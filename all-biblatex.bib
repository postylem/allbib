
@article{abney.s:1991,
  title = {Memory Requirements and Local Ambiguities of Parsing Strategies},
  author = {Abney, Steven P. and Johnson, Mark},
  date = {1991-05},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {20},
  number = {3},
  pages = {233--250},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf01067217},
  url = {https://doi.org/10.1007%2Fbf01067217},
  abstract = {We present a method for calculating lower bounds on the space required and local ambiguities entailed by parsing strategies. A fast, compact natural language parser must implement a strategy with low space requirements and few local ambiguities. It is also widely assumed in the psycholinguistics literature that extremely limited short-term space is available to the human parser, and that sentences containing center-embedded constructions are incomprehensible because processing them requires more space than is available. However, we show that the parsing strategies most psycholinguists assume require less space for processing center-embedded constructions than for processing other perfectly comprehensible constructions. We present alternative strategies for which center-embedded constructions do require more space than other constructions.},
  bdsk-url-2 = {https://doi.org/10.1007/bf01067217},
  date-added = {2022-03-31 09:33:23 -0400},
  date-modified = {2022-03-31 09:38:15 -0400},
  keywords = {memory,parsing,space-complexity}
}

@incollection{abney.s:1991chunks,
  title = {Parsing by Chunks},
  booktitle = {Studies in Linguistics and Philosophy},
  author = {Abney, Steven P.},
  date = {1991},
  pages = {257--278},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-011-3474-3_10},
  url = {https://doi.org/10.1007%2F978-94-011-3474-3_10},
  bdsk-url-2 = {https://doi.org/10.1007/978-94-011-3474-3₁0},
  date-added = {2022-03-31 09:42:19 -0400},
  date-modified = {2022-03-31 09:45:05 -0400},
  keywords = {context free grammar,parsing}
}

@inproceedings{abney.s:1999,
  title = {Relating Probabilistic Grammars and Automata},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}} -},
  author = {Abney, Steven and McAllester, David and Pereira, Fernando},
  date = {1999},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1034678.1034759},
  url = {https://doi.org/10.3115%2F1034678.1034759},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034759},
  date-added = {2022-03-31 09:45:57 -0400},
  date-modified = {2022-03-31 09:47:10 -0400},
  keywords = {automata,context free grammar,parsing,probabilistic context free grammar,push-down automata}
}

@article{adamek.j:2004,
  title = {Abstract and Concrete Categories. {{The}} Joy of Cats},
  author = {Adámek, Jiří and Herrlich, Horst and Strecker, George E},
  date = {2004},
  publisher = {{Citeseer}},
  date-added = {2019-08-24 09:17:33 -0400},
  date-modified = {2019-08-24 09:18:04 -0400},
  keywords = {category theory}
}

@article{adger.d:2009,
  title = {Features in Minimalist Syntax},
  author = {Adger, David and Svenonius, Peter},
  date = {2009},
  journaltitle = {The Oxford Handbook of Minimalist Syntax},
  url = {https://doi.org/10.1093/oxfordhb/9780199549368.013.0002},
  date-added = {2020-02-20 12:37:20 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,minimalist syntax,phi features}
}

@inproceedings{ait-mokhtar.s:1997,
  title = {Incremental Finite-State Parsing},
  booktitle = {Fifth Conference on Applied Natural Language Processing},
  author = {Ait-Mokhtar, Salah and Chanod, Jean-Pierre},
  date = {1997},
  pages = {72--79},
  publisher = {{Association for Computational Linguistics}},
  location = {{Washington, DC, USA}},
  doi = {10.3115/974557.974569},
  url = {https://www.aclweb.org/anthology/A97-1012},
  bdsk-url-2 = {https://doi.org/10.3115/974557.974569}
}

@inproceedings{alemi.a:2016,
  title = {Deep Variational Information Bottleneck},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  date = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=HyxQzBceg},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/AlemiFD017.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@article{alexiadou.a:2014,
  title = {Opaque and Transparent Datives, and How They Behave in Passives},
  author = {Alexiadou, Artemis and Anagnostopoulou, Elena and Sevdali, Christina},
  date = {2014},
  journaltitle = {The Journal of Comparative Germanic Linguistics},
  volume = {17},
  number = {1},
  pages = {1--34},
  publisher = {{Springer}},
  url = {https://doi.org/10.1007/s10828-014-9064-8},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@software{allaire.j:2022,
  title = {Quarto},
  author = {Allaire, J.J. and Teague, Charles and Scheidegger, Carlos and Xie, Yihui and Dervieux, Christophe},
  date = {2022-01},
  doi = {10.5281/zenodo.5960048},
  url = {https://github.com/quarto-dev/quarto-cli},
  bdsk-url-2 = {https://doi.org/10.5281/zenodo.5960048},
  date-added = {2022-05-09 12:21:38 -0400},
  date-modified = {2022-05-09 12:21:39 -0400},
  version = {0.3}
}

@inproceedings{allen.c:2019,
  title = {Analogies Explained: {{Towards}} Understanding Word Embeddings},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning, {{ICML}} 2019, 9-15 June 2019, Long Beach, California, {{USA}}},
  author = {Allen, Carl and Hospedales, Timothy M.},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  date = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {223--231},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v97/allen19a.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/AllenH19.bib},
  timestamp = {Tue, 11 Jun 2019 01:00:00 +0200}
}

@article{altmann.g:1988,
  title = {Interaction with Context during Human Sentence Processing},
  author = {Altmann, Gerry and Steedman, Mark},
  date = {1988-12},
  journaltitle = {Cognition},
  volume = {30},
  number = {3},
  pages = {191--238},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0277(88)90020-0},
  url = {https://doi.org/10.1016%2F0010-0277%2888%2990020-0},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0277(88)90020-0},
  date-added = {2022-04-14 13:35:33 -0400},
  date-modified = {2022-04-14 13:35:37 -0400}
}

@article{amari.s:1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  date = {1992-03},
  journaltitle = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  eventtitle = {{{IEEE Transactions}} on {{Neural Networks}}},
  keywords = {Computer architecture,information geometry,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes},
  file = {/Users/j/Zotero/storage/ZMHDJ3PH/Amari et al. - 1992 - Information geometry of Boltzmann machines.pdf}
}

@book{anagnostopoulou.e:2003,
  title = {The Syntax of Ditransitives: {{Evidence}} from Clitics},
  author = {Anagnostopoulou, Elena},
  date = {2003},
  volume = {54},
  publisher = {{Walter de Gruyter}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:07 -0400},
  project = {Icelandic gluttony},
  keywords = {clitics,hierarchy effects}
}

@article{anagnostopoulou.e:2017,
  title = {The {{Person Case Constraint}}},
  author = {Anagnostopoulou, Elena},
  date = {2017},
  journaltitle = {The Wiley Blackwell Companion to Syntax, Second Edition},
  pages = {1--47},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@book{anderson.j:1990,
  title = {The Adaptive Character of Thought},
  author = {Anderson, John R.},
  date = {1990-01},
  publisher = {{Psychology Press}},
  url = {https://doi.org/10.4324%2F9780203771730},
  bdsk-url-2 = {https://doi.org/10.4324/9780203771730},
  date-added = {2022-04-04 11:54:23 -0400},
  date-modified = {2022-04-04 12:18:17 -0400},
  file = {/Users/j/Zotero/storage/DWKYXFLQ/Anderson - 1990 - The adaptive character of thought.pdf}
}

@article{anderson.j:1991,
  title = {Is Human Cognition Adaptive?},
  author = {Anderson, John R.},
  date = {1991-09},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {14},
  number = {3},
  pages = {471--485},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00070801},
  url = {http://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/is-human-cognition-adaptive/518FCFF303190968CF1F54D2A603C026},
  urldate = {2022-06-12},
  abstract = {Can the output of human cognition be predicted from the assumption that it is an optimal response to the information-processing demands of the environment? A methodology called rational analysis is described for deriving predictions about cognitive phenomena using optimization assumptions. The predictions flow from the statistical structure of the environment and not the assumed structure of the mind. Bayesian inference is used, assuming that people start with a weak prior model of the world which they integrate with experience to develop stronger models of specific aspects of the world. Cognitive performance maximizes the difference between the expected gain and cost of mental effort. (1) Memory performance can be predicted on the assumption that retrieval seeks a maximal trade-off between the probability of finding the relevant memories and the effort required to do so; in (2) categorization performance there is a similar trade-off between accuracy in predicting object features and the cost of hypothesis formation; in (3) casual inference the trade-off is between accuracy in predicting future events and the cost of hypothesis formation; and in (4) problem solving it is between the probability of achieving goals and the cost of both external and mental problem-solving search. The implemention of these rational prescriptions in neurally plausible architecture is also discussed.},
  langid = {english},
  keywords = {Bayes,categorization,causal inference,computation,memory,optimality,problem solving,rational analysis,rationality},
  file = {/Users/j/Zotero/storage/CFWIF6M6/Anderson - 1991 - Is human cognition adaptive.pdf}
}

@article{anderson.j:1991a,
  title = {Reflections of the {{Environment}} in {{Memory}}},
  author = {Anderson, John R. and Schooler, Lael J.},
  date = {1991-11-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {2},
  number = {6},
  pages = {396--408},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1991.tb00174.x},
  url = {https://doi.org/10.1111/j.1467-9280.1991.tb00174.x},
  urldate = {2022-09-08},
  abstract = {Availability of human memories for specific items shows reliable relationships to frequency, recency, and pattern of prior exposures to the item. These relationships have defied a systematic theoretical treatment. A number of environmental sources (New York Times, parental speech, electronic mail) are examined to show that the probability that a memory will be needed also shows reliable relationships to frequency, recency, and pattern of prior exposures. Moreover, the environmental relationships are the same as the memory relationships. It is argued that human memory has the form it does because it is adapted to these environmental relationships. Models for both the environment and human memory are described. Among the memory phenomena addressed are the practice function, the retention function, the effect of spacing of practice, and the relationship between degree of practice and retention.},
  langid = {english},
  file = {/Users/j/Zotero/storage/GH4QBXIU/Anderson and Schooler (1991) Reflections of the Environment in Memory.pdf}
}

@article{anderson.j:1998,
  title = {An {{Integrated Theory}} of {{List Memory}}},
  author = {Anderson, John R. and Bothell, Dan and Lebiere, Christian and Matessa, Michael},
  date = {1998-05-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {341--380},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2553},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X97925535},
  urldate = {2022-09-08},
  abstract = {The ACT-R theory (Anderson, 1993; Anderson \& Lebiere, 1998) is applied to the list memory paradigms of serial recall, recognition memory, free recall, and implicit memory. List memory performance in ACT-R is determined by the level of activation of declarative chunks which encode that items occur in the list. This level of activation is in turn determined by amount of rehearsal, delay, and associative fan from a list node. This theory accounts for accuracy and latency profiles in backward and forward serial recall, set size effects in the Sternberg paradigm, length–strength effects in recognition memory, the Tulving–Wiseman function, serial position, length and practice effects in free recall, and lexical priming in implicit memory paradigms. This wide variety of effects is predicted with minimal parameter variation. It is argued that the strength of the ACT-R theory is that it offers a completely specified processing architecture that serves to integrate many existing models in the literature.},
  langid = {english}
}

@book{anderson.j:1998atomic,
  title = {The Atomic Components of Thought},
  author = {Anderson, John R. and Lebiere, Christian},
  date = {1998},
  publisher = {{Psychology Press}},
  location = {{New York}},
  doi = {10.4324/9781315805696},
  url = {https://doi.org/10.4324/9781315805696},
  isbn = {978-1-315-80569-6},
  langid = {english},
  annotation = {OCLC: 872682504},
  file = {/Users/j/Zotero/storage/NZLCSGS6/Anderson and Lebiere (1998) The atomic components of thought.pdf}
}

@article{anderson.j:2004,
  title = {An {{Integrated Theory}} of the {{Mind}}},
  author = {Anderson, John R. and Bothell, Daniel and Byrne, Michael D. and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  date = {2004},
  journaltitle = {Psychological Review},
  shortjournal = {Psychological Review},
  volume = {111},
  number = {4},
  pages = {1036--1060},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.111.4.1036},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.111.4.1036},
  urldate = {2022-09-17},
  langid = {english},
  file = {/Users/j/Zotero/storage/KE52MBTG/Anderson et al. (2004) An Integrated Theory of the Mind.pdf}
}

@article{andrieu.c:2010PMCMC,
  title = {Particle Markov Chain Monte Carlo Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  date = {2010},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  eprint = {https://rss.onlinelibrary.wiley.com/doi/pdf/10.1111/j.1467-9868.2009.00736.x},
  pages = {269--342},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2009.00736.x},
  abstract = {Summary. Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a Lévy-driven stochastic volatility model.},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2009.00736.x},
  date-added = {2022-05-05 09:30:00 -0400},
  date-modified = {2022-05-05 09:30:49 -0400},
  keywords = {bayesian inference,conditional sequential monte carlo,markov chain monte Carlo,sequential monte carlo,state space models},
  file = {/Users/j/Zotero/storage/GRSNI2MM/Andrieu et al. - 2010 - Particle markov chain monte carlo methods.pdf}
}

@article{angele.b:2015,
  title = {Do Successor Effects in Reading Reflect Lexical Parafoveal Processing? {{Evidence}} from Corpus-Based and Experimental Eye Movement Data},
  author = {Angele, Bernhard and Schotter, Elizabeth R. and Slattery, Timothy J. and Tenenbaum, Tara L. and Bicknell, Klinton and Rayner, Keith},
  date = {2015-02},
  journaltitle = {Journal of Memory and Language},
  volume = {79--80},
  pages = {76--96},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2014.11.003},
  url = {https://doi.org/10.1016%2Fj.jml.2014.11.003},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2014.11.003},
  date-added = {2022-04-21 09:33:17 -0400},
  date-modified = {2022-04-21 09:33:18 -0400}
}

@misc{armengol-estape.j:2021,
  title = {On the Multilingual Capabilities of Very Large-Scale English Language Models},
  author = {Armengol-Estapé, Jordi and de Gibert Bonet, Ona and Melero, Maite},
  options = {useprefix=true},
  date = {2021},
  eprint = {2108.13349},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-12-13 19:48:21 -0500},
  date-modified = {2021-12-13 19:48:22 -0500}
}

@misc{arroyo-fernandez.i:2019,
  title = {On the Possibility of Rewarding Structure Learning Agents: {{Mutual}} Information on Linguistic Random Sets},
  author = {Arroyo-Fernández, Ignacio and Carrasco-Ruíz, Mauricio and Arias-Aguilar, J. Anibal},
  date = {2019},
  eprint = {1910.04023},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2020-01-27 11:44:31 -0500},
  date-modified = {2020-01-27 11:47:03 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised grammar induction}
}

@article{atlamaz.u:2018,
  title = {On Partial Agreement and Oblique Case},
  author = {Atlamaz, Ümit and Baker, Mark},
  date = {2018},
  journaltitle = {Syntax (Oxford, England)},
  shortjournal = {Syntax},
  volume = {21},
  number = {3},
  pages = {195--237},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{attias.h:1999,
  title = {A {{Variational Baysian Framework}} for {{Graphical Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Attias, Hagai},
  editor = {Solla, S. and Leen, T. and Müller, K.},
  date = {1999},
  volume = {12},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1999/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html},
  urldate = {2022-06-27},
  abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
  eventtitle = {Conference on {{Neural Information Processing Systems}}},
  file = {/Users/j/Zotero/storage/RQ4I7UC6/Attias - 1999 - A Variational Baysian Framework for Graphical Mode.pdf}
}

@book{attneave.f:1959,
  title = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  shorttitle = {Applications of Information Theory to Psychology},
  author = {Attneave, Fred},
  date = {1959},
  series = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  pages = {vii, 120},
  publisher = {{Henry Holt}},
  location = {{Oxford, England}},
  abstract = {Summarizes existing informational methods used in psychological research, and illustrates the methods of calculating some of the measures. Chapter 1 develops quantitative expressions of uncertainty and redundancy from qualitative examples. Chapter 2 describes informational methods for analyzing sequences of events. Chapter 3 gives methods of describing rates of transmission of information and reviews pertinent research. Chapter 4 concerns possible applications of information measures, particularly to the study of perceptual problems of patterning. Appendices illustrate the calculation of information measures from variance statistics and provide convenient tables and a nomograph used in calculating information measures. 87 refs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  pagetotal = {vii, 120},
  keywords = {information theory,surprisal}
}

@inproceedings{aurnhammer.c:2019cogsci,
  title = {Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing},
  booktitle = {Proceedings of the 41st Annual Conference of the Cognitive Science Society},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  date = {2019},
  pages = {112--118},
  url = {http://hdl.handle.net/2066/213724},
  date-added = {2021-11-29 11:40:22 -0500},
  date-modified = {2021-11-29 12:44:15 -0500}
}

@article{aurnhammer.c:2019LIG,
  title = {Evaluating Information-Theoretic Measures of Word Prediction in Naturalistic Sentence Reading},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  date = {2019-11},
  journaltitle = {Neuropsychologia},
  volume = {134},
  number = {107198},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.neuropsychologia.2019.107198},
  url = {https://doi.org/10.1016%2Fj.neuropsychologia.2019.107198},
  abstract = {We review information-theoretic measures of cognitive load during sentence processing that have been used to quantify word prediction effort. Two such measures, surprisal and next-word entropy, suffer from shortcomings when employed for a predictive processing view. We propose a novel metric, lookahead information gain, that can overcome these short-comings. We estimate the different measures using probabilistic language models. Subsequently, we put them to the test by analysing how well the estimated measures predict human processing effort in three data sets of naturalistic sentence reading. Our results replicate the well known effect of surprisal on word reading effort, but do not indicate a role of next-word entropy or lookahead information gain. Our computational results suggest that, in a predictive processing system, the costs of predicting may outweigh the gains. This idea poses a potential limit to the value of a predictive mechanism for the processing of language. The result illustrates the unresolved problem of finding estimations of word-by-word prediction that, first, are truly independent of perceptual processing of the to-be-predicted words, second, are statistically reliable predictors of experimental data, and third, can be derived from more general assumptions about the cognitive processes involved.},
  bdsk-url-2 = {https://doi.org/10.1016/j.neuropsychologia.2019.107198},
  date-added = {2021-11-29 11:28:26 -0500},
  date-modified = {2022-04-21 09:12:00 -0400}
}

@book{awodey.s:2010,
  title = {Category Theory},
  author = {Awodey, Steve},
  date = {2010},
  publisher = {{Oxford University Press}},
  date-added = {2019-08-24 09:18:40 -0400},
  date-modified = {2019-08-24 09:19:06 -0400},
  keywords = {category theory}
}

@book{axler.s:2020,
  title = {Measure, {{Integration}} \& {{Real Analysis}}},
  author = {Axler, Sheldon},
  date = {2020},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {282},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-030-33143-6},
  url = {https://measure.axler.net/MIRA.pdf},
  urldate = {2022-06-23},
  isbn = {978-3-030-33142-9},
  langid = {english}
}

@article{aylett.m:2004,
  title = {The Smooth Signal Redundancy Hypothesis: {{A}} Functional Explanation for Relationships between Redundancy, Prosodic Prominence, and Duration in Spontaneous Speech},
  author = {Aylett, Matthew and Turk, Alice},
  date = {2004},
  journaltitle = {Language and Speech},
  volume = {47},
  number = {1},
  eprint = {https://doi.org/10.1177/00238309040470010201},
  pages = {31--56},
  doi = {10.1177/00238309040470010201},
  url = {https://doi.org/10.1177/00238309040470010201},
  abstract = {This paper explores two related factors which influence variation in duration, prosodic structure and redundancy in spontaneous speech. We argue that the constraint of producing robust communication while efficiently expending articulatory effort leads to an inverse relationship between language redundancy and duration. The inverse relationship improves communication robustness by spreading information more evenly across the speech signal, yielding a smoother signal redundancy profile.We argue that prosodic prominence is a linguistic means of achieving smooth signal redundancy. Prosodic prominence increases syllable duration and coincides to a large extent with unpredictable sections of speech, and thus leads to a smoother signal redundancy.The results of linear regressions carried out between measures of redundancy, syllable duration and prosodic structure in a large corpus of spontaneous speech confirm: (1) an inverse relationship between language redundancy and duration, and (2) a strong relationship between prosodic prominence and duration.The fact that a large proportion of the variance predicted by language redundancy and prosodic prominence is nonunique suggests that, in English, prosodic prominence structure is the means with which constraints caused by a robust signal requirement are expressed in spontaneous speech.},
  date-added = {2022-04-27 12:19:58 -0400},
  date-modified = {2022-04-27 12:20:19 -0400},
  keywords = {noisy channel coding}
}

@book{baayen.r:2001book,
  title = {Word {{Frequency Distributions}}},
  author = {Baayen, R. Harald},
  date = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {18},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-010-0844-0},
  url = {http://link.springer.com/10.1007/978-94-010-0844-0},
  urldate = {2022-10-02},
  editorb = {Ide, Nancy and Véronis, Jean},
  editorbtype = {redactor},
  isbn = {978-1-4020-0927-3 978-94-010-0844-0},
  keywords = {best fit,corpus,Estimator},
  file = {/Users/j/Zotero/storage/NPISTVNL/Baayen (2001) Word Frequency Distributions.pdf}
}

@incollection{baayen.r:2001bookch1,
  title = {Word {{Frequencies}}},
  booktitle = {Word {{Frequency Distributions}}},
  author = {Baayen, R. Harald},
  editor = {Baayen, R. Harald},
  date = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  pages = {1--38},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-010-0844-0_1},
  url = {https://doi.org/10.1007/978-94-010-0844-0_1},
  urldate = {2022-10-02},
  abstract = {This chapter introduces two fundamental issues in lexical statistics. The first issue concerns the role of the sample size, the number of words in a text or corpus. The sample size crucially determines a great many measures that have been proposed as characteristic text constants. However, the values of these measures change systematically as a function of the sample size. Similarly, the parameters of many models for word frequency distribution are highly dependent on the sample size. This property sets lexical statistics apart from most other areas in statistics, where an increase in the sample size leads to enhanced accuracy and not to systematic changes in basic measures and parameters.},
  isbn = {978-94-010-0844-0},
  langid = {english},
  file = {/Users/j/Zotero/storage/IRF2DT4A/Baayen (2001) Word Frequencies.pdf}
}

@thesis{bachrach.a:2008phd,
  title = {Imaging Neural Correlates of Syntactic Complexity in a Naturalistic Context},
  author = {Bachrach, Asaf},
  date = {2008},
  institution = {{Massachusetts Institute of Technology}},
  url = {http://hdl.handle.net/1721.1/45900},
  date-added = {2021-06-09 09:00:47 -0400},
  date-modified = {2022-04-20 10:20:04 -0400},
  file = {/Users/j/Zotero/storage/7QTXG8TU/Bachrach - 2008 - Imaging neural correlates of syntactic complexity .pdf}
}

@unpublished{bachrach.a:2009,
  title = {Incremental Prediction in Naturalistic Language Processing: {{An fMRI}} Study},
  author = {Bachrach, Asaf and Roark, Brian and Marantz, Alex and Whitfield-Gabrieli, Susan and Cardenas, Carlos and Gabrieli, John},
  date = {2009}
}

@incollection{bader.m:1994,
  title = {German Verb-Final Clauses and Sentence Processing: {{Evidence}} for Immediate Attachment},
  shorttitle = {German Verb-Final Clauses and Sentence Processing},
  booktitle = {Perspectives on Sentence Processing},
  author = {Bader, Markus and Lasser, Ingeborg},
  editor = {Clifton, Jr., Charles and Frazier, Lyn and Rayner, Keith},
  date = {1994},
  pages = {225--242},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  location = {{Hillsdale, NJ, US}},
  abstract = {one central question in sentence processing concerns the relationship between knowledge of language and the way this knowledge is put to use / this relationship . . . has received a great deal of attention in the discussion of models of the human parsing mechanism [responsible for computing syntactic structures] / despite this attention, the issue of how linguistic knowledge is used during sentence comprehension is far from settled / goal [is] to narrow down the number of possible parsing models by introducing some on-line data from German  discuss a particular class of parsers that assumes both principles and parameters theory and a transparent grammar–parser relationship / call these parsers head-driven licensing parsers / on the basis of experimental evidence from German verb-final structures, we reject the particular interpretation of grammar-parser transparency / introduce certain properties of current syntactic theory and then show how these properties have found their way into head-driven licensing parsers / report an experiment that aims to test the prediction of head-driven licensing parsers for verb-final clauses; namely, that, in these clauses, all attachments have to be delayed until the end of the clause / [Ss were 24 native German speaking university students] (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-1581-8 978-0-8058-1582-5},
  keywords = {Grammar,Psycholinguistics,Sentence Comprehension,Sentence Structure,Syntax,Verbs}
}

@unpublished{bahdanau.d:2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  date = {2016-05-19},
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2022-05-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/S444U3WU/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf}
}

@inproceedings{bailly.r:2020,
  title = {Emergence of Syntax Needs Minimal Supervision},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Bailly, Raphaël and Gábor, Kata},
  date = {2020},
  pages = {477--487},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.46},
  url = {https://www.aclweb.org/anthology/2020.acl-main.46},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.46}
}

@article{baker.j:1979,
  title = {Trainable Grammars for Speech Recognition},
  author = {Baker, J. K.},
  date = {1979-06},
  journaltitle = {The Journal of the Acoustical Society of America},
  volume = {65},
  number = {S1},
  pages = {S132-S132},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2017061},
  url = {https://asa.scitation.org/doi/10.1121/1.2017061},
  urldate = {2022-07-04}
}

@article{balota.d:1985,
  title = {The Interaction of Contextual Constraints and Parafoveal Visual Information in Reading},
  author = {Balota, David A and Pollatsek, Alexander and Rayner, Keith},
  date = {1985},
  journaltitle = {Cognitive Psychology},
  volume = {17},
  number = {3},
  pages = {364--390},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0285(85)90013-1},
  url = {https://doi.org/10.1016%2F0010-0285%2885%2990013-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(85)90013-1},
  date-added = {2021-05-22 15:35:25 -0400},
  date-modified = {2021-05-22 15:35:39 -0400},
  keywords = {predictability,processing}
}

@article{bar-hillel.y:1953,
  title = {A Quasi-Arithmetical Notation for Syntactic Description},
  author = {Bar-Hillel, Yehoshua},
  date = {1953},
  journaltitle = {Language},
  volume = {29},
  number = {1},
  pages = {47},
  publisher = {{JSTOR}},
  doi = {10.2307/410452},
  url = {https://doi.org/10.2307%2F410452},
  bdsk-url-2 = {https://doi.org/10.2307/410452},
  date-added = {2021-06-25 00:50:06 -0400},
  date-modified = {2021-06-25 00:50:07 -0400}
}

@incollection{barber.d:2003,
  title = {The {{IM}} Algorithm: A Variational Approach to Information Maximization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Barber, David and Agakov, Felix},
  date = {2003},
  pages = {201--208},
  url = {https://papers.nips.cc/paper/2410-information-maximization-in-noisy-channels-a-variational-approach.pdf},
  bdsk-url-2 = {https://papers.nips.cc/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},
  date-added = {2019-10-08 23:33:35 -0400},
  date-modified = {2021-03-07 16:09:06 -0500},
  project = {syntactic embedding},
  keywords = {mutual information,variational inference}
}

@article{barnard.g:1946,
  title = {Sequential {{Tests}} in {{Industrial Statistics}}},
  author = {Barnard, G. A.},
  date = {1946},
  journaltitle = {Supplement to the Journal of the Royal Statistical Society},
  volume = {8},
  number = {1},
  pages = {1--21},
  issn = {2517-617X},
  doi = {10.2307/2983610},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.2307/2983610},
  urldate = {2022-07-04},
  abstract = {After an introductory and an historical note, an elementary problem of simple qualitative inspection of a box of components is treated by using a “lattice diagram representation.” This leads to the consideration of sequential tests for such cases. Procedures for determining “Target-Handicap” forms of inspection, and their operating and sample size properties are given. This leads to a consideration of general linear sequential tests, which are those test procedures which can be formulated in terms of a “score.” Such procedures are shown to be similar to classical games of chance, and to physical diffusion processes. The diffusion analogy leads to a differential equation which gives the approximate characteristics of any such linear test. In many cases, Wald's “Probability Ratio Sequential Test” takes the form of a linear test. The conditions for this are determined. The P.R.S. test is seen to be “best possible linear test,” in the sense of minimizing average sample size. The effects of deviations from normality, and general distributions are considered. Reference is made to Wald's work on tests which involve parameters other than those being estimated, and then consideration is restricted to tests for the mean of normal populations where the variance is unknown. Methods of reducing such tests to simple binomial tests are indicated. A number of procedures for use with 2 × 2 comparative trials, and double dichotomies, are given, and their properties discussed. Returning to general inspection problems, the paper indicates that these are not always to be identified with problems involving merely tests of statistical hypotheses. The notions of Consumer's Lot, Producer's Batch, the Lot Quality Curve, the Process Curve, are explained, and their importance indicated. A distinction is made between Acceptance Inspection schemes and Rectifying Inspection schemes, and the notions of Operating Characteristic Curve, Operating Characteristic Matrix, and the Sample Size distribution function are explained. The lattice diagram is used to bring out relationships between notions involved in general inspection, and some other uses are also indicated. Finally, some reflections on the relevance of the matters discussed to matters of current debate among statisticians are given.},
  langid = {english},
  annotation = {\_eprint: https://rss.onlinelibrary.wiley.com/doi/pdf/10.2307/2983610}
}

@inproceedings{barrett.m:2015,
  title = {The {{Dundee}} Treebank},
  booktitle = {The 14th International Workshop on Treebanks and Linguistic Theories ({{TLT}} 14)},
  author = {Barrett, Maria and Agic, Željko and Søgaard, Anders},
  date = {2015},
  pages = {242--248},
  url = {http://tlt14.ipipan.waw.pl/files/4614/5063/3858/TLT14ₚroceedings.pdf#page=249},
  date-added = {2021-09-16 13:19:25 -0400},
  date-modified = {2021-09-16 13:20:18 -0400}
}

@article{baumann.s:2018,
  title = {What Makes a Word Prominent? {{Predicting}} Untrained {{German}} Listeners' Perceptual Judgments},
  author = {Baumann, Stefan and Winter, Bodo},
  date = {2018},
  journaltitle = {Journal of Phonetics},
  volume = {70},
  pages = {20--38},
  publisher = {{Elsevier}},
  url = {https://sfb1252.uni-koeln.de/sites/sfb₁252/user<sub>u</sub>pload/Pdfs<sub>P</sub>ublikationen/Baumann<sub>W</sub>inter₂018<sub>w</sub>hatₘakes<sub>w</sub>ord.pdf},
  bdsk-url-2 = {https://www.sciencedirect.com/science/article/pii/S0095447017301298},
  date-added = {2020-02-27 22:24:39 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {intonation,phonetics,prominence,prosody,random forests}
}

@article{beauchene.c:2022,
  title = {Dynamic {{Cognitive States Predict Individual Variability}} in {{Behavior}} and {{Modulate}} with {{EEG Functional Connectivity}} during {{Working Memory}}},
  author = {Beauchene, Christine and Hinault, Thomas and Sarma, Sridevi V. and Courtney, Susan},
  date = {2022-01-29},
  pages = {2021.08.02.454757},
  publisher = {{bioRxiv}},
  doi = {10.1101/2021.08.02.454757},
  url = {https://www.biorxiv.org/content/10.1101/2021.08.02.454757v3},
  urldate = {2022-06-13},
  abstract = {Fluctuations in strategy, attention, or motivation can cause large variability in performance across task trials. Typically, this variability is treated as noise, and assumed to cancel out, leaving supposedly stable relationships among behavior, neural activity, and experimental task conditions. Those relationships, however, could change with a participant’s internal cognitive states, and variability in performance may carry important information regarding those states, which cannot be directly measured. Therefore, we used a mathematical, state-space modeling framework to estimate internal states from measured behavioral data, quantifying each participant’s sensitivity to factors such as past errors or distractions, to predict their reaction time fluctuations. We show how modeling these states greatly improves trial-by-trial prediction of behavior. Further, we identify EEG functional connectivity features that modulate with each state. These results illustrate the potential of this approach and how it could enable quantification of intra- and inter-individual differences and provide insight into their neural bases. Statement of Relevance Cognitive behavioral performance and its neural bases vary both across individuals and within individuals over time. Understanding this variability may be key to the success of clinical or educational interventions. Internal cognitive states reflecting differences in strategy, attention, and motivation may drive much of these inter- and intra-individual differences, but often cannot be reliably controlled or measured in cognitive neuroscience research. The mathematical modeling framework developed here uses measured data to estimate a participant’s dynamic, internal cognitive states, with each state derived from specific factors hypothesized to affect attention, motivation or strategy. The results highlight potential sources of behavioral variability and reveal EEG features that modulate with each state. Our method quantifies and characterizes individual behavioral differences and highlights their underlying neural mechanisms, which could be used for future targeted training or neuromodulation therapies to improve cognitive performance.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZELCMES7/Beauchene et al. - 2022 - Dynamic Cognitive States Predict Individual Variab.pdf}
}

@thesis{behrenfeldt.j:2009,
  title = {A Linguist's Survey of Pumping Lemmata},
  author = {Behrenfeldt, Johan},
  date = {2009},
  institution = {{University of Gothenburg}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.482.8670},
  date-added = {2020-02-12 12:04:22 -0500},
  date-modified = {2021-03-12 11:46:23 -0500},
  keywords = {formal languages,pumping lemmata}
}

@incollection{bejar.s:2003,
  title = {Person Licensing and the Derivation of {{PCC}} Effects},
  booktitle = {Romance Linguistics: {{Theory}} and Acquisition},
  author = {Béjar, Susana and Rezac, Milan},
  editor = {Perez-Leroux, Ana Teresa and Roberg, Yves},
  date = {2003},
  pages = {49--62},
  publisher = {{J. Benjamins}},
  location = {{Amsterdam}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:56 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{bejar.s:2009,
  title = {Cyclic Agree},
  author = {Béjar, Susana and Rezac, Milan},
  date = {2009},
  journaltitle = {Linguistic Inquiry},
  volume = {40},
  number = {1},
  pages = {35--73},
  publisher = {{MIT Press}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:27:46 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects}
}

@inproceedings{belinkov.y:2017,
  title = {What Do Neural Machine Translation Models Learn about Morphology?},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  date = {2017},
  pages = {861--872},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1080},
  url = {https://www.aclweb.org/anthology/P17-1080},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1080}
}

@inproceedings{bell.a:2003,
  title = {The Co-Information Lattice},
  booktitle = {Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation: {{ICA}}},
  author = {Bell, Anthony J},
  date = {2003},
  volume = {2003},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.320.5264},
  date-added = {2019-05-15 00:08:25 -0400},
  date-modified = {2021-07-19 22:04:55 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,synergy}
}

@report{bennett.v:2010,
  title = {Wasatch {{Solar Project}} Final Report},
  author = {Bennett, Vicki and Bowman, Kate and Wright, Sarah},
  date = {2018},
  number = {DOE-SLC-6903-1},
  institution = {{Salt Lake City Corporation}},
  location = {{Salt Lake City, UT}},
  doi = {10.2172/1474305},
  url = {https://doi.org/10.2172/1474305},
  date-added = {2021-03-12 11:39:14 -0500},
  date-modified = {2021-03-12 11:43:47 -0500}
}

@article{berwick.r:1982,
  title = {Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories},
  author = {Berwick, Robert C. and Weinberg, Amy S.},
  date = {1982},
  journaltitle = {Linguistic Inquiry},
  volume = {13},
  number = {2},
  eprint = {4178272},
  eprinttype = {jstor},
  pages = {165--191},
  publisher = {{The MIT Press}},
  issn = {00243892, 15309150},
  date-added = {2022-03-29 20:33:12 -0400},
  date-modified = {2022-03-29 20:33:17 -0400}
}

@inproceedings{bicknell.k:2009,
  title = {A Model of Local Coherence Effects in Human Sentence Processing as Consequences of Updates from Bottom-up Prior to Posterior Beliefs},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Bicknell, Klinton and Levy, Roger},
  date = {2009-06},
  pages = {665--673},
  publisher = {{Association for Computational Linguistics}},
  location = {{Boulder, Colorado}},
  url = {https://aclanthology.org/N09-1075},
  date-added = {2022-04-21 10:50:28 -0400},
  date-modified = {2022-04-21 10:50:29 -0400},
  file = {/Users/j/Zotero/storage/A5IE9MQL/Bicknell and Levy - 2009 - A model of local coherence effects in human senten.pdf}
}

@inproceedings{bicknell.k:2010,
  title = {A Rational Model of Eye Movement Control in Reading},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Bicknell, Klinton and Levy, Roger},
  date = {2010},
  pages = {1168--1178},
  publisher = {{Association for Computational Linguistics}},
  location = {{Uppsala, Sweden}},
  url = {https://www.aclweb.org/anthology/P10-1119}
}

@thesis{bicknell.k:2011,
  title = {Eye Movements in Reading as Rational Behavior},
  author = {Bicknell, Klinton},
  date = {2011},
  institution = {{University of California, San Diego}},
  url = {https://www.klintonbicknell.com/dl/bicknell_diss.pdf},
  file = {/Users/j/Zotero/storage/MJ65ML8P/Bicknell - 2011 - Eye movements in reading as rational behavior.pdf}
}

@inproceedings{bicknell.k:2012cogsci,
  title = {Word Predictability and Frequency Effects in a Rational Model of Reading},
  booktitle = {Proceedings of the 34th {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Bicknell, Klinton and Levy, Roger},
  date = {2012},
  volume = {34},
  pages = {126--131},
  publisher = {{Cognitive Science Society}},
  location = {{Sapporo, Japan}},
  url = {https://cogsci.mindmodeling.org/2012/papers/0035/},
  abstract = {This paper presents results from the first rational model of eye movement control in reading to make predictions for the full range of the eye movement record. The model identifies the text through Bayesian inference and makes eye movement de-cisions to maximize the efficiency of text identification, go-ing beyond leading approaches which select model parame-ters to maximize the fit to human data. Two simulations with the model demonstrate that it can produce effects of word pre-dictability and frequency on eye movements in reading similar to those produced by humans, providing evidence that many properties of human reading behavior may be understood as following from the nature of efficient text identification.},
  eventtitle = {{{CogSci}} 2021},
  keywords = {surprisal theory},
  file = {/Users/j/Zotero/storage/37JRPKX6/Bicknell and Levy (2012) Word predictability and frequency effects in a rat.pdf}
}

@book{billingsley.p:1995,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  date = {1995},
  edition = {Third edition},
  publisher = {{Wiley}},
  url = {https://books.google.com/books?id=QyXqOXyxEeIC},
  bdsk-url-2 = {https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf},
  date-added = {2021-03-28 11:47:36 -0400},
  date-modified = {2021-08-21 16:16:00 -0400},
  keywords = {measure theory,probability theory}
}

@book{bishop.c:2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  date = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  edition = {1},
  publisher = {{Springer}},
  location = {{New York, USA}},
  url = {https://link.springer.com/book/9780387310732},
  urldate = {2022-06-10},
  isbn = {978-0-387-31073-2},
  langid = {english},
  pagetotal = {XX, 738}
}

@article{bitzer.s:2014,
  title = {Perceptual Decision Making: Drift-Diffusion Model Is Equivalent to a {{Bayesian}} Model},
  shorttitle = {Perceptual Decision Making},
  author = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel, Stefan},
  date = {2014},
  journaltitle = {Frontiers in Human Neuroscience},
  volume = {8},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00102},
  urldate = {2022-07-04},
  abstract = {Behavioral data obtained with perceptual decision making experiments are typically analyzed with the drift-diffusion model. This parsimonious model accumulates noisy pieces of evidence toward a decision bound to explain the accuracy and reaction times of subjects. Recently, Bayesian models have been proposed to explain how the brain extracts information from noisy input as typically presented in perceptual decision making tasks. It has long been known that the drift-diffusion model is tightly linked with such functional Bayesian models but the precise relationship of the two mechanisms was never made explicit. Using a Bayesian model, we derived the equations which relate parameter values between these models. In practice we show that this equivalence is useful when fitting multi-subject data. We further show that the Bayesian model suggests different decision variables which all predict equal responses and discuss how these may be discriminated based on neural correlates of accumulated evidence. In addition, we discuss extensions to the Bayesian model which would be difficult to derive for the drift-diffusion model. We suggest that these and other extensions may be highly useful for deriving new experiments which test novel hypotheses.},
  file = {/Users/j/Zotero/storage/TWAH7Q9N/Bitzer et al. - 2014 - Perceptual decision making drift-diffusion model .pdf}
}

@article{blachman.n:1968,
  title = {The Amount of Information That y Gives about {{X}}},
  author = {Blachman, N.},
  date = {1968},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {1},
  pages = {27--31},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tit.1968.1054094},
  url = {https://doi.org/10.1109%2Ftit.1968.1054094},
  bdsk-url-2 = {https://doi.org/10.1109/tit.1968.1054094},
  date-added = {2021-04-08 14:57:27 -0400},
  date-modified = {2021-04-08 14:57:41 -0400},
  keywords = {entropy,entropy reduction,surprisal},
  file = {/Users/j/Zotero/storage/82WR77XB/Blachman - 1968 - The amount of information that y gives about X.pdf}
}

@misc{black.s:2021GPT-Neo,
  title = {{{GPT-Neo}}: {{Large}} Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella},
  date = {2021-03},
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.5297715},
  url = {https://doi.org/10.5281/zenodo.5297715},
  date-added = {2021-10-12 20:47:28 -0400},
  date-modified = {2022-05-11 20:39:00 -0400},
  version = {1.0}
}

@article{blei.d:2017,
  title = {Variational {{Inference}}: {{A Review}} for {{Statisticians}}},
  shorttitle = {Variational {{Inference}}},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  date = {2017-04-03},
  journaltitle = {Journal of the American Statistical Association},
  shortjournal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  url = {http://arxiv.org/abs/1601.00670},
  urldate = {2022-06-29},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/TSYRFXUY/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@inproceedings{blevins.t:2018,
  title = {Deep {{RNNs}} Encode Soft Hierarchical Syntax},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  date = {2018},
  pages = {14--19},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2003},
  url = {https://www.aclweb.org/anthology/P18-2003},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2003}
}

@article{bobaljik.j:1996,
  title = {Subject Positions and the Roles of {{TP}}},
  author = {Bobaljik, Jonathan David and Jonas, Dianne},
  date = {1996},
  journaltitle = {Linguistic Inquiry},
  volume = {27},
  number = {2},
  eprint = {4178934},
  eprinttype = {jstor},
  pages = {195--236},
  publisher = {{The MIT Press}},
  issn = {00243892, 15309150},
  abstract = {We propose that the specifier of a VP-external functional projection-Tense Phrase-may host subject NPs under certain conditions. We present empirical evidence that nonspecific subject NPs that have elsewhere been analyzed as remaining VP-internal occupy this position. We also offer theoretical arguments that transitive subjects may never remain internal to the VP at S-Structure in languages for which the Extended Projection Principle holds. Extending work by Bures (1992, 1993), we argue further that [Spec, TP] is implicated as a subject position in NP object shift constructions. Parametric availability of this one position accounts for a cluster of properties within the Germanic languages.},
  date-added = {2019-06-14 09:34:00 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {expletives,subject positions}
}

@article{bobaljik.j:2006,
  title = {Where's Phi},
  author = {Bobaljik, Jonathan David},
  date = {2006},
  journaltitle = {Agreement as a postsyntactic operation. Ms. University of Connecticut},
  publisher = {{Citeseer}},
  date-added = {2020-02-02 22:23:49 -0500},
  date-modified = {2020-02-02 22:24:22 -0500},
  project = {Icelandic gluttony},
  keywords = {dative intervention,phi features}
}

@incollection{bod.r:2003,
  title = {Data-Oriented Parsing},
  booktitle = {Data-Oriented Parsing},
  editor = {Bod, Rens and Scha, Remko and Sima'an, Khalil},
  date = {2003},
  publisher = {{CSLI}},
  location = {{Stanford, CA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{bod.r:2006,
  title = {An All-Subtrees Approach to Unsupervised Parsing},
  booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  author = {Bod, Rens},
  date = {2006},
  pages = {865--872},
  publisher = {{Association for Computational Linguistics}},
  location = {{Sydney, Australia}},
  doi = {10.3115/1220175.1220284},
  url = {https://www.aclweb.org/anthology/P06-1109},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220284}
}

@article{boeckx.c:2000,
  title = {Quirky Agreement},
  author = {Boeckx, Cedric},
  date = {2000},
  journaltitle = {Studia linguistica},
  volume = {54},
  number = {3},
  pages = {354--380},
  publisher = {{Wiley Online Library}},
  url = {https://doi.org/10.1111/1467-9582.00070},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{bogacz.r:2017,
  title = {A Tutorial on the Free-Energy Framework for Modelling Perception and Learning},
  author = {Bogacz, Rafal},
  date = {2017-02-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  series = {Model-Based {{Cognitive Neuroscience}}},
  volume = {76},
  pages = {198--211},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2015.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
  urldate = {2022-08-07},
  abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
  langid = {english},
  file = {/Users/j/Zotero/storage/XB6DV3IH/Bogacz - 2017 - A tutorial on the free-energy framework for modell.pdf}
}

@thesis{bonet.m:1991,
  title = {Morphology after Syntax: {{Pronominal}} Clitics in {{Romance}}},
  author = {Bonet, M. Eulália},
  date = {1991},
  institution = {{MIT}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:14:38 -0400},
  project = {Icelandic gluttony},
  keywords = {clitics,hierarchy effects}
}

@article{boston.m:2008,
  title = {Parsing Costs as Predictors of Reading Difficulty: {{An}} Evaluation Using the {{Potsdam Sentence Corpus}}},
  author = {Boston, Marisa Ferrara and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
  date = {2008},
  series = {Zweitveröffentlichungen Der {{Universität Potsdam}} : {{Humanwissenschaftliche Reihe}} - Paper 253},
  url = {https://publishup.uni-potsdam.de/frontdoor/index/index/docId/5511},
  abstract = {The surprisal of a word on a probabilistic grammar constitutes a promising complexity metric for human sentence comprehension difficulty. Using two different grammar types, surprisal is shown to have an effect on fixation durations and regression probabilities in a sample of German readers’ eye movements, the Potsdam Sentence Corpus. A linear mixed-effects model was used to quantify the effect of surprisal while taking into account unigram and bigram frequency, word length, and empirically-derived word predictability; the so-called “early” and “late” measures of processing difficulty both showed an effect of surprisal. Surprisal is also shown to have a small but statistically non-significant effect on empirically-derived predictability itself. This work thus demonstrates the importance of including parsing costs as a predictor of comprehension difficulty in models of reading, and suggests that a simple identification of syntactic parsing costs with early measures and late measures with durations of post-syntactic events may be difficult to uphold.}
}

@incollection{boston.m:2009,
  title = {Dependency Structures Derived from Minimalist Grammars},
  booktitle = {The Mathematics of Language},
  author = {Boston, Marisa Ferrara and Hale, John T. and Kuhlmann, Marco},
  date = {2009},
  pages = {1--12},
  publisher = {{Springer}},
  date-added = {2019-06-15 11:31:22 -0400},
  date-modified = {2022-04-20 13:49:51 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,minimalist grammars}
}

@inproceedings{bouchard-cote.a:2009,
  title = {Randomized Pruning: {{Efficiently}} Calculating Expectations in Large Dynamic Programs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Bouchard-Côté, Alexandre and Petrov, Slav and Klein, Dan},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  date = {2009},
  volume = {22},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2009/file/e515df0d202ae52fcebb14295743063b-Paper.pdf},
  date-added = {2022-03-31 10:05:14 -0400},
  date-modified = {2022-03-31 22:30:35 -0400},
  keywords = {markov chain monte carlo,pruning}
}

@unpublished{boyce.v:2020amlap,
  type = {Talk},
  title = {A-Maze of {{Natural Stories}}: {{Texts}} Are Comprehensible Using the {{Maze}} Task},
  author = {Boyce, Veronica and Levy, Roger},
  options = {useprefix=true},
  date = {2020-09},
  url = {https://amlap2020.github.io/a/154.pdf},
  editora = {von der Malsburg, Titus and Vasishth, Shravan and Wartenburger, Isabell},
  editoratype = {collaborator},
  eventtitle = {26th {{Architectures}} and {{Mechanisms}} for {{Language Processing}} Conference ({{AMLaP}} 26)},
  venue = {{Potsdam, Germany}}
}

@software{boyce.v:2020amlap_github,
  title = {Amaze-Natural-Stories},
  author = {Boyce, Veronica},
  date = {2022-03-27T16:39:22Z},
  origdate = {2020-07-28T17:43:31Z},
  url = {https://github.com/vboyce/amaze-natural-stories},
  urldate = {2022-09-24},
  abstract = {Materials, data, code for A-maze of Natural Stories talk},
  keywords = {amlap}
}

@article{boyce.v:2020jml,
  title = {Maze {{Made Easy}}: {{Better}} and Easier Measurement of Incremental Processing Difficulty},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger},
  date = {2020-04},
  journaltitle = {Journal of Memory and Language},
  volume = {111},
  pages = {104082},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2019.104082},
  url = {https://doi.org/10.1016%2Fj.jml.2019.104082},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2019.104082},
  date-added = {2021-09-30 07:52:16 -0400},
  date-modified = {2022-04-20 13:48:24 -0400}
}

@book{brasoveanu.a:2020,
  title = {Computational {{Cognitive Modeling}} and {{Linguistic Theory}}},
  author = {Brasoveanu, Adrian and Dotlačil, Jakub},
  date = {2020},
  publisher = {{Springer Nature}},
  doi = {10.1007/978-3-030-31846-8},
  url = {https://library.oapen.org/handle/20.500.12657/39529},
  urldate = {2022-09-08},
  abstract = {This open access book introduces a general framework that allows natural language researchers to enhance existing competence theories with fully specified performance and processing components. Gradually developing increasingly complex and cognitively realistic competence-performance models, it provides running code for these models and shows how to fit them to real-time experimental data. This computational cognitive modeling approach opens up exciting new directions for research in formal semantics, and linguistics more generally, and offers new ways of (re)connecting semantics and the broader field of cognitive science. The approach of this book is novel in more ways than one. Assuming the mental architecture and procedural modalities of Anderson’s ACT-R framework, it presents fine-grained computational models of human language processing tasks which make detailed quantitative predictions that can be checked against the results of self-paced reading and other psycho-linguistic experiments. All models are presented as computer programs that readers can run on their own computer and on inputs of their choice, thereby learning to design, program and run their own models. But even for readers who won't do all that, the book will show how such detailed, quantitatively predicting modeling of linguistic processes is possible. A methodological breakthrough and a must for anyone concerned about the future of linguistics! (Hans Kamp) This book constitutes a major step forward in linguistics and psycholinguistics. It constitutes a unique synthesis of several different research traditions: computational models of psycholinguistic processes, and formal models of semantics and discourse processing. The work also introduces a sophisticated python-based software environment for modeling linguistic processes. This book has the potential to revolutionize not only formal models of linguistics, but also models of language processing more generally. (Shravan Vasishth)},
  isbn = {978-3-030-31846-8},
  langid = {english},
  keywords = {ACT-R Based Left-corner Parser,bic Book Industry Communication::C Language::CF linguistics::CFA Philosophy of language,bic Book Industry Communication::C Language::CF linguistics::CFD Psycholinguistics,bic Book Industry Communication::C Language::CF linguistics::CFG Semantics,Cataphoric Presupposition Resolution,Cognitive Aspects of Processing Semantic Representations,discourse analysis,Enriched Semantics,etc,Incremental Dynamic Predicate Logic,Language Interpretation Processes,Linguistics,Meaning Representations in Formal Semantics,Natural Language Processing,Open Access,Philosophy of language,Philosophy of Language,Processing Enriched Logical Forms,Processing of Lexical Semantic and Syntactic Representations,Psycholinguistics,Psycholinguistics and Cognitive Lingusitics,Psycholinguistics on Incremental Interpretation,Real-time Construction of Syntactic Representations,Real-time Semantic Interpretation,Semantics,Semantics and Processing,stylistics},
  annotation = {Accepted: 2020-06-15T15:07:27Z},
  file = {/Users/j/Zotero/storage/BR4RVBDJ/Brasoveanu and Dotlačil (2020) Computational Cognitive Modeling and Linguistic Th.pdf}
}

@article{braun.d:2014,
  title = {Information-{{Theoretic Bounded Rationality}} and ε-{{Optimality}}},
  author = {Braun, Daniel A. and Ortega, Pedro A.},
  date = {2014-08},
  journaltitle = {Entropy},
  volume = {16},
  number = {8},
  pages = {4662--4676},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e16084662},
  url = {https://www.mdpi.com/1099-4300/16/8/4662},
  urldate = {2022-06-08},
  abstract = {Bounded rationality concerns the study of decision makers with limited information processing resources. Previously, the free energy difference functional has been suggested to model bounded rational decision making, as it provides a natural trade-off between an energy or utility function that is to be optimized and information processing costs that are measured by entropic search costs. The main question of this article is how the information-theoretic free energy model relates to simple ε-optimality models of bounded rational decision making, where the decision maker is satisfied with any action in an ε-neighborhood of the optimal utility. We find that the stochastic policies that optimize the free energy trade-off comply with the notion of ε-optimality. Moreover, this optimality criterion even holds when the environment is adversarial. We conclude that the study of bounded rationality based on ε-optimality criteria that abstract away from the particulars of the information processing constraints is compatible with the information-theoretic free energy model of bounded rationality.},
  issue = {8},
  langid = {english},
  keywords = {ambiguity,bounded rationality,decision theory,information theory,probabilistic choice,ε-optimality},
  file = {/Users/j/Zotero/storage/TID5UPAK/Braun and Ortega - 2014 - Information-Theoretic Bounded Rationality and ε-Op.pdf}
}

@book{bresnan.j:2001,
  title = {Lexical-Functional Syntax},
  author = {Bresnan, Joan},
  date = {2001},
  publisher = {{Wiley-Blackwell}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{brody.s:2010,
  title = {It Depends on the Translation: {{Unsupervised}} Dependency Parsing via Word Alignment},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  author = {Brody, Samuel},
  date = {2010},
  pages = {1214--1222},
  publisher = {{Association for Computational Linguistics}},
  location = {{Cambridge, MA}},
  url = {https://www.aclweb.org/anthology/D10-1118}
}

@article{brothers.t:2021,
  title = {Word Predictability Effects Are Linear, Not Logarithmic: {{Implications}} for Probabilistic Models of Sentence Comprehension},
  author = {Brothers, Trevor and Kuperberg, Gina R.},
  date = {2021},
  journaltitle = {Journal of Memory and Language},
  volume = {116},
  pages = {104174},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104174},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300887},
  abstract = {During language comprehension, we routinely use information from the prior context to help identify the meaning of individual words. While measures of online processing difficulty, such as reading times, are strongly influenced by contextual predictability, there is disagreement about the mechanisms underlying this lexical predictability effect, with different models predicting different linking functions – linear (Reichle, Rayner, \& Pollatsek, 2003) or logarithmic (Levy, 2008). To help resolve this debate, we conducted two highly-powered experiments (self-paced reading, N = 216; cross-modal picture naming, N = 36), and a meta-analysis of prior eye-tracking while reading studies (total N = 218). We observed a robust linear relationship between lexical predictability and word processing times across all three studies. Beyond their methodological implications, these findings also place important constraints on predictive processing models of language comprehension. In particular, these results directly contradict the empirical predictions of surprisal theory, while supporting a proportional pre-activation account of lexical prediction effects in comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2020.104174},
  date-added = {2021-03-09 22:52:14 -0500},
  date-modified = {2021-11-14 23:57:23 -0500},
  keywords = {Information theory,Language comprehension,Prediction,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Zotero/storage/4H9J3RQ3/Brothers and Kuperberg - 2021 - Word predictability effects are linear, not logari.pdf}
}

@article{brown.p:1993,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
  date = {1993},
  journaltitle = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {263--311},
  url = {https://www.aclweb.org/anthology/J93-2003}
}

@inproceedings{brown.t:2020GPT3,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  date = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  date-modified = {2022-04-30 13:50:00 -0400},
  keywords = {GPT,GPT3},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100}
}

@article{bruening.b:2012,
  title = {\emph{By} Phrases in Passives and Nominals},
  author = {Bruening, Benjamin},
  date = {2012},
  journaltitle = {Syntax (Oxford, England)},
  shortjournal = {Syntax},
  volume = {16},
  number = {1},
  pages = {1--41},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9612.2012.00171.x},
  url = {https://doi.org/10.1111%2Fj.1467-9612.2012.00171.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2012.00171.x},
  date-added = {2021-03-22 13:11:47 -0400},
  date-modified = {2021-03-24 11:10:53 -0400}
}

@unpublished{bruening.b:2015,
  title = {Idioms: {{Movement}} and Non-Movement Dependencies},
  author = {Bruening, Benjamin},
  date = {2015},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {Idioms}
}

@article{buckley.c:2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  shorttitle = {The Free Energy Principle for Action and Perception},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  date = {2017-12-01},
  journaltitle = {Journal of Mathematical Psychology},
  shortjournal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {55--79},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
  urldate = {2022-07-27},
  abstract = {The ‘free energy principle’ (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian ‘perception as inference’, machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  langid = {english},
  keywords = {Action,Agent-based model,Bayesian brain,free energy principle,Free energy principle,Inference,Perception},
  file = {/Users/j/Zotero/storage/3HKHSLGE/Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf}
}

@misc{burda.y:2015IWAE,
  title = {Importance Weighted Autoencoders},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  date = {2015},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1509.00519},
  url = {https://arxiv.org/abs/1509.00519},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1509.00519},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:17:42 -0400},
  date-modified = {2022-05-05 09:19:51 -0400},
  keywords = {importance weighted autoencoders}
}

@unpublished{burda.y:2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  date = {2016-11-07},
  number = {arXiv:1509.00519},
  eprint = {1509.00519},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1509.00519},
  url = {http://arxiv.org/abs/1509.00519},
  urldate = {2022-05-18},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/KYZW47BL/Burda et al. - 2016 - Importance Weighted Autoencoders.pdf}
}

@inproceedings{buys.j:2015,
  title = {A {{Bayesian}} Model for Generative Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015)},
  author = {Buys, Jan and Blunsom, Phil},
  date = {2015-08},
  pages = {58--67},
  publisher = {{Uppsala University, Uppsala, Sweden}},
  location = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/W15-2108},
  date-added = {2022-04-25 20:09:07 -0400},
  date-modified = {2022-04-25 20:09:41 -0400},
  keywords = {bayesian,dependency parsing},
  file = {/Users/j/Zotero/storage/NG8H3XDE/Buys and Blunsom - 2015 - A Bayesian model for generative transition-based d.pdf}
}

@inproceedings{buys.j:2015short,
  title = {Generative {{Incremental Dependency Parsing}} with {{Neural Networks}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Buys, Jan and Blunsom, Phil},
  date = {2015-07},
  pages = {863--869},
  publisher = {{Association for Computational Linguistics}},
  location = {{Beijing, China}},
  doi = {10.3115/v1/P15-2142},
  url = {https://aclanthology.org/P15-2142},
  urldate = {2022-06-14},
  eventtitle = {{{ACL-IJCNLP}} 2015},
  file = {/Users/j/Zotero/storage/GXIK587E/Buys and Blunsom - 2015 - Generative Incremental Dependency Parsing with Neu.pdf}
}

@thesis{buys.j:2018phd,
  title = {Incremental Generative Models for Syntactic and Semantic Natural Language Processing},
  author = {Buys, Jan},
  date = {2018},
  institution = {{University of Oxford}},
  url = {https://ora.ox.ac.uk/objects/uuid:a9a7b5cf-3bb1-4e08-b109-de06bf387d1d},
  urldate = {2022-06-14},
  abstract = {{$<$}p{$>$}This thesis investigates the role of linguistically-motivated generative models of syntax and semantic structure in natural language processing (NLP). Syntactic well-formedness is crucial in language generation, but most statistical models do not account for the hierarchical structure of sentences. Many applications exhibiting natural language understanding rely on structured semantic representations to enable querying, inference and reasoning. Yet most semantic parsers produce domain-specific or inadequately expressive representations.{$<$}/p{$>$} {$<$}p{$>$}We propose a series of generative transition-based models for dependency syntax which can be applied as both parsers and language models while being amenable to supervised or unsupervised learning. Two models are based on Markov assumptions commonly made in NLP: The first is a Bayesian model with hierarchical smoothing, the second is parameterised by feed-forward neural networks. The Bayesian model enables careful analysis of the structure of the conditioning contexts required for generative parsers, but the neural network is more accurate. As a language model the syntactic neural model outperforms both the Bayesian model and n-gram neural networks, pointing to the complementary nature of distributed and structured representations for syntactic prediction. We propose approximate inference methods based on particle filtering. The third model is parameterised by recurrent neural networks (RNNs), dropping the Markov assumptions. Exact inference with dynamic programming is made tractable here by simplifying the structure of the conditioning contexts.{$<$}/p{$>$} {$<$}p{$>$}We then shift the focus to semantics and propose models for parsing sentences to labelled semantic graphs. We introduce a transition-based parser which incrementally predicts graph nodes (predicates) and edges (arguments). This approach is contrasted against predicting top-down graph traversals. RNNs and pointer networks are key components in approaching graph parsing as an incremental prediction problem. The RNN architecture is augmented to condition the model explicitly on the transition system configuration. We develop a robust parser for Minimal Recursion Semantics, a linguistically-expressive framework for compositional semantics which has previously been parsed only with grammar-based approaches. Our parser is much faster than the grammar-based model, while the same approach improves the accuracy of neural Abstract Meaning Representation parsing.{$<$}/p{$>$}},
  langid = {english},
  pagetotal = {150},
  file = {/Users/j/Zotero/storage/ERI98WK2/Buys - 2018 - Incremental generative models for syntactic and se.pdf}
}

@article{cappe.o:2007,
  title = {An Overview of Existing Methods and Recent Advances in Sequential Monte Carlo},
  author = {Cappe, Olivier and Godsill, Simon J. and Moulines, Eric},
  date = {2007-05},
  journaltitle = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/jproc.2007.893250},
  url = {https://doi.org/10.1109%2Fjproc.2007.893250},
  bdsk-url-2 = {https://doi.org/10.1109/jproc.2007.893250},
  date-added = {2022-03-25 11:26:52 -0400},
  date-modified = {2022-03-25 11:26:52 -0400}
}

@article{carpenter.r:1995,
  title = {Neural Computation of Log Likelihood in Control of Saccadic Eye Movements},
  author = {Carpenter, R. H. S. and Williams, M. L. L.},
  date = {1995-09},
  journaltitle = {Nature},
  volume = {377},
  number = {6544},
  pages = {59--62},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/377059a0},
  url = {https://www.nature.com/articles/377059a0},
  urldate = {2022-06-24},
  abstract = {THE latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals1. In one such model2, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s0 until it reaches a fixed threshold 0, when a saccade is initiated. One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present. Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if s0 simply reflects the prior log likelihood of the stimulus.},
  issue = {6544},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/j/Zotero/storage/6EA288J3/Carpenter and Williams - 1995 - Neural computation of log likelihood in control of.pdf}
}

@report{carroll.g:1992,
  type = {AAAI technical report},
  title = {Two Experiments on Learning Probabilistic Dependency Grammars from Corpora},
  author = {Carroll, Glenn and Charniak, Eugene},
  date = {1992},
  journaltitle = {Working notes of the AAAI workshop statistically-based NLP techniques},
  number = {WS-92-01},
  pages = {1--13},
  institution = {{AAAI}},
  url = {https://aaai.org/Library/Workshops/1992/ws92-01-001.php},
  abstract = {We present a scheme for learning prohabilistic dependency grammars from positive training examples plus constraints on rules. In particular we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.},
  date-added = {2021-04-18 10:28:37 -0400},
  date-modified = {2021-04-18 10:33:11 -0400},
  project = {syntactic embedding},
  keywords = {Dependency Grammar,dependency parsing,dependency structures,mutual information,pmi,unsupervised parsing}
}

@article{casadio.c:2002,
  title = {A Tale of Four Grammars},
  author = {Casadio, Claudia and Lambek, Joachim},
  date = {2002},
  journaltitle = {Studia Logica. An International Journal for Symbolic Logic},
  shortjournal = {Studia Logica},
  volume = {71},
  number = {3},
  pages = {315--329},
  publisher = {{Springer}},
  date-added = {2019-10-25 23:52:18 -0400},
  date-modified = {2019-10-25 23:54:15 -0400},
  keywords = {bilinear logic,category theory,pregroup grammar}
}

@article{chaloner.k:1995,
  title = {Bayesian Experimental Design: {{A}} Review},
  author = {Chaloner, Kathryn and Verdinelli, Isabella},
  date = {1995},
  journaltitle = {Statistical Science},
  volume = {10},
  number = {3},
  eprint = {2246015},
  eprinttype = {jstor},
  pages = {273--304},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision-theoretic approach. This framework casts criteria from the Bayesian literature of design as part of a single coherent approach. The decision-theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility function are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion.},
  date-added = {2021-09-15 10:23:51 -0400},
  date-modified = {2021-09-15 10:23:53 -0400}
}

@inproceedings{chang.h:2022,
  title = {Softmax {{Bottleneck Makes Language Models Unable}} to {{Represent Multi-mode Word Distributions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chang, Haw-Shiuan and McCallum, Andrew},
  date = {2022-05},
  pages = {8048--8073},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.554},
  url = {https://aclanthology.org/2022.acl-long.554},
  urldate = {2022-06-23},
  abstract = {Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.},
  eventtitle = {{{ACL}} 2022},
  file = {/Users/j/Zotero/storage/J66UL3ER/Chang and McCallum - 2022 - Softmax Bottleneck Makes Language Models Unable to.pdf}
}

@article{chang.j:1997,
  title = {Conditioning as Disintegration},
  author = {Chang, Joseph T and Pollard, David},
  date = {1997},
  journaltitle = {Statistica Neerlandica},
  volume = {51},
  number = {3},
  pages = {287--317},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/1467-9574.00056},
  url = {https://doi.org/10.1111/1467-9574.00056},
  date-added = {2021-02-05 12:20:15 -0500},
  date-modified = {2021-02-05 12:22:28 -0500},
  keywords = {probability theory}
}

@article{chater.n:1999,
  title = {Ten Years of the Rational Analysis of Cognition},
  author = {Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, Nick and Oaksford, Mike},
  date = {1999-02-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {2},
  eprint = {10234228},
  eprinttype = {pmid},
  pages = {57--65},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(98)01273-X},
  url = {http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(98)01273-X},
  urldate = {2022-07-07},
  langid = {english},
  keywords = {Memory,Neuroscience,Rational analysis,Rationality,Reasoning},
  file = {/Users/j/Zotero/storage/JZ32KMG5/Chater et al. - 1999 - Ten years of the rational analysis of cognition.pdf}
}

@article{chatterjee.s:2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  date = {2018-04},
  journaltitle = {The Annals of Applied Probability},
  volume = {28},
  number = {2},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/17-aap1326},
  url = {https://doi.org/10.1214%2F17-aap1326},
  file = {/Users/j/Zotero/storage/VMZ3QKKH/Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf}
}

@inproceedings{chen.d:2014,
  title = {A Fast and Accurate Dependency Parser Using Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Chen, Danqi and Manning, Christopher},
  date = {2014},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/d14-1082},
  url = {https://doi.org/10.3115%2Fv1%2Fd14-1082},
  bdsk-url-2 = {https://doi.org/10.3115/v1/d14-1082},
  date-added = {2022-05-06 15:52:04 -0400},
  date-modified = {2022-05-06 15:53:01 -0400},
  keywords = {dependency parsing,dependency structures,parsing,stanford dependencies}
}

@inproceedings{chen.s:1995,
  title = {Bayesian Grammar Induction for Language Modeling},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Stanley F.},
  date = {1995-06-26},
  series = {{{ACL}} '95},
  pages = {228--235},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  doi = {10.3115/981658.981689},
  url = {http://doi.org/10.3115/981658.981689},
  urldate = {2022-07-04},
  abstract = {We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.},
  file = {/Users/j/Zotero/storage/R22E3FH4/Chen - 1995 - Bayesian grammar induction for language modeling.pdf}
}

@inproceedings{chen.x:2016,
  title = {{{InfoGAN}}: {{Interpretable}} Representation Learning by Information Maximizing Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 29: {{Annual}} Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and von Luxburg, Ulrike and Guyon, Isabelle and Garnett, Roman},
  options = {useprefix=true},
  date = {2016},
  pages = {2172--2180},
  url = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChenCDHSSA16.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{cheyette.s:2020,
  title = {A Unified Account of Numerosity Perception},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  date = {2020-12},
  journaltitle = {Nature Human Behaviour},
  shortjournal = {Nat Hum Behav},
  volume = {4},
  number = {12},
  pages = {1265--1272},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00946-0},
  url = {http://colala.berkeley.edu/papers/cheyette2020unified.pdf},
  urldate = {2022-05-19},
  abstract = {People can identify the number of objects in sets of four or fewer items with near-perfect accuracy but exhibit linearly increasing error for larger sets. Some researchers have taken this discontinuity as evidence of two distinct representational systems. Here, we present a mathematical derivation showing that this behaviour is an optimal representation of cardinalities under a limited informational capacity, indicating that this behaviour can emerge from a single system. Our derivation predicts how the amount of information accessible to viewers should influence the perception of quantity for both large and small sets. In a series of four preregistered experiments (N\,=\,100 each), we varied the amount of information accessible to participants in number estimation. We find tight alignment between the model and human performance for both small and large quantities, implicating efficient representation as the common origin behind key phenomena of human and animal numerical cognition.},
  issue = {12},
  langid = {english},
  keywords = {Computational models,Human behaviour,Object vision},
  file = {/Users/j/Zotero/storage/XBEQTIC8/Cheyette and Piantadosi - 2020 - A unified account of numerosity perception.pdf}
}

@book{chierchia.g:2013,
  title = {Logic in Grammar: {{Polarity}}, Free Choice, and Intervention},
  author = {Chierchia, Gennaro},
  date = {2013},
  publisher = {{Oxford}},
  url = {https://doi.org/10.1093/acprof:oso/9780199697977.001.0001},
  date-added = {2019-05-19 22:01:50 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {logic,mathematical linguistics,semantics}
}

@inproceedings{chierchia.g:2019,
  title = {Ultrametric Fitting by Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Chierchia, Giovanni and Perret, Benjamin},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {3175--3186},
  url = {https://proceedings.neurips.cc/paper/2019/hash/b865367fc4c0845c0682bd466e6ebf4c-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChierchiaP19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@unpublished{chomsky.n:1955,
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  date = {1955},
  origdate = {1975},
  url = {http://alpha-leonis.lids.mit.edu/wordpress/?page<sub>i</sub>d=466},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  keywords = {information theory,syntactic information,syntax}
}

@book{chomsky.n:1957,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  date = {1957},
  publisher = {{Mouton and Co.}},
  location = {{The Hague}},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{chomsky.n:1965aspects,
  title = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  date = {1965},
  publisher = {{MIT Press}},
  date-added = {2022-04-14 12:41:20 -0400},
  date-modified = {2022-04-14 12:42:21 -0400},
  keywords = {generative grammar,syntax},
  file = {/Users/j/Zotero/storage/JJRGVPGC/chomsky.n.1965aspects.pdf;/Users/j/Zotero/storage/RTM36H2P/chomsky.n.1965aspects50th.pdf}
}

@book{chomsky.n:1975,
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  date = {1975},
  publisher = {{Springer}},
  url = {http://alpha-leonis.lids.mit.edu/wordpress/?page<sub>i</sub>d=466},
  date-added = {2020-06-05 14:43:06 -0400},
  date-modified = {2020-06-05 14:44:48 -0400},
  project = {syntactic embedding},
  keywords = {syntax}
}

@book{chomsky.n:1995,
  title = {The Minimalist Program},
  author = {Chomsky, Noam},
  date = {1995},
  publisher = {{MIT Press}},
  url = {https://muse.jhu.edu/book/36980},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@incollection{chomsky.n:1995a,
  title = {Bare Phrase Structure},
  booktitle = {Government and Binding Theory and the Minimalist Program},
  author = {Chomsky, Noam},
  editor = {Webelhuth, Gerth},
  date = {1995},
  pages = {383--349},
  publisher = {{Blackwell}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {X-bar}
}

@book{chomsky.n:2002ss2e,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  date = {2002-11},
  edition = {2},
  publisher = {{Mouton de Gruyter}},
  doi = {10.1515/9783110218329},
  url = {https://doi.org/10.1515%2F9783110218329},
  bdsk-url-2 = {https://doi.org/10.1515/9783110218329},
  date-added = {2021-09-24 08:27:37 -0400},
  date-modified = {2021-09-24 08:30:27 -0400},
  file = {/Users/j/Zotero/storage/R93E8RMP/Chomsky - 2002 - Syntactic structures.pdf}
}

@book{chopin.n:2020,
  title = {An Introduction to Sequential Monte Carlo},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  date = {2020},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-47845-2},
  url = {https://doi.org/10.1007%2F978-3-030-47845-2},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-030-47845-2},
  date-added = {2021-03-22 17:03:16 -0400},
  date-modified = {2021-03-22 17:04:36 -0400},
  keywords = {monte carlo,sampling,statistics},
  file = {/Users/j/Zotero/storage/9YYK7L7C/Chopin and Papaspiliopoulos - 2020 - An introduction to sequential monte carlo.pdf}
}

@incollection{chopin.n:2020ch8,
  title = {Importance Sampling},
  booktitle = {Springer Series in Statistics},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  date = {2020},
  pages = {81--103},
  publisher = {{Springer International Publishing}},
  doi = {10.1007/978-3-030-47845-2_8},
  url = {https://doi.org/10.1007%2F978-3-030-47845-2₈},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-030-47845-2₈},
  date-added = {2021-03-28 13:45:29 -0400},
  date-modified = {2021-03-28 13:46:25 -0400},
  keywords = {monte carlo,sampling,statistics}
}

@article{chow.c:1968,
  title = {Approximating Discrete Probability Distributions with Dependence Trees},
  author = {Chow, C and Liu, Cong},
  date = {1968},
  journaltitle = {IEEE transactions on Information Theory},
  volume = {14},
  number = {3},
  pages = {462--467},
  publisher = {{IEEE}},
  date-added = {2019-10-09 20:57:36 -0400},
  date-modified = {2019-10-09 20:58:15 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,mutual information}
}

@misc{chowdhery.a:2022PaLM,
  title = {{{PaLM}}: {{Scaling}} Language Modeling with Pathways},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and Gur-Ari, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and Meier-Hellstern, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  date = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2204.02311},
  url = {https://arxiv.org/abs/2204.02311},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.02311},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-04-19 13:50:14 -0400},
  date-modified = {2022-04-28 12:21:29 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{chung.f:1984,
  title = {On Optimal Linear Arrangements of Trees},
  author = {Chung, F.R.K.},
  date = {1984},
  journaltitle = {Computers and Mathematics with Applications},
  volume = {10},
  number = {1},
  pages = {43--60},
  issn = {0898-1221},
  url = {http://www.sciencedirect.com/science/article/pii/0898122184900853},
  bdsk-url-2 = {https://doi.org/10.1016/0898-1221(84)90085-3},
  date-added = {2019-05-14 23:56:39 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,linearization}
}

@inproceedings{church.k:1990,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  date = {1989},
  pages = {76--83},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, British Columbia, Canada}},
  doi = {10.3115/981623.981633},
  url = {https://www.aclweb.org/anthology/P89-1010},
  bdsk-url-2 = {https://doi.org/10.3115/981623.981633}
}

@inproceedings{clark.c:2019,
  title = {Don't Take the Easy Way out: {{Ensemble}} Based Methods for Avoiding Known Dataset Biases},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  date = {2019},
  pages = {4069--4082},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1418},
  url = {https://www.aclweb.org/anthology/D19-1418},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1418}
}

@inproceedings{clark.k:2019,
  title = {What Does {{BERT}} Look at? {{An}} Analysis of {{BERT}}'s Attention},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  date = {2019},
  pages = {276--286},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4828},
  url = {https://www.aclweb.org/anthology/W19-4828},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4828}
}

@misc{coecke.b:2010,
  title = {Mathematical Foundations for a Compositional Distributional Model of Meaning},
  author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
  date = {2010},
  eprint = {1003.4394},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2019-08-06 08:49:05 +0300},
  date-modified = {2019-08-06 08:50:43 +0300},
  project = {syntactic embedding},
  keywords = {compositionality,distributional models}
}

@inproceedings{coenen.a:2019,
  title = {Visualizing and Measuring the Geometry of {{BERT}}},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Viégas, Fernanda B. and Coenen, Andy and Pearce, Adam and Kim, Been},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {8592--8600},
  url = {https://proceedings.neurips.cc/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ReifYWVCPK19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{cohen.j:1994,
  title = {The Earth Is Round ({{p$<$.05}}).},
  author = {Cohen, Jacob},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0003-066x.49.12.997},
  url = {https://doi.org/10.1037%2F0003-066x.49.12.997},
  bdsk-url-2 = {https://doi.org/10.1037/0003-066x.49.12.997},
  date-added = {2021-08-08 11:16:17 -0400},
  date-modified = {2021-08-08 20:26:36 -0400}
}

@inproceedings{collins.m:2004,
  title = {Incremental Parsing with the Perceptron Algorithm},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Collins, Michael and Roark, Brian},
  date = {2004},
  pages = {111--118},
  location = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1218970},
  url = {https://www.aclweb.org/anthology/P04-1015},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218970}
}

@article{collins.m:2013,
  title = {Information Density and Dependency Length as Complementary Cognitive Models},
  author = {Collins, Michael Xavier},
  date = {2013-09},
  volume = {43},
  number = {5},
  pages = {651--681},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10936-013-9273-3},
  url = {https://doi.org/10.1007%2Fs10936-013-9273-3},
  bdsk-url-2 = {https://doi.org/10.1007/s10936-013-9273-3},
  date-added = {2021-10-18 22:13:42 -0400},
  date-modified = {2021-10-18 22:13:43 -0400}
}

@inproceedings{conneau.a:2018,
  title = {What You Can Cram into a Single \$\&!\#* Vector: {{Probing}} Sentence Embeddings for Linguistic Properties},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Loïc and Baroni, Marco},
  date = {2018},
  pages = {2126--2136},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1198},
  url = {https://www.aclweb.org/anthology/P18-1198},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1198}
}

@misc{coon.j:2018,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  date = {2018},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-02-26 09:53:56 -0500},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@misc{coon.j:2019,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  date = {2019},
  url = {https://ling.auf.net/lingbuzz/004224},
  date-added = {2020-06-16 10:17:01 -0400},
  date-modified = {2020-06-16 10:17:01 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@article{coon.j:2020,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {to appear},
  journaltitle = {Linguistic Inquiry},
  doi = {10.1162/ling_a_00386},
  url = {https://doi.org/10.1162/lingₐ₀0386},
  date-added = {2020-02-03 15:20:36 -0500},
  date-modified = {2020-06-16 10:21:03 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects,person case constraint,phi features,quirky case}
}

@article{costa.f:2003,
  title = {Towards Incremental Parsing of Natural Language Using Recursive Neural Networks},
  author = {Costa, F.},
  date = {2003},
  journaltitle = {Applied Intelligence},
  volume = {19},
  number = {1/2},
  pages = {9--25},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1023860521975},
  url = {https://doi.org/10.1023%2Fa%3A1023860521975},
  bdsk-url-2 = {https://doi.org/10.1023/a:1023860521975},
  date-added = {2021-06-22 15:15:28 -0400},
  date-modified = {2021-06-22 15:16:11 -0400},
  file = {/Users/j/Zotero/storage/Q8KJXIDW/Costa (2003) Towards incremental parsing of natural language us.pdf}
}

@book{cover.t:2006,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  date = {2006},
  edition = {2},
  publisher = {{Wiley}},
  doi = {10.1002/047174882x},
  url = {https://doi.org/10.1002%2F047174882x},
  bdsk-url-2 = {https://doi.org/10.1002/047174882x},
  date-added = {2022-04-28 11:28:46 -0400},
  date-modified = {2022-04-28 11:29:11 -0400}
}

@inproceedings{cramer.b:2007,
  title = {Limitations of Current Grammar Induction Algorithms},
  booktitle = {Proceedings of the {{ACL}} 2007 Student Research Workshop},
  author = {Cramer, Bart},
  date = {2007},
  pages = {43--48},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/P07-3008}
}

@article{crameri.f:2020,
  title = {The Misuse of Colour in Science Communication},
  author = {Crameri, Fabio and Shephard, Grace E. and Heron, Philip J.},
  date = {2020-10-28},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {11},
  number = {1},
  pages = {5444},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19160-7},
  url = {https://www.nature.com/articles/s41467-020-19160-7},
  urldate = {2022-10-12},
  abstract = {The accurate representation of data is essential in science communication. However, colour maps that visually distort data through uneven colour gradients or are unreadable to those with colour-vision deficiency remain prevalent in science. These include, but are not limited to, rainbow-like and red–green colour maps. Here, we present a simple guide for the scientific use of colour. We show how scientifically derived colour maps report true data variations, reduce complexity, and are accessible for people with colour-vision deficiencies. We highlight ways for the scientific community to identify and prevent the misuse of colour in science, and call for a proactive step away from colour misuse among the community, publishers, and the press.},
  issue = {1},
  langid = {english},
  keywords = {Scientific community,Software}
}

@book{crooks.g:2019,
  title = {Field {{Guide}} to {{Continuous Probability Distributions}}},
  author = {Crooks, Gavin E.},
  date = {2019},
  edition = {1.0.0},
  publisher = {{Berkeley Institute for Theoretical Science}},
  url = {http://threeplusone.com/pubs/fieldguide/},
  urldate = {2022-06-22},
  abstract = {Over 170 continuous univariate probability distributions (and at least as many synonyms) organized into 20 families.},
  isbn = {978-1-73393-810-5},
  langid = {english}
}

@incollection{culicover.p:1999,
  title = {Syntactic Nuts: {{Hard}} Cases in Syntax},
  booktitle = {Syntactic Nuts: {{Hard}} Cases in Syntax.},
  author = {Culicover, Peter},
  date = {1999},
  series = {Foundations of Syntax},
  publisher = {{Oxford University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{culicover.p:2005,
  title = {Simpler Syntax},
  booktitle = {Simpler Syntax},
  author = {Culicover, Peter and Jackendoff, Ray},
  date = {2005},
  publisher = {{Oxford University Press}},
  location = {{Oxford}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:43 -0400}
}

@inproceedings{cusumano-towner.m:2018,
  title = {Incremental Inference for Probabilistic Programs},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {Cusumano-Towner, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
  date = {2018},
  series = {{{PLDI}} 2018},
  pages = {571--585},
  publisher = {{Association for Computing Machinery}},
  location = {{New York, NY, USA}},
  doi = {10.1145/3192366.3192399},
  url = {https://doi.org/10.1145/3192366.3192399},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
  date-added = {2022-05-03 21:06:22 -0400},
  date-modified = {2022-05-03 21:07:50 -0400},
  isbn = {978-1-4503-5698-5},
  pagetotal = {15},
  keywords = {incremental computation,probabilistic programming,sequential Monte Carlo}
}

@inproceedings{dai.z:2019,
  title = {Transformer-{{XL}}: {{Attentive}} Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
  date = {2019},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/p19-1285},
  url = {https://doi.org/10.18653%2Fv1%2Fp19-1285},
  bdsk-url-2 = {https://doi.org/10.18653/v1/p19-1285},
  date-added = {2021-11-30 13:59:20 -0500},
  date-modified = {2021-11-30 13:59:33 -0500}
}

@article{danon.g:2006,
  title = {Caseless Nominals and the Projection of {{DP}}},
  author = {Danon, Gabi},
  date = {2006},
  journaltitle = {Natural Language \& Linguistic Theory},
  volume = {24},
  number = {4},
  pages = {977},
  issn = {1573-0859},
  doi = {10.1007/s11049-006-9005-6},
  url = {https://doi.org/10.1007/s11049-006-9005-6},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:43:43 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case,subject positions}
}

@article{danon.g:2011,
  title = {Agreement and {{DP-Internal}} Feature Distribution},
  author = {Danon, Gabi},
  date = {2011},
  journaltitle = {Syntax (Oxford, England)},
  shortjournal = {Syntax},
  volume = {14},
  number = {4},
  pages = {297--317},
  doi = {10.1111/j.1467-9612.2011.00154.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9612.2011.00154.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2011.00154.x},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:45:11 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,subject positions}
}

@inproceedings{dasgupta.s:2016,
  title = {A Cost Function for Similarity-Based Hierarchical Clustering},
  booktitle = {Proceedings of the 48th Annual {{ACM SIGACT}} Symposium on Theory of Computing, {{STOC}} 2016, Cambridge, {{MA}}, {{USA}}, June 18-21, 2016},
  author = {Dasgupta, Sanjoy},
  editor = {Wichs, Daniel and Mansour, Yishay},
  date = {2016},
  pages = {118--127},
  publisher = {{ACM}},
  doi = {10.1145/2897518.2897527},
  url = {https://doi.org/10.1145/2897518.2897527},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/stoc/Dasgupta16.bib},
  timestamp = {Tue, 06 Nov 2018 00:00:00 +0100}
}

@inproceedings{dathathri.s:2020,
  title = {Plug and {{Play Language Models}}: {{A Simple Approach}} to {{Controlled Text Generation}}},
  shorttitle = {Plug and {{Play Language Models}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  date = {2020-04-23},
  url = {https://openreview.net/forum?id=H1edEyBKDS},
  urldate = {2022-07-11},
  abstract = {We control the topic and sentiment of text generation (almost) without any training.},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZWAD349P/Dathathri et al. - 2020 - Plug and Play Language Models A Simple Approach t.pdf}
}

@article{dawid.a:2004,
  title = {Probability, Causality and the Empirical World: {{A Bayes}}–de {{Finetti}}–{{Popper}}– Borel Synthesis},
  author = {Dawid, A. P.},
  date = {2004-02},
  journaltitle = {Statistical Science},
  volume = {19},
  number = {1},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/088342304000000125},
  url = {https://doi.org/10.1214%2F088342304000000125},
  bdsk-url-2 = {https://doi.org/10.1214/088342304000000125},
  date-added = {2022-04-13 19:51:33 -0400},
  date-modified = {2022-04-13 19:51:34 -0400},
  file = {/Users/j/Zotero/storage/VQYK37KT/Dawid - 2004 - Probability, causality and the empirical world A .pdf}
}

@inproceedings{deal.a:2015,
  title = {Interaction and Satisfaction in {{φ}}-Agreement},
  booktitle = {Proceedings of {{NELS}}},
  author = {Deal, Amy Rose},
  date = {2015},
  volume = {45},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:25 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@article{decaire.r:2017,
  title = {On Optionality in {{Mohawk}} Noun Incorporation},
  author = {DeCaire, Ryan and Johns, Alana and Kučerová, Ivona},
  date = {2017-12-14},
  journaltitle = {Toronto Working Papers in Linguistics},
  volume = {39},
  issn = {1718-3510},
  url = {https://twpl.library.utoronto.ca/index.php/twpl/article/view/28604},
  urldate = {2022-05-30},
  abstract = {Noun incorporation is a phenomenon much discussed within Iroquoian language literature. In this paper, we consider noun incorporation in Mohawk, a language within the Iroquoian language family, and argue that what has often been considered to be optional noun incorporation is in fact primarily determined by the information structure of the clause. We show that with the exception of lexically-determined verbs that always or never incorporate, every verb may or may not incorporate its nominal object. We analyse the incorporated version as the default structure. The non-incorporated counterpart is licensed only under particular information-structure properties. We provide evidence that noun excorporation arises whenever the verb or the object noun is focused, and in turn moves to the left periphery.},
  langid = {english},
  keywords = {excorporation,iroquoian,Mohawk,noun incorporation},
  file = {/Users/j/Zotero/storage/VYJWIBAA/DeCaire et al. - 2017 - On optionality in Mohawk noun incorporation.pdf}
}

@article{del-moral.p:2006,
  title = {Sequential Monte Carlo Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  date = {2006-06},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  url = {https://doi.org/10.1111%2Fj.1467-9868.2006.00553.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
  date-added = {2022-04-30 17:47:24 -0400},
  date-modified = {2022-04-30 22:51:54 -0400},
  keywords = {particle filtering,sampling,sequential monte carlo},
  file = {/Users/j/Zotero/storage/IVS4EKHZ/Del Moral et al. - 2006 - Sequential monte carlo samplers.pdf;/Users/j/Zotero/storage/ZCZ3LNH2/Del Moral et al. - 2006 - Sequential monte carlo samplers.pdf}
}

@misc{deletang.g:2022,
  title = {Neural {{Networks}} and the {{Chomsky Hierarchy}}},
  author = {Delétang, Grégoire and Ruoss, Anian and Grau-Moya, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  date = {2022-10-06},
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.02098},
  urldate = {2022-10-11},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (10250 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {/Users/j/Zotero/storage/NN7TDKA4/Delétang et al. (2022) Neural Networks and the Chomsky Hierarchy.pdf}
}

@inproceedings{demarneffe.m:2006stanforddep,
  title = {Generating Typed Dependency Parses from Phrase Structure Parses},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({{LREC}}'06)},
  author = {de Marneffe, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
  options = {useprefix=true},
  date = {2006},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Genoa, Italy}},
  url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/440ₚdf.pdf}
}

@inproceedings{demarneffe.m:2008,
  title = {The {{Stanford}} Typed Dependencies Representation},
  booktitle = {Coling 2008: {{Proceedings}} of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation},
  author = {de Marneffe, Marie-Catherine and Manning, Christopher D.},
  options = {useprefix=true},
  date = {2008},
  pages = {1--8},
  publisher = {{Coling 2008 Organizing Committee}},
  location = {{Manchester, UK}},
  url = {https://www.aclweb.org/anthology/W08-1301}
}

@manual{demarneffe.m:2008sdmanual,
  type = {manual},
  title = {Stanford Typed Dependencies Manual},
  author = {de Marneffe, Marie-Catherine and Manning, Christopher},
  options = {useprefix=true},
  date = {2008},
  url = {https://nlp.stanford.edu/software/dependenciesₘanual.pdf},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-03-12 10:47:01 -0500},
  organization = {{Stanford NLP}},
  project = {syntactic embedding},
  version = {Stanford Parser v.3.7.0},
  keywords = {dependency structures,stanford dependencies}
}

@article{demberg.v:2008,
  title = {Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity},
  author = {Demberg, Vera and Keller, Frank},
  date = {2008},
  journaltitle = {Cognition},
  volume = {109},
  number = {2},
  pages = {193--210},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027708001741},
  abstract = {We evaluate the predictions of two theories of syntactic processing complexity, dependency locality theory (DLT) and surprisal, against the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text. Our results show that DLT integration cost is not a significant predictor of reading times for arbitrary words in the corpus. However, DLT successfully predicts reading times for nouns. We also find evidence for integration cost effects at auxiliaries, not predicted by DLT. For surprisal, we demonstrate that an unlexicalized formulation of surprisal can predict reading times for arbitrary words in the corpus. Comparing DLT integration cost and surprisal, we find that the two measures are uncorrelated, which suggests that a complete theory will need to incorporate both aspects of processing complexity. We conclude that eye-tracking corpora, which provide reading time data for naturally occurring, contextualized sentences, can complement experimental evidence as a basis for theories of processing complexity.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2008.07.008},
  keywords = {Corpus data,Dependency locality theory,Eye-tracking,Processing complexity,Surprisal},
  file = {/Users/j/Zotero/storage/RD6WXH4Z/Demberg and Keller - 2008 - Data from eye-tracking corpora as evidence for the.pdf}
}

@inproceedings{devlin.j:2019,
  title = {{{BERT}}: {{Pre-training}} of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  date = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  url = {https://www.aclweb.org/anthology/N19-1423},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423},
  file = {/Users/j/Zotero/storage/FZG9EPZ3/Devlin et al. - 2019 - BERT Pre-training of deep bidirectional transform.pdf}
}

@article{diaconis.p:2008,
  title = {The Markov Chain Monte Carlo Revolution},
  author = {Diaconis, Persi},
  date = {2008-11},
  journaltitle = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {179--205},
  publisher = {{American Mathematical Society (AMS)}},
  doi = {10.1090/s0273-0979-08-01238-x},
  url = {https://doi.org/10.1090%2Fs0273-0979-08-01238-x},
  bdsk-url-2 = {https://doi.org/10.1090/s0273-0979-08-01238-x},
  date-added = {2022-03-17 13:44:06 -0400},
  date-modified = {2022-03-17 13:44:07 -0400},
  file = {/Users/j/Zotero/storage/VMFKC8TS/Diaconis - 2008 - The markov chain monte carlo revolution.pdf}
}

@article{dotlacil.j:2021,
  title = {Parsing as a {{Cue-Based Retrieval Model}}},
  author = {Dotlačil, Jakub},
  date = {2021},
  journaltitle = {Cognitive Science},
  volume = {45},
  number = {8},
  pages = {e13020},
  issn = {1551-6709},
  doi = {10.1111/cogs.13020},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13020},
  urldate = {2022-07-01},
  abstract = {This paper develops a novel psycholinguistic parser and tests it against experimental and corpus reading data. The parser builds on the recent research into memory structures, which argues that memory retrieval is content-addressable and cue-based. It is shown that the theory of cue-based memory systems can be combined with transition-based parsing to produce a parser that, when combined with the cognitive architecture ACT-R, can model reading and predict online behavioral measures (reading times and regressions). The parser's modeling capacities are tested against self-paced reading experimental data (Grodner \& Gibson, 2005), eye-tracking experimental data (Staub, 2011), and a self-paced reading corpus (Futrell et al., 2018).},
  langid = {english},
  keywords = {ACT-R,Computational psycholinguistics,Cue-based retrieval,Memory retrieval,Modeling reading data,Processing},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.13020},
  file = {/Users/j/Zotero/storage/CE7V9VPS/Dotlačil - 2021 - Parsing as a Cue-Based Retrieval Model.pdf}
}

@article{douc.r:2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  author = {Douc, Randal and Cappé, Olivier and Moulines, Eric},
  date = {2005},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.CS/0507025},
  url = {https://arxiv.org/abs/cs/0507025},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.CS/0507025},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  date-added = {2022-03-29 20:05:42 -0400},
  date-modified = {2022-03-29 20:05:43 -0400},
  keywords = {and Science (cs.CE),Computational Engineering,Finance,FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/ECTS7DHP/Douc et al. - 2005 - Comparison of resampling schemes for particle filt.pdf}
}

@incollection{doucet.a:2001,
  title = {An Introduction to Sequential {{Monte Carlo}} Methods},
  booktitle = {Sequential Monte Carlo Methods in Practice},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  date = {2001},
  pages = {3--14},
  publisher = {{Springer New York}},
  doi = {10.1007/978-1-4757-3437-9_1},
  date-added = {2021-03-22 19:46:53 -0400},
  date-modified = {2021-03-22 19:46:54 -0400},
  file = {/Users/j/Zotero/storage/597H86JN/Doucet et al. - 2001 - An introduction to sequential monte carlo methods.pdf}
}

@book{doucet.a:2001smcbook,
  title = {Sequential {{Monte Carlo}} Methods in Practice},
  editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  date = {2001},
  series = {Statistics for Engineering and Information Science},
  publisher = {{Springer}},
  location = {{New York}},
  doi = {10.1007/978-1-4757-3437-9},
  url = {https://doi.org/10.1007%2F978-1-4757-3437-9},
  bdsk-url-2 = {https://doi.org/10.1007/978-1-4757-3437-9},
  date-added = {2022-03-25 22:05:58 -0400},
  date-modified = {2022-03-25 22:10:04 -0400},
  file = {/Users/j/Zotero/storage/E66PEYMC/Doucet et al. - 2001 - Sequential monte carlo methods in practice.pdf}
}

@incollection{doucet.a:2011,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {The {{Oxford Handbook}} of {{Nonlinear Filtering}}},
  author = {Doucet, Arnaud and Johansen, Adam M.},
  editor = {Crisan, Dan and Rozovskiĭ, Boris},
  date = {2011-04-15},
  series = {Oxford {{Handbooks}}},
  pages = {656--704},
  publisher = {{Oxford University Press}},
  url = {http://www.stats.ox.ac.uk/~doucet/doucet_johansen_tutorialPF2011.pdf},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision,　econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
  annotation = {Note: Version 1.1 – December 2008 with typographical corrections March 2012},
  file = {/Users/j/Zotero/storage/AQ62L6MY/Doucet and Johansen - 2011 - A tutorial on particle filtering and smoothing fi.pdf;/Users/j/Zotero/storage/U3NCLMN4/doucet.a.2011scan.pdf}
}

@inproceedings{dozat.t:2016,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Dozat, Timothy and Manning, Christopher D.},
  date = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=Hk95PK9le},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@incollection{drieghe.d:2011,
  title = {Parafoveal-on-Foveal Effects on Eye Movements during Reading},
  booktitle = {The Oxford Handbook of Eye Movements},
  author = {Drieghe, Denis},
  editor = {Liversedge, Simon P. and Gilchrist, Iain and Everling, Stefan},
  date = {2011},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199539789.013.0046},
  url = {https://doi.org/10.1093%2Foxfordhb%2F9780199539789.013.0046},
  bdsk-url-2 = {https://doi.org/10.1093/oxfordhb/9780199539789.013.0046},
  date-added = {2022-04-21 11:02:00 -0400},
  date-modified = {2022-04-21 11:03:00 -0400}
}

@inproceedings{du.w:2020,
  title = {Exploiting Syntactic Structure for Better Language Modeling: {{A}} Syntactic Distance Approach},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Du, Wenyu and Lin, Zhouhan and Shen, Yikang and O'Donnell, Timothy J. and Bengio, Yoshua and Zhang, Yue},
  date = {2020},
  pages = {6611--6628},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.591},
  url = {https://www.aclweb.org/anthology/2020.acl-main.591},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.591},
  date-modified = {2022-05-17 08:09:30 -0400}
}

@article{dupoux.e:2018,
  title = {Cognitive Science in the Era of Artificial Intelligence: {{A}} Roadmap for Reverse-Engineering the Infant Language-Learner},
  author = {Dupoux, Emmanuel},
  date = {2018-04},
  journaltitle = {Cognition},
  volume = {173},
  pages = {43--59},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.cognition.2017.11.008},
  url = {https://doi.org/10.1016%2Fj.cognition.2017.11.008},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2017.11.008},
  date-added = {2021-10-04 22:09:38 -0400},
  date-modified = {2021-10-04 22:09:56 -0400}
}

@inproceedings{dyer.c:2016,
  title = {Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
  date = {2016},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/n16-1024},
  url = {https://doi.org/10.18653%2Fv1%2Fn16-1024},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n16-1024},
  date-added = {2021-09-13 21:32:13 -0400},
  date-modified = {2021-09-13 21:32:16 -0400}
}

@article{earley.j:1970,
  title = {An Efficient Context-Free Parsing Algorithm},
  author = {Earley, Jay},
  date = {1970-02-01},
  journaltitle = {Communications of the ACM},
  shortjournal = {Commun. ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  url = {https://doi.org/10.1145/362007.362035},
  urldate = {2022-06-13},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  file = {/Users/j/Zotero/storage/3W34H2B6/Earley - 1970 - An efficient context-free parsing algorithm.pdf}
}

@article{ebbinghaus.h:1885,
  title = {Memory: {{A Contribution}} to {{Experimental Psychology}}},
  shorttitle = {Memory},
  author = {Ebbinghaus, Hermann},
  translator = {Ruger, Henry A. and Bussenius, Clara E.},
  date = {2013},
  origdate = {1885},
  journaltitle = {Annals of Neurosciences},
  shortjournal = {ANS},
  series = {Annals {{Classics}}},
  volume = {20},
  number = {4},
  pages = {155--156},
  issn = {09727531, 09763260},
  doi = {10.5214/ans.0972.7531.200408},
  url = {http://annalsofneurosciences.org/journal/index.php/annal/article/view/540},
  urldate = {2022-06-13},
  abstract = {The language of life as well as of science in attributing a memory to the mind attempts to point out the facts and their interpretation somewhat as follows: Mental states of every kind, -- sensations, feelings, ideas, -- which were at one time present in consciousness and then have disappeared from it, have not with their disappearance absolutely ceased to exist. Although the inwardly-turned look may no longer be able to find them, nevertheless they have not been utterly destroyed and annulled, but in a certain manner they continue to exist, stored up, so to speak, in the memory. We cannot, of course, directly observe their present existence, but it is revealed by the effects which come to our knowledge with a certainty like that with which we infer the existence of the stars below the horizon. These effects are of different kinds.},
  langid = {english},
  file = {/Users/j/Zotero/storage/5EM4LARF/Ebbinghaus - 2013 - Memory A Contribution to Experimental Psychology.pdf}
}

@article{eberhard.k:1995,
  title = {Eye Movements as a Window into Real-Time Spoken Language Comprehension in Natural Contexts},
  author = {Eberhard, Kathleen M. and Spivey-Knowlton, Michael J. and Sedivy, Julie C. and Tanenhaus, Michael K.},
  date = {1995-11-01},
  journaltitle = {Journal of Psycholinguistic Research},
  shortjournal = {J Psycholinguist Res},
  volume = {24},
  number = {6},
  pages = {409--436},
  issn = {1573-6555},
  doi = {10.1007/BF02143160},
  url = {https://doi.org/10.1007/BF02143160},
  urldate = {2022-10-13},
  abstract = {When listeners follow spoken instructions to manipulate real objects, their eye movements to the objects are closely time locked to the referring words. We review five experiments showing that this time-locked characteristic of eye movements provides a detailed profile of the processes that underlie real-time spoken language comprehension. Together, the first four experiments showed that listerners immediately integrated lexical, sublexical, and prosodic information in the spoken input with information from the visual context to reduce the set of referents to the intended one. The fifth experiment demonstrated that a visual referential context affected the initial structuring of the linguistic input, eliminating even strong syntactic preferences that result in clear garden paths when the referential context is introduced linguistically. We argue that context affected the earliest moments of language processing because it was highly accessible and relevant to the behavioral goals of the listener.},
  langid = {english},
  keywords = {Cognitive Psychology,Initial Structure,Language Comprehension,Language Processing,Real Object},
  file = {/Users/j/Zotero/storage/TZ2DTPRA/Eberhard et al. (1995) Eye movements as a window into real-time spoken la.pdf}
}

@article{ehrlich.s:1981,
  title = {Contextual Effects on Word Perception and Eye Movements during Reading},
  author = {Ehrlich, Susan F. and Rayner, Keith},
  date = {1981},
  journaltitle = {Journal of Memory and Language},
  volume = {20},
  number = {6},
  pages = {641},
  publisher = {{Academic Press}},
  doi = {10.1016/S0022-5371(81)90220-6},
  url = {https://doi.org/10.1016/S0022-5371(81)90220-6},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@inproceedings{eisner.j:1996,
  title = {Three New Probabilistic Models for Dependency Parsing: {{An}} Exploration},
  booktitle = {{{COLING}} 1996 Volume 1: {{The}} 16th International Conference on Computational Linguistics},
  author = {Eisner, Jason M.},
  date = {1996},
  url = {https://www.aclweb.org/anthology/C96-1058}
}

@report{eisner.j:1997,
  type = {Technical report},
  title = {An Empirical Comparison of Probability Models for Dependency Grammar},
  author = {Eisner, Jason},
  date = {1997},
  number = {IRCS-96-11},
  eprint = {cmp-lg/9706004},
  eprinttype = {arxiv},
  institution = {{Institute for Research in Cognitive Science, University of Pennsylvania}},
  url = {https://arxiv.org/pdf/cmp-lg/9706004.pdf},
  archiveprefix = {arXiv},
  date-added = {2020-02-24 16:29:57 -0500},
  date-modified = {2021-09-09 23:03:05 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm,projective dependencies,technical report},
  file = {/Users/j/Zotero/storage/WR55UZ3E/Eisner - 1997 - An empirical comparison of probability models for .pdf}
}

@inproceedings{eisner.j:2016,
  title = {Inside-{{Outside}} and {{Forward-Backward Algorithms Are Just Backprop}} (Tutorial Paper)},
  booktitle = {Proceedings of the {{Workshop}} on {{Structured Prediction}} for {{NLP}}},
  author = {Eisner, Jason},
  date = {2016-11},
  pages = {1--17},
  publisher = {{Association for Computational Linguistics}},
  location = {{Austin, TX}},
  doi = {10.18653/v1/W16-5901},
  url = {https://aclanthology.org/W16-5901},
  urldate = {2022-07-05},
  file = {/Users/j/Zotero/storage/B55HLLIY/Eisner - 2016 - Inside-Outside and Forward-Backward Algorithms Are.pdf}
}

@article{engbert.r:2005,
  title = {{{SWIFT}}: {{A}} Dynamical Model of Saccade Generation during Reading.},
  author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
  date = {2005},
  journaltitle = {Psychological Review},
  volume = {112},
  number = {4},
  pages = {777--813},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0033-295x.112.4.777},
  url = {https://doi.org/10.1037%2F0033-295x.112.4.777},
  bdsk-url-2 = {https://doi.org/10.1037/0033-295x.112.4.777},
  date-added = {2021-06-02 14:39:17 -0400},
  date-modified = {2021-06-02 14:39:49 -0400},
  keywords = {eye-tracking,frequency effects,predictability}
}

@article{engelmann.f:2013,
  title = {A {{Framework}} for {{Modeling}} the {{Interaction}} of {{Syntactic Processing}} and {{Eye Movement Control}}},
  author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
  date = {2013-07},
  journaltitle = {Topics in Cognitive Science},
  shortjournal = {Top Cogn Sci},
  volume = {5},
  number = {3},
  pages = {452--474},
  issn = {17568757},
  doi = {10.1111/tops.12026},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12026},
  urldate = {2022-08-28},
  langid = {english},
  file = {/Users/j/Zotero/storage/WN3VF4TX/Engelmann et al. - 2013 - A Framework for Modeling the Interaction of Syntac.pdf}
}

@thesis{engelmann.f:2016PhD,
  type = {phdthesis},
  title = {Toward an Integrated Model of Sentence Processing in Reading},
  author = {Engelmann, Felix},
  date = {2016},
  institution = {{Universität Potsdam}},
  location = {{Potsdam, Germany}},
  url = {https://publishup.uni-potsdam.de/frontdoor/index/index/docId/10086},
  urldate = {2022-10-12},
  abstract = {In experiments investigating sentence processing, eye movement measures such as fixation durations and regression proportions while reading are commonly used to draw conclusions about processing difficulties. However, these measures are the result of an interaction of multiple cognitive levels and processing strategies and thus are only indirect indicators of processing difficulty. In order to properly interpret an eye movement response, one has to understand the underlying principles of adaptive processing such as trade-off mechanisms between reading speed and depth of comprehension that interact with task demands and individual differences. Therefore, it is necessary to establish explicit models of the respective mechanisms as well as their causal relationship with observable behavior. There are models of lexical processing and eye movement control on the one side and models on sentence parsing and memory processes on the other. However, no model so far combines both sides with explicitly defined linking assumptions. In this thesis, a model is developed that integrates oculomotor control with a parsing mechanism and a theory of cue-based memory retrieval. On the basis of previous empirical findings and independently motivated principles, adaptive, resource-preserving mechanisms of underspecification are proposed both on the level of memory access and on the level of syntactic parsing. The thesis first investigates the model of cue-based retrieval in sentence comprehension of Lewis \& Vasishth (2005) with a comprehensive literature review and computational modeling of retrieval interference in dependency processing. The results reveal a great variability in the data that is not explained by the theory. Therefore, two principles, 'distractor prominence' and 'cue confusion', are proposed as an extension to the theory, thus providing a more adequate description of systematic variance in empirical results as a consequence of experimental design, linguistic environment, and individual differences. In the remainder of the thesis, four interfaces between parsing and eye movement control are defined: Time Out, Reanalysis, Underspecification, and Subvocalization. By comparing computationally derived predictions with experimental results from the literature, it is investigated to what extent these four interfaces constitute an appropriate elementary set of assumptions for explaining specific eye movement patterns during sentence processing. Through simulations, it is shown how this system of in itself simple assumptions results in predictions of complex, adaptive behavior.  In conclusion, it is argued that, on all levels, the sentence comprehension mechanism seeks a balance between necessary processing effort and reading speed on the basis of experience, task demands, and resource limitations. Theories of linguistic processing therefore need to be explicitly defined and implemented, in particular with respect to linking assumptions between observable behavior and underlying cognitive processes. The comprehensive model developed here integrates multiple levels of sentence processing that hitherto have only been studied in isolation. The model is made publicly available as an expandable framework for future studies of the interactions between parsing, memory access, and eye movement control.},
  langid = {english},
  keywords = {ACT-R},
  file = {/Users/j/Zotero/storage/QR59CZFZ/Engelmann (2016) Toward an integrated model of sentence processing .pdf}
}

@article{engelmann.f:2019,
  title = {The {{Effect}} of {{Prominence}} and {{Cue Association}} on {{Retrieval Processes}}: {{A Computational Account}}},
  shorttitle = {The {{Effect}} of {{Prominence}} and {{Cue Association}} on {{Retrieval Processes}}},
  author = {Engelmann, Felix and Jäger, Lena A. and Vasishth, Shravan},
  date = {2019},
  journaltitle = {Cognitive Science},
  volume = {43},
  number = {12},
  pages = {e12800},
  issn = {1551-6709},
  doi = {10.1111/cogs.12800},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12800},
  urldate = {2022-10-12},
  abstract = {We present a comprehensive empirical evaluation of the ACT-R–based model of sentence processing developed by Lewis and Vasishth (2005) (LV05). The predictions of the model are compared with the results of a recent meta-analysis of published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject–verb dependencies (Jäger, Engelmann, \& Vasishth, 2017). The comparison shows that the model has only partial success in explaining the data; and we propose that its prediction space is restricted by oversimplifying assumptions. We then implement a revised model that takes into account differences between individual experimental designs in terms of the prominence of the target and the distractor in memory- and context-dependent cue-feature associations. The predictions of the original and the revised model are quantitatively compared with the results of the meta-analysis. Our simulations show that, compared to the original LV05 model, the revised model accounts for the data better. The results suggest that effects of prominence and variable cue-feature associations need to be considered in the interpretation of existing empirical results and in the design and planning of future experiments. With regard to retrieval interference in sentence processing and to the broader field of psycholinguistic studies, we conclude that well-specified models in tandem with high-powered experiments are needed in order to uncover the underlying cognitive processes.},
  langid = {english},
  keywords = {ACT-R,Computational modeling,Cue-based retrieval,Dependency completion,Retrieval interference,Sentence processing},
  file = {/Users/j/Zotero/storage/P3AY5IXV/Engelmann et al. (2019) The Effect of Prominence and Cue Association on Re.pdf}
}

@article{ennever.t:2017,
  title = {A Replicable Acoustic Measure of Lenition and the Nature of Variability in {{Gurindji}} Stops},
  author = {Ennever, Thomas and Meakins, Felicity and Round, Erich R.},
  date = {2017-08},
  journaltitle = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  number = {1},
  publisher = {{Open Library of the Humanities}},
  doi = {10.5334/labphon.18},
  url = {https://doi.org/10.5334%2Flabphon.18},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.18},
  date-added = {2022-05-10 10:59:44 -0400},
  date-modified = {2022-05-10 10:59:54 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/M7JP2B7J/Ennever et al. - 2017 - A replicable acoustic measure of lenition and the .pdf}
}

@misc{erion.g:2019,
  title = {Learning Explainable Models Using Attribution Priors},
  author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  date = {2019},
  eprint = {1906.10670},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:04:57 -0400},
  date-modified = {2019-07-05 11:06:04 -0400},
  project = {syntactic embedding},
  keywords = {gradient attribution priors}
}

@article{estes.w:1959,
  title = {Foundations of Linear Models},
  author = {Estes, William K and Suppes, Patrick},
  date = {1959},
  journaltitle = {Studies in mathematical learning theory},
  pages = {137--179},
  publisher = {{Stanford University Press Stanford}},
  date-added = {2021-05-31 13:41:13 -0400},
  date-modified = {2021-05-31 13:41:26 -0400},
  keywords = {probability matching}
}

@book{fano.r:1961,
  title = {Transmission of Information: A Statistical Theory of Communications},
  author = {Fano, Robert M},
  date = {1961},
  edition = {1},
  publisher = {{MIT Press}},
  location = {{Cambdridge, Mass}},
  url = {https://b-ok.cc/book/5577269/50d40b},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-07 09:14:32 -0400}
}

@article{fasiolo.m:2020,
  title = {Fast Calibrated Additive Quantile Regression},
  author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Raphaël and Goude, Yannig},
  date = {2020-03},
  journaltitle = {Journal of the American Statistical Association},
  volume = {116},
  number = {535},
  pages = {1402--1412},
  publisher = {{Informa UK Limited}},
  issn = {1537-274X},
  doi = {10.1080/01621459.2020.1725521},
  url = {http://dx.doi.org/10.1080/01621459.2020.1725521},
  date-added = {2022-03-10 22:30:30 -0500},
  date-modified = {2022-03-10 22:30:33 -0500}
}

@misc{feder.a:2021,
  title = {Causal Inference in Natural Language Processing: {{Estimation}}, Prediction, Interpretation and Beyond},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and Wood-Doughty, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2109.00725},
  url = {https://arxiv.org/abs/2109.00725},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2109.00725},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-10 11:31:28 -0400},
  date-modified = {2022-05-10 11:31:44 -0400},
  keywords = {causality,machine learning,natural language processing},
  file = {/Users/j/Zotero/storage/CKBHHB2T/Feder et al. - 2021 - Causal inference in natural language processing E.pdf}
}

@inproceedings{feller.w:1949,
  title = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Proceedings of the [{{First}}] Berkeley Symposium on Mathematical Statistics and Probability},
  author = {Feller, William},
  date = {1949},
  pages = {403--432},
  url = {https://digitalassets.lib.berkeley.edu/math/ucb/text/mathₛ1ₐrticle-21.pdf},
  date-added = {2022-04-30 11:52:57 -0400},
  date-modified = {2022-04-30 11:55:11 -0400},
  organization = {{University of California Press}},
  keywords = {diffusion processes}
}

@article{fenk.a:1980,
  title = {Konstanz Im Kurzzeitgedächtnis - Konstanz Im Sprachlichen Informationsfluß?},
  author = {Fenk, August and Fenk-Oczlon, Gertraud},
  date = {1980-01},
  journaltitle = {Zeitschrift für experimentelle und angewandte Psychologie},
  volume = {27},
  number = {3},
  pages = {400--414},
  date-added = {2021-10-26 14:23:04 -0400},
  date-modified = {2021-10-27 09:20:37 -0400}
}

@inproceedings{fernandez-monsalve.i:2012,
  title = {Lexical Surprisal as a General Predictor of Reading Time},
  booktitle = {Proceedings of the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fernandez Monsalve, Irene and Frank, Stefan L. and Vigliocco, Gabriella},
  date = {2012-04},
  pages = {398--408},
  publisher = {{Association for Computational Linguistics}},
  location = {{Avignon, France}},
  url = {https://aclanthology.org/E12-1041},
  date-added = {2021-11-29 10:29:43 -0500},
  date-modified = {2021-11-29 10:29:44 -0500}
}

@article{ferreira.f:2001,
  title = {Misinterpretations of {{Garden-Path Sentences}}: {{Implications}} for {{Models}} of {{Sentence Processing}} and {{Reanalysis}}},
  shorttitle = {Misinterpretations of {{Garden-Path Sentences}}},
  author = {Ferreira, Fernanda and Christianson, Kiel and Hollingworth, Andrew},
  date = {2001-01-01},
  journaltitle = {Journal of Psycholinguistic Research},
  shortjournal = {J Psycholinguist Res},
  volume = {30},
  number = {1},
  pages = {3--20},
  issn = {1573-6555},
  doi = {10.1023/A:1005290706460},
  url = {https://doi.org/10.1023/A:1005290706460},
  urldate = {2022-06-11},
  abstract = {Theories of sentence comprehension have addressed both initial parsing processes and mechanisms responsible for reanalysis. Three experiments are summarized that were designed to investigate the reanalysis and interpretation of relatively difficult garden-path sentences (e.g., While Anna dressed the baby spit up on the bed). After reading such sentences, participants correctly believed that the baby spit up on the bed; however, they often confidently, yet incorrectly, believed that Anna dressed the baby. These results demonstrate that garden-path reanalysis is not an all-or-nothing process and that thematic roles initially assigned for the subordinate clause verb are not consistently revised. The implications of the partial reanalysis phenomenon for Fodor and Inoue's (1998) model of reanalysis and sentence processing are discussed. In addition, we discuss the possibility that language processing often creates “good enough” structures rather than ideal structures.},
  langid = {english},
  keywords = {parsing,reanalysis,semantics,syntactic ambiguity},
  file = {/Users/j/Zotero/storage/AQCBQ96B/Ferreira et al. - 2001 - Misinterpretations of Garden-Path Sentences Impli.pdf}
}

@article{ferreira.f:2002,
  title = {Good-{{Enough Representations}} in {{Language Comprehension}}},
  author = {Ferreira, Fernanda and Bailey, Karl G.D. and Ferraro, Vittoria},
  date = {2002-02-01},
  journaltitle = {Current Directions in Psychological Science},
  shortjournal = {Curr Dir Psychol Sci},
  volume = {11},
  number = {1},
  pages = {11--15},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1111/1467-8721.00158},
  url = {https://doi.org/10.1111/1467-8721.00158},
  urldate = {2022-06-11},
  abstract = {People comprehend utterances rapidly and without conscious effort. Traditional theories assume that sentence processing is algorithmic and that meaning is derived compositionally. The language processor is believed to generate representations of the linguistic input that are complete, detailed, and accurate. However, recent findings challenge these assumptions. Investigations of the misinterpretation of both garden-path and passive sentences have yielded support for the idea that the meaning people obtain for a sentence is often not a reflection of its true content. Moreover, incorrect interpretations may persist even after syntactic reanalysis has taken place. Our good-enough approach to language comprehension holds that language processing is sometimes only partial and that semantic representations are often incomplete. Future work will elucidate the conditions under which sentence processing is simply good enough.},
  langid = {english},
  keywords = {language comprehension,linguistic ambiguity,satisficing,syntax},
  file = {/Users/j/Zotero/storage/5D9WINXM/Ferreira et al. - 2002 - Good-Enough Representations in Language Comprehens.pdf}
}

@incollection{ferreira.f:2016,
  title = {Chapter {{Six}} - {{Prediction}}, {{Information Structure}}, and {{Good-Enough Language Processing}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Ferreira, Fernanda and Lowder, Matthew W.},
  editor = {Ross, Brian H.},
  date = {2016-01-01},
  volume = {65},
  pages = {217--247},
  publisher = {{Academic Press}},
  doi = {10.1016/bs.plm.2016.04.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0079742116300020},
  urldate = {2022-06-14},
  abstract = {The good-enough language processing approach emphasizes people's tendency to generate superficial and even inaccurate interpretations of sentences. At the same time, a number of researchers have argued that prediction plays a key role in comprehension, allowing people to anticipate features of the input and even specific upcoming words based on sentential constraint. In this chapter, we review evidence from our lab supporting both approaches, even though at least superficially these two perspectives seem incompatible. We then argue that what allows us to link good-enough processing and prediction is the concept of information structure, which states that sentences are organized to convey both given or presupposed information, and new or focused information. Our fundamental proposal is that given or presupposed information is processed in a good-enough manner, while new or focused information is the target of the comprehender's prediction efforts. The result is a theory that brings together three different literatures that have been treated almost entirely independently, and which can be evaluated using a combination of behavioral, computational, and neural methods.},
  langid = {english},
  keywords = {Comprehension,Good-enough processing,Information structure,Language processing,Prediction}
}

@article{ferrer-i-cancho.r:2015,
  title = {The Placement of the Head That Minimizes Online Memory: A Complex Systems Approach},
  author = {Ferrer-i-Cancho, Ramon},
  date = {2015},
  journaltitle = {Language Dynamics and Change},
  volume = {5},
  number = {1},
  pages = {114--137},
  publisher = {{Brill}},
  date-added = {2019-05-15 00:10:35 -0400},
  date-modified = {2019-06-17 21:57:08 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,memory}
}

@article{floridi.l:2020,
  title = {{{GPT-3}}: {{Its}} Nature, Scope, Limits, and Consequences},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  date = {2020},
  journaltitle = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s11023-020-09548-1},
  url = {https://doi.org/10.1007%2Fs11023-020-09548-1},
  bdsk-url-2 = {https://doi.org/10.1007/s11023-020-09548-1},
  date-added = {2021-06-05 22:29:51 -0400},
  date-modified = {2021-06-05 22:29:53 -0400}
}

@book{folland.g:1999,
  title = {Real {{Analysis}}: {{Modern Techniques}} and {{Their Applications}}, 2nd {{Edition}} | {{Wiley}}},
  shorttitle = {Real {{Analysis}}},
  author = {Folland, Gerald B.},
  date = {1999-04},
  series = {Pure and {{Applied Mathematics}}: {{A Wiley Series}} of {{Texts}}, {{Monographs}} and {{Tracts}}},
  edition = {2nd Edition},
  url = {https://www.wiley.com/en-us/Real+Analysis%3A+Modern+Techniques+and+Their+Applications%2C+2nd+Edition-p-9780471317166},
  urldate = {2022-06-23},
  abstract = {An in-depth look at real analysis and its applications-now expanded and revised. This new edition of the widely used analysis book continues to cover real analysis in greater detail and at a more advanced level than most books on the subject. Encompassing several subjects that underlie much of modern analysis, the book focuses on measure and integration theory, point set topology, and the basics of functional analysis. It illustrates the use of the general theories and introduces readers to other branches of analysis such as Fourier analysis, distribution theory, and probability theory. This edition is bolstered in content as well as in scope-extending its usefulness to students outside of pure analysis as well as those interested in dynamical systems. The numerous exercises, extensive bibliography, and review chapter on sets and metric spaces make Real Analysis: Modern Techniques and Their Applications, Second Edition invaluable for students in graduate-level analysis courses. New features include: * Revised material on the n-dimensional Lebesgue integral. * An improved proof of Tychonoffs theorem. * Expanded material on Fourier analysis. * A newly written chapter devoted to distributions and differential equations. * Updated material on Hausdorff dimension and fractal dimension.},
  isbn = {978-0-471-31716-6},
  langid = {english},
  pagetotal = {416},
  file = {/Users/j/Zotero/storage/NYLUISAM/folland.g.1999RealAnalysis.djvu}
}

@inproceedings{fossum.v:2012,
  title = {Sequential vs. {{Hierarchical}} Syntactic Models of Human Incremental Sentence Processing},
  booktitle = {Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({{CMCL}} 2012)},
  author = {Fossum, Victoria and Levy, Roger},
  date = {2012-06},
  pages = {61--69},
  publisher = {{Association for Computational Linguistics}},
  location = {{Montréal, Canada}},
  url = {https://aclanthology.org/W12-1706},
  date-added = {2021-11-29 10:02:26 -0500},
  date-modified = {2021-11-29 10:02:27 -0500}
}

@inproceedings{foster.a:2019,
  title = {Variational Bayesian Optimal Experimental Design},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah D.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {14036--14047},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d55cbf210f175f4a37916eafe6c04f0d-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/FosterJBHTRG19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{fox.c:2012,
  title = {A Tutorial on Variational {{Bayesian}} Inference},
  author = {Fox, Charles W. and Roberts, Stephen J.},
  date = {2012-08-01},
  journaltitle = {Artificial Intelligence Review},
  shortjournal = {Artif Intell Rev},
  volume = {38},
  number = {2},
  pages = {85--95},
  issn = {1573-7462},
  doi = {10.1007/s10462-011-9236-8},
  url = {https://doi.org/10.1007/s10462-011-9236-8},
  urldate = {2022-06-27},
  abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  langid = {english},
  keywords = {Mean-field,Tutorial,Variational Bayes},
  file = {/Users/j/Zotero/storage/W2PZQNV6/Fox and Roberts - 2012 - A tutorial on variational Bayesian inference.pdf}
}

@article{fox.d:2003,
  title = {Adapting the Sample Size in Particle Filters through {{KLD-Sampling}}},
  author = {Fox, Dieter},
  date = {2003-12},
  journaltitle = {The International Journal of Robotics Research},
  volume = {22},
  number = {12},
  pages = {985--1003},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0278364903022012001},
  url = {https://doi.org/10.1177%2F0278364903022012001},
  bdsk-url-2 = {https://doi.org/10.1177/0278364903022012001},
  date-added = {2022-05-05 09:45:16 -0400},
  date-modified = {2022-05-05 09:46:04 -0400},
  keywords = {bayes filtering,KLD-sampling,particle filtering}
}

@article{frank.s:2009cogsci,
  title = {Surprisal-Based Comparison between a Symbolic and a Connectionist Model of Sentence Processing},
  author = {Frank, Stefan L.},
  date = {2009},
  journaltitle = {Proceedings of the 31st Annual Meeting of the Cognitive Science Society},
  url = {https://escholarship.org/uc/item/02v5m1hf},
  urldate = {2022-10-12},
  langid = {english},
  file = {/Users/j/Zotero/storage/UZHHW2J6/Frank (2009) Surprisal-based comparison between a symbolic and .pdf}
}

@article{frank.s:2011,
  title = {Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author = {Frank, Stefan L. and Bod, Rens},
  date = {2011-05},
  journaltitle = {Psychological Science},
  volume = {22},
  number = {6},
  pages = {829--834},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0956797611409589},
  url = {https://doi.org/10.1177%2F0956797611409589},
  bdsk-url-2 = {https://doi.org/10.1177/0956797611409589},
  date-added = {2021-11-29 10:04:00 -0500},
  date-modified = {2021-11-29 10:04:02 -0500}
}

@article{frank.s:2013corpus,
  title = {Reading Time Data for Evaluating Broad-Coverage Models of {{English}} Sentence Processing},
  author = {Frank, Stefan L. and Monsalve, Irene Fernandez and Thompson, Robin L. and Vigliocco, Gabriella},
  date = {2013-02},
  journaltitle = {Behavior Research Methods},
  volume = {45},
  number = {4},
  pages = {1182--1190},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-012-0313-y},
  url = {https://doi.org/10.3758%2Fs13428-012-0313-y},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-012-0313-y},
  date-added = {2021-09-16 13:31:43 -0400},
  date-modified = {2021-12-15 09:07:19 -0500}
}

@inproceedings{frank.s:2013surp,
  title = {Word Surprisal Predicts {{N400}} Amplitude during Reading},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  date = {2013},
  pages = {878--883},
  publisher = {{Association for Computational Linguistics}},
  location = {{Sofia, Bulgaria}},
  url = {https://www.aclweb.org/anthology/P13-2152},
  date-added = {2021-12-15 09:07:40 -0500},
  date-modified = {2021-12-15 09:07:44 -0500}
}

@article{frazier.l:1978,
  title = {The Sausage Machine: {{A}} New Two-Stage Parsing Model},
  shorttitle = {The Sausage Machine},
  author = {Frazier, Lyn and Fodor, Janet Dean},
  date = {1978-01-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {6},
  number = {4},
  pages = {291--325},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(78)90002-1},
  url = {https://www.sciencedirect.com/science/article/pii/0010027778900021},
  urldate = {2022-06-12},
  abstract = {It is proposed that the human sentence parsing device assigns phrase structure to word strings in two steps. The first stage parser assigns lexical and phrasal nodes to substrings of roughly six words. The second stage parser then adds higher nodes to link these phrasal packages together into a complete phrase marker. This model of the parser is compared with ATN models, and with the two-stage models of Kimball (1973) and Fodor, Bever and Garrett (1974). Our assumption that the units which are shunted from the first stage to the second stage are defined by their length, rather than by their syntactic type, explains the effects of constituent length on perceptual complexity in center embedded sentences and in sentences of the kind that fall under Kimball's principle of Right Association. The particular division of labor between the two parsing units allows us to explain, without appeal to any ad hoc parsing strategies, why the parser makes certain ‘shortsighted’ errors even though, in general, it is able to make intelligent use of all the information that is available to it. Résumé Dans cet article on propose un mécanisme de segmentation des énoncés qui assigne en deux étapes une structure syntagmatique aux suites de mots. La première méthode de segmentation assigne des noeuds lexicaux et syntagmatiques à des suites de 6 mots environ. La seconde ajoute des noeuds à un niveau supérieur pour lier ces blocs syntagmatiques et obtenir ainsi un marqueur syntagmatique complet. Ce modèle de segmentation est comparé d'une part aux modèles ATN et d'autre part au modèle en deux étapes de Kimball (1973) et Fodor, Bever et Garrett (1974). Nous pensons que les unités qui passent du ler au 2è niveau sont caractérisées par leur longueur plutôt que par leur forme syntaxique. Ceci expliquerait les effects de la longueur des constituants sur la complexité perceptuelle des phrases enclassées et des phrases du type de celles qui tombent sous le principe de l'association à droite de Kimball. La distinction spécifique du travail entre les deux unités de segmentation permet d'expliquer, sans faire intervenir des stratégies ad hoc, certaines erreurs de segmentation même si, en général, il est possible de faire un usage intelligent de toutes les informations disponibles.},
  langid = {english}
}

@article{frazier.l:1982,
  title = {Making and Correcting Errors during Sentence Comprehension: {{Eye}} Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  date = {1982-04},
  journaltitle = {Cognitive Psychology},
  volume = {14},
  number = {2},
  pages = {178--210},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0285(82)90008-1},
  url = {https://doi.org/10.1016%2F0010-0285%2882%2990008-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(82)90008-1},
  date-added = {2022-05-06 15:36:26 -0400},
  date-modified = {2022-05-06 15:36:46 -0400},
  keywords = {eye-tracking,regressions}
}

@article{frazier.l:1987,
  title = {Syntactic Processing: {{Evidence}} from Dutch},
  shorttitle = {Syntactic Processing},
  author = {Frazier, Lyn},
  date = {1987-12-01},
  journaltitle = {Natural Language \& Linguistic Theory},
  shortjournal = {Nat Lang Linguist Theory},
  volume = {5},
  number = {4},
  pages = {519--559},
  issn = {1573-0859},
  doi = {10.1007/BF00138988},
  url = {https://doi.org/10.1007/BF00138988},
  urldate = {2022-10-13},
  langid = {english},
  keywords = {Artificial Intelligence,eager processing,Syntactic Processing},
  file = {/Users/j/Zotero/storage/SYEKGJMU/Frazier (1987) Syntactic processing Evidence from dutch.pdf}
}

@inproceedings{freer.f:2010,
  title = {When Are Probabilistic Programs Probably Computationally Tractable?},
  booktitle = {{{NIPS}} Workshop on {{Monte Carlo}} Methods for Modern Applications},
  author = {Freer, Cameron and Mansinghka, Vikash K. and Roy, Daniel},
  date = {2010},
  url = {http://citeseerx.ist.psu.edu/viewdoc/versions?doi=10.1.1.187.1495},
  date-added = {2021-03-09 22:52:16 -0500},
  date-modified = {2022-04-14 10:57:16 -0400},
  keywords = {probabilistic programming,rejection sampling,sampling,surprisal}
}

@article{friston.k:2012,
  title = {A {{Free Energy Principle}} for {{Biological Systems}}},
  author = {Friston, Karl},
  date = {2012-11},
  journaltitle = {Entropy},
  volume = {14},
  number = {11},
  pages = {2100--2121},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e14112100},
  url = {https://www.mdpi.com/1099-4300/14/11/2100},
  urldate = {2022-06-10},
  abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
  issue = {11},
  langid = {english},
  keywords = {Bayesian,ergodicity,free energy,random dynamical system,self-organization,surprise},
  file = {/Users/j/Zotero/storage/929TWEGN/Karl - 2012 - A Free Energy Principle for Biological Systems.pdf}
}

@article{fromkin.v:1971,
  title = {The {{Non-Anomalous Nature}} of {{Anomalous Utterances}}},
  author = {Fromkin, Victoria A.},
  date = {1971},
  journaltitle = {Language},
  volume = {47},
  number = {1},
  eprint = {412187},
  eprinttype = {jstor},
  pages = {27--52},
  publisher = {{Linguistic Society of America}},
  issn = {0097-8507},
  doi = {10.2307/412187},
  abstract = {An analysis of speech errors provides evidence for the psychological reality of theoretical linguistic concepts such as distinctive features, morpheme structure constraints, abstract underlying forms, phonological rules, and syntactic and semantic features. Furthermore, such errors reveal that linguistic performance is highly rule-governed, and that in many cases it is grammatical rules which constrain or monitor actual speech production. While a model of linguistic competence is independent of temporal constraints, a model of linguistic performance must provide information as to the sequencing of events in real time. To explain the occurrence of particular kinds of errors, a specific ordering of rules is posited, which ordering may or may not coincide with the organization of a grammar.},
  keywords = {speech errors}
}

@inproceedings{futrell.r:2017,
  title = {Noisy-Context Surprisal as a Human Sentence Processing Cost Model},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Volume}} 1, Long Papers},
  author = {Futrell, Richard and Levy, Roger},
  date = {2017},
  pages = {688--698},
  publisher = {{Association for Computational Linguistics}},
  location = {{Valencia, Spain}},
  url = {https://www.aclweb.org/anthology/E17-1065}
}

@thesis{futrell.r:2017phd,
  title = {Memory and Locality in Natural Language},
  author = {Futrell, Richard},
  date = {2017},
  institution = {{Massachusetts Institute of Technology / Massachusetts Institute of Technology. Department of Brain and Cognitive Sciences}},
  url = {http://hdl.handle.net/1721.1/114075},
  abstract = {I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning.},
  date-added = {2022-04-11 23:46:38 -0400},
  date-modified = {2022-04-11 23:50:11 -0400},
  keywords = {information theory,noisy channel coding},
  file = {/Users/j/Zotero/storage/63RJV3GR/Futrell - 2017 - Memory and locality in natural language.pdf}
}

@inproceedings{futrell.r:2018,
  title = {The {{Natural Stories}} Corpus},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven and Fedorenko, Evelina},
  date = {2018},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Miyazaki, Japan}},
  url = {https://www.aclweb.org/anthology/L18-1012},
  date-modified = {2021-12-02 00:12:56 -0500}
}

@inproceedings{futrell.r:2019,
  title = {Syntactic Dependencies Correspond to Word Pairs with High Mutual Information},
  booktitle = {Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, {{SyntaxFest}} 2019)},
  author = {Futrell, Richard and Qian, Peng and Gibson, Edward and Fedorenko, Evelina and Blank, Idan},
  date = {2019},
  pages = {3--13},
  publisher = {{Association for Computational Linguistics}},
  location = {{Paris, France}},
  doi = {10.18653/v1/W19-7703},
  url = {https://www.aclweb.org/anthology/W19-7703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-7703}
}

@article{futrell.r:2020,
  title = {Lossy-Context Surprisal: {{An}} Information-Theoretic Model of Memory Effects in Sentence Processing},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger},
  date = {2020},
  journaltitle = {Cognitive Science},
  volume = {44},
  number = {3},
  pages = {e12814},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/cogs.12814},
  url = {https://doi.org/10.1111/cogs.12814},
  date-added = {2020-03-27 16:35:14 -0400},
  date-modified = {2022-04-20 13:48:30 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lossy context surprisal,mutual information,processing},
  file = {/Users/j/Zotero/storage/F4RL26FY/Futrell et al. - 2020 - Lossy-context surprisal An information-theoretic .pdf}
}

@article{futrell.r:2021,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  date = {2021-03-01},
  journaltitle = {Language Resources and Evaluation},
  shortjournal = {Lang Resources \& Evaluation},
  volume = {55},
  number = {1},
  pages = {63--77},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09503-7},
  url = {https://doi.org/10.1007/s10579-020-09503-7},
  urldate = {2022-06-09},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  keywords = {cognitive modeling,psycholinguistics,reading time,self-paced reading time},
  file = {/Users/j/Zotero/storage/3GDJYJRE/Futrell et al. - 2021 - The Natural Stories corpus a reading-time corpus .pdf}
}

@incollection{gamut.l:1991,
  title = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  booktitle = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  author = {Gamut, L. T. F.},
  date = {1991},
  publisher = {{University of Chicago Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@misc{gao.l:2020ThePile,
  title = {The Pile: {{An 800GB}} Dataset of Diverse Text for Language Modeling},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  date = {2020},
  eprint = {2101.00027},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-11-30 10:16:24 -0500},
  date-modified = {2021-11-30 10:19:58 -0500}
}

@misc{gao.l:2021blogGPT3sizes,
  title = {On the Sizes of {{OpenAI API}} Models},
  author = {Gao, Leo},
  date = {2021-05},
  url = {https://blog.eleuther.ai/gpt3-model-sizes/},
  date-added = {2021-12-13 19:27:28 -0500},
  date-modified = {2021-12-13 19:29:08 -0500},
  howpublished = {Blog post}
}

@inproceedings{gauthier.j:2020syntaxgym,
  title = {{{SyntaxGym}}: {{An}} Online Platform for Targeted Evaluation of Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: {{System}} Demonstrations},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  date = {2020},
  pages = {70--76},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-demos.10},
  url = {https://www.aclweb.org/anthology/2020.acl-demos.10},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-demos.10}
}

@incollection{gazdar.g:1985,
  title = {Generalized Phrase Structure Grammar},
  booktitle = {Generalized Phrase Structure Grammar},
  author = {Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K. and Sag, Ivan A.},
  date = {1985},
  publisher = {{Harvard University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:14 -0400},
  keywords = {GPSG}
}

@article{geman.s:1984,
  title = {Stochastic Relaxation, Gibbs Distributions, and the Bayesian Restoration of Images},
  author = {Geman, S. and Geman, D.},
  date = {1984},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  doi = {10.1109/TPAMI.1984.4767596},
  url = {https://doi.org/10.1109/TPAMI.1984.4767596},
  date-added = {2021-03-17 15:08:02 -0400},
  date-modified = {2021-03-17 15:09:10 -0400},
  keywords = {bayesian,gibbs sampling,markov random fields,stochastic processes}
}

@article{geman.s:1984a,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  date = {1984-11},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (“annealing”), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel “relaxation” algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  eventtitle = {{{IEEE Transactions}} on {{Pattern Analysis}} and {{Machine Intelligence}}},
  keywords = {Additive noise,Annealing,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,image restoration,Image restoration,line process,MAP estimate,Markov random field,Markov random fields,relaxation,scene modeling,spatial degradation,Stochastic processes,Temperature distribution},
  file = {/Users/j/Zotero/storage/X8ULM23N/Geman and Geman - 1984 - Stochastic Relaxation, Gibbs Distributions, and th.pdf}
}

@unpublished{genewein.t:2013,
  title = {Abstraction in Decision-Makers with Limited Information Processing Capabilities},
  author = {Genewein, Tim and Braun, Daniel A.},
  date = {2013-12-19},
  number = {arXiv:1312.4353},
  eprint = {1312.4353},
  eprinttype = {arxiv},
  primaryclass = {cs, math, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1312.4353},
  url = {http://arxiv.org/abs/1312.4353},
  urldate = {2022-06-09},
  abstract = {A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Statistics - Machine Learning},
  annotation = {note: Presented at the NIPS 2013 Workshop on Planning with Information Constraints},
  file = {/Users/j/Zotero/storage/EY79F5ND/Genewein and Braun - 2013 - Abstraction in decision-makers with limited inform.pdf}
}

@article{genewein.t:2015,
  title = {Bounded {{Rationality}}, {{Abstraction}}, and {{Hierarchical Decision-Making}}: {{An Information-Theoretic Optimality Principle}}},
  shorttitle = {Bounded {{Rationality}}, {{Abstraction}}, and {{Hierarchical Decision-Making}}},
  author = {Genewein, Tim and Leibfried, Felix and Grau-Moya, Jordi and Braun, Daniel Alexander},
  date = {2015},
  journaltitle = {Frontiers in Robotics and AI},
  volume = {2},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2015.00027},
  urldate = {2022-06-07},
  abstract = {Abstraction and hierarchical information processing are hallmarks of human and animal intelligence underlying the unrivaled flexibility of behavior in biological systems. Achieving such flexibility in artificial systems is challenging, even with more and more computational power. Here, we investigate the hypothesis that abstraction and hierarchical information processing might in fact be the consequence of limitations in information-processing power. In particular, we study an information-theoretic framework of bounded rational decision-making that trades off utility maximization against information-processing costs. We apply the basic principle of this framework to perception-action systems with multiple information-processing nodes and derive bounded-optimal solutions. We show how the formation of abstractions and decision-making hierarchies depends on information-processing costs. We illustrate the theoretical ideas with example simulations and conclude by formalizing a mathematically unifying optimization principle that could potentially be extended to more complex systems.},
  keywords = {bounded rationality,information theory,lossy compression,rate-distortion theory},
  file = {/Users/j/Zotero/storage/AS9HLP5H/Genewein et al. - 2015 - Bounded Rationality, Abstraction, and Hierarchical.pdf;/Users/j/Zotero/storage/NYI26A99/Genewein et al. - 2015 - Bounded Rationality, Abstraction, and Hierarchical.pdf}
}

@inproceedings{georgi.d:2011,
  title = {Deriving the Distribution of Person Portmanteaux by Relativized Probing},
  booktitle = {Proceedings of the North-Eastern Linguistic Society},
  author = {Georgi, Doreen},
  date = {2011},
  volume = {42},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:19 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@incollection{gershman.s:2012,
  title = {Perception, Action and Utility: {{The}} Tangled Skein},
  booktitle = {Principles of Brain Dynamics: {{Global}} State Interactions},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  date = {2012},
  series = {Computational {{Neuroscience Series}}},
  pages = {293--312},
  publisher = {{MIT Press}},
  doi = {10.7551/mitpress/9108.003.0015},
  url = {https://doi.org/10.7551/mitpress/9108.003.0015},
  isbn = {978-0-262-01764-0},
  file = {/Users/j/Zotero/storage/B9IW77BL/Gershman and Daw - 2012 - Perception, action and utility The tangled skein.pdf}
}

@article{gershman.s:2019,
  title = {What Does the Free Energy Principle Tell Us about the Brain?},
  author = {Gershman, Samuel J.},
  date = {2019-01-23},
  doi = {10.48550/arXiv.1901.07945},
  url = {https://arxiv.org/abs/1901.07945v5},
  urldate = {2022-07-27},
  abstract = {The free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
  langid = {english},
  keywords = {Bayesian brain,free energy principle,inference,predictive coding},
  file = {/Users/j/Zotero/storage/JP945JX9/Gershman - 2019 - What does the free energy principle tell us about .pdf}
}

@thesis{gibson.e:1991phd,
  type = {phdthesis},
  title = {A Computational Theory of Human Linguistic Processing: {{Memory}} Limitations and Processing Breakdown},
  shorttitle = {A Computational Theory of Human Linguistic Processing},
  author = {Gibson, Edward},
  date = {1991},
  institution = {{Carnegie Mellon University}},
  location = {{United States -- Pennsylvania}},
  url = {https://www.proquest.com/docview/303922361/abstract/D91794B7808B4DE7PQ/1},
  urldate = {2022-06-14},
  abstract = {This thesis gives a theory of sentence comprehension that attempts to explain a number of linguistic performance effects, including garden-path effects, preferred readings for ambiguous input and processing overload effects. It is hypothesized that the human parser heuristically determines its options based upon evaluation of possible representations with respect to lexical, syntactic, semantic and pragmatic properties, each of which is associated with a weight. Processing overload effects are explained by the assumption of the existence of a maximum load corresponding to the limited capacity of short term memory: a structure becomes unacceptable at a particular parse state if the combination of the processing weights associated with its properties at that state is greater than the available capacity. Furthermore, it is assumed that the language processor is an automatic device that maintains only the best of the set of all compatible representations for an input string. This thesis assumes a formulation of representational evaluation within a parallel framework: one structure is preferred over another if the processing load associated with the first structure is markedly lower than the processing load associated with the second. Thus a garden path effect results if the unpreferred structure is necessary for a successful parse of the input. Four properties of linguistic representations are presented within this framework. The first two--the Properties of Thematic Reception and Transmission--derivable from the \$\textbackslash theta\$-Criterion from Government-Binding (GB) Theory (Chomsky (1981)); the third--the Property of Lexical Requirement--derivable from the Projection Principle of GB Theory; and the fourth--the Property of Recency Preference--prefers local attachments over more distant attachments (cf. Kimball (1973), Frazier (1979)). This thesis shows how these properties interact to give a partially unified theory of many performance effects.},
  isbn = {9798617014688},
  langid = {english},
  pagetotal = {318},
  keywords = {Applied sciences,computational linguistics,Language,literature and linguistics,memory,parsing,processing,psycholinguistics},
  file = {/Users/j/Zotero/storage/YBKJ9FUN/Gibson - A computational theory of human linguistic process.pdf}
}

@article{gibson.e:1998,
  title = {Linguistic Complexity: Locality of Syntactic Dependencies},
  author = {Gibson, Edward},
  date = {1998-08-01},
  journaltitle = {Cognition},
  shortjournal = {Cognition},
  volume = {68},
  number = {1},
  pages = {1--76},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00034-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027798000341},
  abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory – the Syntactic Prediction Locality Theory (SPLT) – has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
  keywords = {Computational resources,Linguistic complexity,Sentence processing,Syntactic dependency}
}

@article{gibson.e:1999,
  title = {Memory Limitations and Structural Forgetting: {{The}} Perception of Complex Ungrammatical Sentences as Grammatical},
  author = {Gibson, Edward and Thomas, James},
  date = {1999-06},
  journaltitle = {Language and Cognitive Processes},
  volume = {14},
  number = {3},
  pages = {225--248},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/016909699386293},
  url = {https://doi.org/10.1080%2F016909699386293},
  bdsk-url-2 = {https://doi.org/10.1080/016909699386293},
  date-added = {2022-04-19 22:36:47 -0400},
  date-modified = {2022-04-19 22:36:48 -0400}
}

@incollection{gibson.e:2000,
  title = {The Dependency Locality Theory: {{A}} Distance-Based Theory of Linguistic Complexity},
  shorttitle = {The Dependency Locality Theory},
  booktitle = {Image, Language, Brain:  {{Papers}} from the First Mind Articulation Project Symposium},
  author = {Gibson, Edward},
  editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  date = {2000},
  pages = {94--126},
  publisher = {{The MIT Press}},
  location = {{Cambridge, MA, US}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.592.5833&rank=1&q=The%20dependency%20locality%20theory:%20A%20distance-based%20theory%20of%20linguistic%20complexity.&osm=&ossid=},
  abstract = {Discusses the dependency locality theory (DLT) of human computational resources in sentence parsing that relies on 2 kinds of resource use. One of the key ideas underlying the theory is locality, such that the cost of integrating 2 elements (such as a head and a dependent, or a pronominal referent to its antecedent) depends on the distance between the 2. The remainder of the chapter reviews some empirical observations regarding the proceeding difficulty associated with unambiguous structures. It is shown that the DLT accounts for the complexity effect in these structures as well as preferences in ambiguous structures.},
  isbn = {978-0-262-13371-5},
  keywords = {Linguistics,Psychological Theories,Sentence Comprehension,Sentence Structure,Syntax}
}

@article{gibson.e:2013,
  title = {A Noisy-Channel Account of Crosslinguistic Word-Order Variation},
  author = {Gibson, Edward and Piantadosi, Steven T. and Brink, Kimberly and Bergen, Leon and Lim, Eunice and Saxe, Rebecca},
  date = {2013-05},
  journaltitle = {Psychological Science},
  volume = {24},
  number = {7},
  pages = {1079--1088},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0956797612463705},
  url = {https://doi.org/10.1177%2F0956797612463705},
  bdsk-url-2 = {https://doi.org/10.1177/0956797612463705},
  date-added = {2022-04-19 22:26:57 -0400},
  date-modified = {2022-05-02 14:46:14 -0400},
  keywords = {noisy channel coding}
}

@article{gibson.e:2013pnas,
  title = {Rational Integration of Noisy Evidence and Prior Semantic Expectations in Sentence Interpretation},
  author = {Gibson, Edward and Bergen, Leon and Piantadosi, Steven T.},
  date = {2013-05},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {20},
  pages = {8051--8056},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1216438110},
  url = {https://doi.org/10.1073%2Fpnas.1216438110},
  keywords = {noisy channel coding},
  file = {/Users/j/Zotero/storage/E6DQDNMG/Gibson et al. - 2013 - Rational integration of noisy evidence and prior s.pdf}
}

@inproceedings{gildea.d:2007,
  title = {Optimizing Grammars for Minimum Dependency Length},
  booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  author = {Gildea, Daniel and Temperley, David},
  date = {2007},
  pages = {184--191},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/P07-1024}
}

@article{gildea.d:2010,
  title = {Do Grammars Minimize Dependency Length?},
  author = {Gildea, Daniel and Temperley, David},
  date = {2010},
  journaltitle = {Cognitive Science},
  volume = {34},
  number = {2},
  pages = {286--310},
  publisher = {{Wiley Online Library}},
  date-added = {2019-05-14 23:50:31 -0400},
  date-modified = {2019-06-17 21:56:52 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,projectivity}
}

@article{gleitman.l:2019,
  title = {The Impossibility of Language Acquisition (and How They Do It)},
  author = {Gleitman, Lila R. and Liberman, Mark Y. and McLemore, Cynthia A. and Partee, Barbara H.},
  date = {2019},
  journaltitle = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {1--24},
  publisher = {{Annual Reviews}},
  doi = {10.1146/annurev-linguistics-011718-011640},
  url = {https://doi.org/10.1146%2Fannurev-linguistics-011718-011640},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011640},
  date-added = {2021-08-17 09:52:01 -0400},
  date-modified = {2021-08-17 09:52:03 -0400}
}

@misc{godfrey.j:1993switchboard,
  title = {Switchboard-1 Release 2},
  author = {Godfrey, John J. and Holliman, Edward},
  date = {1993},
  number = {LDC97S62},
  publisher = {{Linguistic Data Consortium}},
  doi = {10.35111/SW3H-RW02},
  url = {https://catalog.ldc.upenn.edu/LDC97S62},
  bdsk-url-2 = {https://doi.org/10.35111/SW3H-RW02},
  date-added = {2022-05-06 14:21:03 -0400},
  date-modified = {2022-05-06 14:23:45 -0400},
  howpublished = {Web Download},
  keywords = {dataset,speech errors}
}

@inproceedings{gogate.v:2007,
  title = {{{SampleSearch}}: {{A}} Scheme That Searches for Consistent Samples},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  author = {Gogate, Vibhav and Dechter, Rina},
  editor = {Meila, Marina and Shen, Xiaotong},
  date = {2007-03-21/2007-03-24},
  series = {Proceedings of Machine Learning Research},
  volume = {2},
  pages = {147--154},
  publisher = {{PMLR}},
  location = {{San Juan, Puerto Rico}},
  url = {https://proceedings.mlr.press/v2/gogate07a.html},
  abstract = {Sampling from belief networks which have a substantial number of zero probabilities is problematic. MCMC algorithms like Gibbs sampling do not converge and importance sampling schemes generate many zero weight samples that are rejected, yielding an inefficient sampling process (the rejection problem). In this paper, we propose to augment importance sampling with systematic constraint-satisfaction search in order to overcome the rejection problem. The resulting SampleSearch scheme can be made unbiased by using a computationally expensive weighting scheme. To overcome this an approximation is proposed such that the resulting estimator is asymptotically unbiased. Our empirical results demonstrate the potential of our new scheme.},
  date-added = {2022-05-05 09:35:36 -0400},
  date-modified = {2022-05-05 09:37:37 -0400},
  pdf = {http://proceedings.mlr.press/v2/gogate07a/gogate07a.pdf},
  keywords = {sample search}
}

@book{goldberg.y:2017,
  title = {Neural Network Methods for Natural Language Processing},
  author = {Goldberg, Yoav},
  date = {2017},
  publisher = {{Morgan and Claypool Publishers}},
  date-added = {2019-05-17 21:08:13 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  keywords = {machine learning,neural networks,recurrent neural networks,sequence to sequence models,word embeddings}
}

@misc{goldberg.y:2019,
  title = {Assessing {{BERT}}'s Syntactic Abilities},
  author = {Goldberg, Yoav},
  date = {2019},
  eprint = {1901.05287},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{goldstein.a:2021,
  title = {Thinking Ahead: Spontaneous Prediction in Context as a Keystone of Language in Humans and Machines},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  date = {2021},
  journaltitle = {bioRxiv : the preprint server for biology},
  shortjournal = {bioRxiv},
  eprint = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477.full.pdf},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.02.403477},
  url = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM’s next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM’s contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.Competing Interest StatementThe authors have declared no competing interest.},
  bdsk-url-2 = {https://doi.org/10.1101/2020.12.02.403477},
  date-added = {2021-06-09 15:53:23 -0400},
  date-modified = {2021-06-09 15:53:24 -0400},
  elocation-id = {2020.12.02.403477},
  file = {/Users/j/Zotero/storage/YG4G4JNR/Goldstein et al. - 2021 - Thinking ahead spontaneous prediction in context .pdf}
}

@inproceedings{goodkind.a:2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th Workshop on Cognitive Modeling and Computational Linguistics ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  date = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/w18-0102},
  url = {https://doi.org/10.18653%2Fv1%2Fw18-0102},
  bdsk-url-2 = {https://doi.org/10.18653/v1/w18-0102},
  date-added = {2021-11-29 10:00:16 -0500},
  date-modified = {2021-11-29 10:00:18 -0500}
}

@misc{goodkind.a:2021,
  title = {Local Word Statistics Affect Reading Times Independently of Surprisal},
  author = {Goodkind, Adam and Bicknell, Klinton},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2103.04469},
  url = {https://arxiv.org/abs/2103.04469},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2103.04469},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-09 17:20:07 -0400},
  date-modified = {2022-05-09 17:21:18 -0400},
  keywords = {causal bottleneck},
  file = {/Users/j/Zotero/storage/H3U3VVSI/Goodkind and Bicknell - 2021 - Local word statistics affect reading times indepen.pdf}
}

@article{goodman.j:1999,
  title = {Semiring Parsing},
  author = {Goodman, Joshua},
  date = {1999},
  journaltitle = {Computational Linguistics},
  volume = {25},
  number = {4},
  pages = {573--606},
  url = {https://www.aclweb.org/anthology/J99-4004}
}

@article{goodman.j:2001,
  title = {A Bit of Progress in Language Modeling},
  author = {Goodman, Joshua T.},
  date = {2001},
  journaltitle = {Computer Speech \& Language},
  volume = {15},
  number = {4},
  pages = {403--434},
  issn = {0885-2308},
  doi = {10.1006/csla.2001.0174},
  url = {https://doi.org/10.1006/csla.2001.0174},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-09 15:53:09 -0400},
  opturl = {http://www.sciencedirect.com/science/article/pii/S0885230801901743}
}

@inproceedings{goodwin.e:2020,
  title = {Probing {{Linguistic Systematicity}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  date = {2020-07},
  pages = {1958--1969},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.177},
  url = {https://aclanthology.org/2020.acl-main.177},
  urldate = {2022-05-19},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.},
  eventtitle = {{{ACL}} 2020},
  keywords = {natural language inference,natural logic,systematicity}
}

@inproceedings{gorla.j:2007,
  title = {Two Approaches for Building an Unsupervised Dependency Parser and Their Other Applications},
  booktitle = {{{PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE}}},
  author = {Gorla, Jagadeesh and Goyal, Amit and Sangal, Rajeev},
  date = {2007},
  volume = {22},
  number = {2},
  pages = {1860},
  url = {https://www.aaai.org/Papers/AAAI/2007/AAAI07-305.pdf},
  date-added = {2020-04-23 11:09:55 -0400},
  date-modified = {2020-04-23 11:10:41 -0400},
  organization = {{Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised parsing},
  file = {/Users/j/Zotero/storage/3KCRD7XK/Gorla et al. - 2007 - Two approaches for building an unsupervised depend.pdf}
}

@article{gottwald.s:2020,
  title = {The Two Kinds of Free Energy and the {{Bayesian}} Revolution},
  author = {Gottwald, Sebastian and Braun, Daniel A.},
  date = {2020-12-03},
  journaltitle = {PLOS Computational Biology},
  shortjournal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008420},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008420},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008420},
  urldate = {2022-07-11},
  abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information processing of perception.},
  langid = {english},
  keywords = {Decision making,Entropy,Free energy,Helmholtz free energy,Information processing,Kullback Leibler divergence,Optimization,Probability distribution},
  file = {/Users/j/Zotero/storage/C39AKFN2/Gottwald and Braun - 2020 - The two kinds of free energy and the Bayesian revo.pdf}
}

@misc{goyal.k:2021,
  title = {Exposing the Implicit Energy Networks behind Masked Language Models via {{Metropolis}}–{{Hastings}}},
  author = {Goyal, Kartik and Dyer, Chris and Berg-Kirkpatrick, Taylor},
  date = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2106.02736},
  url = {https://arxiv.org/abs/2106.02736},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.02736},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 12:27:57 -0400},
  date-modified = {2022-04-09 00:57:41 -0400},
  keywords = {energy networks,masked language models,metropolis hastings},
  file = {/Users/j/Zotero/storage/JY2YPR3J/Goyal et al. - 2021 - Exposing the implicit energy networks behind maske.pdf}
}

@article{graf.t:2017,
  title = {Relative Clauses as a Benchmark for {{Minimalist}} Parsing},
  author = {Graf, Thomas and Monette, James and Zhang, Chong},
  date = {2017-07-03},
  journaltitle = {Journal of Language Modelling},
  shortjournal = {JLM},
  volume = {5},
  number = {1},
  issn = {2299-8470, 2299-856X},
  doi = {10.15398/jlm.v5i1.157},
  url = {https://jlm.ipipan.waw.pl/index.php/JLM/article/view/157},
  urldate = {2022-09-30},
  abstract = {Minimalist grammars have been used recently in a series of papers to explain well-known contrasts in human sentence processing in terms of subtle structural differences. These proposals combine a top-down parser with complexity metrics that relate parsing difficulty to memory usage. So far, though, there has been no large-scale exploration of the space of viable metrics. Building on this earlier work, we compare the ability of 1600 metrics to derive several processing effects observed with relative clauses, many of which have been proven difficult to unify. We show that among those 1600 candidates, a few metrics (and only a few) can provide a unified account of all these contrasts. This is a welcome result for two reasons: First, it provides a novel account of extensively studied psycholinguistic data. Second, it significantly limits the number of viable metrics that may be applied to other phenomena, thus reducing theoretical indeterminacy.},
  file = {/Users/j/Zotero/storage/FBS723CR/Graf et al. (2017) Relative clauses as a benchmark for Minimalist par.pdf}
}

@incollection{grice.h:1975,
  title = {Logic and {{Conversation}}},
  booktitle = {Speech {{Acts}}},
  author = {Grice, H. P.},
  editor = {Cole, Peter and Morgan, Jerry L.},
  date = {1975-12-12},
  pages = {41--58},
  publisher = {{BRILL}},
  doi = {10.1163/9789004368811_003},
  url = {https://brill.com/view/book/edcoll/9789004368811/BP000003.xml},
  urldate = {2022-09-30},
  isbn = {978-90-04-36881-1 978-90-04-36857-6}
}

@misc{griffith.v:2012,
  title = {Quantifying Synergistic Mutual Information},
  author = {Griffith, Virgil and Koch, Christof},
  date = {2012},
  eprint = {1205.4265},
  eprinttype = {arxiv},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2020-07-06 08:48:33 -0400},
  date-modified = {2020-07-06 08:49:07 -0400},
  project = {information-compositionality}
}

@article{grodner.d:2003,
  title = {Against Repair-Based Reanalysis in Sentence Comprehension},
  author = {Grodner, Daniel and Gibson, Edward and Argaman, Vered and Babyonyshev, Maria},
  date = {2003},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {141--166},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1022496223965},
  url = {https://doi.org/10.1023%2Fa%3A1022496223965},
  bdsk-url-2 = {https://doi.org/10.1023/a:1022496223965},
  date-added = {2021-03-18 11:28:51 -0400},
  date-modified = {2021-03-18 11:30:16 -0400},
  keywords = {reading time,self-paced reading}
}

@book{grune.d:2008,
  title = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  date = {2008},
  publisher = {{Springer New York}},
  doi = {10.1007/978-0-387-68954-8},
  url = {https://doi.org/10.1007%2F978-0-387-68954-8},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-68954-8},
  date-added = {2022-03-31 10:23:54 -0400},
  date-modified = {2022-03-31 10:24:43 -0400},
  keywords = {book,parsing}
}

@misc{grunwald.p:2004,
  title = {Shannon Information and Kolmogorov Complexity},
  author = {Grunwald, Peter and Vitanyi, Paul},
  date = {2004},
  eprint = {cs/0410002},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  date-added = {2019-09-13 08:19:19 -0400},
  date-modified = {2019-09-13 08:20:10 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity,mutual information,rate-distortion theory,shannon entropy}
}

@misc{grunwald.p:2004a,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  date = {2004},
  eprint = {math/0406077},
  eprinttype = {arxiv},
  url = {https://arxiv.org/pdf/math/0406077.pdf},
  archiveprefix = {arXiv},
  date-added = {2020-02-20 11:40:38 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {minimum description length,mutual information},
  file = {/Users/j/Zotero/storage/Y9C8S7ZU/Grunwald - 2004 - A tutorial introduction to the minimum description.pdf}
}

@inproceedings{gulordava.k:2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  date = {2018},
  pages = {1195--1205},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1108},
  url = {https://www.aclweb.org/anthology/N18-1108},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1108}
}

@article{gutknecht.a:2021,
  title = {Bits and Pieces: Understanding Information Decomposition from Part-Whole Relationships and Formal Logic},
  author = {Gutknecht, A. J. and Wibral, M. and Makkeh, A.},
  date = {2021-07},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {477},
  number = {2251},
  pages = {20210110},
  publisher = {{The Royal Society}},
  doi = {10.1098/rspa.2021.0110},
  url = {https://doi.org/10.1098%2Frspa.2021.0110},
  bdsk-url-2 = {https://doi.org/10.1098/rspa.2021.0110},
  date-added = {2022-04-18 11:14:51 -0400},
  date-modified = {2022-04-18 11:15:12 -0400},
  keywords = {partial information decomposition}
}

@book{hacking.i:2006,
  title = {The Emergence of Probability: {{A}} Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference},
  author = {Hacking, Ian},
  date = {2006},
  edition = {2},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  doi = {10.1017/CBO9780511817557},
  url = {https://doi.org/10.1017/CBO9780511817557},
  date-added = {2021-02-16 15:11:17 -0500},
  date-modified = {2021-02-16 15:11:20 -0500}
}

@unpublished{hahn.m:2019,
  title = {Estimating Predictive Rate-Distortion Curves via Neural Variational Inference},
  author = {Hahn, Michael and Futrell, Richard},
  date = {2019},
  date-added = {2019-06-11 14:15:47 -0400},
  date-modified = {2019-06-17 21:56:11 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory,variational inference}
}

@unpublished{hahn.m:2022,
  title = {Modeling {{Task Effects}} in {{Human Reading}} with {{Neural Attention}}},
  author = {Hahn, Michael and Keller, Frank},
  date = {2022-04-30},
  number = {arXiv:1808.00054},
  eprint = {1808.00054},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.1808.00054},
  url = {http://arxiv.org/abs/1808.00054},
  urldate = {2022-06-07},
  abstract = {Humans read by making a sequence of fixations and saccades. They often skip words, without apparent detriment to understanding. We offer a novel explanation for skipping: readers optimize a tradeoff between performing a language-related task and fixating as few words as possible. We propose a neural architecture that combines an attention module (deciding whether to skip words) and a task module (memorizing the input). We show that our model predicts human skipping behavior, while also modeling reading times well, even though it skips 40\% of the input. A key prediction of our model is that different reading tasks should result in different skipping behaviors. We confirm this prediction in an eye-tracking experiment in which participants answers questions about a text. We are able to capture these experimental results using the our model, replacing the memorization module with a task module that performs neural question answering.},
  archiveprefix = {arXiv},
  keywords = {eye-tracking},
  file = {/Users/j/Zotero/storage/Y8BQBU3C/Hahn and Keller - 2022 - Modeling Task Effects in Human Reading with Neural.pdf}
}

@inproceedings{hale.j:2001,
  title = {A Probabilistic {{Earley}} Parser as a Psycholinguistic Model},
  booktitle = {Second Meeting of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Hale, John T.},
  date = {2001},
  url = {https://www.aclweb.org/anthology/N01-1021},
  date-modified = {2022-04-20 13:49:59 -0400},
  file = {/Users/j/Zotero/storage/JVG9SXWX/Hale - 2001 - A probabilistic Earley parser as a psycholinguisti.pdf}
}

@article{hale.j:2003,
  title = {The Information Conveyed by Words in Sentences},
  author = {Hale, John T.},
  date = {2003},
  journaltitle = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {101--123},
  publisher = {{Springer}},
  doi = {10.1023/A:1022492123056},
  url = {https://doi.org/10.1023/A:1022492123056},
  date-added = {2021-04-08 14:24:37 -0400},
  date-modified = {2022-04-20 13:49:43 -0400},
  keywords = {entropy reduction}
}

@thesis{hale.j:2003phd,
  title = {Grammar, Uncertainty and Sentence Processing},
  author = {Hale, John T.},
  date = {2003},
  institution = {{Johns Hopkins University}},
  location = {{Baltimore, Maryland}},
  url = {https://www.proquest.com/docview/288510490},
  abstract = {Toward a probabilistic theory of human sentence processing, this dissertation proposes a definition of computational work done in the course of analyzing sentences generated by formal grammars. It applies the idea of entropy from information theory to the set of derivations compatible with an initial substring of a sentence. Given a probabilistic grammar, this permits the set of such compatible derivations to be viewed as a random variable, and the change in uncertainty about the outcomes to be calculated. This definition of computational work is examined as a cognitive model of human sentence processing difficulty. To apply the model, a variety of existing syntactic proposals for English sentences are cast as probabilistic Generalized Phrase Structure Grammars (Gazdar et al., 1985) and probabilistic Minimalist Grammars (Stabler, 1997). It is shown that the amount of predicted processing effort in relative clauses correlates with the Accessibility Hierarchy of relativized grammatical relations (Keenan and Comrie, 1977) on a Kaynian (1994) view of relative clause structure. Results from three new on-line sentence reading experiments suggest that while genitivity has the role suggested by the Accessibility Hierarchy, extraction from oblique does not. Evidence is also found for a direct object/indirect object processing asymmetry, which can be derived from the proposed cognitive model under the assumption of a lexicalized probabilistic grammar.},
  date-added = {2022-04-14 15:27:00 -0400},
  date-modified = {2022-04-14 15:31:29 -0400},
  isbn = {978-0-496-55064-7},
  file = {/Users/j/Zotero/storage/KGA6L2HA/Hale - 2003 - Grammar, uncertainty and sentence processing.pdf}
}

@inproceedings{hale.j:2004,
  title = {The Information-Processing Difficulty of Incremental Parsing},
  booktitle = {Proceedings of the Workshop on Incremental Parsing: {{Bringing}} Engineering and Cognition Together},
  author = {Hale, John T.},
  date = {2004-07},
  pages = {58--65},
  publisher = {{Association for Computational Linguistics}},
  location = {{Barcelona, Spain}},
  url = {https://aclanthology.org/W04-0309},
  date-added = {2022-04-14 13:31:29 -0400},
  date-modified = {2022-04-20 13:50:05 -0400}
}

@article{hale.j:2006,
  title = {Uncertainty about the Rest of the Sentence},
  author = {Hale, John T.},
  date = {2006},
  journaltitle = {Cognitive Science},
  volume = {30},
  number = {4},
  pages = {643--672},
  publisher = {{Wiley}},
  doi = {10.1207/s15516709cog0000_64},
  url = {https://doi.org/10.1207%2Fs15516709cog0000₆4},
  bdsk-url-2 = {https://doi.org/10.1207/s15516709cog0000₆4},
  date-added = {2021-03-18 10:37:45 -0400},
  date-modified = {2022-04-20 13:50:10 -0400},
  keywords = {entropy reduction,processing}
}

@book{hale.j:2014,
  title = {Automaton {{Theories}} of {{Human Sentence Comprehension}}},
  author = {Hale, John T.},
  date = {2014-09},
  series = {{{CSLI Studies}} in {{Computational Linguistics}}},
  publisher = {{CSLI Publications, Center for the Study of Language and Information}},
  location = {{Stanford, CA}},
  url = {https://csli.sites.stanford.edu/publications/csli-studies-computational-linguistics/automaton-theories-human-sentence-comprehension},
  urldate = {2022-07-01},
  abstract = {Different kinds of grammars may actually be used in models of perceptual processing. By relating grammars to cognitive architecture, John T. Hale shows step-by-step how incremental parsing works and how specific learning rules might lead to frequency-sensitive preferences. Along the way, this book reconsiders garden-pathing, the parallel/serial distinction and information-theoretical complexity metrics, such as surprisal. This book is a must for cognitive scientists of language.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZKXUQCX8/Hale - 2014 - Automaton Theories of Human Sentence Comprehension.pdf}
}

@inproceedings{hall.d:2014,
  title = {On Substance in Phonology},
  booktitle = {Proceedings of the 2014 Annual Conference of the {{Canadian Linguistic Association}}},
  author = {Hall, Daniel Currie},
  date = {2014},
  date-added = {2019-06-17 08:36:30 -0400},
  date-modified = {2019-06-17 08:37:21 -0400},
  keywords = {substance free phonology}
}

@article{halle.m:1994,
  title = {Some Key Features of {{Distributed Morphology}}},
  author = {Halle, Morris and Marantz, Alec},
  date = {1994},
  journaltitle = {MIT working papers in linguistics},
  volume = {21},
  number = {275},
  pages = {88},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@misc{hamilton.w:2017,
  title = {Representation Learning on Graphs: {{Methods}} and Applications},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  date = {2017},
  eprint = {1709.05584},
  eprinttype = {arxiv},
  primaryclass = {cs.SI},
  archiveprefix = {arXiv},
  date-added = {2021-08-03 10:09:17 -0400},
  date-modified = {2021-08-03 10:10:59 -0400},
  project = {syntactic embedding},
  keywords = {graph embedding}
}

@incollection{harb.b:2005,
  title = {Approximating the Best-Fit Tree under {{L}} p Norms},
  booktitle = {Approximation, Randomization and Combinatorial Optimization. {{Algorithms}} and Techniques},
  author = {Harb, Boulos and Kannan, Sampath and McGregor, Andrew},
  date = {2005},
  pages = {123--133},
  publisher = {{Springer}},
  date-added = {2019-07-17 17:56:30 -0400},
  date-modified = {2019-07-17 17:56:53 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{harley.h:2002,
  title = {Person and Number in Pronouns: {{A}} Feature-Geometric Analysis},
  author = {Harley, Heidi and Ritter, Elizabeth},
  date = {2002},
  journaltitle = {Language},
  volume = {78},
  number = {3},
  pages = {482--526},
  publisher = {{Linguistic Society of America}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:21:59 -0400},
  project = {Icelandic gluttony},
  keywords = {feature geometry,phi features,pronouns}
}

@inproceedings{harremoes.p:2007,
  title = {The Information Bottleneck Revisited or How to Choose a Good Distortion Measure},
  booktitle = {2007 {{IEEE}} International Symposium on Information Theory},
  author = {Harremoës, Peter and Tishby, Naftali},
  date = {2007},
  pages = {566--570},
  doi = {10.1109/ISIT.2007.4557285},
  url = {https://doi.org/10.1109/ISIT.2007.4557285},
  date-added = {2019-05-15 00:02:01 -0400},
  date-modified = {2020-07-22 11:17:35 -0400},
  organization = {{IEEE}},
  project = {syntactic embedding},
  keywords = {information bottleneck,KL divergence},
  file = {/Users/j/Zotero/storage/8XQF8FIW/Harremoës and Tishby - 2007 - The information bottleneck revisited or how to cho.pdf}
}

@inproceedings{hartmann.j:2016,
  title = {Evading Agreement: {{A}} New Perspective on Low Nominative Agreement in {{Icelandic}}},
  booktitle = {Proceedings of the 46th Annual Meeting of the North East Linguistic Society ({{NELS}})},
  author = {Hartmann, Jutta M and Heycock, Caroline},
  editor = {Hammerly, Christopher and Prickett, Brandon},
  date = {2016},
  volume = {2},
  pages = {67--80},
  publisher = {{GLSA Publications}},
  date-added = {2020-01-22 18:01:44 -0500},
  date-modified = {2020-02-01 19:41:52 -0500},
  project = {Icelandic gluttony},
  keywords = {invisible dative,low nominative}
}

@book{hastie.t:1990,
  title = {Generalized Additive Models},
  author = {Hastie, T.J. and Tibshirani, R.J.},
  date = {1990-10},
  publisher = {{Routledge}},
  doi = {10.1201/9780203753781},
  url = {https://doi.org/10.1201%2F9780203753781},
  bdsk-url-2 = {https://doi.org/10.1201/9780203753781},
  date-added = {2022-01-05 22:06:57 -0500},
  date-modified = {2022-01-05 22:10:25 -0500}
}

@article{havelka.j:2007,
  title = {Mathematical Properties of Dependency Trees and Their Application to Natural Language Syntax},
  author = {Havelka, Jiří},
  date = {2007},
  publisher = {{Univerzita Karlova, Matematicko-fyzikální fakulta}},
  date-added = {2020-02-26 18:33:58 -0500},
  date-modified = {2020-02-26 18:36:25 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projectivity}
}

@article{heathcote.a:2012,
  title = {Linear {{Deterministic Accumulator Models}} of {{Simple Choice}}},
  author = {Heathcote, Andrew and Love, Jonathon},
  date = {2012},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychology},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00292},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00292/abstract},
  urldate = {2022-08-13},
  file = {/Users/j/Zotero/storage/APYGQJTQ/Heathcote and Love - 2012 - Linear Deterministic Accumulator Models of Simple .pdf}
}

@book{heim.i:1998,
  title = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  date = {1998},
  publisher = {{Blackwell}},
  date-added = {2019-05-19 21:59:01 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {0-631-19712-5},
  keywords = {semantics}
}

@incollection{heim.i:2000,
  title = {Semantics in Generative Grammar},
  booktitle = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  date = {2000},
  publisher = {{Blackwell Publishing}},
  location = {{Malden, MA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:16:57 -0400},
  keywords = {Semantics}
}

@inproceedings{hewitt.j:2019,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  date = {2019},
  pages = {4129--4138},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1419},
  url = {https://www.aclweb.org/anthology/N19-1419},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1419}
}

@inproceedings{hewitt.j:2019selectivity,
  title = {Designing and Interpreting Probes with Control Tasks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Hewitt, John and Liang, Percy},
  date = {2019},
  pages = {2733--2743},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1275},
  url = {https://www.aclweb.org/anthology/D19-1275},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1275}
}

@article{hidaka.s:2013,
  title = {A {{Computational Model Associating Learning Process}}, {{Word Attributes}}, and {{Age}} of {{Acquisition}}},
  author = {Hidaka, Shohei},
  date = {2013-11-01},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {8},
  number = {11},
  pages = {e76242},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076242},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0076242},
  urldate = {2022-09-28},
  abstract = {We propose a new model-based approach linking word learning to the age of acquisition (AoA) of words; a new computational tool for understanding the relationships among word learning processes, psychological attributes, and word AoAs as measures of vocabulary growth. The computational model developed describes the distinct statistical relationships between three theoretical factors underpinning word learning and AoA distributions. Simply put, this model formulates how different learning processes, characterized by change in learning rate over time and/or by the number of exposures required to acquire a word, likely result in different AoA distributions depending on word type. We tested the model in three respects. The first analysis showed that the proposed model accounts for empirical AoA distributions better than a standard alternative. The second analysis demonstrated that the estimated learning parameters well predicted the psychological attributes, such as frequency and imageability, of words. The third analysis illustrated that the developmental trend predicted by our estimated learning parameters was consistent with relevant findings in the developmental literature on word learning in children. We further discuss the theoretical implications of our model-based approach.},
  langid = {english},
  keywords = {Children,Learning,Learning curves,Linguistic morphology,Semantics,Social psychology,Statistical distributions,Vocabulary},
  file = {/Users/j/Zotero/storage/3SSR2NIZ/Hidaka (2013) A Computational Model Associating Learning Process.pdf}
}

@inproceedings{ho.j:2020,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  date = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2022-07-07},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/Users/j/Zotero/storage/JKXPLCHC/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hochreiter.s:1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
  date = {1997-11},
  journaltitle = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162%2Fneco.1997.9.8.1735},
  bdsk-url-2 = {https://doi.org/10.1162/neco.1997.9.8.1735},
  date-added = {2021-12-01 18:30:30 -0500},
  date-modified = {2021-12-01 18:30:32 -0500}
}

@book{hodges.w:2008,
  title = {Model {{Theory}}},
  author = {Hodges, Wilfrid},
  date = {2008-06-19},
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge}},
  isbn = {978-0-521-06636-5},
  langid = {english},
  pagetotal = {788}
}

@article{hoffman.m:2013,
  title = {Stochastic {{Variational Inference}}},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  date = {2013},
  journaltitle = {Journal of Machine Learning Research},
  volume = {14},
  number = {40},
  pages = {1303--1347},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v14/hoffman13a.html},
  urldate = {2022-06-30},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {/Users/j/Zotero/storage/N8K45R6U/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf}
}

@incollection{hofmann.m:2017,
  title = {Benchmarking N-Grams, {{Topic Models}} and {{Recurrent Neural Networks}} by {{Cloze Completions}}, {{EEGs}} and {{Eye Movements}}},
  booktitle = {Cognitive {{Approach}} to {{Natural Language Processing}}},
  author = {Hofmann, Markus J. and Biemann, Chris and Remus, Steffen},
  date = {2017},
  pages = {197--215},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-78548-253-3.50010-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B978178548253350010X},
  urldate = {2022-08-28},
  isbn = {978-1-78548-253-3},
  langid = {english}
}

@article{hofmann.m:2022,
  title = {Language {{Models Explain Word Reading Times Better Than Empirical Predictability}}},
  author = {Hofmann, Markus J. and Remus, Steffen and Biemann, Chris and Radach, Ralph and Kuchinke, Lars},
  date = {2022-02-02},
  journaltitle = {Frontiers in Artificial Intelligence},
  shortjournal = {Front. Artif. Intell.},
  volume = {4},
  pages = {730570},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.730570},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2021.730570/full},
  urldate = {2022-08-28},
  abstract = {Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.},
  file = {/Users/j/Zotero/storage/VW34WVNF/Hofmann et al. - 2022 - Language Models Explain Word Reading Times Better .pdf}
}

@article{holly.j:2001,
  title = {Pictures of Ultrametric Spaces, the p-Adic Numbers, and Valued Fields},
  author = {Holly, Jan E},
  date = {2001},
  journaltitle = {The American Mathematical Monthly},
  volume = {108},
  number = {8},
  pages = {721--728},
  publisher = {{Taylor \& Francis}},
  date-added = {2019-06-11 08:49:52 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry,ultrametric}
}

@article{holmberg.a:2003,
  title = {Agreement and Movement in {{Icelandic}} Raising Constructions},
  author = {Holmberg, Anders and Hróarsdóttir, Þorbjörg},
  date = {2003},
  journaltitle = {Lingua. International review of general linguistics. Revue internationale de linguistique générale},
  shortjournal = {Lingua},
  volume = {113},
  number = {10},
  pages = {997--1019},
  publisher = {{Elsevier}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:24:18 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@inproceedings{hoogeboom.e:2021,
  title = {Argmax Flows and Multinomial Diffusion: {{Learning}} Categorical Distributions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forré, Patrick and Welling, Max},
  editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
  date = {2021},
  url = {https://openreview.net/forum?id=6nbpPqUCIi7},
  date-added = {2022-03-31 10:59:19 -0400},
  date-modified = {2022-03-31 11:00:13 -0400},
  keywords = {denoising}
}

@thesis{hoover.i:2022phd,
  type = {phdthesis},
  title = {Effective {{Equidistribution}} on {{Hilbert Modular Varieties}}},
  author = {Hoover, Ian},
  date = {2022-08},
  institution = {{Boston College}},
  doi = {2345/bc-ir:109520},
  url = {http://dlib.bc.edu/islandora/object/bc-ir:109520},
  urldate = {2022-09-03},
  abstract = {We compute effective error rates for the equidistribution of translates of diagonal orbits on Hilbert modular varieties. The translation is determined by \$n\$ real parameters and our results require the assumption that all parameters are non-zero. The error rate is given in explicit polynomial terms of the translation parameters and Sobolev type norms of the test functions. The effective equidistribution is applied to give counting estimates for binary quadratic forms of square discriminant over real number rings.},
  langid = {english},
  pagetotal = {53},
  file = {/Users/j/Zotero/storage/MIHBUTMM/Hoover - 2022 - Effective Equidistribution on Hilbert Modular Vari.pdf}
}

@unpublished{hoover.j:2020wccflhandout,
  type = {Handout},
  title = {Accounting for Variation in Number Agreement in {{Icelandic DAT-NOM}} Constructions},
  author = {Hoover, Jacob Louis},
  date = {2020-03-07},
  doi = {10.14288/1.0389856},
  url = {https://doi.library.ubc.ca/10.14288/1.0389856},
  bdsk-url-2 = {https://doi.org/10.14288/1.0389856},
  date-added = {2021-04-08 21:14:55 -0400},
  date-modified = {2021-11-01 08:16:07 -0400},
  eventtitle = {38th {{West Coast Conference}} on {{Formal Linguistics}}},
  howpublished = {WCCFL2020 Talk handout},
  project = {Icelandic gluttony},
  keywords = {agreement,feature geometry}
}

@misc{hoover.j:2021arxiv,
  title = {Linguistic {{Dependencies}} and {{Statistical Dependence}}},
  author = {Hoover, Jacob Louis and Sordoni, Alessandro and Du, Wenyu and O'Donnell, Timothy J.},
  date = {2021},
  eprint = {2104.08685},
  eprinttype = {arxiv},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08685},
  url = {http://arxiv.org/abs/2104.08685},
  urldate = {2022-05-18},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \$\textbackslash approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {/Users/j/Zotero/storage/UXJMS7PY/Hoover et al. - 2021 - Linguistic Dependencies and Statistical Dependence.pdf}
}

@inproceedings{hoover.j:2021emnlp,
  title = {Linguistic {{Dependencies}} and {{Statistical Dependence}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hoover, Jacob Louis and Du, Wenyu and Sordoni, Alessandro and O'Donnell, Timothy J.},
  date = {2021-11},
  pages = {2941--2963},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.234},
  url = {https://aclanthology.org/2021.emnlp-main.234},
  urldate = {2022-05-19},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \$\textbackslash approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  date-added = {2021-11-07 10:33:16 -0500},
  date-modified = {2022-05-17 08:09:49 -0400},
  eventtitle = {{{EMNLP}} 2021},
  keywords = {dependency grammar,linguistic dependencies},
  file = {/Users/j/Zotero/storage/CR64LXWY/Hoover et al. (2021) Linguistic Dependencies and Statistical Dependence.pdf}
}

@inproceedings{hoover.j:2021wccfl,
  title = {Accounting for Variation in Number Agreement in {{Icelandic}} Dative-Nominative Constructions},
  booktitle = {Proceedings of the 38th {{West Coast Conference}} on {{Formal Linguistics}}},
  author = {Hoover, Jacob Louis},
  editor = {Soo, Rachel and Chow, Una Y. and Nederveen, Sander},
  date = {2021},
  pages = {231--241},
  publisher = {{Cascadilla Proceedings Project}},
  location = {{Somerville, Mass., USA}},
  url = {http://www.lingref.com/cpp/wccfl/38/abstract3568.html},
  abstract = {Icelandic dative-nominative constructions exhibit a syntactic hierarchy effect known as the Person Restriction: only third person nominatives may control agreement. In these constructions, there is variation between speakers in the extent to which the verb agrees with the nominative for number. Sigurðsson \& Holmberg (2008) explain this variation as arising due to differences between varieties in the timing of subject raising, using a split phi-probe. This paper revises their approach, using the feature gluttony mechanism for Agree developed in Coon \& Keine (2020), and a split phi-probe in which person probing precedes number probing. Within this framework, the observed variation can be captured by allowing variability two independent parameters: the timing of EPP subject raising, and the visibility of a number feature on dative DPs. The proposed mechanism describes the variation, including predicting the observed optional agreement in certain cases that previous literature had struggled to account for, and makes additional predictions about the differences between varieties in cases of syncretism within the verbal paradigm. An investigation into these predictions should allow this already well-studied area of Icelandic grammar to continue to be a useful test-case for crosslinguistic assumptions about the mechanism of Agree, and the status of dative arguments.},
  eventtitle = {{{WCCFL38}}},
  annotation = {Note: www.lingref.com, document 3568},
  file = {/Users/j/Zotero/storage/XA3QY83Q/Hoover (2021) Accounting for variation in number agreement in Ic.pdf}
}

@unpublished{hoover.j:2022amlap,
  type = {Poster},
  title = {With Better Language Models, Processing Time Is Superlinear in Surprisal},
  author = {Hoover, Jacob Louis},
  date = {2022-09-06},
  url = {https://virtual.oxfordabstracts.com/#/event/3067/submission/297},
  editora = {Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  editoratype = {collaborator},
  eventtitle = {Architectures and {{Mechanisms}} for {{Language Processing}} ({{AMLaP}}) 28},
  venue = {{York, England}}
}

@article{hoyer.p:2004,
  title = {Non-Negative Matrix Factorization with Sparseness Constraints},
  author = {Hoyer, Patrik O},
  date = {2004},
  journaltitle = {Journal of machine learning research},
  volume = {5},
  pages = {1457--1469},
  url = {http://www.jmlr.org/papers/v5/hoyer04a.html},
  date-added = {2020-07-26 15:09:12 -0400},
  date-modified = {2020-07-26 15:10:47 -0400},
  issue = {Nov},
  project = {syntactic embedding},
  keywords = {sparseness}
}

@misc{htut.p:2019,
  title = {Do Attention Heads in {{BERT}} Track Syntactic Dependencies?},
  author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
  date = {2019},
  eprint = {1911.12246},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{hu.j:2020systematic,
  title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
  date = {2020},
  pages = {1725--1744},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.158},
  url = {https://www.aclweb.org/anthology/2020.acl-main.158},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.158}
}

@inproceedings{hu.x:2021,
  title = {{{R2D2}}: {{Recursive}} Transformer Based on Differentiable Tree for Interpretable Hierarchical Language Modeling},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and de Melo, Gerard},
  options = {useprefix=true},
  date = {2021-08},
  pages = {4897--4908},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.acl-long.379},
  url = {https://aclanthology.org/2021.acl-long.379},
  abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.379},
  date-added = {2022-04-28 10:19:40 -0400},
  date-modified = {2022-04-28 10:19:58 -0400},
  keywords = {chart parsing,pruning}
}

@misc{hu.x:2022,
  title = {Fast-{{R2D2}}: {{A}} Pretrained Recursive Neural Network Based on Pruned {{CKY}} for Grammar Induction and Text Representation},
  author = {Hu, Xiang and Mi, Haitao and Li, Liang and de Melo, Gerard},
  options = {useprefix=true},
  date = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2203.00281},
  url = {https://arxiv.org/abs/2203.00281},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.00281},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-28 10:20:35 -0400},
  date-modified = {2022-04-28 10:20:59 -0400},
  keywords = {chart parsing,pruning,sampling},
  file = {/Users/j/Zotero/storage/DQPTD2F6/Hu et al. - 2022 - Fast-R2D2 A pretrained recursive neural network b.pdf}
}

@inproceedings{huang.c:2021,
  title = {A Variational Perspective on Diffusion-Based Generative Models and Score Matching},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron C},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  date = {2021},
  volume = {34},
  pages = {22863--22876},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/c11abfd29e4d9b4d4b566b01114d8486-Abstract.html},
  file = {/Users/j/Zotero/storage/25BH7NUL/Huang et al. - 2021 - A variational perspective on diffusion-based gener.pdf}
}

@thesis{huang.l:2008phd,
  title = {Forest-Based Algorithms in Natural Language Processing},
  author = {Huang, Liang},
  date = {2008},
  institution = {{University of Pennsylvania}},
  url = {https://www.proquest.com/docview/304495357},
  date-added = {2022-03-31 09:58:59 -0400},
  date-modified = {2022-04-26 21:20:55 -0400},
  keywords = {parsing},
  file = {/Users/j/Zotero/storage/FI56RTQ3/Huang - 2008 - Forest-based algorithms in natural language proces.pdf}
}

@inproceedings{huang.l:2010,
  title = {Dynamic Programming for Linear-Time Incremental Parsing},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Huang, Liang and Sagae, Kenji},
  date = {2010-07},
  pages = {1077--1086},
  publisher = {{Association for Computational Linguistics}},
  location = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/P10-1110},
  date-added = {2022-04-26 21:01:34 -0400},
  date-modified = {2022-04-26 21:02:08 -0400},
  keywords = {dependency parsing,dynamic programming}
}

@article{huddleston.r:2002,
  title = {The Cambridge Grammar of English},
  author = {Huddleston, Rodney and Pullum, Geoffrey K and others},
  date = {2002},
  journaltitle = {Language. Cambridge: Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{hudson.r:1984,
  title = {Word Grammar},
  author = {Hudson, Richard A},
  date = {1984},
  publisher = {{Blackwell Oxford}},
  url = {https://archive.org/details/wordgrammar0000huds},
  date-added = {2021-07-17 10:26:33 -0400},
  date-modified = {2021-07-17 10:48:55 -0400}
}

@article{hughes.b:2004,
  title = {Trees and Ultrametric Spaces: A Categorical Equivalence},
  author = {Hughes, Bruce},
  date = {2004},
  journaltitle = {Advances in Mathematics},
  volume = {189},
  number = {1},
  pages = {148--191},
  publisher = {{Elsevier}},
  url = {https://doi.org/10.1016/j.aim.2003.11.008},
  date-added = {2019-07-10 18:13:04 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{huijben.i:2022,
  title = {A Review of the Gumbel-Max Trick and Its Extensions for Discrete Stochasticity in Machine Learning},
  author = {Huijben, Iris A.M. and Kool, Wouter and Paulus, Max Benedikt and Sloun, Ruud JG Van},
  date = {2022},
  journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tpami.2022.3157042},
  url = {https://doi.org/10.1109%2Ftpami.2022.3157042},
  bdsk-url-2 = {https://doi.org/10.1109/tpami.2022.3157042},
  date-added = {2022-04-10 19:20:21 -0400},
  date-modified = {2022-04-10 19:26:14 -0400},
  file = {/Users/j/Zotero/storage/PGXVRCZU/Huijben et al. - 2022 - A review of the gumbel-max trick and its extension.pdf}
}

@article{hupkes.d:2018,
  title = {Visualisation and'diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure},
  author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  date = {2018},
  journaltitle = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {907--926},
  date-added = {2019-06-17 18:51:15 -0400},
  date-modified = {2019-06-17 18:52:12 -0400},
  project = {syntactic embedding},
  keywords = {implicit information probing}
}

@inproceedings{hwa.r:1999,
  title = {Supervised Grammar Induction Using Training Data with Limited Constituent Information},
  booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics},
  author = {Hwa, Rebecca},
  date = {1999},
  pages = {73--79},
  publisher = {{Association for Computational Linguistics}},
  location = {{College Park, Maryland, USA}},
  doi = {10.3115/1034678.1034699},
  url = {https://www.aclweb.org/anthology/P99-1010},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034699}
}

@article{itti.l:2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  date = {2009-06-02},
  journaltitle = {Vision Research},
  shortjournal = {Vision Research},
  series = {Visual {{Attention}}: {{Psychophysics}}, Electrophysiology and Neuroimaging},
  volume = {49},
  number = {10},
  pages = {1295--1306},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2008.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0042698908004380},
  urldate = {2022-08-07},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer’s beliefs yield surprise, irrespectively of how rare or informative in Shannon’s sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  langid = {english},
  keywords = {Attention,Bayes theorem,Eye movements,Free viewing,Information theory,Natural vision,Novelty,Saliency,Surprise},
  file = {/Users/j/Zotero/storage/PBY9SBAG/Itti and Baldi - 2009 - Bayesian surprise attracts human attention.pdf}
}

@incollection{jackendoff.r:2002,
  title = {Foundations of Language},
  booktitle = {Foundations of Language},
  author = {Jackendoff, Ray},
  date = {2002},
  publisher = {{Oxford University Press}},
  location = {{New York}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:52 -0400},
  group = {Cognitive Science, Language},
  readinglist = {Thesis},
  keywords = {Parallel Architecture}
}

@thesis{jaeger.t:2006,
  title = {Redundancy and Syntactic Reduction in Spontaneous Speech},
  author = {Jaeger, Tim Florian},
  date = {2006},
  institution = {{Stanford University Stanford, CA}},
  file = {/Users/j/Zotero/storage/YDQHTXHN/Jaeger (2006) Redundancy and syntactic reduction in spontaneous .pdf}
}

@article{jager.l:2015,
  title = {Retrieval Interference in Reflexive Processing: Experimental Evidence from {{Mandarin}}, and Computational Modeling},
  shorttitle = {Retrieval Interference in Reflexive Processing},
  author = {Jäger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  date = {2015},
  journaltitle = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00617},
  urldate = {2022-10-12},
  abstract = {We conducted two eye-tracking experiments investigating the processing of the Mandarin reflexive ziji in order to tease apart structurally constrained accounts from standard cue-based accounts of memory retrieval. In both experiments, we tested whether structurally inaccessible distractors that fulfill the animacy requirement of ziji influence processing times at the reflexive. In Experiment 1, we manipulated animacy of the antecedent and a structurally inaccessible distractor intervening between the antecedent and the reflexive. In conditions where the accessible antecedent mismatched the animacy cue, we found inhibitory interference whereas in antecedent-match conditions, no effect of the distractor was observed. In Experiment 2, we tested only antecedent-match configurations and manipulated locality of the reflexive-antecedent binding (Mandarin allows non-local binding). Participants were asked to hold three distractors (animate vs. inanimate nouns) in memory while reading the target sentence. We found slower reading times when animate distractors were held in memory (inhibitory interference). Moreover, we replicated the locality effect reported in previous studies. These results are incompatible with structure-based accounts. However, the cue-based ACT-R model of Lewis and Vasishth (2005) cannot explain the observed pattern either. We therefore extend the original ACT-R model and show how this model not only explains the data presented in this article, but is also able to account for previously unexplained patterns in the literature on reflexive processing.},
  keywords = {ACT-R},
  file = {/Users/j/Zotero/storage/W9IFD7Q4/Jäger et al. (2015) Retrieval interference in reflexive processing ex.pdf}
}

@misc{jakulin.a:2003,
  title = {Quantifying and Visualizing Attribute Interactions},
  author = {Jakulin, Aleks and Bratko, Ivan},
  date = {2003},
  eprint = {cs/0308002},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  date-added = {2021-07-19 22:11:58 -0400},
  date-modified = {2021-07-19 22:12:25 -0400},
  keywords = {co-information,interaction information,multivariate mututal information}
}

@inproceedings{jang.e:2017,
  title = {Categorical Reparameterization with Gumbel-Softmax},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  date = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  date-added = {2022-04-10 19:10:47 -0400},
  date-modified = {2022-04-10 19:20:19 -0400},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200}
}

@article{jarnik.v:1930,
  title = {O Jistém Problému Minimálním.({{Z}} Dopisu Panu {{O}}. {{Borŭvkovi}})},
  author = {Jarník, Vojtěch},
  date = {1930},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@misc{jasra.a:2013,
  title = {The Alive Particle Filter},
  author = {Jasra, Ajay and Lee, Anthony and Yau, Christopher and Zhang, Xiaole},
  date = {2013},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1304.0151},
  url = {https://arxiv.org/abs/1304.0151},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1304.0151},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:49:28 -0400},
  date-modified = {2022-05-05 09:49:46 -0400},
  keywords = {particle filtering},
  file = {/Users/j/Zotero/storage/VLKBETLS/Jasra et al. - 2013 - The alive particle filter.pdf}
}

@article{jelinek.f:1976,
  title = {Continuous Speech Recognition by Statistical Methods},
  author = {Jelinek, F.},
  date = {1976-04},
  journaltitle = {Proceedings of the IEEE},
  volume = {64},
  number = {4},
  pages = {532--556},
  issn = {1558-2256},
  doi = {10.1109/PROC.1976.10159},
  abstract = {Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.},
  eventtitle = {Proceedings of the {{IEEE}}},
  keywords = {Acoustic devices,Automatic speech recognition,Decoding,Loudspeakers,Natural languages,noisy-channel,Signal processing,Speech processing,Speech recognition,Statistical analysis,Statistics}
}

@inproceedings{jin.l:2020,
  title = {Memory-Bounded Neural Incremental Parsing for Psycholinguistic Prediction},
  booktitle = {Proceedings of the 16th International Conference on Parsing Technologies and the {{IWPT}} 2020 Shared Task on Parsing into Enhanced Universal Dependencies},
  author = {Jin, Lifeng and Schuler, William},
  date = {2020-07},
  pages = {48--61},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.iwpt-1.6},
  url = {https://aclanthology.org/2020.iwpt-1.6},
  abstract = {Syntactic surprisal has been shown to have an effect on human sentence processing, and can be predicted from prefix probabilities of generative incremental parsers. Recent state-of-the-art incremental generative neural parsers are able to produce accurate parses and surprisal values but have unbounded stack memory, which may be used by the neural parser to maintain explicit in-order representations of all previously parsed words, inconsistent with results of human memory experiments. In contrast, humans seem to have a bounded working memory, demonstrated by inhibited performance on word recall in multi-clause sentences (Bransford and Franks, 1971), and on center-embedded sentences (Miller and Isard,1964). Bounded statistical parsers exist, but are less accurate than neural parsers in predict-ing reading times. This paper describes a neural incremental generative parser that is able to provide accurate surprisal estimates and can be constrained to use a bounded stack. Results show that the accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to un-bounded working memory.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.iwpt-1.6},
  date-added = {2021-09-13 19:25:38 -0400},
  date-modified = {2021-09-13 19:25:40 -0400}
}

@book{johnson.d:1980,
  title = {Arc Pair Grammar},
  author = {Johnson, David E. and Postal, Paul M.},
  date = {1980},
  publisher = {{Princeton University Press}},
  location = {{Princeton, New Jersey}},
  area = {Linguistics, Syntax},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{johnson.m:2004,
  title = {A {{TAG-based}} Noisy-Channel Model of Speech Repairs},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Johnson, Mark and Charniak, Eugene},
  date = {2004-07},
  pages = {33--39},
  location = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1218960},
  url = {https://aclanthology.org/P04-1005},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218960},
  date-added = {2022-05-03 15:15:11 -0400},
  date-modified = {2022-05-03 15:18:08 -0400},
  keywords = {noisy channel coding,tree adjoining grammars,tree transducers}
}

@thesis{johnson.m:2014phd,
  type = {Thesis},
  title = {Bayesian Time Series Models and Scalable Inference},
  author = {Johnson, Matthew James},
  date = {2014},
  institution = {{Massachusetts Institute of Technology}},
  url = {https://dspace.mit.edu/handle/1721.1/89993},
  urldate = {2022-07-05},
  abstract = {With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and their Bayesian nonparametric extensions. The HMM is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model (HDP-HMM), have been applied in many settings. HSMMs and HDP-HSMMs extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which HSMM message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference (SVI) framework to develop scalable inference for the HMM, HSMM, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models (LDA), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes.},
  langid = {english},
  annotation = {Accepted: 2014-09-19T21:33:09Z},
  file = {/Users/j/Zotero/storage/7N5NAEDX/Johnson - 2014 - Bayesian time series models and scalable inference.pdf;/Users/j/Zotero/storage/X34XRVGE/Johnson - 2014 - Bayesian time series models and scalable inference.pdf}
}

@article{johnson.s:1967,
  title = {Hierarchical Clustering Schemes},
  author = {Johnson, Stephen C},
  date = {1967},
  journaltitle = {Psychometrika},
  volume = {32},
  number = {3},
  pages = {241--254},
  publisher = {{Springer-Verlag}},
  url = {https://doi.org/10.1007/BF02289588},
  date-added = {2019-06-15 10:38:05 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {1},
  keywords = {hierarchical clustering,ultrametric}
}

@article{jozefowicz.r:2016,
  title = {Exploring the Limits of Language Modeling},
  author = {Józefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  date = {2016},
  journaltitle = {CoRR},
  volume = {abs/1602.02410},
  eprint = {1602.02410},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1602.02410},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/JozefowiczVSSW16},
  date-added = {2019-06-23 21:20:27 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {convolutional,language modeling,LSTM},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  file = {/Users/j/Zotero/storage/STG35LBG/Józefowicz et al. - 2016 - Exploring the limits of language modeling.pdf}
}

@article{jurafsky.d:1996,
  title = {A {{Probabilistic Model}} of {{Lexical}} and {{Syntactic Access}} and {{Disambiguation}}},
  author = {Jurafsky, Daniel},
  date = {1996},
  journaltitle = {Cognitive Science},
  volume = {20},
  number = {2},
  pages = {137--194},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2002_1},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2002_1},
  urldate = {2022-07-23},
  abstract = {The problems of access—retrieving linguistic structure from some mental grammar —and disambiguation—choosing among these structures to correctly parse ambiguous linguistic input—are fundamental to language understanding. The literature abounds with psychological results on lexical access, the access of idioms, syntactic rule access, parsing preferences, syntactic disambiguation, and the processing of garden-path sentences. Unfortunately, it has been difficult to combine models which account for these results to build a general, uniform model of access and disambiguation at the lexical, idiomatic, and syntactic levels. For example, psycholinguistic theories of lexical access and idiom access and parsing theories of syntactic rule access have almost no commonality in methodology or coverage of psycholinguistic data. This article presents a single probabilistic algorithm which models both the access and disambiguation of linguistic knowledge. The algorithm is based on a parallel parser which ranks constructions for access, and interpretations for disambiguation, by their conditional probability. Low-ranked constructions and interpretations are pruned through beam-search; this pruning accounts, among other things, for the garden-path effect. I show that this motivated probabilistic treatment accounts for a wide variety of psycholinguistic results, arguing for a more uniform representation of linguistic knowledge and for the use of probabilistically-enriched grammars and interpreters as models of human knowledge of and processing of language.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog2002\_1},
  file = {/Users/j/Zotero/storage/ZARYLTGK/Jurafsky - 1996 - A Probabilistic Model of Lexical and Syntactic Acc.pdf}
}

@book{jurafsky.d:2009slp2,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, Daniel and Martin, James H.},
  date = {2009},
  edition = {2},
  publisher = {{Pearson Prentice Hall}},
  url = {https://home.cs.colorado.edu/ martin/slp.html},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kahane.s:1997,
  title = {Bubble Trees and Syntactic Representations},
  booktitle = {Proceedings of Mathematics of Language (Mol5) Meeting},
  author = {Kahane, Sylvain},
  date = {1997},
  pages = {70--76},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.7125&rep=rep1&type=pdf},
  date-added = {2021-07-17 10:51:30 -0400},
  date-modified = {2021-07-17 10:52:15 -0400},
  file = {/Users/j/Zotero/storage/UEJTS85H/Kahane - 1997 - Bubble trees and syntactic representations.pdf}
}

@unpublished{kahardipraja.p:2021,
  title = {Towards {{Incremental Transformers}}: {{An Empirical Analysis}} of {{Transformer Models}} for {{Incremental NLU}}},
  shorttitle = {Towards {{Incremental Transformers}}},
  author = {Kahardipraja, Patrick and Madureira, Brielen and Schlangen, David},
  date = {2021-09-15},
  number = {arXiv:2109.07364},
  eprint = {2109.07364},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2109.07364},
  url = {http://arxiv.org/abs/2109.07364},
  urldate = {2022-05-18},
  abstract = {Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5JDRJ39Y/Kahardipraja et al. - 2021 - Towards Incremental Transformers An Empirical Ana.pdf}
}

@article{kalin.l:2018,
  title = {Licensing and {{Differential Object Marking}}: {{The}} View from {{Neo-Aramaic}}},
  author = {Kalin, Laura},
  date = {2018},
  journaltitle = {Syntax (Oxford, England)},
  shortjournal = {Syntax},
  volume = {21},
  number = {2},
  pages = {112--159},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case}
}

@article{kamide.y:1999,
  title = {Incremental {{Pre-head Attachment}} in {{Japanese Parsing}}},
  author = {Kamide, Yuki and Mitchell, Don C.},
  date = {1999-10-01},
  journaltitle = {Language and Cognitive Processes},
  volume = {14},
  number = {5-6},
  pages = {631--662},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/016909699386211},
  url = {https://doi.org/10.1080/016909699386211},
  urldate = {2022-10-13},
  abstract = {The present study addresses the question of whether structural analyses of verb-arguments are postponed up until the head verb has been processed (head-driven parsing accounts) or initiated prior to the appearance of the verb (pre-head attachment accounts). To explore this question in relation to a head-final language, a Japanese dative argument attachment ambiguity was examined in both a questionnaire study (Experiment 1) and a self-paced reading test (Experiment 2). The data suggested that the dative argument attachment ambiguity is resolved in the manner predicted by pre-head attachment accounts. The results were incompatible with most variants of the head-driven parsing model, and were not of the form currently predicted by constraint-satisfaction models. We end by discussing the general theoretical implications of the findings.},
  keywords = {eager processing},
  annotation = {\_eprint: https://doi.org/10.1080/016909699386211},
  file = {/Users/j/Zotero/storage/RVVYX9B6/Kamide and Mitchell (1999) Incremental Pre-head Attachment in Japanese Parsin.pdf}
}

@article{kamide.y:2008,
  title = {Anticipatory {{Processes}} in {{Sentence Processing}}},
  author = {Kamide, Yuki},
  date = {2008},
  journaltitle = {Language and Linguistics Compass},
  volume = {2},
  number = {4},
  pages = {647--670},
  issn = {1749-818X},
  doi = {10.1111/j.1749-818X.2008.00072.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-818X.2008.00072.x},
  urldate = {2022-06-11},
  abstract = {Anticipation is an essential ability for the human cognitive system to survive in its surrounding environment. The present article will review previous research on anticipatory processes in sentence processing (comprehension). I start by pointing out past research carried out with inadequate methods, then move on to reviewing recent research with relatively new, more appropriate methods, specifically, the so-called ‘visual-world’ eye-tracking paradigm, and neuropsychological techniques. I then discuss remaining unresolved issues, both methodological and theoretical.},
  langid = {english},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1749-818X.2008.00072.x},
  file = {/Users/j/Zotero/storage/G4V9J49D/Kamide - 2008 - Anticipatory Processes in Sentence Processing.pdf}
}

@article{kartsaklis.d:2019,
  title = {Linguistic Matrix Theory},
  author = {Kartsaklis, Dimitrios and Ramgoolam, Sanjaye and Sadrzadeh, Mehrnoosh},
  date = {2019},
  journaltitle = {Annales de l'Institut Henri Poincaré D},
  publisher = {{European Mathematical Publishing House}},
  issn = {2308-5827},
  url = {http://dx.doi.org/10.4171/aihpd/75},
  date-added = {2019-08-06 08:52:19 +0300},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {physics}
}

@article{katz.j:2019,
  title = {The Phonetics and Phonology of Lenition: {{A Campidanese Sardinian}} Case Study},
  author = {Katz, Jonah and Pitzanti, Gianmarco},
  date = {2019-09},
  journaltitle = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {10},
  number = {1},
  pages = {16},
  publisher = {{Open Library of the Humanities}},
  doi = {10.5334/labphon.184},
  url = {https://doi.org/10.5334%2Flabphon.184},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.184},
  date-added = {2022-05-10 10:57:54 -0400},
  date-modified = {2022-05-10 10:58:06 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/JAUFQPV3/Katz and Pitzanti - 2019 - The phonetics and phonology of lenition A Campida.pdf}
}

@article{kawabata.t:1992,
  title = {The Structure of the {{I-measure}} of a {{Markov}} Chain},
  author = {Kawabata, T. and Yeung, R.W.},
  date = {1992-05},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {3},
  pages = {1146--1149},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.135658},
  url = {https://doi.org/10.1109%2F18.135658},
  bdsk-url-2 = {https://doi.org/10.1109/18.135658},
  date-added = {2021-09-21 17:47:00 -0400},
  date-modified = {2021-09-21 17:47:01 -0400}
}

@incollection{kay.p:2005,
  title = {Argument Structure Constructions and the {{Argument}}–{{Adjunct}} Distinction},
  booktitle = {Grammatical {{Constructions}}: {{Back}} to the Roots},
  author = {Kay, Paul},
  editor = {Fried, Mirjam and Boas, Hans C.},
  date = {2005},
  volume = {4},
  pages = {71--98},
  publisher = {{John Benjamins Publishing Company}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:32 -0400},
  readinglist = {Adjunction},
  keywords = {Argument/Modifier}
}

@inproceedings{kazantseva.a:2018,
  title = {Kawennón:Nis: The {{Wordmaker}} for {{Kanyen}}'kéha},
  shorttitle = {Kawennón:Nis},
  booktitle = {Proceedings of the {{Workshop}} on {{Computational Modeling}} of {{Polysynthetic Languages}}},
  author = {Kazantseva, Anna and Maracle, Owennatekha Brian and Maracle, Ronkwe'tiyóhstha Josiah and Pine, Aidan},
  date = {2018-08},
  pages = {53--64},
  publisher = {{Association for Computational Linguistics}},
  location = {{Santa Fe, New Mexico, USA}},
  url = {https://aclanthology.org/W18-4806},
  urldate = {2022-06-04},
  abstract = {In this paper we describe preliminary work on Kawennón:nis, a verb conjugator for Kanyen'kéha (Ohsweken dialect). The project is the result of a collaboration between Onkwawenna Kentyohkwa Kanyen'kéha immersion school and the Canadian National Research Council's Indigenous Language Technology lab. The purpose of Kawennón:nis is to build on the educational successes of the Onkwawenna Kentyohkwa school and develop a tool that assists students in learning how to conjugate verbs in Kanyen'kéha; a skill that is essential to mastering the language. Kawennón:nis is implemented with both web and mobile front-ends that communicate with an application programming interface that in turn communicates with a symbolic language model implemented as a finite state transducer. Eventually, it will serve as a foundation for several other applications for both Kanyen'kéha and other Iroquoian languages.},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/CDLZ5N9N/Kazantseva et al. - 2018 - Kawennónnis the Wordmaker for Kanyen'kéha.pdf}
}

@inproceedings{kennedy.a:2003,
  title = {The {{Dundee}} Corpus},
  booktitle = {Proceedings of the 12th {{European}} Conference on Eye Movement},
  author = {Kennedy, Alan and Hill, Robin and Pynte, Joël},
  date = {2003},
  date-added = {2021-06-02 17:24:08 -0400},
  date-modified = {2021-06-02 17:25:50 -0400}
}

@article{kennedy.a:2013,
  title = {Frequency and Predictability Effects in the {{Dundee Corpus}}: {{An}} Eye Movement Analysis},
  author = {Kennedy, Alan and Pynte, Joël and Murray, Wayne S. and Paul, Shirley-Anne},
  date = {2013},
  journaltitle = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {3},
  pages = {601--618},
  publisher = {{SAGE Publications}},
  doi = {10.1080/17470218.2012.676054},
  url = {https://doi.org/10.1080%2F17470218.2012.676054},
  bdsk-url-2 = {https://doi.org/10.1080/17470218.2012.676054},
  date-added = {2021-06-02 17:10:01 -0400},
  date-modified = {2021-06-02 17:10:34 -0400}
}

@misc{kim.t:2020chartbased,
  title = {Chart-Based Zero-Shot Constituency Parsing on Multiple Languages},
  author = {Kim, Taeuk and Li, Bowen and Lee, Sang-goo},
  date = {2020},
  eprint = {2004.13805},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kim.t:2020pretrained,
  title = {Are Pre-Trained Language Models Aware of Phrases? {{Simple}} but Strong Baselines for Grammar Induction},
  booktitle = {8th International Conference on Learning Representations, {{ICLR}} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  author = {Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  date = {2020},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=H1xPR3NtPB},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KimCEL20.bib},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200}
}

@inproceedings{kim.y:2015,
  title = {Character-Aware Neural Language Models},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {{USA}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David A. and Rush, Alexander M.},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  date = {2016},
  pages = {2741--2749},
  publisher = {{AAAI Press}},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12489},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aaai/KimJSR16.bib},
  timestamp = {Fri, 15 Nov 2019 00:00:00 +0100}
}

@inproceedings{kingma.d:2013,
  title = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Welling, Max},
  editor = {Bengio, Yoshua and LeCun, Yann},
  date = {2014},
  url = {http://arxiv.org/abs/1312.6114},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
  file = {/Users/j/Zotero/storage/F8H45BMD/Kingma and Welling - 2014 - Auto-encoding variational bayes.pdf}
}

@thesis{kingma.d:2017,
  title = {Variational Inference \& Deep Learning: {{A}} New Synthesis},
  author = {Kingma, Diederik P},
  date = {2017},
  institution = {{University of Amsterdam}},
  url = {https://pure.uva.nl/ws/files/17891313/Thesis.pdf},
  date-added = {2019-10-08 21:58:23 -0400},
  date-modified = {2021-03-12 11:48:12 -0500},
  project = {syntactic embedding},
  keywords = {autoencoders,variational inference},
  file = {/Users/j/Zotero/storage/CZSPTAWT/Kingma - 2017 - Variational inference & deep learning A new synth.pdf}
}

@inproceedings{kipf.t:2017,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Kipf, Thomas N. and Welling, Max},
  date = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{kitaev.n:2018,
  title = {Constituency Parsing with a Self-Attentive Encoder},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kitaev, Nikita and Klein, Dan},
  date = {2018},
  pages = {2676--2686},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1249},
  url = {https://www.aclweb.org/anthology/P18-1249},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1249},
  file = {/Users/j/Zotero/storage/MJEFTKPU/Kitaev and Klein - 2018 - Constituency parsing with a self-attentive encoder.pdf}
}

@inproceedings{kitaev.n:2019,
  title = {Multilingual {{Constituency Parsing}} with {{Self-Attention}} and {{Pre-Training}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
  date = {2019-07},
  pages = {3499--3505},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1340},
  url = {https://aclanthology.org/P19-1340},
  urldate = {2022-05-18},
  abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2\% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
  eventtitle = {{{ACL}} 2019},
  keywords = {parsing},
  file = {/Users/j/Zotero/storage/ETEMZZ8N/Kitaev et al. - 2019 - Multilingual Constituency Parsing with Self-Attent.pdf}
}

@inproceedings{kitaev.n:2022,
  title = {Learned {{Incremental Representations}} for {{Parsing}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kitaev, Nikita and Lu, Thomas and Klein, Dan},
  date = {2022-05},
  pages = {3086--3095},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.220},
  urldate = {2022-05-18},
  abstract = {We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.},
  eventtitle = {{{ACL}} 2022},
  file = {/Users/j/Zotero/storage/Y5TAURC3/Kitaev et al. - 2022 - Learned Incremental Representations for Parsing.pdf}
}

@inproceedings{klein.d:2002parserFactored,
  title = {Fast Exact Inference with a Factored Model for Natural Language Parsing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Klein, Dan and Manning, Christopher D},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  date = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2002/file/6c97cd07663b099253bc569fe8d342bb-Paper.pdf},
  date-added = {2022-05-06 15:57:44 -0400},
  date-modified = {2022-05-06 16:01:12 -0400},
  keywords = {stanford dependencies,stanford parser},
  file = {/Users/j/Zotero/storage/NSJVN9IE/Klein and Manning - 2002 - Fast exact inference with a factored model for nat.pdf}
}

@inproceedings{klein.d:2003parserPCFG,
  title = {Accurate Unlexicalized Parsing},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  author = {Klein, Dan and Manning, Christopher D.},
  date = {2003-07},
  pages = {423--430},
  publisher = {{Association for Computational Linguistics}},
  location = {{Sapporo, Japan}},
  doi = {10.3115/1075096.1075150},
  url = {https://aclanthology.org/P03-1054},
  bdsk-url-2 = {https://doi.org/10.3115/1075096.1075150},
  date-added = {2022-05-06 16:00:23 -0400},
  date-modified = {2022-05-06 16:00:53 -0400}
}

@inproceedings{klein.d:2004induction,
  title = {Corpus-Based Induction of Syntactic Structure: {{Models}} of Dependency and Constituency},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Klein, Dan and Manning, Christopher},
  date = {2004},
  pages = {478--485},
  location = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1219016},
  url = {https://www.aclweb.org/anthology/P04-1061},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1219016}
}

@article{kliegl.r:2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  date = {2004},
  journaltitle = {European Journal of Cognitive Psychology},
  volume = {16},
  number = {1-2},
  pages = {262--284},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/09541440340000213},
  url = {https://doi.org/10.1080%2F09541440340000213},
  bdsk-url-2 = {https://doi.org/10.1080/09541440340000213},
  date-added = {2021-06-02 17:15:22 -0400},
  date-modified = {2021-06-02 17:15:24 -0400}
}

@unpublished{kollar.t:2017,
  title = {Generalized Grounding Graphs: {{A}} Probabilistic Framework for Understanding Grounded Commands},
  author = {Kollar, Thomas and Tellex, Stefanie and Walter, Matthew and Huang, Albert and Bachrach, Abraham and Hemachandra, Sachi and Brunskill, Emma and Banerjee, Ashis and Roy, Deb and Teller, Seth and others},
  date = {2017},
  eprint = {1712.01097},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  date-added = {2020-07-28 16:14:45 -0400},
  date-modified = {2020-07-28 16:15:35 -0400},
  project = {syntactic embedding},
  keywords = {robotics,semantics}
}

@article{kolmogorov.a:1968,
  title = {Logical Basis for Information Theory and Probability Theory},
  author = {Kolmogorov, Andrei},
  date = {1968},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {5},
  pages = {662--664},
  publisher = {{IEEE}},
  date-added = {2019-09-13 08:11:08 -0400},
  date-modified = {2019-09-13 08:11:46 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@article{kolmogorov.a:1968a,
  title = {Three Approaches to the Quantitative Definition of Information},
  author = {Kolmogorov, Andrei Nikolaevich},
  date = {1968},
  journaltitle = {International journal of computer mathematics},
  volume = {2},
  number = {1-4},
  pages = {157--168},
  publisher = {{Taylor \& Francis}},
  url = {https://www.tandfonline.com/doi/pdf/10.1080/00207166808803030},
  date-added = {2019-09-13 08:14:37 -0400},
  date-modified = {2019-09-13 08:15:41 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@article{kong.a:1994,
  title = {Sequential Imputations and Bayesian Missing Data Problems},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  date = {1994},
  journaltitle = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1994.10476469},
  pages = {278--288},
  publisher = {{Taylor \& Francis}},
  doi = {10.1080/01621459.1994.10476469},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476469},
  abstract = {Abstract For missing data problems, Tanner and Wong have described a data augmentation procedure that approximates the actual posterior distribution of the parameter vector by a mixture of complete data posteriors. Their method of constructing the complete data sets is closely related to the Gibbs sampler. Both required iterations, and, similar to the EM algorithm, convergence can be slow. We introduce in this article an alternative procedure that involves imputing the missing data sequentially and computing appropriate importance sampling weights. In many applications this new procedure works very well without the need for iterations. Sensitivity analysis, influence analysis, and updating with new data can be performed cheaply. Bayesian prediction and model selection can also be incorporated. Examples taken from a wide range of applications are used for illustration.},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1994.10476469},
  date-added = {2022-05-07 10:36:04 -0400},
  date-modified = {2022-05-07 10:36:22 -0400},
  keywords = {sequential importance sampling,sequential imputation,sequential Monte Carlo}
}

@inproceedings{konieczny.l:2003,
  title = {Anticipation of Clause-Final Heads. {{Evidence}} from Eye-Tracking and {{SRNs}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Cognitive Science}}},
  author = {Konieczny, Lars and Döring, Philipp},
  date = {2003-06-02/2003-06-04},
  pages = {13--17},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Melbourne, Australia and St. Petersburg, Russia}},
  url = {https://www.researchgate.net/publication/235711358},
  abstract = {In a Simple Recurrent Network simulation and an eye- tracking study, we investigated the processing of clause- final verbs. Following the integration cost hypothesis (Gibson, 1998), processing verbs should be the harder, the more complement integrations have to take place. In contrast, probabilistic prediction-based models, like Simple Recurrent Networks (SRNs, Elman, 1990), might anticipate verbs the better, the more dependents have been encountered beforehand. We trained SRNs with a subset of the German language to establish basic dependency relationships between verbs and their arguments in both verb-second and verb-final constructions. The test results established a clear anticipation hypothesis: the more arguments precede the verb, the lower the prediction error and hence, predicted reading times. The data from an eye-tracking experiment confirm the anticipation hypothesis: Clause final verbs are read faster when an additional Dative, instead of a noun-modifying Genitive, is read beforehand. Adverbial PP-adjuncts, in contrast to Noun-modifying PPs, however, did not affect reading times. In general, the results support a restricted anticipation hypothesis.},
  eventtitle = {{{ICCS}}/{{ASCS}} 2003}
}

@inproceedings{koo.t:2007,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  date = {2007},
  pages = {141--150},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/D07-1015}
}

@book{kozen.d:1997,
  title = {Automata and Computability},
  author = {Kozen, Dexter C.},
  date = {1997},
  publisher = {{Springer}},
  date-added = {2019-05-19 21:40:11 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  keywords = {automata,complexity,computability}
}

@book{kracht.m:2003,
  title = {The Mathematics of Language},
  author = {Kracht, Marcus},
  date = {2003},
  series = {Studies in Generative Grammar},
  number = {63},
  publisher = {{Mouton De Gruyter}},
  date-added = {2019-05-19 21:51:49 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {3-11-017620-3 978-3-11-017620-9},
  keywords = {automata,complexity,formal languages,mathematical linguistics,model theory}
}

@article{kubler.s:2009,
  title = {Dependency Parsing},
  author = {Kübler, Sandra and McDonald, Ryan and Nivre, Joakim},
  date = {2009},
  journaltitle = {Synthesis lectures on human language technologies},
  volume = {1},
  number = {1},
  pages = {1--127},
  publisher = {{Morgan \& Claypool Publishers}},
  date-added = {2020-02-26 14:44:36 -0500},
  date-modified = {2020-02-26 14:45:01 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm}
}

@article{kucerova.i:2016,
  title = {Long-Distance Agreement in {{Icelandic}}: Locality Restored},
  author = {Kučerová, Ivona},
  date = {2016},
  journaltitle = {The Journal of Comparative Germanic Linguistics},
  volume = {19},
  number = {1},
  pages = {49--74},
  publisher = {{Springer}},
  url = {http://ling.auf.net/lingbuzz/002237/current.pdf},
  date-added = {2020-02-26 09:11:49 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,object shift,phi features},
  file = {/Users/j/Zotero/storage/9PYVFJSG/Kučerová - 2016 - Long-distance agreement in Icelandic locality res.pdf}
}

@book{kuhlmann.m:2010,
  title = {Dependency Structures and Lexicalized Grammars: {{An}} Algebraic Approach},
  author = {Kuhlmann, Marco},
  date = {2010},
  volume = {6270},
  publisher = {{Springer}},
  url = {https://www.ida.liu.se/ marku61/pdf/kuhlmann2010dependency.pdf},
  date-added = {2020-02-26 18:37:01 -0500},
  date-modified = {2021-07-16 11:22:03 -0400},
  isbn = {978-3-642-14568-1},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projective dependencies,projectivity}
}

@article{kullback.s:1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  date = {1951-03},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
  urldate = {2022-10-11},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/j/Zotero/storage/G42NQYWW/Kullback and Leibler (1951) On Information and Sufficiency.pdf}
}

@book{kullback.s:1959,
  title = {Information {{Theory}} and {{Statistics}}},
  author = {Kullback, Solomon},
  date = {1959},
  origdate = {1968},
  edition = {1968 Dover republication of 1959 (Wiley) first edition},
  publisher = {{Peter Smith}},
  location = {{New York, NY, USA}},
  isbn = {978-0-8446-5625-0},
  langid = {english},
  pagetotal = {409},
  keywords = {Information theory},
  file = {/Users/j/Zotero/storage/JLNQK6GX/kullback.s.1959.djvu}
}

@inproceedings{kuncoro.a:2018,
  title = {{{LSTMs}} Can Learn Syntax-Sensitive Dependencies Well, but Modeling Structure Makes Them Better},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John T. and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  date = {2018},
  pages = {1426--1436},
  publisher = {{Association for Computational Linguistics}},
  location = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1132},
  url = {https://www.aclweb.org/anthology/P18-1132},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1132},
  date-modified = {2022-04-20 13:50:17 -0400}
}

@article{kuperberg.g:2016,
  title = {What Do We Mean by Prediction in Language Comprehension?},
  author = {Kuperberg, Gina R. and Jaeger, T. Florian},
  date = {2016},
  journaltitle = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  eprint = {https://doi.org/10.1080/23273798.2015.1102299},
  pages = {32--59},
  publisher = {{Routledge}},
  doi = {10.1080/23273798.2015.1102299},
  url = {https://doi.org/10.1080/23273798.2015.1102299},
  date-modified = {2021-06-05 22:29:28 -0400}
}

@inproceedings{kuribayashi.t:2021,
  title = {Lower Perplexity Is Not Always Human-Like},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  date = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.405},
  url = {https://doi.org/10.18653%2Fv1%2F2021.acl-long.405},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.405},
  date-added = {2021-12-02 19:40:09 -0500},
  date-modified = {2021-12-02 19:40:25 -0500}
}

@inproceedings{kurihara.k:2004,
  title = {An Application of the Variational {{Bayesian}} Approach to Probabilistic Context-Free Grammars},
  booktitle = {In {{International Joint Conference}} on {{Natural Language Processing Workshop Beyond Shallow Analyses}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  date = {2004},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.4186},
  abstract = {We present an efficient learning algorithm for probabilistic context-free grammars based on the variational Bayesian approach. Although the maximum likelihood method has traditionally been used for learning probabilistic language models, Bayesian learning is, in principle, less likely to cause overfitting problems than the maximum likelihood method. We show that the computational complexity of our algorithm is equal to that of the Inside-Outside algorithm. We also report results of experiments to compare precisions of the Inside-Outside algorithm and our algorithm. 1},
  eventtitle = {{{IJCNLP-04 Workshow Beyond Shallow Analyses}}},
  file = {/Users/j/Zotero/storage/GIIQM9EA/Kurihara - 2004 - 2004. An application of the variational Bayesian a.pdf}
}

@incollection{kurihara.k:2006,
  title = {Variational {{Bayesian Grammar Induction}} for {{Natural Language}}},
  booktitle = {Grammatical {{Inference}}: {{Algorithms}} and {{Applications}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  editor = {Sakakibara, Yasubumi and Kobayashi, Satoshi and Sato, Kengo and Nishino, Tetsuro and Tomita, Etsuji},
  date = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--96},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/11872436_8},
  abstract = {This paper presents a new grammar induction algorithm for probabilistic context-free grammars (PCFGs). There is an approach to PCFG induction that is based on parameter estimation. Following this approach, we apply the variational Bayes to PCFGs. The variational Bayes (VB) is an approximation of Bayesian learning. It has been empirically shown that VB is less likely to cause overfitting. Moreover, the free energy of VB has been successfully used in model selection. Our algorithm can be seen as a generalization of PCFG induction algorithms proposed before. In the experiments, we empirically show that induced grammars achieve better parsing results than those of other PCFG induction algorithms. Based on the better parsing results, we give examples of recursive grammatical structures found by the proposed algorithm.},
  isbn = {978-3-540-45265-2},
  langid = {english},
  keywords = {Bayesian Learning,Noun Phrase,Parse Tree,Training Corpus,Wall Street Journal},
  file = {/Users/j/Zotero/storage/LNILI62V/Kurihara and Sato - 2006 - Variational Bayesian Grammar Induction for Natural.pdf}
}

@article{lambek.j:1958,
  title = {The Mathematics of Sentence Structure},
  author = {Lambek, Joachim},
  date = {1958},
  journaltitle = {The American Mathematical Monthly},
  volume = {65},
  number = {3},
  eprint = {2310058},
  eprinttype = {jstor},
  pages = {154--170},
  publisher = {{Taylor \& Francis, Ltd. on behalf of the Mathematical Association of America}},
  doi = {10.1080/00029890.1958.11989160},
  bdsk-url-2 = {https://doi.org/10.1080/00029890.1958.11989160},
  date-added = {2019-08-26 14:46:48 -0400},
  date-modified = {2021-06-25 00:48:42 -0400},
  keywords = {category theory,pregroup grammar}
}

@inproceedings{lambek.j:1999,
  title = {Type Grammar Revisited},
  booktitle = {International Conference on Logical Aspects of Computational Linguistics},
  author = {Lambek, Joachim},
  date = {1999},
  pages = {1--27},
  date-added = {2019-08-26 22:09:20 -0400},
  date-modified = {2019-08-26 22:09:55 -0400},
  organization = {{Springer}},
  keywords = {pregroup grammar}
}

@article{lambek.j:2001,
  title = {Type Grammars as Pregroups},
  author = {Lambek, Joachim},
  date = {2001},
  journaltitle = {Grammars},
  volume = {4},
  pages = {21--39},
  date-added = {2019-08-26 21:51:00 -0400},
  date-modified = {2019-08-26 21:51:54 -0400},
  keywords = {pregroup grammar}
}

@article{lambek.j:2012,
  title = {Logic and Grammar},
  author = {Lambek, Joachim},
  date = {2012},
  journaltitle = {Studia Logica: An International Journal for Symbolic Logic},
  volume = {100},
  number = {4},
  eprint = {23262129},
  eprinttype = {jstor},
  pages = {667--681},
  publisher = {{Springer}},
  issn = {00393215, 15728730},
  abstract = {Grammar can be formulated as a kind of substructural propositional logic. In support of this claim, we survey bare Gentzen style deductive systems and two kinds of non-commutative linear logic: intuitionistic and compact bilinear logic. We also glance at their categorical refinements.},
  date-added = {2019-08-26 21:59:20 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {pregroup grammar}
}

@inproceedings{lample.g:2019,
  title = {Cross-Lingual Language Model Pretraining},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Conneau, Alexis and Lample, Guillaume},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {7057--7067},
  url = {https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ConneauL19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@book{lanchier.n:2017,
  title = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  date = {2017},
  series = {Universitext},
  publisher = {{Springer International}},
  doi = {10.1007/978-3-319-50038-6},
  url = {https://doi.org/10.1007%2F978-3-319-50038-6},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6},
  date-added = {2022-04-07 10:01:23 -0400},
  date-modified = {2022-04-14 10:21:03 -0400}
}

@incollection{lanchier.n:2017ch1,
  title = {Basics of Measure and Probability Theory},
  booktitle = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  date = {2017},
  series = {Universitext},
  pages = {3--24},
  publisher = {{Springer International}},
  doi = {10.1007/978-3-319-50038-6_1},
  url = {https://doi.org/10.1007%2F978-3-319-50038-6₁},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6₁},
  date-added = {2022-04-07 10:02:11 -0400},
  date-modified = {2022-04-14 10:20:52 -0400}
}

@article{lari.k:1991,
  title = {Applications of Stochastic Context-Free Grammars Using the {{Inside-Outside}} Algorithm},
  author = {Lari, K. and Young, S. J.},
  date = {1991-07-01},
  journaltitle = {Computer Speech \& Language},
  shortjournal = {Computer Speech \& Language},
  volume = {5},
  number = {3},
  pages = {237--257},
  issn = {0885-2308},
  doi = {10.1016/0885-2308(91)90009-F},
  url = {https://www.sciencedirect.com/science/article/pii/088523089190009F},
  urldate = {2022-07-04},
  abstract = {This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.},
  langid = {english},
  file = {/Users/j/Zotero/storage/SGHR95TR/Lari and Young - 1991 - Applications of stochastic context-free grammars u.pdf}
}

@article{lau.j:2016,
  title = {Grammaticality, Acceptability, and Probability: {{A}} Probabilistic View of Linguistic Knowledge},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  date = {2016-10},
  volume = {41},
  number = {5},
  pages = {1202--1241},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12414},
  url = {https://doi.org/10.1111%2Fcogs.12414},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12414},
  date-added = {2021-10-19 00:11:36 -0400},
  date-modified = {2021-10-19 00:11:37 -0400}
}

@book{lazore.d:1993,
  title = {The Mohawk Language Standardisation Project Conference Report, Aug. 9-10, 1993},
  author = {Lazore, Dorothy Karihwénhawe},
  editor = {Jacobs, Annette Kaia'titáhkhe and Thompson, Nancy Kahawinónkie and Leaf, Minnie Kaià:khons},
  date = {1993},
  publisher = {{Literacy and Basic Skills Section, Ministry of Education and Training}},
  location = {{Toronto}},
  url = {http://kanienkeha.net/the-mohawk-language-standardisation-project/},
  date-added = {2022-05-03 17:07:15 -0400},
  date-modified = {2022-05-03 17:17:25 -0400},
  isbn = {0-7778-6105-4},
  langid = {english},
  organization = {{Mohawk Language Standardisation Conference (1993 : Tyendinaga Indian Reserve)}},
  keywords = {kanien'keha,mohawk language,standardization}
}

@article{lebesgue.h:1902,
  title = {Intégrale, Longueur, Aire},
  author = {Lebesgue, H.},
  date = {1902-12-01},
  journaltitle = {Annali di Matematica Pura ed Applicata (1898-1922)},
  shortjournal = {Annali di Matematica, Serie III},
  volume = {7},
  number = {1},
  pages = {231--359},
  issn = {0373-3114},
  doi = {10.1007/BF02420592},
  url = {https://doi.org/10.1007/BF02420592},
  urldate = {2022-06-22},
  langid = {french}
}

@inproceedings{lebrun.b:2022,
  title = {Evaluating Distributional Distortion in Neural Language Modeling},
  booktitle = {International Conference on Learning Representations},
  author = {LeBrun, Benjamin and Sordoni, Alessandro and O'Donnell, Timothy J.},
  date = {2022},
  url = {https://openreview.net/forum?id=bTteFbU99ye}
}

@article{leemis.l:2008,
  title = {Univariate {{Distribution Relationships}}},
  author = {Leemis, Lawrence M. and McQueston, Jacquelyn T.},
  date = {2008-02-01},
  journaltitle = {The American Statistician},
  volume = {62},
  number = {1},
  pages = {45--53},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313008X270448},
  url = {https://doi.org/10.1198/000313008X270448},
  urldate = {2022-06-20},
  abstract = {Probability distributions are traditionally treated separately in introductory mathematical statistics textbooks. A figure is presented here that shows properties that individual distributions possess and many of the relationships between these distributions.},
  keywords = {Asymptotic relationships,Distribution properties,Limiting distributions,Stochastic parameters,Transformations},
  annotation = {\_eprint: https://doi.org/10.1198/000313008X270448},
  file = {/Users/j/Zotero/storage/T9J6S7Z7/Leemis and McQueston - 2008 - Univariate Distribution Relationships.pdf}
}

@book{legate.j:2014,
  title = {Voice and v: {{Lessons}} from Acehnese},
  author = {Legate, Julie Anne},
  date = {2014},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262028141.001.0001},
  url = {https://doi.org/10.7551%2Fmitpress%2F9780262028141.001.0001},
  bdsk-url-2 = {https://doi.org/10.7551/mitpress/9780262028141.001.0001},
  date-added = {2021-03-22 00:34:12 -0400},
  date-modified = {2021-03-22 13:11:36 -0400},
  keywords = {argument structure,voice}
}

@article{legate.j:2020,
  title = {On Passives of Passives},
  author = {Legate, Julie Anne and Akkuş, Faruk and Šereikaitė, Milena and Ringe, Don},
  date = {2020},
  journaltitle = {Language},
  volume = {96},
  number = {4},
  pages = {771--818},
  publisher = {{Project Muse}},
  doi = {10.1353/lan.2020.0062},
  url = {https://doi.org/10.1353%2Flan.2020.0062},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0062},
  date-added = {2021-03-20 12:21:19 -0400},
  date-modified = {2021-03-20 12:21:35 -0400},
  keywords = {argument structure,passives}
}

@book{levelt.w:1974,
  title = {Formal Grammars in Linguistics and Psycholinguistics: {{Volume}} 3: {{Psycholinguistic}} Applications},
  author = {Levelt, Willem JM},
  date = {1974},
  series = {{{JANUA LINGUARUM}}},
  volume = {192},
  number = {3},
  publisher = {{Mouton}},
  location = {{The Hague}},
  date-added = {2019-06-11 14:51:49 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@incollection{levin.b:2005,
  title = {Argument Realization: {{Research}} Surveys in Linguistics},
  booktitle = {Argument Realization: {{Research}} Surveys in Linguistics},
  author = {Levin, Beth and Rappaport Hovav, Malka},
  date = {2005},
  publisher = {{Cambridge University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:20 -0400},
  readinglist = {Thesis}
}

@inproceedings{levshina.n:2017,
  title = {Communicative Efficiency and Syntactic Predictability: {{A}} Cross-Linguistic Study Based on the {{Universal Dependencies}} Corpora},
  booktitle = {Proceedings of the {{NoDaLiDa}} 2017 Workshop on Universal Dependencies, 22 May, Gothenburg Sweden},
  author = {Levshina, Natalia},
  date = {2017},
  number = {135},
  pages = {72--78},
  url = {http://www.ep.liu.se/ecp/article.asp?issue=135&article=009&volume=},
  date-added = {2020-04-05 12:14:22 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  organization = {{Linköping University Electronic Press}},
  project = {syntactic embedding},
  keywords = {dependency structures,information theory,random forests}
}

@inproceedings{levy.o:2014,
  title = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27: {{Annual}} Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  date = {2014},
  pages = {2177--2185},
  url = {https://proceedings.neurips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyG14.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{levy.o:2014dependency,
  title = {Dependency-Based Word Embeddings},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Levy, Omer and Goldberg, Yoav},
  date = {2014},
  pages = {302--308},
  publisher = {{Association for Computational Linguistics}},
  location = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2050},
  url = {https://www.aclweb.org/anthology/P14-2050},
  bdsk-url-2 = {https://doi.org/10.3115/v1/P14-2050}
}

@thesis{levy.r:2005,
  title = {Probabilistic Models of Word Order and Syntactic Discontinuity},
  author = {Levy, Roger},
  date = {2005},
  institution = {{Stanford University}},
  url = {https://www.proquest.com/dissertations-theses/probabilistic-models-word-order-syntactic/docview/305432573/se-2?accountid=12339},
  abstract = {This thesis takes up the problem of syntactic comprehension, or parsing—how an agent (human or machine) with knowledge of a specific language goes about inferring the hierarchical structural relationships underlying a surface string in the language. I take the position that probabilistic models of combining evidential information are cognitively plausible and practically useful for syntactic comprehension. In particular, the thesis applies probabilistic methods in investigating the relationship between word order and psycholinguistic models of comprehension; and in the practical problems of accuracy and efficiency in parsing sentences with syntactic discontinuity. On the psychological side, the thesis proposes a theory of expectation-based processing difficulty as a consequence of probabilistic syntactic disambiguation: the ease of processing a word during comprehension is determined primarily by the degree to which that word is expected. I identify a class of syntactic phenomena, associated primarily with verb-final clause order, where the predictions of expectation-based processing diverge most sharply from more established locality-based theories of processing difficulty. Using existing probabilistic parsing algorithms and syntactically annotated data sources, I show that the expectation-based theory matches a range of established experimental psycholinguistic results better than locality-based theories. The comparison of probabilistic- and locality-driven processing theories is a crucial area of psycholinguistic research due to its implications for the relationship between linguistic production and comprehension, and more generally for theories of modularity in cognitive science. The thesis also takes up the problem of probabilistic models for discontinuous constituency, when phrases do not consist of continuous substrings of a sentence. Discontinuity poses a computational challenge in parsing, because it expands the set of possible substructures in a sentence beyond the bound, quadratic in sentence length, on the set of possible continuous constituents. For discontinuous constituency, I investigate the problem of accuracy employing discriminative classifiers organized on principles of syntactic theory and used to introduce discontinuous relationships into otherwise strictly context-free phrase structure trees; and the problem of efficiency in joint inference over both continuous and discontinuous structures, using probabilistic instantiations of mildly context-sensitive grammatical formalisms and factorizing grammatical generalizations into probabilistic components of dominance and linear order.},
  date-added = {2021-09-18 22:16:45 -0400},
  date-modified = {2022-04-04 13:25:27 -0400},
  isbn = {978-0-542-28638-4},
  keywords = {Applied sciences,Cognitive psychology,Cognitive therapy,Computer science,Language,Linguistics,literature and linguistics,Natural language processing,Parsing,Probabilistic,Psychology,Syntactic discontinuity,Word order},
  file = {/Users/j/Zotero/storage/9EWYEL2I/Levy - 2005 - Probabilistic models of word order and syntactic d.pdf}
}

@inproceedings{levy.r:2006,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Advances in Neural Information Processing Systems 19, Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 4-7, 2006},
  author = {Levy, Roger and Jaeger, T. Florian},
  editor = {Schölkopf, Bernhard and Platt, John C. and Hofmann, Thomas},
  date = {2006},
  pages = {849--856},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2006/hash/c6a01432c8138d46ba39957a8250e027-Abstract.html},
  biburl = {https://dblp.org/rec/conf/nips/LevyJ06.bib},
  date-added = {2021-10-18 21:45:39 -0400},
  date-modified = {2021-10-18 21:46:13 -0400}
}

@article{levy.r:2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  date = {2008},
  journaltitle = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027707001436},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159–166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2007.05.006},
  date-added = {2021-01-14 13:02:24 -0500},
  date-modified = {2021-03-09 22:53:26 -0500},
  keywords = {Frequency,Information theory,Parsing,Prediction,processing,Sentence processing,surprisal,Syntactic complexity,Syntax,Word order},
  file = {/Users/j/Zotero/storage/CTRQQCHF/Levy (2008) Expectation-based syntactic comprehension.pdf}
}

@inproceedings{levy.r:2008noisy,
  title = {A Noisy-Channel Model of Human Sentence Comprehension under Uncertain Input},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Levy, Roger},
  date = {2008-10},
  pages = {234--243},
  publisher = {{Association for Computational Linguistics}},
  location = {{Honolulu, Hawaii}},
  url = {https://aclanthology.org/D08-1025},
  date-added = {2022-04-11 23:17:10 -0400},
  date-modified = {2022-04-11 23:17:30 -0400}
}

@inproceedings{levy.r:2008particle,
  title = {Modeling the Effects of Memory on Human Online Sentence Processing with Particle Filters},
  booktitle = {Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008},
  author = {Levy, Roger and Reali, Florencia and Griffiths, Thomas L.},
  editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, Léon},
  date = {2008},
  pages = {937--944},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2008/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyRG08.bib},
  date-modified = {2022-05-12 19:43:45 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{levy.r:2009pnas,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  date = {2009},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {50},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0907664106},
  pages = {21086--21090},
  doi = {10.1073/pnas.0907664106},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.0907664106},
  abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.0907664106},
  date-added = {2022-04-27 22:17:38 -0400},
  date-modified = {2022-04-27 22:18:16 -0400},
  keywords = {memory,noisy channel coding}
}

@inproceedings{levy.r:2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Levy, Roger},
  date = {2011-06},
  pages = {1055--1065},
  publisher = {{Association for Computational Linguistics}},
  location = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1106},
  date-added = {2022-04-27 08:47:55 -0400},
  date-modified = {2022-04-27 08:49:23 -0400},
  keywords = {noisy channel coding}
}

@incollection{levy.r:2013,
  title = {Memory and Surprisal in Human Sentence Comprehension},
  booktitle = {Sentence Processing},
  author = {Levy, Roger},
  editor = {van Gompel, Roger P. G.},
  options = {useprefix=true},
  date = {2013},
  pages = {78--114},
  publisher = {{Psychology Press}},
  url = {https://www.mit.edu/ rplevy/papers/levy-2013-memory-and-surprisal-corrected.pdf},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-05-03 14:37:38 -0400},
  file = {/Users/j/Zotero/storage/6LQVKMGP/Levy - 2013 - Memory and surprisal in human sentence comprehensi.pdf}
}

@inproceedings{levy.r:2018cogsci,
  title = {Communicative Efficiency, Uniform Information Density, and the Rational Speech Act Theory.},
  booktitle = {{{CogSci2018}}},
  author = {Levy, Roger},
  date = {2018},
  url = {https://osf.io/9kamx/},
  date-added = {2021-10-18 21:36:04 -0400},
  date-modified = {2021-10-18 21:38:58 -0400}
}

@misc{lew.a:2022RAVI,
  title = {Recursive Monte Carlo and Variational Inference with Auxiliary Variables},
  author = {Lew, Alexander K. and Cusumano-Towner, Marco and Mansinghka, Vikash K.},
  date = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2203.02836},
  url = {https://arxiv.org/abs/2203.02836},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.02836},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-05 09:09:41 -0400},
  date-modified = {2022-05-05 09:12:32 -0400},
  keywords = {monte carlo,recursive auxiliary-variable inference,variational inference},
  file = {/Users/j/Zotero/storage/UHYKBG9B/Lew et al. - 2022 - Recursive monte carlo and variational inference wi.pdf}
}

@article{lewis.m:2014,
  title = {Combined Distributional and Logical Semantics},
  author = {Lewis, Mike and Steedman, Mark},
  date = {2013},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {179--192},
  doi = {10.1162/tacl_a_00219},
  url = {https://www.aclweb.org/anthology/Q13-1015},
  file = {/Users/j/Zotero/storage/TDDALVLI/Lewis and Steedman - 2013 - Combined distributional and logical semantics.pdf}
}

@inproceedings{lewis.m:2019bart,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  date = {2020},
  pages = {7871--7880},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.703},
  url = {https://www.aclweb.org/anthology/2020.acl-main.703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.703}
}

@article{lewis.r:2005,
  title = {An {{Activation-Based Model}} of {{Sentence Processing}} as {{Skilled Memory Retrieval}}},
  author = {Lewis, Richard L. and Vasishth, Shravan},
  date = {2005},
  journaltitle = {Cognitive Science},
  volume = {29},
  number = {3},
  pages = {375--419},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_25},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_25},
  urldate = {2022-07-15},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated control structure that subserve sentence comprehension. The theory is derived from the application of independently motivated principles of memory and cognitive skill to the specialized task of sentence parsing. The resulting theory construes sentence processing as a series of skilled associative memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive principles are formalized in computational form in the Adaptive Control of Thought–Rational (ACT–R) architecture, and our process model is realized in ACT–R. We present the results of 6 sets of simulations: 5 simulation sets provide quantitative accounts of the effects of length and structural interference on both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-embedding difficulty is a novel one that derives from the model' complete reliance on discriminating retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT–R defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference are the key factors shaping working memory in sentence processing. We contrast the theory and empirical predictions with several related accounts of sentence-processing complexity.},
  langid = {english},
  keywords = {ACT-R,Activation,Cognitive architectures,Cognitive modeling,Decay,Interference,Parsing,Sentence processing,Syntax,Working memory},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog0000\_25},
  file = {/Users/j/Zotero/storage/MBNQSLBP/Lewis and Vasishth (2005) An Activation-Based Model of Sentence Processing a.pdf}
}

@article{lewis.r:2006,
  title = {Computational Principles of Working Memory in Sentence Comprehension},
  author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
  date = {2006-10-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {10},
  pages = {447--454},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.08.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661306002142},
  urldate = {2022-09-08},
  abstract = {Understanding a sentence requires a working memory of the partial products of comprehension, so that linguistic relations between temporally distal parts of the sentence can be rapidly computed. We describe an emerging theoretical framework for this working memory system that incorporates several independently motivated principles of memory: a sharply limited attentional focus, rapid retrieval of item (but not order) information subject to interference from similar items, and activation decay (forgetting over time). A computational model embodying these principles provides an explanation of the functional capacities and severe limitations of human processing, as well as accounts of reading times. The broad implication is that the detailed nature of crosslinguistic sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension.},
  langid = {english},
  keywords = {ACT-R,comprehension,memory,sentence processing},
  file = {/Users/j/Zotero/storage/JESGWVM3/Lewis et al. (2006) Computational principles of working memory in sent.pdf}
}

@inproceedings{li.b:2020headsup,
  title = {Heads-up! {{Unsupervised}} Constituency Parsing via Self-Attention Heads},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  author = {Li, Bowen and Kim, Taeuk and Amplayo, Reinald Kim and Keller, Frank},
  date = {2020},
  pages = {409--424},
  publisher = {{Association for Computational Linguistics}},
  location = {{Suzhou, China}},
  url = {https://www.aclweb.org/anthology/2020.aacl-main.43}
}

@thesis{li.b:2022PhD,
  type = {phdthesis},
  title = {Integrating {{Linguistic Theory}} and {{Neural Language Models}}},
  author = {Li, Bai},
  date = {2022-07-20},
  eprint = {2207.09643},
  eprinttype = {arxiv},
  primaryclass = {cs},
  institution = {{University of Toronto}},
  location = {{Toronto}},
  url = {http://arxiv.org/abs/2207.09643},
  urldate = {2022-07-22},
  abstract = {Transformer-based language models have recently achieved remarkable results in many natural language tasks. However, performance on leaderboards is generally achieved by leveraging massive amounts of training data, and rarely by encoding explicit linguistic knowledge into neural models. This has led many to question the relevance of linguistics for modern natural language processing. In this dissertation, I present several case studies to illustrate how theoretical linguistics and neural language models are still relevant to each other. First, language models are useful to linguists by providing an objective tool to measure semantic distance, which is difficult to do using traditional methods. On the other hand, linguistic theory contributes to language modelling research by providing frameworks and sources of data to probe our language models for specific aspects of language understanding. This thesis contributes three studies that explore different aspects of the syntax-semantics interface in language models. In the first part of my thesis, I apply language models to the problem of word class flexibility. Using mBERT as a source of semantic distance measurements, I present evidence in favour of analyzing word class flexibility as a directional process. In the second part of my thesis, I propose a method to measure surprisal at intermediate layers of language models. My experiments show that sentences containing morphosyntactic anomalies trigger surprisals earlier in language models than semantic and commonsense anomalies. Finally, in the third part of my thesis, I adapt several psycholinguistic studies to show that language models contain knowledge of argument structure constructions. In summary, my thesis develops new connections between natural language processing, linguistic theory, and psycholinguistics to provide fresh perspectives for the interpretation of language models.},
  archiveprefix = {arXiv},
  pagetotal = {104},
  version = {1},
  keywords = {Computer Science - Computation and Language}
}

@misc{li.j:2016a,
  title = {Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author = {Li, Jiwei and Jurafsky, Dan},
  date = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1601.00372},
  url = {https://arxiv.org/abs/1601.00372},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1601.00372},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:37:17 -0400},
  date-modified = {2022-05-15 15:40:00 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Zotero/storage/JWLLWW5N/Li and Jurafsky - 2016 - Mutual information and diverse decoding improve ne.pdf}
}

@misc{li.j:2016b,
  title = {A Simple, Fast Diverse Decoding Algorithm for Neural Generation},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  date = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1611.08562},
  url = {https://arxiv.org/abs/1611.08562},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.08562},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:39:25 -0400},
  date-modified = {2022-05-15 15:40:09 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Zotero/storage/NMKZMPD4/Li et al. - 2016 - A simple, fast diverse decoding algorithm for neur.pdf}
}

@book{li.m:2008,
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  author = {Li, Ming and Vitányi, Paul and others},
  date = {2008},
  volume = {3},
  publisher = {{Springer}},
  date-added = {2019-09-13 08:17:22 -0400},
  date-modified = {2019-09-13 08:17:36 -0400},
  project = {information-entropy},
  keywords = {kolmogorov complexity}
}

@inproceedings{li.x:2019,
  title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Li, Xiang Lisa and Eisner, Jason},
  date = {2019},
  pages = {2744--2754},
  publisher = {{Association for Computational Linguistics}},
  location = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1276},
  url = {https://www.aclweb.org/anthology/D19-1276},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1276}
}

@unpublished{li.x:2022,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  date = {2022-05-27},
  number = {arXiv:2205.14217},
  eprint = {2205.14217},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.14217},
  url = {http://arxiv.org/abs/2205.14217},
  urldate = {2022-06-13},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Zotero/storage/L4E57MR4/Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf}
}

@article{liberti.l:2016,
  title = {Six Mathematical Gems from the History of Distance Geometry},
  author = {Liberti, Leo and Lavor, Carlile},
  date = {2016},
  journaltitle = {International Transactions in Operational Research},
  volume = {23},
  number = {5},
  pages = {897--920},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-11 11:26:58 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry}
}

@inproceedings{lin.c:2018,
  title = {Neural Particle Smoothing for Sampling from Conditional Sequence Models},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Lin, Chu-Cheng and Eisner, Jason},
  date = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/n18-1085},
  url = {https://doi.org/10.18653%2Fv1%2Fn18-1085},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n18-1085},
  date-added = {2022-03-31 10:36:15 -0400},
  date-modified = {2022-03-31 10:36:38 -0400},
  keywords = {parsing,sampling}
}

@article{linzen.t:2015,
  title = {Uncertainty and Expectation in Sentence Processing: {{Evidence}} from Subcategorization Distributions},
  author = {Linzen, Tal and Jaeger, T. Florian},
  date = {2015},
  journaltitle = {Cognitive Science},
  volume = {40},
  number = {6},
  pages = {1382--1411},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12274},
  url = {https://doi.org/10.1111%2Fcogs.12274},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12274},
  date-added = {2021-03-18 10:32:01 -0400},
  date-modified = {2021-03-18 10:37:45 -0400},
  keywords = {expectation,processing}
}

@article{linzen.t:2016,
  title = {Assessing the Ability of {{LSTMs}} to Learn Syntax-Sensitive Dependencies},
  author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  date = {2016},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {521--535},
  doi = {10.1162/tacl_a_00115},
  url = {https://www.aclweb.org/anthology/Q16-1037},
  bdsk-url-2 = {https://doi.org/10.1162/taclₐ₀0115},
  file = {/Users/j/Zotero/storage/6XDCR75F/Linzen et al. - 2016 - Assessing the ability of LSTMs to learn syntax-sen.pdf}
}

@unpublished{linzen.t:2018,
  title = {What Can Linguistics and Deep Learning Contribute to Each Other?},
  author = {Linzen, Tal},
  date = {2018},
  eprint = {1809.04179},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  date-added = {2019-06-13 08:03:15 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {recurrent neural networks}
}

@article{lipman.b:1995,
  title = {Information {{Processing}} and {{Bounded Rationality}}: {{A Survey}}},
  shorttitle = {Information {{Processing}} and {{Bounded Rationality}}},
  author = {Lipman, Barton L.},
  date = {1995},
  journaltitle = {The Canadian Journal of Economics / Revue canadienne d'Economique},
  volume = {28},
  number = {1},
  eprint = {136022},
  eprinttype = {jstor},
  pages = {42--67},
  publisher = {{[Wiley, Canadian Economics Association]}},
  issn = {0008-4085},
  doi = {10.2307/136022},
  abstract = {This paper surveys recent attempts to formulate a plausible and tractable model of bounded rationality. I focus in particular on models that view bounded rationality as stemming from limited information processing. I discuss partitional models (such as computability, automata, perceptrons, and optimal networks), non-partitional models, and axiomatic approaches. /// Transformation de l'information et rationalité limitée: une revue de la littérature. Ce mémoire examine certaines tentatives récentes pour formuler un modèle plausible et utilisable de la rationalité limitée. L'auteur s'attache en particulier aux modèles qui présentent la rationalité limitée comme un phénomène émanant de la limitation dans la capacité à transformer l'information. L'auteur discute les modèles qu'on appelle `partitionnels' (computabilité, automates, perceptrons, réseaux optimaux), les modèles `non partitionnels' ainsi que les approches axiomatiques.},
  file = {/Users/j/Zotero/storage/ZDL2QRR9/Lipman - 1995 - Information Processing and Bounded Rationality A .pdf}
}

@article{liu.j:1998,
  title = {Rejection Control and Sequential Importance Sampling},
  author = {Liu, Jun S. and Chen, Rong and Wong, Wing Hung},
  date = {1998-09},
  journaltitle = {Journal of the American Statistical Association},
  volume = {93},
  number = {443},
  pages = {1022--1031},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/01621459.1998.10473764},
  url = {https://doi.org/10.1080%2F01621459.1998.10473764},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1998.10473764},
  date-added = {2022-05-05 09:40:36 -0400},
  date-modified = {2022-05-05 09:42:57 -0400},
  keywords = {importance sampling,rejection controlled sequential importance sampling,sequential importance sampling,sequential monte carlo}
}

@article{liu.j:2017,
  title = {In-Order Transition-Based Constituent Parsing},
  author = {Liu, Jiangming and Zhang, Yue},
  date = {2017},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {413--424},
  doi = {10.1162/tacl_a_00070},
  url = {https://www.aclweb.org/anthology/Q17-1029},
  bdsk-url-2 = {https://doi.org/10.1162/taclₐ₀0070},
  file = {/Users/j/Zotero/storage/5T8Q2YHA/Liu and Zhang - 2017 - In-order transition-based constituent parsing.pdf}
}

@misc{liu.q:2020,
  title = {A Survey on Contextual Embeddings},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  date = {2020},
  eprint = {2003.07278},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {word embeddings}
}

@inproceedings{liu.z:2021,
  title = {Morphological {{Segmentation}} for {{Seneca}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}}},
  author = {Liu, Zoey and Jimerson, Robert and Prud'hommeaux, Emily},
  date = {2021-06},
  pages = {90--101},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2021.americasnlp-1.10},
  url = {https://aclanthology.org/2021.americasnlp-1.10},
  urldate = {2022-06-06},
  abstract = {This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning.},
  eventtitle = {{{AmericasNLP-NAACL}} 2021},
  keywords = {computational revitalization,iroquoian,morphology},
  file = {/Users/j/Zotero/storage/XPP5CDYA/Liu et al. - 2021 - Morphological Segmentation for Seneca.pdf}
}

@article{lo.s:2015,
  title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
  author = {Lo, Steson and Andrews, Sally},
  date = {2015-08},
  journaltitle = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers Media SA}},
  doi = {10.3389/fpsyg.2015.01171},
  url = {https://doi.org/10.3389%2Ffpsyg.2015.01171},
  bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2015.01171},
  date-added = {2022-02-23 22:30:34 -0500},
  date-modified = {2022-02-23 22:30:36 -0500},
  file = {/Users/j/Zotero/storage/ZK6XN5P6/Lo and Andrews - 2015 - To transform or not to transform using generalize.pdf}
}

@article{lomashvili.l:2011,
  title = {Phases and Templates in {{Georgian}} Agreement},
  author = {Lomashvili, Leila and Harley, Heidi},
  date = {2011},
  journaltitle = {Studia Linguistica},
  volume = {65},
  number = {3},
  pages = {233--267},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:50 -0400},
  project = {Icelandic gluttony},
  keywords = {phase theory,phi features}
}

@misc{lou.p:2018,
  title = {Disfluency Detection Using a Noisy Channel Model and a Deep Neural Language Model},
  author = {Lou, Paria Jamshid and Johnson, Mark},
  date = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1808.09091},
  url = {https://arxiv.org/abs/1808.09091},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.09091},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-27 10:29:36 -0400},
  date-modified = {2022-04-27 10:29:37 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/367YWNTS/Lou and Johnson - 2018 - Disfluency detection using a noisy channel model a.pdf}
}

@incollection{lounsbury.f:1954,
  title = {Transitional Probability, Linguistic Structure, and Systems of Habit-Family Hierarchies},
  booktitle = {Psycholinguistics},
  author = {Lounsbury, Floyd G},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  date = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {93--101},
  publisher = {{Waverly Press Baltimore}},
  url = {https://publish.iupress.indiana.edu/read/db9a6002-fd15-44c0-a60e-65b6af037d2b/section/cebab226-583c-4176-a54c-0c8461c45bbc#fn47},
  date-added = {2022-04-14 23:38:02 -0400},
  date-modified = {2022-04-14 23:51:56 -0400},
  keywords = {entropy reduction}
}

@inproceedings{lueckmann.j:2021,
  title = {Benchmarking Simulation-Based Inference},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {{AISTATS}} 2021, April 13-15, 2021, Virtual Event},
  author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David S. and Gonçalves, Pedro J. and Macke, Jakob H.},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  date = {2021},
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {343--351},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v130/lueckmann21a.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aistats/LueckmannBGGM21.bib},
  timestamp = {Wed, 14 Apr 2021 01:00:00 +0200}
}

@article{luke.s:2017,
  title = {The {{Provo Corpus}}: {{A}} Large Eye-Tracking Corpus with Predictability Norms},
  author = {Luke, Steven G. and Christianson, Kiel},
  date = {2017-05},
  volume = {50},
  number = {2},
  pages = {826--833},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-017-0908-4},
  url = {https://doi.org/10.3758%2Fs13428-017-0908-4},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-017-0908-4},
  date-added = {2021-10-19 00:07:37 -0400},
  date-modified = {2021-10-19 00:07:38 -0400}
}

@inproceedings{luong.t:2015,
  title = {Evaluating Models of Computation and Storage in Human Sentence Processing},
  booktitle = {Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning},
  author = {Luong, Thang and O'Donnell, Timothy and Goodman, Noah},
  date = {2015-09},
  pages = {14--21},
  publisher = {{Association for Computational Linguistics}},
  location = {{Lisbon, Portugal}},
  doi = {10.18653/v1/W15-2403},
  url = {https://aclanthology.org/W15-2403},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W15-2403},
  date-added = {2022-05-02 11:30:20 -0400},
  date-modified = {2022-05-17 08:07:37 -0400},
  keywords = {fragment grammars,incrementality,parsing}
}

@book{lurie.j:2009,
  title = {Higher Topos Theory (Preprint)},
  author = {Lurie, Jacob},
  date = {2009},
  publisher = {{Princeton University Press}},
  date-added = {2019-08-24 09:21:19 -0400},
  date-modified = {2019-08-24 09:23:18 -0400},
  keywords = {category theory,topos theory}
}

@book{mackay.d:2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David J. C.},
  date = {2003},
  publisher = {{Cambridge university press}},
  url = {https://www.inference.org.uk/itila/},
  date-added = {2020-02-16 21:05:41 -0500},
  date-modified = {2020-04-29 12:55:23 -0400},
  project = {information-entropy},
  keywords = {information theory}
}

@inproceedings{madureira.b:2020,
  title = {Incremental Processing in the Age of Non-Incremental Encoders: {{An}} Empirical Assessment of Bidirectional Models for Incremental {{NLU}}},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Madureira, Brielen and Schlangen, David},
  date = {2020},
  pages = {357--374},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.26},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.26},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.26}
}

@article{maehara.h:2013,
  title = {Euclidean Embeddings of Finite Metric Spaces},
  author = {Maehara, Hiroshi},
  date = {2013},
  journaltitle = {Discrete Mathematics},
  volume = {313},
  number = {23},
  pages = {2848--2856},
  publisher = {{Elsevier}},
  date-added = {2019-06-13 07:52:44 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {euclidean space,geometry}
}

@inproceedings{magerman.d:1990,
  title = {Parsing a Natural Language Using Mutual Information Statistics.},
  booktitle = {{{AAAI}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  date = {1990},
  volume = {90},
  pages = {984--989},
  url = {https://www.aaai.org/Library/AAAI/1990/aaai90-147.php},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:27:23 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,word association}
}

@inproceedings{magerman.d:1991,
  title = {\emph{P}earl: A Probabilistic Chart Parser},
  shorttitle = {\emph{P}earl},
  booktitle = {Proceedings of the Fifth Conference on {{European}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  date = {1991-04-09},
  series = {{{EACL}} '91},
  pages = {15--20},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  doi = {10.3115/977180.977184},
  url = {http://doi.org/10.3115/977180.977184},
  urldate = {2022-06-13},
  abstract = {This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.},
  file = {/Users/j/Zotero/storage/AF9TTST8/Magerman and Marcus - 1991 - Pearl a probabilistic chart parser.pdf}
}

@article{makkeh.a:2021,
  title = {Introducing a Differentiable Measure of Pointwise Shared Information},
  author = {Makkeh, Abdullah and Gutknecht, Aaron J. and Wibral, Michael},
  date = {2021-03},
  journaltitle = {Physical Review E},
  volume = {103},
  number = {3},
  publisher = {{American Physical Society (APS)}},
  doi = {10.1103/physreve.103.032149},
  url = {https://doi.org/10.1103%2Fphysreve.103.032149},
  bdsk-url-2 = {https://doi.org/10.1103/physreve.103.032149},
  date-added = {2022-04-18 11:15:53 -0400},
  date-modified = {2022-04-18 11:16:04 -0400},
  keywords = {partial information decomposition}
}

@book{malchukov.a:2012,
  title = {The Oxford Handbook of Case},
  author = {Malchukov, Andrej L. and Spencer, Andrew},
  date = {2012},
  publisher = {{Oxford University Press}},
  url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199206476.001.0001/oxfordhb-9780199206476},
  date-added = {2020-02-03 16:08:35 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  isbn = {978-0-19-920647-6},
  project = {Icelandic gluttony},
  keywords = {case}
}

@article{manning.c:2020,
  title = {Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision},
  author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  date = {2020},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30046--30054},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1907367117},
  url = {https://doi.org/10.1073%2Fpnas.1907367117},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1907367117},
  date-added = {2021-07-16 19:46:55 -0400},
  date-modified = {2021-07-16 19:46:57 -0400}
}

@inproceedings{mansinghka.v:2009,
  title = {Exact and Approximate Sampling by Systematic Stochastic Search},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  author = {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},
  editor = {van Dyk, David and Welling, Max},
  options = {useprefix=true},
  date = {2009-04-16/2009-04-18},
  series = {Proceedings of Machine Learning Research},
  volume = {5},
  pages = {400--407},
  publisher = {{PMLR}},
  location = {{Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA}},
  url = {https://proceedings.mlr.press/v5/mansinghka09a.html},
  abstract = {We introduce ₐdaptive sequential rejection sampling\textsubscript{,} an algorithm for generating exact samples from high-dimensional, discrete distributions, building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions, sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation, and a randomized version of the directed arc consistency algorithm with backtracking when applied to deterministic problems. In this paper, we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on ferromagnetic Isings and other probabilistic graphical models, obtaining exact and approximate samples in a range of situations.},
  date-added = {2022-05-05 09:38:21 -0400},
  date-modified = {2022-05-05 09:39:35 -0400},
  pdf = {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},
  keywords = {adaptive sequential rejection sampling}
}

@thesis{marcken.c:1996,
  title = {Unsupervised Language Acquisition},
  author = {de Marcken, Carl},
  options = {useprefix=true},
  date = {1996},
  institution = {{Massachusetts Institute of Technology, Cambridge, MA, USA}},
  url = {http://hdl.handle.net/1721.1/10640},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/phd/ndltd/Marcken96},
  date-added = {2020-01-27 11:53:05 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {information theory,unsupervised grammar induction},
  timestamp = {Mon, 08 May 2017 16:29:45 +0200}
}

@incollection{marcken.c:1999,
  title = {On the Unsupervised Induction of Phrase-Structure Grammars},
  booktitle = {Natural Language Processing Using Very Large Corpora},
  author = {de Marcken, C.},
  editor = {Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  options = {useprefix=true},
  date = {1999},
  pages = {191--208},
  publisher = {{Springer Netherlands}},
  location = {{Dordrecht}},
  doi = {10.1007/978-94-017-2390-9_12},
  url = {https://doi.org/10.1007/978-94-017-2390-9₁2},
  abstract = {Researchers investigating the acquisition of phrase-structure grammars from raw text have had only mixed success. In particular, unsupervised learning techniques, such as the inside-outside algorithm (Baker, 1979) for estimating the parameters of stochastic context-free grammars (SCFGs), tend to produce grammars that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to constrain the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint, and currently the best performing parsers are trained on treebanks (Black et al., 1992; Magerman, 1995).},
  date-added = {2020-01-27 11:50:06 -0500},
  date-modified = {2021-07-16 11:27:47 -0400},
  isbn = {978-94-017-2390-9},
  project = {syntactic embedding},
  keywords = {information theory,unsupervised grammar induction}
}

@thesis{marcus.m:1978phd,
  title = {A Theory of Syntactic Recognition for Natural Language.},
  author = {Marcus, Mitchell P.},
  date = {1978},
  institution = {{Massachusetts Institute of Technology}},
  url = {http://hdl.handle.net/1721.1/16176},
  date-added = {2022-03-31 11:14:02 -0400},
  date-modified = {2022-04-26 21:21:13 -0400},
  file = {/Users/j/Zotero/storage/CQ6QYUBY/Marcus - 1978 - A theory of syntactic recognition for natural lang.pdf}
}

@book{marcus.m:1980phdbook,
  title = {Theory of Syntactic Recognition for Natural Languages},
  author = {Marcus, Mitchell P.},
  date = {1980},
  publisher = {{MIT Press}},
  location = {{Cambridge, MA, USA}},
  date-added = {2022-03-31 11:15:48 -0400},
  date-modified = {2022-04-26 21:21:21 -0400},
  isbn = {0-262-13149-8}
}

@inproceedings{marcus.m:1994,
  title = {The {{Penn Treebank}}: {{Annotating}} Predicate Argument Structure},
  booktitle = {Human {{Language Technology}}: {{Proceedings}} of a {{Workshop}} Held at {{Plainsboro}}, {{New Jersey}}, {{March}} 8-11, 1994},
  author = {Marcus, Mitchell P. and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  date = {1994},
  url = {https://www.aclweb.org/anthology/H94-1020}
}

@thesis{marecek.d:2012,
  title = {Unsupervised Dependency Parsing},
  author = {Mareček, David},
  date = {2012},
  institution = {{Charles University}},
  location = {{Prague, Czech Republic}},
  url = {http://ufal.mff.cuni.cz/biblio/attachments/2012-marecek-m1481417340536440366.pdf},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-09-07 16:53:21 -0400},
  pagetotal = {118},
  project = {syntactic embedding},
  keywords = {dependency parsing,unsupervised parsing},
  file = {/Users/j/Zotero/storage/32CIN6HJ/Mareček - 2012 - Unsupervised dependency parsing.pdf}
}

@inproceedings{marecek.d:2018,
  title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
  booktitle = {Proceedings of the 2018 {{EMNLP}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mareček, David and Rosa, Rudolf},
  date = {2018-11},
  pages = {347--349},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5444},
  url = {https://aclanthology.org/W18-5444},
  abstract = {This is a work in progress about extracting the sentence tree structures from the encoder's self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W18-5444},
  date-added = {2021-09-08 00:32:46 -0400},
  date-modified = {2021-09-08 00:32:47 -0400}
}

@inproceedings{marecek.d:2019,
  title = {From Balustrades to Pierre Vinken: {{Looking}} for Syntax in Transformer Self-Attentions},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mareček, David and Rosa, Rudolf},
  date = {2019},
  pages = {263--275},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4827},
  url = {https://www.aclweb.org/anthology/W19-4827},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4827}
}

@article{marneffe.m:2019,
  title = {Dependency Grammar},
  author = {de Marneffe, Marie-Catherine and Nivre, Joakim},
  options = {useprefix=true},
  date = {2019},
  journaltitle = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {197--218},
  publisher = {{Annual Reviews}},
  doi = {10.1146/annurev-linguistics-011718-011842},
  url = {https://doi.org/10.1146%2Fannurev-linguistics-011718-011842},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011842},
  date-added = {2021-07-16 19:34:18 -0400},
  date-modified = {2021-07-16 19:34:19 -0400}
}

@article{marr.d:1976,
  title = {From {{Understanding Computation}} to {{Understanding Neural Circuitry}}},
  author = {Marr, D. and Poggio, T.},
  date = {1976-05-01},
  url = {https://dspace.mit.edu/handle/1721.1/5782},
  urldate = {2022-06-06},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  langid = {american},
  annotation = {Accepted: 2004-10-01T20:36:50Z},
  file = {/Users/j/Zotero/storage/6D4ZPKH7/Marr and Poggio - 1976 - From Understanding Computation to Understanding Ne.pdf}
}

@book{marr.d:1982,
  title = {Vision: {{A}} Computational Investigation into the Human Representation and Processing of Visual Information},
  author = {Marr, David},
  date = {1982},
  publisher = {{W. H. Freeman}},
  location = {{San Francisco, CA}},
  date-added = {2021-12-01 19:20:38 -0500},
  date-modified = {2021-12-01 19:21:38 -0500}
}

@article{marslen-wilson.w:1973,
  title = {Linguistic Structure and Speech Shadowing at Very Short Latencies},
  author = {Marslen-Wilson, William D.},
  date = {1973-08},
  journaltitle = {Nature},
  volume = {244},
  number = {5417},
  pages = {522--523},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/244522a0},
  url = {https://doi.org/10.1038%2F244522a0},
  bdsk-url-2 = {https://doi.org/10.1038/244522a0},
  date-added = {2022-04-14 13:38:15 -0400},
  date-modified = {2022-05-02 14:45:30 -0400},
  keywords = {incrementality},
  file = {/Users/j/Zotero/storage/XDKKSWUI/Marslen-Wilson - 1973 - Linguistic structure and speech shadowing at very .pdf}
}

@article{marslen-wilson.w:1975,
  title = {Sentence Perception as an Interactive Parallel Process},
  author = {Marslen-Wilson, William D.},
  date = {1975-07},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {189},
  number = {4198},
  pages = {226--228},
  publisher = {{American Association for the Advancement of Science (AAAS)}},
  doi = {10.1126/science.189.4198.226},
  url = {https://doi.org/10.1126%2Fscience.189.4198.226},
  bdsk-url-2 = {https://doi.org/10.1126/science.189.4198.226},
  date-added = {2022-04-14 13:38:57 -0400},
  date-modified = {2022-05-02 14:45:37 -0400},
  keywords = {incrementality},
  file = {/Users/j/Zotero/storage/AZY7WBPS/Marslen-Wilson - 1975 - Sentence perception as an interactive parallel pro.pdf}
}

@incollection{martino.l:2018,
  title = {Adaptive {{Rejection Sampling Methods}}},
  booktitle = {Independent {{Random Sampling Methods}}},
  author = {Martino, Luca and Luengo, David and Míguez, Joaquín},
  editor = {Martino, Luca and Luengo, David and Míguez, Joaquín},
  date = {2018},
  series = {Statistics and {{Computing}}},
  pages = {115--157},
  publisher = {{Springer International Publishing}},
  location = {{Cham}},
  doi = {10.1007/978-3-319-72634-2_4},
  url = {https://doi.org/10.1007/978-3-319-72634-2_4},
  urldate = {2022-07-05},
  abstract = {This chapter is devoted to describing the class of the adaptive rejection sampling (ARS) schemes. These (theoretically) universal methods are very efficient samplers that update the proposal density whenever a generated sample is rejected in the RS test. In this way, they can produce i.i.d. samples from the target with an increasing acceptance rate that can converge to 1. As a by-product, these techniques also generate a sequence of proposal pdfs converging to the true shape of the target density. Another advantage of the ARS samplers is that, when they can be applied, the user only has to select a set of initial conditions. After the initialization, they are completely automatic, self-tuning algorithms (i.e., no parameters need to be adjusted by the user) regardless of the specific target density. However, the need to construct a suitable sequence of proposal densities restricts the practical applicability of this methodology. As a consequence, ARS schemes are often tailored to specific classes of target distributions. Indeed, the construction of the proposal is particularly hard in multidimensional spaces. Hence, ARS algorithms are usually designed only for drawing from univariate densities.},
  isbn = {978-3-319-72634-2},
  langid = {english},
  file = {/Users/j/Zotero/storage/LF75KLR6/Martino et al. - 2018 - Adaptive Rejection Sampling Methods.pdf}
}

@inproceedings{marvin.r:2018,
  title = {Targeted Syntactic Evaluation of Language Models},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Marvin, Rebecca and Linzen, Tal},
  date = {2018},
  pages = {1192--1202},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1151},
  url = {https://www.aclweb.org/anthology/D18-1151},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1151}
}

@unpublished{marvin.r:2018a,
  title = {Targeted {{Syntactic Evaluation}} of {{Language Models}}},
  author = {Marvin, Rebecca and Linzen, Tal},
  date = {2018-08-27},
  eprint = {1808.09031},
  eprinttype = {arxiv},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.09031},
  urldate = {2020-02-16},
  abstract = {We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5NMKJ8NZ/Marvin and Linzen - 2018 - Targeted Syntactic Evaluation of Language Models.pdf}
}

@article{mazur.b:2008,
  title = {When Is One Thing Equalto Some Other Thing?},
  author = {Mazur, Barry},
  date = {2008},
  journaltitle = {Proof and other dilemmas: Mathematics and philosophy},
  volume = {59},
  pages = {221},
  publisher = {{MAA}},
  date-added = {2019-08-24 09:29:47 -0400},
  date-modified = {2019-08-24 09:29:57 -0400},
  keywords = {category theory}
}

@inproceedings{mccann.b:2017,
  title = {Learned in Translation: {{Contextualized}} Word Vectors},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, {{CA}}, {{USA}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  options = {useprefix=true},
  date = {2017},
  pages = {6294--6305},
  url = {https://proceedings.neurips.cc/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/McCannBXS17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@incollection{mcconnell-ginet.s:2000,
  title = {Meaning and Grammar: {{An}} Introduction to Semantics},
  booktitle = {Meaning and Grammar: {{An}} Introduction to Semantics},
  author = {McConnell-Ginet, Sally and Chierchia, Gennaro},
  date = {2000},
  publisher = {{MIT Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{mcdonald.m:1977,
  title = {Iontenwennaweienstahkhwa' Mohawk Spelling Dictionary},
  author = {McDonald, Mary and Barnes, Ann and Cook, Louise and Herne, Jean and Jacobs, Rita and Jock, Louise and LaFrance, Harriett and Ransom, Elaine and Sinclair, Winnie and Tarbell, Elizabeth},
  editor = {Mithun, Marianne},
  date = {1977-09},
  number = {Bulletin 429},
  publisher = {{The University of the State of New York, State Education Department}},
  location = {{Albany, N.Y.}},
  url = {http://kanienkeha.net/wp-content/uploads/2015/10/Mohawk-Spelling-Dictionary.pdf},
  date-added = {2022-05-11 11:20:30 -0400},
  date-modified = {2022-05-11 11:26:42 -0400},
  keywords = {kanien'keha},
  file = {/Users/j/Zotero/storage/C5Q2JARF/McDonald et al. - 1977 - Iontenwennaweienstahkhwa' mohawk spelling dictiona.pdf}
}

@inproceedings{mcdonald.r:2005,
  title = {Non-Projective Dependency Parsing Using Spanning Tree Algorithms},
  booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Hajič, Jan},
  date = {2005},
  pages = {523--530},
  publisher = {{Association for Computational Linguistics}},
  location = {{Vancouver, British Columbia, Canada}},
  url = {https://www.aclweb.org/anthology/H05-1066}
}

@inproceedings{mcdonald.r:2005a,
  title = {Online Large-Margin Training of Dependency Parsers},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({{ACL}}'05)},
  author = {McDonald, Ryan and Crammer, Koby and Pereira, Fernando},
  date = {2005},
  pages = {91--98},
  publisher = {{Association for Computational Linguistics}},
  location = {{Ann Arbor, Michigan}},
  doi = {10.3115/1219840.1219852},
  url = {https://www.aclweb.org/anthology/P05-1012},
  bdsk-url-2 = {https://doi.org/10.3115/1219840.1219852}
}

@article{mcdonald.s:2003,
  title = {Low-Level Predictive Inference in Reading: The Influence of Transitional Probabilities on Eye Movements},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  date = {2003-07},
  journaltitle = {Vision Research},
  volume = {43},
  number = {16},
  pages = {1735--1751},
  publisher = {{Elsevier BV}},
  doi = {10.1016/s0042-6989(03)00237-2},
  url = {https://doi.org/10.1016%2Fs0042-6989%2803%2900237-2},
  bdsk-url-2 = {https://doi.org/10.1016/s0042-6989(03)00237-2},
  date-added = {2022-05-15 22:54:21 -0400},
  date-modified = {2022-05-15 22:54:23 -0400}
}

@article{mcfadden.t:2018,
  title = {What the {{EPP}} and Comp-Trace Effects Have in Common: {{Constraining}} Silent Elements at the Edge},
  author = {McFadden, Thomas and Sundaresan, Sandhya},
  date = {2018},
  journaltitle = {Glossa: a journal of general linguistics},
  volume = {3},
  number = {1},
  publisher = {{Ubiquity Press}},
  date-added = {2020-02-02 08:05:04 -0500},
  date-modified = {2020-02-02 08:05:55 -0500},
  keywords = {EPP}
}

@misc{mcguffie.k:2020,
  title = {The Radicalization Risks of {{GPT-3}} and Advanced Neural Language Models},
  author = {McGuffie, Kris and Newhouse, Alex},
  date = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2009.06807},
  url = {https://arxiv.org/abs/2009.06807},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.06807},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-15 11:01:44 -0400},
  date-modified = {2022-03-15 11:01:46 -0400},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/Q8BIE2E9/McGuffie and Newhouse - 2020 - The radicalization risks of GPT-3 and advanced neu.pdf}
}

@article{mehta.p:2019,
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  date = {2019-05-30},
  journaltitle = {Physics Reports},
  shortjournal = {Physics Reports},
  series = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  volume = {810},
  pages = {1--124},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0370157319300766},
  urldate = {2022-07-11},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias–variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton–proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
  langid = {english},
  keywords = {boltzmann machines,energy models},
  file = {/Users/j/Zotero/storage/9B8KKHJB/Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine .pdf}
}

@inproceedings{meister.c:2020,
  title = {If Beam Search Is the Answer, What Was the Question?},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
  date = {2020},
  pages = {2173--2185},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.170},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.170},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.170},
  date-modified = {2022-03-31 09:40:23 -0400},
  keywords = {beam search,parsing}
}

@article{meister.c:2020tacl,
  title = {Best-First Beam Search},
  author = {Meister, Clara and Vieira, Tim and Cotterell, Ryan},
  date = {2020-12},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {795--809},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/tacl_a_00346},
  date-added = {2022-03-31 09:48:44 -0400},
  date-modified = {2022-03-31 09:55:48 -0400},
  keywords = {beam search,memory,parsing,space-complexity,time-complexity},
  file = {/Users/j/Zotero/storage/CBY2CCF3/Meister et al. - 2020 - Best-first beam search.pdf}
}

@inproceedings{meister.c:2021,
  title = {Revisiting the Uniform Information Density Hypothesis},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Meister, Clara and Pimentel, Tiago and Haller, Patrick and Jäger, Lena and Cotterell, Ryan and Levy, Roger},
  date = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.emnlp-main.74},
  url = {https://doi.org/10.18653%2Fv1%2F2021.emnlp-main.74},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.74},
  date-added = {2022-04-15 16:06:48 -0400},
  date-modified = {2022-04-15 16:06:50 -0400}
}

@book{melcuk.i:1988,
  title = {Dependency Syntax : {{Theory}} and Practice},
  author = {Mel'čuk, Igor A.},
  date = {1988},
  series = {{{SUNY}} Series in Linguistics},
  publisher = {{State University of New York Press}},
  location = {{Albany, N.Y.}},
  url = {http://www.sunypress.edu/p-164-dependency-syntax.aspx},
  abstract = {This work presents the first sustained examination of Dependency Syntax. In clear and stimulating analyses Mel'cuk promotes syntactic description in terms of dependency rather than in terms of more familiar phrase-structure. The notions of dependency relations and dependency structure are introduced and substantiated, and the advantages of dependency representation are demonstrated by applying it to a number of popular linguistic problems, e.g. grammatical subject and ergative construction. A wide array of linguistic data is used – the well-known (Dyirbal), the less known (Lezgian), and the more recent (Alutor). Several "exotic" cases of Russian are discussed to show how dependency can be used to solve difficult technical problems. The book is not only formal and rigorous, but also strongly theory-oriented and data-based. Special attention is paid to linguistic terminology, specifically to its logical consistency. The dependency formalism is presented within the framework of a new semantics-oriented general linguistic theory, Meaning-Text theory.},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-17 10:33:46 -0400},
  isbn = {978-0-88706-450-0},
  keywords = {Dependency Grammar}
}

@article{menne.m:2012,
  title = {An Overview of the Global Historical Climatology Network-Daily Database},
  author = {Menne, Matthew J. and Durre, Imke and Vose, Russell S. and Gleason, Byron E. and Houston, Tamara G.},
  date = {2012-07},
  journaltitle = {Journal of Atmospheric and Oceanic Technology},
  volume = {29},
  number = {7},
  pages = {897--910},
  publisher = {{American Meteorological Society}},
  doi = {10.1175/jtech-d-11-00103.1},
  url = {https://doi.org/10.1175%2Fjtech-d-11-00103.1},
  bdsk-url-2 = {https://doi.org/10.1175/jtech-d-11-00103.1},
  date-added = {2021-12-03 19:02:07 -0500},
  date-modified = {2021-12-03 19:02:09 -0500}
}

@inproceedings{merkx.d:2021,
  title = {Human Sentence Processing: {{Recurrence}} or Attention?},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Merkx, Danny and Frank, Stefan L.},
  date = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.cmcl-1.2},
  url = {https://doi.org/10.18653%2Fv1%2F2021.cmcl-1.2},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.2},
  date-added = {2021-11-29 10:15:11 -0500},
  date-modified = {2021-11-29 10:15:12 -0500}
}

@article{meylan.s:2021,
  title = {The Challenges of Large-Scale, Web-Based Language Datasets: {{Word}} Length and Predictability Revisited},
  author = {Meylan, Stephan C. and Griffiths, Thomas L.},
  date = {2021},
  journaltitle = {Cognitive Science},
  volume = {45},
  number = {6},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12983},
  url = {https://doi.org/10.1111%2Fcogs.12983},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12983},
  date-added = {2021-07-25 10:58:38 -0400},
  date-modified = {2021-07-25 10:58:39 -0400}
}

@book{michelson.k:2016,
  title = {Iroquoian {{Languages}}},
  author = {Michelson, Karin},
  date = {2016-08-05},
  publisher = {{Oxford University Press}},
  doi = {10.1093/acrefore/9780199384655.013.47},
  url = {https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-47},
  urldate = {2022-05-30},
  abstract = {The Iroquoian languages are spoken today in New York State, Ontario, Quebec, Wisconsin, North Carolina, and Oklahoma. The languages share a relatively small segment inventory, a challenging accentual system, polysynthetic morphology, a complex system of pronominal affixes, an unusual kinship terminology, and a syntax that functions almost exclusively to combine the meaning of two expressions. Some of the languages have been documented since contact with Europeans in the 16th century. There exists substantial scholarly linguistic work on most of the languages, and solid teaching materials continue to be developed.},
  isbn = {978-0-19-938465-5},
  langid = {english},
  keywords = {iroquoian}
}

@inproceedings{mikolov.t:2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. {{Proceedings}} of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  author = {Mikolov, Tomás and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
  editor = {Burges, Christopher J. C. and Bottou, Léon and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  date = {2013},
  pages = {3111--3119},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{mikolov.t:2013a,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  date = {2013},
  pages = {746--751},
  publisher = {{Association for Computational Linguistics}},
  location = {{Atlanta, Georgia}},
  url = {https://www.aclweb.org/anthology/N13-1090}
}

@article{miller.g:1957,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information.},
  shorttitle = {The Magical Number Seven, plus or Minus Two},
  author = {Miller, George A.},
  date = {1957-02-01},
  journaltitle = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {81},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/h0043158},
  url = {https://psycnet.apa.org/fulltext/1957-02914-001.pdf},
  urldate = {2022-09-25},
  file = {/Users/j/Zotero/storage/2F7M8P8S/Miller (The magical number seven, plus or minus two Some .pdf}
}

@incollection{miller.g:1963,
  title = {Finitary Models of Language Users},
  booktitle = {Handbook of Mathematical Psychology},
  author = {Miller, George A. and Chomsky, Noam},
  editor = {Luce, D.},
  date = {1963},
  pages = {2--419},
  publisher = {{John Wiley \& Sons.}},
  url = {https://www.semanticscholar.org/paper/Finitary-models-of-language-users-Miller-Chomsky/4f3695d5dd36bb0abd91c02d2725463fca556f46},
  date-added = {2022-03-31 11:48:29 -0400},
  date-modified = {2022-03-31 11:48:31 -0400}
}

@inproceedings{milward.d:1995,
  title = {Incremental Interpretation of Categorial Grammar},
  booktitle = {Seventh Conference of the {{European}} Chapter of the Association for Computational Linguistics},
  author = {Milward, David},
  date = {1995},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://www.aclweb.org/anthology/E95-1017}
}

@inproceedings{mitchell.j:2010,
  title = {Syntactic and Semantic Factors in Processing Difficulty: {{An}} Integrated Measure},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Mitchell, Jeff and Lapata, Mirella and Demberg, Vera and Keller, Frank},
  date = {2010},
  pages = {196--206},
  publisher = {{Association for Computational Linguistics}},
  location = {{Uppsala, Sweden}},
  url = {https://www.aclweb.org/anthology/P10-1021}
}

@incollection{mithun.m:2014,
  title = {Syntactic and Prosodic Structures: {{Segmentation}}, Integration, and in Between},
  booktitle = {Spoken {{Corpora}} and {{Linguistic Studies}}},
  author = {Mithun, Marianne},
  editor = {Raso, Tommaso and Mello, Heliana},
  date = {2014},
  series = {Studies in {{Corpus Linguistics}}},
  volume = {61},
  pages = {297--330},
  publisher = {{John Benjamins Publishing Company}},
  url = {http://mithun.faculty.linguistics.ucsb.edu/pdfs/Mithun%202014%20Syntactic%20and%20prosodic%20structures.pdf},
  abstract = {In this paper the focus is on syntactic and prosodic structures in a language that is typologically quite different from the majority languages of Europe and Asia. Mohawk, a language of the Iroquoian family, is indigenous to northeastern North America. Examples cited here are drawn from unscripted conversations. Though much of the grammatical structure of Mohawk differs substantially from that of European languages, many of the devices exploited by speakers to shape the flow of information converge.},
  langid = {english},
  keywords = {iroquoian},
  file = {/Users/j/Zotero/storage/IZGCP3WT/Mithun - Syntactic and prosodic structures.pdf}
}

@incollection{mithun.m:2020,
  title = {Discourse Particle Position and Information Structure},
  booktitle = {Information-{{Structural Perspectives}} on {{Discourse Particles}}},
  author = {Mithun, Marianne},
  editor = {Modicom, Pierre-Yves and Duplâtre, Olivier},
  date = {2020-03-04},
  series = {Studies in {{Language Companion Series}}},
  number = {213},
  pages = {27--46},
  publisher = {{John Benjamins Publishing Company}},
  doi = {10.1075/slcs.213.01mit},
  url = {https://benjamins.com/catalog/slcs.213.01mit},
  urldate = {2022-05-30},
  abstract = {Discourse markers differ cross-linguistically not only in their functions but also in their positions within the sentence. Some are sentence-initial, some are sentence-final, and some occur in what has been termed the ‘middle-field’. But many appear simply in second position in the sentence. In many cases the positions of the markers can be explained in terms of the source constructions from which they emerged. Here one likely pathway of development is traced in Mohawk, indigenous to North America, illustrated with a pervasive marker of discourse coherence. Patterns in the modern language suggest that it and others emerged from marked information structures, which, over time, evolved into basic clause structures via familiar mechanisms of grammaticalization.},
  langid = {english},
  keywords = {iroquoian}
}

@article{mollica.f:2017,
  title = {How {{Data Drive Early Word Learning}}: {{A Cross-Linguistic Waiting Time Analysis}}},
  author = {Mollica, Francis and Piantadosi, Steven T.},
  date = {2017-09-01},
  journaltitle = {Open Mind},
  shortjournal = {Open Mind},
  volume = {1},
  number = {2},
  pages = {67--77},
  issn = {2470-2986},
  doi = {10.1162/OPMI_a_00006},
  url = {https://doi.org/10.1162/OPMI_a_00006},
  urldate = {2022-09-28},
  abstract = {The extent to which word learning is delayed by maturation as opposed to accumulating data is a longstanding question in language acquisition. Further, the precise way in which data influence learning on a large scale is unknown—experimental results reveal that children can rapidly learn words from single instances as well as by aggregating ambiguous information across multiple situations. We analyze Wordbank, a large cross-linguistic dataset of word acquisition norms, using a statistical waiting time model to quantify the role of data in early language learning, building off Hidaka (2013). We find that the model both fits and accurately predicts the shape of children’s growth curves. Further analyses of model parameters suggest a primarily data-driven account of early word learning. The parameters of the model directly characterize both the amount of data required and the rate at which informative data occurs. With high statistical certainty, words require on the order of ∼ 10 learning instances, which occur on average once every two months. Our method is extremely simple, statistically principled, and broadly applicable to modeling data-driven learning effects in development.},
  file = {/Users/j/Zotero/storage/PLAJ32R8/Mollica and Piantadosi (2017) How Data Drive Early Word Learning A Cross-Lingui.pdf}
}

@article{montague.r:1970,
  title = {Universal Grammar},
  author = {Montague, Richard},
  date = {1970},
  journaltitle = {Theoria: a Swedish journal of philosophy and psychology},
  shortjournal = {Theoria},
  volume = {36},
  number = {3},
  pages = {373--398},
  doi = {10.1111/j.1755-2567.1970.tb00434.x},
  url = {https://doi.org/10.1111/j.1755-2567.1970.tb00434.x},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@incollection{moortgat.m:1997,
  title = {Categorial Type Logics},
  booktitle = {Handbook of Logic and Language},
  author = {Moortgat, M.},
  date = {1997},
  pages = {93--177},
  publisher = {{Elsevier}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:01 -0400}
}

@article{murray.w:2004,
  title = {Serial Mechanisms in Lexical Access: The Rank Hypothesis.},
  author = {Murray, Wayne S and Forster, Kenneth I},
  date = {2004},
  journaltitle = {Psychological Review},
  volume = {111},
  number = {3},
  pages = {721},
  publisher = {{American Psychological Association}},
  doi = {10.1037/0033-295X.111.3.721},
  url = {https://doi.org/10.1037/0033-295X.111.3.721},
  date-added = {2021-02-16 16:15:20 -0500},
  date-modified = {2021-02-16 16:16:45 -0500},
  keywords = {frequency effects,psycholinguistics,rank hypothesis,word access},
  file = {/Users/j/Zotero/storage/UQ9RCT8E/Murray and Forster - 2004 - Serial mechanisms in lexical access the rank hypo.pdf}
}

@misc{naesseth.c:2017VSMC,
  title = {Variational Sequential Monte Carlo},
  author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
  date = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1705.11140},
  url = {https://arxiv.org/abs/1705.11140},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.11140},
  date-added = {2022-05-03 21:09:24 -0400},
  date-modified = {2022-05-05 09:15:25 -0400},
  keywords = {sequential monte carlo,variational inference,variational sequential monte carlo},
  file = {/Users/j/Zotero/storage/6U8M48YV/Naesseth et al. - 2017 - Variational sequential monte carlo.pdf}
}

@inproceedings{narayanan.s:1998,
  title = {Bayesian Models of Human Sentence Processing},
  booktitle = {Procedings of Twentieth Annual Conference of the Cognitive Science Society: {{University}} of Wisconsin-Madison},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  editor = {Gernsbacher, Morton Ann and Derry, Sharon J.},
  date = {1998},
  pages = {752--757},
  publisher = {{Lawrence Erlbaum Associates}},
  location = {{Mahwah, NJ}},
  url = {https://web.stanford.edu/~jurafsky/srini2.pdf},
  date-added = {2021-03-09 22:52:27 -0500},
  date-modified = {2021-03-09 22:52:27 -0500},
  keywords = {bayesian,processing}
}

@inproceedings{narayanan.s:2001,
  title = {A {{Bayesian Model Predicts Human Parse Preference}} and {{Reading Times}} in {{Sentence Processing}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Narayanan, S. and Jurafsky, Daniel},
  date = {2001},
  volume = {14},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2001/hash/f15d337c70078947cfe1b5d6f0ed3f13-Abstract.html},
  urldate = {2022-06-28},
  abstract = {Narayanan and Jurafsky (1998) proposed that human language compre- hension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian de- cision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.},
  file = {/Users/j/Zotero/storage/AT6RBGTU/Narayanan and Jurafsky - 2001 - A Bayesian Model Predicts Human Parse Preference a.pdf}
}

@unpublished{narayanan.s:2004,
  title = {A {{Bayesian}} Model of Human Sentence Processing},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  date = {2004},
  url = {https://web.stanford.edu/~jurafsky/narayananjurafsky04.pdf},
  date-added = {2021-03-09 22:52:28 -0500},
  date-modified = {2021-03-10 12:32:00 -0500}
}

@inproceedings{naseem.t:2012,
  title = {Selective Sharing for Multilingual Dependency Parsing},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Naseem, Tahira and Barzilay, Regina and Globerson, Amir},
  date = {2012-07},
  pages = {629--637},
  publisher = {{Association for Computational Linguistics}},
  location = {{Jeju Island, Korea}},
  url = {https://aclanthology.org/P12-1066},
  date-added = {2022-04-04 12:41:51 -0400},
  date-modified = {2022-04-04 12:41:52 -0400}
}

@article{neal.r:2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  date = {2003-06},
  journaltitle = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation,rejection sampling,sampling,slice sampling},
  file = {/Users/j/Zotero/storage/XAHUUUPG/Neal - 2003 - Slice sampling.pdf}
}

@article{nevins.a:2011,
  title = {Multiple Agree with Clitics: {{Person}} Complementarity vs. Omnivorous Number},
  author = {Nevins, Andrew},
  date = {2011},
  journaltitle = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {939--971},
  publisher = {{Springer}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:19 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,omnivorous agree}
}

@book{newell.a:1972,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert Alexander},
  date = {1972},
  volume = {104},
  number = {9},
  publisher = {{Prentice-hall Englewood Cliffs, NJ}}
}

@incollection{newell.a:1973,
  title = {Production {{Systems}}: {{Models}} of {{Control Structures}}},
  shorttitle = {Production {{Systems}}},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  editor = {Chase, William G.},
  date = {1973-01-01},
  pages = {463--526},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-170150-5.50016-0},
  url = {https://www.sciencedirect.com/science/article/pii/B9780121701505500160},
  urldate = {2022-07-15},
  abstract = {This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works—on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.},
  isbn = {978-0-12-170150-5},
  langid = {english}
}

@incollection{newell.a:1981,
  title = {Mechanisms of {{Skill Acquisition}} and the {{Law}} of {{Practice}}},
  shorttitle = {Mechanisms of {{Skill Acquisition}} and the {{Law}} of {{Practice}}},
  booktitle = {Cognitive {{Skills}} and {{Their Acquisition}}},
  author = {Newell, Allen and Paul, Rosenbloom},
  editor = {Anderson, John R.},
  date = {1981},
  publisher = {{Psychology Press}},
  doi = {10.4324/9780203728178-6},
  url = {https://www.taylorfrancis.com/books/9781135830885/chapters/10.4324/9780203728178-6},
  abstract = {Practice makes perfect. Correcting the overstatement of a maxim: Almost always, practice brings improvement, and more practice brings more improvement. We all expect improvement with practice to be ubiquitous, though obviously limits exist both in scope and extent. Take only the experimental laboratory: We do not expect people to perform an experimental task correctly without at least some practice; and we design all our psychology experiments with one eye to the confounding influence of practice effects.},
  isbn = {978-0-203-72817-8}
}

@book{newell.a:1994,
  title = {Unified {{Theories}} of {{Cognition}}},
  author = {Newell, Allen},
  date = {1994},
  eprint = {1lbY14DmV2cC},
  eprinttype = {googlebooks},
  publisher = {{Harvard University Press}},
  abstract = {Psychology is now ready for unified theories of cognition--so says Allen Newell, a leading investigator in computer science and cognitive psychology. Not everyone will agree on a single set of mechanisms that will explain the full range of human cognition, but such theories are within reach and we should strive to articulate them.In this book, Newell makes the case for unified theories by setting forth a candidate. After reviewing the foundational concepts of cognitive science--knowledge, representation, computation, symbols, architecture, intelligence, and search--Newell introduces Soar, an architecture for general cognition. A pioneer system in artificial intelligence, Soar is the first problem solver to create its own subgoals and learn continuously from its own experience.Newell shows how Soar's ability to operate within the real-time constraints of intelligent behavior, such as immediate-response and item-recognition tasks, illustrates important characteristics of the human cognitive structure. Throughout, Soar remains an exemplar: we know only enough to work toward a fully developed theory of cognition, but Soar's success so far establishes the viability of the enterprise.Given its integrative approach, Unified Theories of Cognition will be of tremendous interest to researchers in a variety of fields, including cognitive science, artificial intelligence, psychology, and computer science. This exploration of the nature of mind, one of the great problems of philosophy, should also transcend disciplines and attract a large scientific audience.},
  isbn = {978-0-674-92101-6},
  langid = {english},
  pagetotal = {580},
  keywords = {Psychology / General}
}

@inproceedings{nguyen.l:2012,
  title = {Accurate Unbounded Dependency Recovery Using Generalized Categorial Grammars},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Nguyen, Luan and Van Schijndel, Marten and Schuler, William},
  date = {2012},
  pages = {2125--2140},
  publisher = {{The COLING 2012 Organizing Committee}},
  location = {{Mumbai, India}},
  url = {https://www.aclweb.org/anthology/C12-1130}
}

@article{nicenboim.b:2016,
  title = {When {{High-Capacity Readers Slow Down}} and {{Low-Capacity Readers Speed Up}}: {{Working Memory}} and {{Locality Effects}}},
  shorttitle = {When {{High-Capacity Readers Slow Down}} and {{Low-Capacity Readers Speed Up}}},
  author = {Nicenboim, Bruno and Logačev, Pavel and Gattei, Carolina and Vasishth, Shravan},
  date = {2016-03-08},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00280},
  url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.00280/abstract},
  urldate = {2022-08-13},
  file = {/Users/j/Zotero/storage/4F2YFMHQ/Nicenboim et al. - 2016 - When High-Capacity Readers Slow Down and Low-Capac.pdf}
}

@article{nicenboim.b:2018,
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  author = {Nicenboim, Bruno and Vasishth, Shravan},
  date = {2018-04},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {99},
  pages = {1--34},
  issn = {0749596X},
  doi = {10.1016/j.jml.2017.08.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X16301577},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/MWGYVZ7N/Nicenboim and Vasishth - 2018 - Models of retrieval in sentence comprehension A c.pdf}
}

@inproceedings{nichol.a:2021,
  title = {Improved {{Denoising Diffusion Probabilistic Models}}},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  date = {2021-07-01},
  pages = {8162--8171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/nichol21a.html},
  urldate = {2022-07-07},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  eventtitle = {International {{Conference}} on {{Machine Learning}}},
  langid = {english},
  keywords = {diffusion processes},
  file = {/Users/j/Zotero/storage/5R9MDK85/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/j/Zotero/storage/N2AKD33G/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@inproceedings{NIPS2001_f15d337c,
  title = {A Bayesian Model Predicts Human Parse Preference and Reading Times in Sentence Processing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Narayanan, S. and Jurafsky, Daniel},
  editor = {Dietterich, T. and Becker, S. and Ghahramani, Z.},
  date = {2001},
  volume = {14},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2001/file/f15d337c70078947cfe1b5d6f0ed3f13-Paper.pdf}
}

@article{nivre.j:2008,
  title = {Algorithms for {{Deterministic Incremental Dependency Parsing}}},
  author = {Nivre, Joakim},
  date = {2008-12-01},
  journaltitle = {Computational Linguistics},
  shortjournal = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {513--553},
  issn = {0891-2017},
  doi = {10.1162/coli.07-056-R1-07-027},
  url = {https://aclanthology.org/J08-4003},
  urldate = {2022-06-19},
  abstract = {Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.},
  file = {/Users/j/Zotero/storage/YK68V25N/Nivre - 2008 - Algorithms for Deterministic Incremental Dependenc.pdf}
}

@inproceedings{nivre.j:2016universal-dependencies,
  title = {Universal {{Dependencies}} v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({{LREC}}'16)},
  author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Hajič, Jan and Manning, Christopher D. and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
  options = {useprefix=true},
  date = {2016},
  pages = {1659--1666},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Portorož, Slovenia}},
  url = {https://www.aclweb.org/anthology/L16-1262}
}

@misc{nivre.j:2017,
  title = {Universal Dependencies 2.0 – {{CoNLL}} 2017 Shared Task Development and Test Data},
  author = {Nivre, Joakim and Agić, Željko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Burchardt, Aljoscha and Candito, Marie and Caron, Gauthier and Cebiroğlu Eryiğit, Gülşen and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Cinková, Silvie and Çöltekin, Çağrı and Connor, Miriam and de Marneffe, Marie-Catherine and de Paiva, Valeria and Diaz de Ilarraza, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Eli, Marhaba and Elkahky, Ali and Erjavec, Tomaž and Farkas, Richárd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cláudia and Gajdošová, Katarína and Galbraith, Daniel and Garcia, Marcos and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and Gökırmak, Memduh and Goldberg, Yoav and Gómez Guinovart, Xavier and Gonzáles Saavedra, Berta and Grioni, Matias and Grūztis, Normunds and Guillaume, Bruno and Habash, Nizar and Hajič, Jan and Hajič jr., Jan and Hà Mỹ, Linh and Harris, Kim and Haug, Dag and Hladká, Barbora and Hlaváčová, Jaroslava and Hohle, Petter and Ion, Radu and Irimia, Elena and Johannsen, Anders and Jørgensen, Fredrik and Kaşıkara, Hüner and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerová, Václava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and Lê Hồng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Ljubešić, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and Mărănduc, Cătălina and Mareček, David and Marheinecke, Katrin and Martínez Alonso, Héctor and Martins, André and Mašek, Jan and Matsumoto, Yuji and McDonald, Ryan and Mendonça, Gustavo and Missilä, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shunsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Mustafina, Nina and Müürisep, Kaili and Nainwani, Pinkey and Nedoluzhko, Anna and Nguyễn Thị, Lương and Nguyễn Thị Minh, Huyền and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and Øvrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Popel, Martin and Pretkalniņa, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Real, Livy and Reddy, Siva and Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and Rosa, Rudolf and Rovati, Davide and Saleh, Shadi and Sanguinetti, Manuela and Saulte, Baiba and Sawanakunanon, Yanin and Schuster, Sebastian and Seddah, Djamé and Seeker, Wolfgang and Seraji, Mojgan and Shakurova, Lena and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simkó, Katalin and Šimková, Mária and Simov, Kiril and Smith, Aaron and Stella, Antonio and Strnadová, Jana and Suhr, Alane and Sulubacak, Umut and Szántó, Zsolt and Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Urešová, Zdeňka and Uria, Larraitz and Uszkoreit, Hans and van Noord, Gertjan and Varga, Viktor and Vincze, Veronika and Washington, Jonathan North and Yu, Zhuoran and Žabokrtský, Zdeněk and Zeman, Daniel and Zhu, Hanzhi},
  options = {useprefix=true},
  date = {2017},
  url = {http://hdl.handle.net/11234/1-2184},
  copyright = {Licence Universal Dependencies v2.0},
  date-added = {2021-04-30 13:00:05 -0400},
  date-modified = {2021-04-30 13:02:24 -0400},
  keywords = {dependency parsing,dependency structures,universal dependencies}
}

@inproceedings{nivre.j:2020,
  title = {Universal {{Dependencies}} v2: {{An}} Evergrowing Multilingual Treebank Collection},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Nivre, Joakim and de Marneffe, Marie-Catherine and Ginter, Filip and Hajič, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
  options = {useprefix=true},
  date = {2020},
  pages = {4034--4043},
  publisher = {{European Language Resources Association}},
  location = {{Marseille, France}},
  url = {https://www.aclweb.org/anthology/2020.lrec-1.497},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@article{norris.d:2006,
  title = {The {{Bayesian}} Reader: {{Explaining}} Word Recognition as an Optimal {{Bayesian}} Decision Process},
  shorttitle = {The {{Bayesian}} Reader},
  author = {Norris, Dennis},
  date = {2006},
  journaltitle = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {327--357},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.113.2.327},
  abstract = {This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. This leads to the development of a computational model of word recognition, the Bayesian reader. The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Lexical Decision,Models,Reading,Semantics,Statistical Probability,Word Recognition},
  file = {/Users/j/Zotero/storage/99ILH9NN/Norris - 2006 - The Bayesian reader Explaining word recognition a.pdf}
}

@article{norris.d:2009,
  title = {Putting It All Together: {{A}} Unified Account of Word Recognition and Reaction-Time Distributions},
  shorttitle = {Putting It All Together},
  author = {Norris, Dennis},
  date = {2009},
  journaltitle = {Psychological Review},
  volume = {116},
  number = {1},
  pages = {207--219},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/a0014259},
  abstract = {R. Ratcliff, P. Gomez, and G. McKoon (2004) suggested much of what goes on in lexical decision is attributable to decision processes and may not be particularly informative about word recognition. They proposed that lexical decision should be characterized by a decision process, taking the form of a drift-diffusion model (R. Ratcliff, 1978), that operates on the output of lexical model. The present article argues that the distinction between perception and decision making is unnecessary and that it is possible to give a unified account of both lexical processing and decision making. This claim is supported by formal arguments and reinforced by simulations showing how the Bayesian Reader model (D. Norris, 2006) can be extended to fit the data on reaction time distributions collected by Ratcliff, Gomez, and McKoon simply by adding extra sources of noise. The Bayesian Reader gives an integrated explanation of both word recognition and decision making, using fewer parameters than the diffusion model. It can be thought of as a Bayesian diffusion model, which subsumes Ratcliff's drift-diffusion model as a special case. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Lexical Decision,Models,Reaction Time,Word Recognition}
}

@report{odonnell.t:2009,
  type = {Technical report},
  title = {Fragment {{Grammars}}: {{Exploring Computation}} and {{Reuse}} in {{Language}}},
  shorttitle = {Fragment {{Grammars}}},
  author = {O'Donnell, Timothy J. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  date = {2009-03-31},
  series = {{{MIT Computer Science}} and {{Artificial Intelligence Laboratory Technical Report Series}}},
  number = {MIT-CSAIL-TR-2009-013},
  institution = {{MIT Computer Science and Artificial Intelligence Laboratory}},
  url = {https://dspace.mit.edu/handle/1721.1/44963},
  urldate = {2022-06-15},
  abstract = {Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP) and in particular using a probabilistic variant of Lisp known as the Church programming language (Goodman, Mansinghka, Roy, Bonawitz, \& Tenenbaum, 2008). We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them---adaptor grammars (Johnson, Griffiths, \& Goldwater, 2007). We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.},
  langid = {english},
  annotation = {Accepted: 2009-03-31T05:00:03Z},
  file = {/Users/j/Zotero/storage/YNH3B97V/O'Donnell et al. - 2009 - Fragment Grammars Exploring Computation and Reuse.pdf}
}

@article{odonnell.t:2011cogsci,
  title = {Productivity and {{Reuse}} in {{Language}}},
  author = {O'Donnell, Timothy and Snedeker, Jesse and Tenenbaum, Joshua and Goodman, Noah},
  date = {2011},
  journaltitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {33},
  number = {33},
  url = {https://escholarship.org/uc/item/4t2312gv},
  urldate = {2022-06-15},
  abstract = {Author(s): O'Donnell, Timothy; Snedeker, Jesse; Tenenbaum, Joshua; Goodman, Noah},
  langid = {english},
  file = {/Users/j/Zotero/storage/N7ZT4BP6/O'Donnell et al. - 2011 - Productivity and Reuse in Language.pdf}
}

@book{odonnell.t:2015phdbook,
  title = {Productivity and {{Reuse}} in {{Language}}: {{A Theory}} of {{Linguistic Computation}} and {{Storage}}},
  author = {O'Donnell, Timothy J.},
  date = {2015-08-28},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262028844.001.0001},
  url = {https://doi.org/10.7551/mitpress/9780262028844.001.0001},
  urldate = {2022-06-16},
  abstract = {A proposal for a formal model, Fragment Grammars, that treats productivity and reuse as the target of inference in a probabilistic framework.Language allows us to express and comprehend an unbounded number of thoughts. This fundamental and much-celebrated property is made possible by a division of labor between a large inventory of stored items (e.g., affixes, words, idioms) and a computational system that productively combines these stored units on the fly to create a potentially unlimited array of new expressions. A language learner must discover a language's productive, reusable units and determine which computational processes can give rise to new expressions. But how does the learner differentiate between the reusable, generalizable units (for example, the affix -ness, as in coolness, orderliness, cheapness) and apparent units that do not actually generalize in practice (for example, -th, as in warmth but not coolth)? In this book, Timothy O'Donnell proposes a formal computational model, Fragment Grammars, to answer these questions. This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distribution of productive word-formation processes and reusable units in a given language.O'Donnell compares this model to a number of other theoretical and mathematical models, applying them to the English past tense and English derivational morphology, and showing that Fragment Grammars unifies a number of superficially distinct empirical phenomena in these domains and justifies certain seemingly ad hoc assumptions in earlier theories.},
  isbn = {978-0-262-32680-3}
}

@book{ontario:2011,
  title = {Native {{Languages}}: {{A Support Document}} for the {{Teaching}} of {{Language Patterns}}: {{Oneida}}, {{Cayuga}}, and {{Mohawk}}},
  author = {{Ontario Ministry of Education}},
  date = {2011},
  series = {The {{Ontario Curriculum}}: {{Grades}} 1 to 12},
  publisher = {{Ontario: Queen's Printer for Ontario}},
  langid = {english},
  file = {/Users/j/Zotero/storage/3UCUY6DE/Native Languages A Support Document for the Teach.pdf}
}

@misc{oord.a:2018,
  title = {Representation Learning with Contrastive Predictive Coding},
  author = {van den Oord, Aaron and Li, Yazhe and Vinyals, Oriol},
  options = {useprefix=true},
  date = {2018},
  eprint = {1807.03748},
  eprinttype = {arxiv},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:07:24 -0400},
  date-modified = {2019-07-05 11:08:29 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,representation learning}
}

@article{ortega.p:2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information-Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  date = {2013-05-08},
  journaltitle = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {469},
  number = {2153},
  pages = {20120683},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2012.0683},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2012.0683},
  urldate = {2022-06-09},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
  keywords = {bounded rationality,decision-making,information processing},
  file = {/Users/j/Zotero/storage/KRCB8XUT/Ortega and Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf}
}

@article{oxford.w:2019,
  title = {Inverse Marking and Multiple Agree in Algonquin},
  author = {Oxford, Will},
  date = {2019},
  journaltitle = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {3},
  pages = {955--996},
  doi = {10.1007/s11049-018-9428-x},
  url = {https://doi.org/10.1007/s11049-018-9428-x},
  abstract = {This paper shows that inverse marking and portmanteau agreement are in complementary distribution in Algonquin: inverse marking is possible only in contexts where portmanteau agreement is not. This correlation holds despite intralanguage variation in both phenomena. The paper proposes that the two phenomena pattern together because both are determined by the outcome of the Agree operation on Infl. When Infl enters a Multiple Agree relation with both arguments, the realization of portmanteau agreement morphology is possible. When Infl agrees only with the object, it duplicates the result of an earlier object agreement operation on Voice. The presence of identical features on Infl and Voice triggers an impoverishment operation that deletes the features of Voice, resulting in its spellout as an underspecified elsewhere form—which is the exponent that we know descriptively as the inverse marker. This analysis explains why inverse marking and portmanteau agreement never co-occur in Algonquin: the two phenomena are determined by alternative outcomes of the Agree operation on Infl. The analysis also enables a simple account of the intralanguage variation in the patterning of the two phenomena, which is shown to follow from variation in the specification of the probe on Infl.},
  da = {2019/08/01},
  date-added = {2020-06-16 10:51:08 -0400},
  date-modified = {2020-06-16 10:52:50 -0400},
  isbn = {1573-0859},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@inproceedings{paiva-alves.e:1996,
  title = {The Selection of the Most Probable Dependency Structure in {{Japanese}} Using Mutual Information},
  booktitle = {34th Annual Meeting of the Association for Computational Linguistics},
  author = {de Paiva Alves, Eduardo},
  options = {useprefix=true},
  date = {1996},
  pages = {372--374},
  publisher = {{Association for Computational Linguistics}},
  location = {{Santa Cruz, California, USA}},
  doi = {10.3115/981863.981919},
  url = {https://www.aclweb.org/anthology/P96-1055},
  bdsk-url-2 = {https://doi.org/10.3115/981863.981919}
}

@inproceedings{pal.c:2006,
  title = {Sparse Forward-Backward Using Minimum Divergence Beams for Fast Training of Conditional Random Fields},
  booktitle = {{{IEEE}} International Conference on Acoustics Speed and Signal Processing Proceedings},
  author = {Pal, C. and Sutton, C. and McCallum, A.},
  date = {2006},
  volume = {V},
  pages = {581--584},
  publisher = {{IEEE}},
  doi = {10.1109/icassp.2006.1661342},
  url = {https://doi.org/10.1109%2Ficassp.2006.1661342},
  bdsk-url-2 = {https://doi.org/10.1109/icassp.2006.1661342},
  date-added = {2022-03-25 11:41:07 -0400},
  date-modified = {2022-03-25 11:45:04 -0400}
}

@book{palermo.d:1964,
  title = {Word Association Norms: {{Grade}} School through College},
  author = {Palermo, David and Jenkins, James},
  date = {1964},
  publisher = {{U. Minnesota Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@inproceedings{park.y:2009,
  title = {Minimal-Length Linearizations for Mildly Context-Sensitive Dependency Trees},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Park, Y. Albert and Levy, Roger},
  date = {2009},
  pages = {335--343},
  publisher = {{Association for Computational Linguistics}},
  location = {{Boulder, Colorado}},
  url = {https://www.aclweb.org/anthology/N09-1038}
}

@inproceedings{park.y:2011,
  title = {Automated Whole Sentence Grammar Correction Using a Noisy Channel Model},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Park, Y. Albert and Levy, Roger},
  date = {2011-06},
  pages = {934--944},
  publisher = {{Association for Computational Linguistics}},
  location = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1094},
  date-added = {2022-04-11 23:09:22 -0400},
  date-modified = {2022-04-11 23:09:27 -0400}
}

@article{partee.b:1990,
  title = {Mathematical Methods in Linguistics},
  author = {Manaster Ramer, Alexis},
  date = {1992},
  journaltitle = {Computational Linguistics},
  volume = {18},
  number = {1},
  url = {https://www.aclweb.org/anthology/J92-1009}
}

@inproceedings{paskin.m:2002,
  title = {Grammatical Bigrams},
  booktitle = {Advances in Neural Information Processing Systems 14 [{{Neural}} Information Processing Systems: {{Natural}} and Synthetic, {{NIPS}} 2001, December 3-8, 2001, Vancouver, British Columbia, Canada]},
  author = {Paskin, Mark A.},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  date = {2001},
  pages = {91--97},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2001/hash/89885ff2c83a10305ee08bd507c1049c-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/Paskin01.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@thesis{pentangelo.j:2020,
  type = {phdthesis},
  title = {360° {{Video}} and {{Language Documentation}}: {{Towards}} a {{Corpus}} of {{Kanien}}’kéha ({{Mohawk}})},
  shorttitle = {360° {{Video}} and {{Language Documentation}}},
  author = {Pentangelo, Joseph and link will open in a new window Link to external site, this},
  date = {2020},
  institution = {{City University of New York}},
  location = {{United States -- New York}},
  url = {https://www.proquest.com/docview/2459231889/abstract/2D8FDD4353574D95PQ/1},
  urldate = {2022-05-31},
  abstract = {Robust documentation is a major goal of documentary linguistics. Recognizing spoken language as a multimodal phenomenon, researchers working in this field broadly agree that video is an improvement over audio-only recording. At the same time, video is limited by the format’s frame, which permits only a relatively small portion of the visual field to be recorded at any given time. This results in much data being lost, as the documenter must decide where to aim their camera, necessarily leaving out more than they record. In this dissertation, I apply 360º video to language documentation for the first time. 360º video, which is one variety of virtual reality, improves upon traditional video by drastically expanding the frame, recording in all directions surrounding the camera. In this way, a maximum of visual data is recorded, and there is no need for the camera to be redirected as participants take turns speaking or move around the space. I recorded over 10 hours of 360º video with ambisonic audio, containing mostly naturalistic conversation in the Akwesasne variety of Kanien’kéha (Mohawk), an endangered Northern Iroquoian language spoken in New York State, Ontario, and Quebec. Most of the existing documentation of Kanien’kéha outside of this corpus is formal or non-naturalistic. The resulting corpus thus serves a dual purpose: it is both a demonstration of the capabilities of 360º video for language documentation, and a contribution to the documentation of Kanien’kéha. This dissertation includes a brief grammatical description of Kanien’kéha phonology and morphology, a discussion of the interplay between technology and language documentation throughout North American history, an exploration of the significance of 360º video to documentary linguistics, a brief analysis of gesture and intonation in the present corpus, and an assessment of the suitability of ambisonic audio for linguistic analysis. Directions for potential future research are indicated throughout.},
  isbn = {9798678110015},
  langid = {english},
  pagetotal = {240},
  keywords = {Akwesasne,Documentary linguistics,Iroquoian,Language documentation,Virtual reality},
  file = {/Users/j/Zotero/storage/QBSYW5RY/Pentangelo and Link to external site - 360° Video and Language Documentation Towards a C.pdf}
}

@article{pereira.f:2000,
  title = {Formal Grammar and Information Theory: {{Together}} Again?},
  author = {Pereira, Fernando C. N.},
  date = {2000},
  journaltitle = {Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences},
  volume = {358},
  number = {1769},
  pages = {1239--1253},
  doi = {10.1098/rsta.2000.0583},
  url = {https://doi.org/10.1098/rsta.2000.0583},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {information theory},
  file = {/Users/j/Zotero/storage/KZIT4MEK/Pereira - 2000 - Formal grammar and information theory Together ag.pdf}
}

@article{perfors2011learnability,
  title = {The Learnability of Abstract Syntactic Principles},
  author = {Perfors, Amy and Tenenbaum, Joshua B and Regier, Terry},
  date = {2011},
  journaltitle = {Cognition},
  volume = {118},
  number = {3},
  pages = {306--338},
  publisher = {{Elsevier}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{peters.m:2018,
  title = {Deep Contextualized Word Representations},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  date = {2018},
  pages = {2227--2237},
  publisher = {{Association for Computational Linguistics}},
  location = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1202},
  url = {https://www.aclweb.org/anthology/N18-1202},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1202}
}

@inproceedings{peters.m:2018a,
  title = {Dissecting Contextual Word Embeddings: {{Architecture}} and Representation},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  date = {2018},
  pages = {1499--1509},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1179},
  url = {https://www.aclweb.org/anthology/D18-1179},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1179}
}

@article{piantadosi.s:2011,
  title = {Word Lengths Are Optimized for Efficient Communication},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  date = {2011},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3526--3529},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1012551108},
  url = {https://doi.org/10.1073%2Fpnas.1012551108},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1012551108},
  date-added = {2021-07-25 11:06:49 -0400},
  date-modified = {2021-07-25 11:06:50 -0400}
}

@article{piantadosi.s:2014,
  title = {Zipf’s Word Frequency Law in Natural Language: {{A}} Critical Review and Future Directions},
  shorttitle = {Zipf’s Word Frequency Law in Natural Language},
  author = {Piantadosi, Steven T.},
  date = {2014-10},
  journaltitle = {Psychonomic bulletin \& review},
  shortjournal = {Psychon Bull Rev},
  volume = {21},
  number = {5},
  eprint = {24664880},
  eprinttype = {pmid},
  pages = {1112--1130},
  issn = {1069-9384},
  doi = {10.3758/s13423-014-0585-6},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/},
  urldate = {2022-09-27},
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf ’ s law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf’s law and are then used to evaluate many of the theoretical explanations of Zipf’s law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf’s law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  pmcid = {PMC4176592},
  file = {/Users/j/Zotero/storage/4JZT9KFV/Piantadosi (2014) Zipf’s word frequency law in natural language A c.pdf}
}

@article{pickering.m:2013,
  title = {An Integrated Theory of Language Production and Comprehension},
  author = {Pickering, Martin J. and Garrod, Simon},
  date = {2013},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {36},
  number = {4},
  pages = {329--347},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/S0140525X12001495},
  url = {https://doi.org/10.1017/S0140525X12001495},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding},
  file = {/Users/j/Zotero/storage/YWM6TEES/Pickering and Garrod - 2013 - An integrated theory of language production and co.pdf}
}

@article{pickering.m:2018predicting,
  title = {Predicting While Comprehending Language: {{A}} Theory and Review.},
  author = {Pickering, Martin J and Gambi, Chiara},
  date = {2018},
  journaltitle = {Psychological Bulletin},
  volume = {144},
  number = {10},
  pages = {1002},
  publisher = {{American Psychological Association}},
  doi = {10.1037/bul0000158},
  url = {https://doi.org/10.1037/bul0000158},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@inproceedings{pine.a:2022,
  title = {Requirements and {{Motivations}} of {{Low-Resource Speech Synthesis}} for {{Language Revitalization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pine, Aidan and Wells, Dan and Brinklow, Nathan and Littell, Patrick and Richmond, Korin},
  date = {2022-05},
  pages = {7346--7359},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.507},
  url = {https://aclanthology.org/2022.acl-long.507},
  urldate = {2022-06-04},
  abstract = {This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien'kéha, Gitksan \& SENĆOŦEN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.},
  eventtitle = {{{ACL}} 2022},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/LMSBKQLF/Pine et al. - 2022 - Requirements and Motivations of Low-Resource Speec.pdf}
}

@article{poole.e:2016,
  title = {Deconstructing Subjecthood},
  author = {Poole, Ethan},
  date = {2016},
  journaltitle = {Ms., UMass Amherst.},
  url = {http://ling.auf.net/lingbuzz/003197},
  date-added = {2019-06-14 09:32:06 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,subject positions}
}

@inproceedings{poppels.t:2016,
  title = {Structure-Sensitive {{Noise Inference}}: {{Comprehenders Expect Exchange Errors}}},
  booktitle = {Proceedings of the 38th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Poppels, Till and Levy, Roger},
  date = {2016},
  pages = {378--383},
  publisher = {{Cognitive Science Society}},
  location = {{Austin, TX}},
  url = {https://cogsci.mindmodeling.org/2016/papers/0077/},
  abstract = {Previous research has found that comprehenders are willing to adopt non-literal interpretations of sentences whose literal reading is unlikely. Several studies found evidence that comprehenders decide whether a given utterance should be taken at face value in accordance with principles of Bayesian rationality, by weighing the prior probability of potential interpretations against the degree to which they are (in)consistent with the literal form of the utterance. While all of these results are consistent with string-edit noise models, many error processes are known to be sensitive to the underlying linguistic structure of the intended utterance. Here, we explore the case of exchange errors and provide experimental evidence that comprehenders' noise model is structure-sensitive. Our results add further support to the noisy-channel theory of language comprehension, extend the set of known noise operations to include positional exchanges, and show that comprehenders' noise models are well-adapted to structure-sensitive sources of signal corruption.},
  eventtitle = {{{CogSci}} 2016},
  file = {/Users/j/Zotero/storage/RK2HV63N/Poppels and Levy - 2016 - Structure-sensitive Noise Inference Comprehenders.pdf}
}

@article{post.e:1943,
  title = {Formal {{Reductions}} of the {{General Combinatorial Decision Problem}}},
  author = {Post, Emil L.},
  date = {1943},
  journaltitle = {American Journal of Mathematics},
  volume = {65},
  number = {2},
  eprint = {2371809},
  eprinttype = {jstor},
  pages = {197--215},
  publisher = {{Johns Hopkins University Press}},
  issn = {0002-9327},
  doi = {10.2307/2371809},
  file = {/Users/j/Zotero/storage/XIZKXKKT/Post - 1943 - Formal Reductions of the General Combinatorial Dec.pdf}
}

@article{post.m:2013,
  title = {Bayesian Tree Substitution Grammars as a Usage-Based Approach},
  author = {Post, Matt and Gildea, Daniel},
  date = {2013},
  journaltitle = {Language and Speech},
  volume = {56},
  number = {3},
  pages = {291--308},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:27 -0400},
  keywords = {Tree Substitution Grammar}
}

@misc{prasad.g:2019readingMVRR,
  title = {Rapid Syntactic Adaptation in Self-Paced Reading: Detectable, but Requires Many Participants.},
  author = {Prasad, Grusha and Linzen, Tal},
  date = {2019},
  publisher = {{Center for Open Science}},
  doi = {10.31234/osf.io/9ptg4},
  url = {https://doi.org/10.31234%2Fosf.io%2F9ptg4},
  bdsk-url-2 = {https://doi.org/10.31234/osf.io/9ptg4},
  date-added = {2021-03-18 11:13:22 -0400},
  date-modified = {2021-03-18 17:40:38 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing,reading time,self-paced reading}
}

@misc{prasad.g:2019readingNPS-NPZ,
  title = {How Much Harder Are Hard Garden-Path Sentences than Easy Ones?},
  author = {Prasad, Grusha and Linzen, Tal},
  date = {2019},
  journaltitle = {CogSci},
  url = {https://osf.io/syh3j/},
  date-added = {2021-03-18 11:20:31 -0400},
  date-modified = {2021-03-18 17:38:35 -0400},
  howpublished = {OSF preprint},
  keywords = {processing,reading time,self-paced reading}
}

@article{preminger.o:2011,
  title = {Asymmetries between Person and Number in Syntax: A Commentary on {{Baker}}'s {{SCOPA}}},
  author = {Preminger, Omer},
  date = {2011},
  journaltitle = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {917--937},
  publisher = {{Springer}},
  date-added = {2020-02-25 21:43:01 -0500},
  date-modified = {2020-02-26 09:11:06 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement,phi features}
}

@book{preminger.o:2014,
  title = {Agreement and Its Failures},
  author = {Preminger, Omer},
  date = {2014},
  volume = {68},
  publisher = {{MIT Press}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@article{prim.r:1957,
  title = {Shortest Connection Networks and Some Generalizations},
  author = {Prim, R. C.},
  date = {1957},
  journaltitle = {The Bell System Technical Journal},
  volume = {36},
  number = {6},
  pages = {1389--1401},
  doi = {10.1002/j.1538-7305.1957.tb01515.x},
  url = {https://doi.org/10.1002/j.1538-7305.1957.tb01515.x},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{priva.u:2020,
  title = {The Causal Structure of Lenition: {{A}} Case for the Causal Precedence of Durational Shortening},
  author = {Priva, Uriel Cohen and Gleason, Emily},
  date = {2020},
  journaltitle = {Language},
  volume = {96},
  number = {2},
  pages = {413--448},
  publisher = {{Project Muse}},
  doi = {10.1353/lan.2020.0025},
  url = {https://doi.org/10.1353%2Flan.2020.0025},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0025},
  date-added = {2022-05-10 10:31:58 -0400},
  date-modified = {2022-05-10 10:32:14 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/YKU8493F/Priva and Gleason - 2020 - The causal structure of lenition A case for the c.pdf}
}

@inproceedings{przepiorkowski.a:2018arguments-adjuncts-ud,
  title = {Arguments and Adjuncts in {{Universal Dependencies}}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {Przepiórkowski, Adam and Patejuk, Agnieszka},
  date = {2018},
  pages = {3837--3852},
  publisher = {{Association for Computational Linguistics}},
  location = {{Santa Fe, New Mexico, USA}},
  url = {https://www.aclweb.org/anthology/C18-1324}
}

@article{rabagliati.h:2016,
  title = {Learning to Predict or Predicting to Learn?},
  author = {Rabagliati, Hugh and Gambi, Chiara and Pickering, J},
  date = {2016},
  journaltitle = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {94--105},
  doi = {10.1080/23273798.2015.1077979},
  url = {https://doi.org/10.1080/23273798.2015.1077979},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@book{rabinovich.m:2012,
  title = {Principles of {{Brain Dynamics}}: {{Global State Interactions}}},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  date = {2012-07-06},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9108.001.0001},
  url = {https://doi.org/10.7551/mitpress/9108.001.0001},
  urldate = {2022-07-08},
  abstract = {Experimental and theoretical approaches to global brain dynamics that draw on the latest research in the field.The consideration of time or dynamics is fundamental for all aspects of mental activity—perception, cognition, and emotion—because the main feature of brain activity is the continuous change of the underlying brain states even in a constant environment. The application of nonlinear dynamics to the study of brain activity began to flourish in the 1990s when combined with empirical observations from modern morphological and physiological observations. This book offers perspectives on brain dynamics that draw on the latest advances in research in the field. It includes contributions from both theoreticians and experimentalists, offering an eclectic treatment of fundamental issues.Topics addressed range from experimental and computational approaches to transient brain dynamics to the free-energy principle as a global brain theory. The book concludes with a short but rigorous guide to modern nonlinear dynamics and their application to neural dynamics.},
  isbn = {978-0-262-30558-7}
}

@article{radford.a:2018,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  date = {2018},
  url = {https://cdn.openai.com/research-covers/language-unsupervised/language<sub>u</sub>nderstandingₚaper.pdf},
  date-added = {2019-06-24 19:01:10 -0400},
  date-modified = {2021-11-30 10:47:13 -0500},
  project = {syntactic embedding},
  keywords = {generative pre-training,GPT,GPT1,transfer learning}
}

@misc{radford.a:2019,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  date = {2019},
  url = {https://openai.com/blog/better-language-models/},
  bdsk-url-2 = {https://cdn.openai.com/better-language-models/languageₘodelsₐre\textsubscript{u}nsupervisedₘultitaskₗearners.pdf},
  date-added = {2019-06-15 18:07:44 -0400},
  date-modified = {2021-11-30 13:45:33 -0500},
  howpublished = {OpenAI},
  project = {syntactic embedding},
  keywords = {GPT,GPT2}
}

@article{ramgoolam.s:2019,
  title = {Permutation Invariant Gaussian Matrix Models},
  author = {Ramgoolam, Sanjaye},
  date = {2019},
  journaltitle = {Nuclear Physics B},
  pages = {114682},
  publisher = {{Elsevier}},
  date-added = {2019-08-06 08:51:05 +0300},
  date-modified = {2019-08-06 08:52:06 +0300},
  project = {syntactic embedding},
  keywords = {gaussian matrix models,physics}
}

@article{rasmussen.n:2018,
  title = {Left-{{Corner Parsing With Distributed Associative Memory Produces Surprisal}} and {{Locality Effects}}},
  author = {Rasmussen, Nathan E. and Schuler, William},
  date = {2018},
  journaltitle = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1009--1042},
  issn = {1551-6709},
  doi = {10.1111/cogs.12511},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12511},
  urldate = {2022-06-13},
  abstract = {This article describes a left-corner parser implemented within a cognitively and neurologically motivated distributed model of memory. This parser's approach to syntactic ambiguity points toward a tidy account both of surprisal effects and of locality effects, such as the parsing breakdowns caused by center embedding. The model provides an algorithmic-level (Marr, 1982) account of these breakdowns: The structure of the parser's memory and the nature of incremental parsing produce a smooth degradation of processing accuracy for longer center embeddings, and a steeper degradation when they are nested, in line with recall observations by Miller and Isard (1964) and speed-accuracy trade-off observations by McElree et al. (2003). Modeling results show that this effect is distinct from the effects of ambiguity and exceeds the effect of mere sentence length.},
  langid = {english},
  keywords = {Computational modeling,Computer simulation,Language understanding,Linguistics,Locality,Memory,Sentence processing,Surprisal,Syntax},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12511},
  file = {/Users/j/Zotero/storage/P2Q26JLY/Rasmussen and Schuler - 2018 - Left-Corner Parsing With Distributed Associative M.pdf}
}

@misc{rasooli.m:2015,
  title = {Yara {{Parser}}: {{A Fast}} and {{Accurate Dependency Parser}}},
  shorttitle = {Yara {{Parser}}},
  author = {Rasooli, Mohammad Sadegh and Tetreault, Joel},
  date = {2015-03-24},
  number = {arXiv:1503.06733},
  eprint = {1503.06733},
  eprinttype = {arxiv},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1503.06733},
  urldate = {2022-05-17},
  abstract = {Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https://github.com/yahoo/YaraParser.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5F6CI55T/Rasooli and Tetreault - 2015 - Yara Parser A Fast and Accurate Dependency Parser.pdf;/Users/j/Zotero/storage/SP9552SL/1503.html}
}

@article{ratcliff.r:1978,
  title = {A Theory of Memory Retrieval},
  author = {Ratcliff, Roger},
  date = {1978},
  journaltitle = {Psychological Review},
  volume = {85},
  number = {2},
  pages = {59--108},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.85.2.59},
  abstract = {Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe–memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe–memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study–test, and continuous) and to speed–accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Memory,Theories}
}

@thesis{ravishankar.m:1996,
  title = {Efficient Algorithms for Speech Recognition.},
  author = {Ravishankar, Mosur K},
  date = {1996-05},
  institution = {{Carnegie Mellon University, Department of Computer Science}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.3560},
  date-added = {2022-03-25 23:17:15 -0400},
  date-modified = {2022-03-25 23:19:35 -0400}
}

@inproceedings{rehurek.r:2010gensim,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {Řehůřek, Radim and Sojka, Petr},
  date = {2010},
  pages = {45--50},
  publisher = {{ELRA}},
  location = {{Valletta, Malta}},
  url = {http://is.muni.cz/publication/884893/en},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@article{reichle.e:2003,
  title = {The {{E-Z Reader}} Model of Eye-Movement Control in Reading: {{Comparisons}} to Other Models},
  author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
  date = {2003},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {26},
  number = {4},
  pages = {445--476},
  publisher = {{Cambridge University Press (CUP)}},
  doi = {10.1017/s0140525x03000104},
  url = {https://doi.org/10.1017%2Fs0140525x03000104},
  bdsk-url-2 = {https://doi.org/10.1017/s0140525x03000104},
  date-added = {2021-05-22 14:59:06 -0400},
  date-modified = {2022-05-06 15:37:48 -0400},
  keywords = {eye-tracking,processing}
}

@article{reiss.c:2018,
  title = {Substance Free Phonology},
  author = {Reiss, Charles},
  editor = {S. J. Hannahs, A. R. K. Bosch},
  date = {2017},
  journaltitle = {The Routledge handbook of phonological theory},
  pages = {425--452},
  publisher = {{Routledge New York}},
  date-added = {2019-06-17 08:29:14 -0400},
  date-modified = {2019-06-17 08:38:02 -0400},
  isbn = {9781138025813},
  keywords = {substance free phonology}
}

@inproceedings{renyi.a:1961,
  title = {On Measures of Entropy and Information},
  booktitle = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
  author = {Rényi, Alfréd},
  editor = {Neyman, Jerzy},
  date = {1961},
  volume = {1},
  pages = {547--561},
  url = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181},
  date-added = {2021-10-27 09:20:40 -0400},
  date-modified = {2021-10-27 09:27:42 -0400},
  organization = {{University of California Press}}
}

@article{rezac.m:2008,
  title = {Phi-Agree and Theta-Related Case},
  author = {Rezac, Milan},
  date = {2008},
  journaltitle = {Phi theory: Phi-features across interfaces and modules},
  pages = {83--129},
  publisher = {{Oxford University Press Oxford}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:25:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@unpublished{rezac.m:2016,
  title = {The Ways of Referential Deficiency: {{Impersonal}} {{{\emph{on}}}} and Its Kin},
  author = {Rezac, Milan and Jouitteau, Mélanie},
  date = {2016},
  url = {https://www.iker.cnrs.fr/IMG/pdf/rezac<sub>j</sub>ouitteau.impersonal.pdf},
  date-added = {2021-03-21 13:51:16 -0400},
  date-modified = {2021-03-21 14:01:49 -0400},
  keywords = {breton,impersonals,phi features}
}

@book{riehl.e:2017,
  title = {Category Theory in Context},
  author = {Riehl, Emily},
  date = {2017},
  publisher = {{Courier Dover Publications}},
  date-added = {2019-08-24 09:26:31 -0400},
  date-modified = {2019-08-24 09:26:50 -0400},
  keywords = {category theory}
}

@article{ritchie.d:1986,
  title = {Shannon and {{Weaver}}: {{Unravelling}} the Paradox of Information},
  author = {Ritchie, David},
  date = {1986-04},
  journaltitle = {Communication Research},
  volume = {13},
  number = {2},
  pages = {278--298},
  publisher = {{SAGE Publications}},
  doi = {10.1177/009365086013002007},
  url = {https://doi.org/10.1177%2F009365086013002007},
  abstract = {A case is presented for separating Shannon's (1949) paper on information theory from Weaver's introduction, which is shown to contain distortions, as well as proofs by coincidence and homonym. Shannon's mathematical tools and methods are distinguished from his theory, which consists of 23 theorems setting forth the conditions for maximum efficiency in electromechanical signal transmission. Attempts to apply Shannon's theory to our field are reviewed, along with previous critiques, and it is recommended that future uses of Shannon's theory adopt a more methodologically rigorous approach. In particular, it is argued that Shannon's assumptions must be shown to hold before his theorems can be successfully applied.},
  bdsk-url-2 = {https://doi.org/10.1177/009365086013002007},
  date-added = {2022-04-07 12:59:21 -0400},
  date-modified = {2022-04-07 13:01:58 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{roark.b:2001,
  title = {Probabilistic Top-down Parsing and Language Modeling},
  author = {Roark, Brian},
  date = {2001},
  journaltitle = {Computational Linguistics},
  volume = {27},
  number = {2},
  pages = {249--276},
  doi = {10.1162/089120101750300526},
  url = {https://www.aclweb.org/anthology/J01-2004},
  bdsk-url-2 = {https://doi.org/10.1162/089120101750300526},
  file = {/Users/j/Zotero/storage/WG7I7NQH/Roark - 2001 - Probabilistic top-down parsing and language modeli.pdf}
}

@inproceedings{roark.b:2009,
  title = {Deriving Lexical and Syntactic Expectation-Based Measures for Psycholinguistic Modeling via Incremental Top-down Parsing},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
  date = {2009},
  pages = {324--333},
  publisher = {{Association for Computational Linguistics}},
  location = {{Singapore}},
  url = {https://www.aclweb.org/anthology/D09-1034}
}

@report{roark.b:2011techreport,
  type = {Technical report},
  title = {Expected Surprisal and Entropy},
  author = {Roark, Brian},
  date = {2011},
  number = {CSLU-11-004},
  institution = {{Center for Spoken Language Processing, Oregon Health and Science University}},
  url = {https://lanzaroark.org/docs/techrpt-CSLU-11-004.pdf},
  date-added = {2021-06-16 09:42:11 -0400},
  date-modified = {2021-06-16 09:48:54 -0400},
  file = {/Users/j/Zotero/storage/LEN85CGT/Roark - 2011 - Expected surprisal and entropy.pdf}
}

@article{rogers.j:2003,
  title = {Syntactic Structures as Multi-Dimensional Trees},
  author = {Rogers, James},
  date = {2003},
  journaltitle = {Research on Language and Computation},
  volume = {1},
  number = {3-4},
  pages = {265--305},
  publisher = {{Springer}},
  date-added = {2019-06-15 11:50:06 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {automata,control languages}
}

@incollection{rohatgi.v:2015,
  title = {Some {{Special Distributions}}},
  booktitle = {An {{Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Rohatgi, Vijay K. and Saleh, A. K. Md. Ehsanes},
  date = {2015},
  pages = {173--244},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118799635.ch5},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/9781118799635.ch5},
  urldate = {2022-07-20},
  abstract = {This chapter focuses on some commonly occurring probability distributions and investigates their basic properties. The results of this chapter will be of considerable use in theoretical as well as practical applications. The chapter begins with some univariate and multivariate discrete distributions and proceeds with some continuous models. It then deals with bivariate and multivariate normal distributions and subsequently discusses the exponential family of distributions. Finally, the chapter records briefly some of the several other distributions which are related to these special distributions and their important characteristics.},
  isbn = {978-1-118-79963-5},
  langid = {english},
  keywords = {bivariate normal distribution,continuous distributions,discrete distributions,exponential distribution,multivariate normal distribution},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1002/9781118799635.ch5},
  file = {/Users/j/Zotero/storage/7RDPDKEA/2015 - Some Special Distributions.pdf}
}

@misc{rosa.r:2019short,
  title = {Inducing Syntactic Trees from {{BERT}} Representations},
  author = {Rosa, Rudolf and Mareček, David},
  date = {2019},
  eprint = {1906.11511},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-16 11:51:58 -0400}
}

@incollection{rosenbloom.p:1987,
  title = {Learning by {{Chunking}}: {{A Production System Model}} of {{Practice}}},
  booktitle = {Production {{System Models}} of {{Learning}} and {{Development}}},
  author = {Rosenbloom, Paul and Newell, Allen},
  editor = {Klahr, David and Langley, Patrick W. and Neches, Robert T.},
  date = {1987-01-26},
  pages = {221--286},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/5605.003.0007},
  url = {https://doi.org/10.7551/mitpress/5605.003.0007},
  urldate = {2022-09-25},
  isbn = {978-0-262-31596-8}
}

@inproceedings{rosenkrantz.d:1970,
  title = {Deterministic Left Corner Parsing},
  booktitle = {11th Annual Symposium on Switching and Automata Theory (Swat 1970)},
  author = {Rosenkrantz, D. J. and Lewis, P. M.},
  date = {1970},
  pages = {139--152},
  doi = {10.1109/SWAT.1970.5},
  url = {https://doi.org/10.1109/SWAT.1970.5},
  date-added = {2022-03-11 22:35:17 -0500},
  date-modified = {2022-03-11 22:35:18 -0500}
}

@book{rosenthal.j:2006,
  title = {A First Look at Rigorous Probability Theory},
  author = {Rosenthal, Jeffrey S},
  date = {2006-11},
  edition = {2},
  publisher = {{World Scientific}},
  doi = {10.1142/6300},
  url = {https://doi.org/10.1142%2F6300},
  bdsk-url-2 = {https://doi.org/10.1142/6300},
  bdsk-url-3 = {http://probability.ca/jeff/grprobbook.html},
  date-added = {2021-10-15 14:24:25 -0400},
  date-modified = {2021-10-15 14:25:40 -0400}
}

@article{rouder.j:2015,
  title = {The {{Lognormal Race}}: {{A Cognitive-Process Model}} of {{Choice}} and {{Latency}} with {{Desirable Psychometric Properties}}},
  shorttitle = {The {{Lognormal Race}}},
  author = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  date = {2015-06},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {80},
  number = {2},
  pages = {491--513},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9396-3},
  url = {http://link.springer.com/10.1007/s11336-013-9396-3},
  urldate = {2022-08-13},
  langid = {english}
}

@book{royden.h:2010,
  title = {Real Analysis},
  author = {Royden, H. L. and Fitzpatrick, Patrick},
  date = {2010},
  edition = {4th ed},
  publisher = {{Prentice Hall}},
  location = {{Boston}},
  abstract = {Real Analysis, Fourth Edition, covers the basic material that every reader should know in the classical theory of functions of a real variable, measure and integration theory, and some of the more important and elementary topics in general topology and normed linear space theory. This text assumes a general background in mathematics and familiarity with the fundamental concepts of analysis. Classical theory of functions, including the classical Banach spaces; General topology and the theory of general Banach spaces; Abstract treatment of measure and integration. For all readers interested in real analysis},
  isbn = {978-0-13-143747-0},
  langid = {english},
  annotation = {OCLC: 456836719}
}

@article{ryskin.r:2021,
  title = {An {{ERP}} Index of Real-Time Error Correction within a Noisy-Channel Framework of Human Communication},
  author = {Ryskin, Rachel and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Fedorenko, Evelina and Gibson, Edward},
  date = {2021-07-30},
  journaltitle = {Neuropsychologia},
  shortjournal = {Neuropsychologia},
  volume = {158},
  pages = {107855},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107855},
  url = {https://www.sciencedirect.com/science/article/pii/S0028393221001068},
  urldate = {2022-06-24},
  abstract = {Recent evidence suggests that language processing is well-adapted to noise in the input (e.g., spelling or speech errors, misreading or mishearing) and that comprehenders readily correct the input via rational inference over possible intended sentences given probable noise corruptions. In the current study, we probed the processing of noisy linguistic input, asking whether well-studied ERP components may serve as useful indices of this inferential process. In particular, we examined sentences where semantic violations could be attributed to noise—for example, in “The storyteller could turn any incident into an amusing antidote”, where the implausible word “antidote” is orthographically and phonologically close to the intended “anecdote”. We found that the processing of such sentences—where the probability that the message was corrupted by noise exceeds the probability that it was produced intentionally and perceived accurately—was associated with a reduced (less negative) N400 effect and an increased P600 effect, compared to semantic violations which are unlikely to be attributed to noise (“The storyteller could turn any incident into an amusing hearse”). Further, the magnitudes of these ERP effects were correlated with the probability that the comprehender retrieved a plausible alternative. This work thus adds to the growing body of literature that suggests that many aspects of language processing are optimized for dealing with noise in the input, and opens the door to electrophysiologic investigations of the computations that support the processing of imperfect input.},
  langid = {english},
  keywords = {electroencephalography,event-related potential,N400,noisy-channel,P600},
  file = {/Users/j/Zotero/storage/85E24ABK/Ryskin et al. - 2021 - An ERP index of real-time error correction within .pdf}
}

@book{sag.i:2003,
  title = {Syntactic Theory: {{A}} Formal Introduction},
  author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M.},
  date = {2003},
  edition = {2},
  publisher = {{CSLI}},
  location = {{Stanford, CA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{sag.i:2012,
  title = {Sign-Based Construction Grammar: {{An}} Informal Synopsis},
  booktitle = {Sign–{{Based}} Construction Grammar},
  author = {Sag, Ivan A.},
  editor = {Boas, Hans abd Sag, Ivan A.},
  date = {2012},
  pages = {101--107},
  publisher = {{CSLI Publications}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:05 -0400}
}

@article{sainburg.t:2019,
  title = {Parallels in the Sequential Organization of Birdsong and Human Speech},
  author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q},
  date = {2019},
  journaltitle = {Nature communications},
  volume = {10},
  publisher = {{Nature Publishing Group}},
  date-added = {2019-10-01 16:50:44 -0400},
  date-modified = {2019-10-01 16:51:36 -0400},
  keywords = {birdsong,mutual information,structure,syntax}
}

@inproceedings{salimans.t:2015MCVI,
  title = {Markov Chain Monte Carlo and Variational Inference: {{Bridging}} the Gap},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  editor = {Bach, Francis and Blei, David},
  date = {2015-07-07/2015-07-09},
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {1218--1226},
  publisher = {{PMLR}},
  location = {{Lille, France}},
  url = {https://proceedings.mlr.press/v37/salimans15.html},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  date-added = {2022-05-05 11:01:25 -0400},
  date-modified = {2022-05-05 11:02:29 -0400},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf},
  keywords = {markov chain monte carlo,markov chain variational inference,variational inference}
}

@misc{salle.a:2019,
  title = {Why so down? {{The}} Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics},
  author = {Salle, Alexandre and Villavicencio, Aline},
  date = {2019},
  eprint = {1908.06941},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-07-13 08:39:45 -0400},
  date-modified = {2020-07-13 08:40:45 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,pmi}
}

@article{sanford.a:2002,
  title = {Depth of Processing in Language Comprehension: Not Noticing the Evidence},
  shorttitle = {Depth of Processing in Language Comprehension},
  author = {Sanford, Anthony J. and Sturt, Patrick},
  date = {2002-09-01},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {9},
  pages = {382--386},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01958-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661302019587},
  urldate = {2022-06-14},
  abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
  langid = {english},
  keywords = {Language interpretation,Representation,Semantic anomalies,Text-change-blindness,Underspecification}
}

@misc{sanh.v:2019distilbert,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  date = {2019},
  eprint = {1910.01108},
  eprinttype = {arxiv},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@inproceedings{saphra.n:2018,
  title = {Understanding Learning Dynamics of Language Models with {{SVCCA}}},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Saphra, Naomi and Lopez, Adam},
  date = {2019},
  pages = {3257--3267},
  publisher = {{Association for Computational Linguistics}},
  location = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1329},
  url = {https://www.aclweb.org/anthology/N19-1329},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1329}
}

@inproceedings{savinov.n:2022,
  title = {Step-Unrolled Denoising Autoencoders for Text Generation},
  booktitle = {International Conference on Learning Representations},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and van den Oord, Aaron},
  options = {useprefix=true},
  date = {2022},
  url = {https://openreview.net/forum?id=T0GpzBQ1Fg6},
  date-added = {2022-05-04 10:51:51 -0400},
  date-modified = {2022-05-04 10:52:30 -0400},
  keywords = {autoencoders,denoising,diffusion processes,language modeling,SUNDAE}
}

@incollection{scha.r:1990,
  title = {Taaltheorie En Taaltechnologie; Competence En Performance.},
  booktitle = {Computertoepassingen in de Neerlandistiek},
  author = {Scha, Remko},
  editor = {de Kort, R. and Leerdam, G.L.J.},
  options = {useprefix=true},
  date = {1990},
  pages = {7--22},
  publisher = {{Landelijke Vereniging van Neerlandici}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  pass = {1},
  readinglist = {Thesis}
}

@thesis{schabes.y:1990phd,
  title = {Mathematical and Computational Aspects of Lexicalized Grammars},
  author = {Schabes, Yves},
  date = {1990},
  institution = {{University of Pennsylvania}},
  url = {https://repository.upenn.edu/dissertations/AAI9101213},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2022-04-26 21:21:04 -0400},
  project = {syntactic embedding}
}

@misc{schick-poland.k:2021,
  title = {A Partial Information Decomposition for Discrete and Continuous Variables},
  author = {Schick-Poland, Kyle and Makkeh, Abdullah and Gutknecht, Aaron J. and Wollstadt, Patricia and Sturm, Anja and Wibral, Michael},
  date = {2021},
  eprint = {2106.12393},
  eprinttype = {arxiv},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-30 17:30:27 -0400},
  date-modified = {2021-09-30 17:30:29 -0400}
}

@article{schijndel.m:2013,
  title = {A Model of Language Processing as Hierarchic Sequential Prediction},
  author = {van Schijndel, Marten and Exley, Andy and Schuler, William},
  options = {useprefix=true},
  date = {2013-06},
  journaltitle = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {522--540},
  publisher = {{Wiley}},
  doi = {10.1111/tops.12034},
  url = {https://doi.org/10.1111%2Ftops.12034},
  bdsk-url-2 = {https://doi.org/10.1111/tops.12034},
  date-added = {2021-09-13 21:29:14 -0400},
  date-modified = {2021-09-13 21:29:17 -0400}
}

@inproceedings{schijndel.m:2015,
  title = {Hierarchic Syntax Improves Reading Time Prediction},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {van Schijndel, Marten and Schuler, William},
  options = {useprefix=true},
  date = {2015},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/n15-1183},
  url = {https://doi.org/10.3115%2Fv1%2Fn15-1183},
  bdsk-url-2 = {https://doi.org/10.3115/v1/n15-1183},
  date-added = {2021-09-13 21:36:26 -0400},
  date-modified = {2021-09-13 21:36:30 -0400}
}

@inproceedings{schijndel.m:2017,
  title = {Approximations of Predictive Entropy Correlate with Reading Times},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Cognitive Science Society}}},
  author = {van Schijndel, Marten and Schuler, William},
  editor = {Gunzelmann, Glenn and Andrew Howes, Thora Tenbrink and Davelaar, Eddy},
  options = {useprefix=true},
  date = {2017},
  pages = {1266--1271},
  location = {{London, United Kingdom}},
  url = {https://cogsci.mindmodeling.org/2017/papers/0242/index.html},
  date-added = {2022-04-21 09:42:08 -0400},
  date-modified = {2022-04-21 09:44:53 -0400},
  organization = {{Cognitive Science Society}},
  keywords = {entropy reduction,predictability,predictive entropy}
}

@inproceedings{schijndel.m:2018,
  title = {Modeling Garden Path Effects without Explicit Hierarchical Syntax.},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {van Schijndel, Marten and Linzen, Tal},
  editor = {{Rogers} and {Rau} and {Zhu} and {Kalish}},
  options = {useprefix=true},
  date = {2018},
  location = {{Madison, Wisconsin}},
  url = {https://cogsci.mindmodeling.org/2018/papers/0496/},
  date-added = {2021-03-18 10:48:52 -0400},
  date-modified = {2021-03-18 11:56:18 -0400},
  isbn = {978-0-9911967-8-4}
}

@inproceedings{schijndel.m:2018a,
  title = {A Neural Model of Adaptation in Reading},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {van Schijndel, Marten and Linzen, Tal},
  options = {useprefix=true},
  date = {2018},
  pages = {4704--4710},
  publisher = {{Association for Computational Linguistics}},
  location = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1499},
  url = {https://www.aclweb.org/anthology/D18-1499},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1499}
}

@inproceedings{schijndel.m:2019,
  title = {Can Entropy Explain Successor Surprisal Effects in Reading?},
  booktitle = {Proceedings of the Society for Computation in Linguistics ({{SCiL}}), {{NYC}}, January 3–6, 2019},
  author = {van Schijndel, Marten and Linzen, Tal},
  options = {useprefix=true},
  date = {2019},
  volume = {2},
  number = {2},
  pages = {1--7},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/QTBB-9D05},
  url = {https://scholarworks.umass.edu/scil/vol2/iss1/2/},
  bdsk-url-2 = {https://doi.org/10.7275/QTBB-9D05},
  date-added = {2022-04-21 09:21:31 -0400},
  date-modified = {2022-04-21 09:39:00 -0400}
}

@misc{schijndel.m:2020psyarxiv,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {van Schijndel, Marten and Linzen, Tal},
  options = {useprefix=true},
  date = {2020},
  doi = {10.31234/osf.io/sgbqy},
  url = {https://doi.org/10.31234/osf.io/sgbqy},
  bdsk-url-1 = {psyarxiv.com/sgbqy},
  date-added = {2021-03-10 11:20:44 -0500},
  date-modified = {2021-03-18 17:20:22 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing}
}

@article{schijndel.m:2021,
  title = {Single-{{Stage Prediction Models Do Not Explain}} the {{Magnitude}} of {{Syntactic Disambiguation Difficulty}}},
  author = {van Schijndel, Marten and Linzen, Tal},
  options = {useprefix=true},
  date = {2021},
  journaltitle = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12988},
  issn = {1551-6709},
  doi = {10.1111/cogs.12988},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12988},
  urldate = {2022-10-11},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  keywords = {Garden paths,Information theory,Neural networks,Self-paced reading,Surprisal},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/cogs.12988},
  file = {/Users/j/Zotero/storage/99C587CD/van Schijndel and Linzen (2021) Single-Stage Prediction Models Do Not Explain the .pdf}
}

@article{schooler.l:1997,
  title = {The {{Role}} of {{Process}} in the {{Rational Analysis}} of {{Memory}}},
  author = {Schooler, Lael J. and Anderson, John R.},
  date = {1997-04},
  journaltitle = {Cognitive Psychology},
  shortjournal = {Cognitive Psychology},
  volume = {32},
  number = {3},
  pages = {219--250},
  issn = {00100285},
  doi = {10.1006/cogp.1997.0652},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028597906526},
  urldate = {2022-09-27},
  langid = {english},
  file = {/Users/j/Zotero/storage/Q8WPISAE/Schooler and Anderson (1997) The Role of Process in the Rational Analysis of Me.pdf}
}

@inproceedings{schuler.w:2008,
  title = {Toward a Psycholinguistically-Motivated Model of Language Processing},
  booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  author = {Schuler, William and AbdelRahman, Samir and Miller, Tim and Schwartz, Lane},
  date = {2008-08},
  pages = {785--792},
  publisher = {{Coling 2008 Organizing Committee}},
  location = {{Manchester, UK}},
  url = {https://aclanthology.org/C08-1099},
  date-added = {2022-05-02 11:58:18 -0400},
  date-modified = {2022-05-02 11:58:33 -0400},
  keywords = {incrementality,parsing,parsing algorithm}
}

@article{schutze.c:2003,
  title = {Syncretism and Double Agreement with {{Icelandic}} Nominative Objects},
  author = {Schütze, Carson T},
  date = {2003},
  bdsk-url-1 = {escholarship.org/uc/item/8vn9b04q},
  date-added = {2020-03-05 10:17:01 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {syncretism}
}

@article{schwartz.m:2008,
  title = {The Importance of Stupidity in Scientific Research},
  author = {Schwartz, Martin A.},
  date = {2008-06-01},
  journaltitle = {Journal of Cell Science},
  shortjournal = {Journal of Cell Science},
  volume = {121},
  number = {11},
  pages = {1771},
  issn = {0021-9533},
  doi = {10.1242/jcs.033340},
  url = {https://doi.org/10.1242/jcs.033340},
  urldate = {2022-06-16},
  keywords = {scientific method,stupidity},
  file = {/Users/j/Zotero/storage/VUWEZK4C/Schwartz - 2008 - The importance of stupidity in scientific research.pdf}
}

@misc{schwartz.r:2019,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  date = {2019},
  eprint = {1907.10597},
  eprinttype = {arxiv},
  primaryclass = {cs.CY},
  archiveprefix = {arXiv},
  date-added = {2019-09-30 11:48:17 -0400},
  date-modified = {2019-09-30 11:50:22 -0400},
  keywords = {energy,environmental impact}
}

@article{sebastiani.p:2000,
  title = {Maximum Entropy Sampling and Optimal Bayesian Experimental Design},
  author = {Sebastiani, Paola and Wynn, Henry P.},
  date = {2000},
  journaltitle = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {62},
  number = {1},
  eprint = {2680683},
  eprinttype = {jstor},
  pages = {145--157},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {13697412, 14679868},
  abstract = {When Shannon entropy is used as a criterion in the optimal design of experiments, advantage can be taken of the classical identity representing the joint entropy of parameters and observations as the sum of the marginal entropy of the observations and the preposterior conditional entropy of the parameters. Following previous work in which this idea was used in spatial sampling, the method is applied to standard parameterized Bayesian optimal experimental design. Under suitable conditions, which include non-linear as well as linear regression models, it is shown in a few steps that maximizing the marginal entropy of the sample is equivalent to minimizing the pre-posterior entropy, the usual Bayesian criterion, thus avoiding the use of conditional distributions. It is shown using this marginal formulation that under normality assumptions every standard model which has a two-point prior distribution on the parameters gives an optimal design supported on a single point. Other results include a new asymptotic formula which applies as the error variance is large and bounds on support size.},
  date-added = {2021-09-15 10:25:32 -0400},
  date-modified = {2021-09-15 10:25:34 -0400}
}

@inproceedings{shain.c:2018,
  title = {Deep Syntactic Annotations for Broad-Coverage Psycholinguistic Modeling},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Shain, Cory and Schijndel, Marten Van},
  editor = {Devereux, Barry and Shutova, Ekaterina and Huang, Chu-Ren},
  date = {2018},
  pages = {33--37},
  publisher = {{European Language Resources Association (ELRA)}},
  location = {{Paris, France}},
  url = {http://lrec-conf.org/workshops/lrec2018/W9/pdf/9<sub>W</sub>9.pdf},
  date-added = {2021-06-08 09:41:36 -0400},
  date-modified = {2021-06-08 09:43:23 -0400},
  isbn = {979-10-95546-08-5},
  langid = {english}
}

@inproceedings{shain.c:2018a,
  title = {Deconvolutional Time Series Regression: {{A}} Technique for Modeling Temporally Diffuse Effects},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Shain, Cory and Schuler, William},
  date = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/d18-1288},
  url = {https://doi.org/10.18653%2Fv1%2Fd18-1288},
  bdsk-url-2 = {https://doi.org/10.18653/v1/d18-1288},
  date-added = {2021-09-18 22:25:03 -0400},
  date-modified = {2021-09-18 22:25:04 -0400}
}

@article{shannon.c:1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  date = {1948-06},
  journaltitle = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  url = {https://doi.org/10.1002%2Fj.1538-7305.1948.tb01338.x},
  bdsk-url-2 = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  date-added = {2022-04-07 13:03:48 -0400},
  date-modified = {2022-05-04 18:57:35 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{shannon.c:1948reprint,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  date = {1948},
  journaltitle = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Nokia Bell Labs}},
  url = {http://people.math.harvard.edu/ ctm/home/text/others/shannon/entropy/entropy.pdf},
  date-added = {2020-07-27 17:34:08 -0400},
  date-modified = {2022-04-27 12:41:34 -0400},
  project = {information-entropy},
  keywords = {asymptotic equipartition property,information theory}
}

@report{sheldon.d:2013,
  type = {Unpublished report},
  title = {Discrete Adaptive Rejection Sampling},
  author = {Sheldon, Daniel R.},
  date = {2013},
  number = {UM-CS-2013-012},
  institution = {{College of Information and Computer Sciences, University of Massachusetts Amherst}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.685.7109},
  abstract = {Adaptive rejection sampling (ARS) is an algorithm by Gilks and Wild for drawing samples from a continuous log-concave probability distribution with only black-box access to a function that computes the (unnormalized) density function. The ideas extend in a straightforward way to discrete log-concave distributions, but some details of the extension and its implementation can be tricky. This report provides the details of a discrete ARS algorithm. A companion implementation in C, with a MATLAB interface, accompanies the report. 1},
  file = {/Users/j/Zotero/storage/WX4SZ4PI/Sheldon - 2013 - Discrete adaptive rejection sampling.pdf}
}

@inproceedings{shen.t:2020,
  title = {Blank Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  date = {2020},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2020.emnlp-main.420},
  url = {https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.420},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.420},
  date-added = {2022-04-05 19:37:55 -0400},
  date-modified = {2022-04-05 19:38:07 -0400}
}

@inproceedings{shen.y:2017,
  title = {Neural Language Modeling by Jointly Learning Syntax and Lexicon},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron C.},
  date = {2018},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=rkgOLb-0W},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenLHC18.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{shen.y:2018,
  title = {Ordered Neurons: {{Integrating}} Tree Structures into Recurrent Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron C.},
  date = {2019},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=B1l6qiR5F7},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  file = {/Users/j/Zotero/storage/ATDQRUJ9/Shen et al. - 2019 - Ordered neurons Integrating tree structures into .pdf}
}

@article{shepard.r:1987,
  title = {Toward a {{Universal Law}} of {{Generalization}} for {{Psychological Science}}},
  author = {Shepard, Roger N.},
  date = {1987-09-11},
  journaltitle = {Science},
  volume = {237},
  number = {4820},
  pages = {1317--1323},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.3629243},
  url = {http://www.science.org/doi/10.1126/science.3629243},
  urldate = {2022-10-11},
  file = {/Users/j/Zotero/storage/4QYJFWMP/Shepard (1987) Toward a Universal Law of Generalization for Psych.pdf}
}

@book{shields.p:1996,
  title = {The Ergodic Theory of Discrete Sample Paths},
  author = {Shields, Paul C},
  date = {1996},
  volume = {13},
  publisher = {{American Mathematical Soc.}},
  date-added = {2020-08-08 19:47:15 -0400},
  date-modified = {2020-08-08 19:49:28 -0400},
  project = {information-compositionality},
  keywords = {entropy,information theory}
}

@article{shmueli.g:2010,
  title = {To {{Explain}} or to {{Predict}}?},
  author = {Shmueli, Galit},
  date = {2010-08-01},
  journaltitle = {Statistical Science},
  shortjournal = {Statist. Sci.},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
  urldate = {2022-09-26},
  file = {/Users/j/Zotero/storage/UHNA8XZV/Shmueli (2010) To Explain or to Predict.pdf}
}

@article{sigurdsson.h:1996,
  title = {Icelandic Finite Verb Agreement},
  author = {Sigurðsson, Halldór Ármann},
  date = {1996},
  journaltitle = {Working papers in Scandinavian syntax},
  volume = {57},
  pages = {1--46},
  publisher = {{Department of Scandinavian Languages}},
  url = {https://lucris.lub.lu.se/ws/files/4562723/8500167.pdf},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,syncretism},
  file = {/Users/j/Zotero/storage/TVMYQEYL/Sigurðsson - 1996 - Icelandic finite verb agreement.pdf}
}

@incollection{sigurdsson.h:2000,
  title = {The Locus of Case and Agreement},
  author = {Sigurðsson, Halldór Ármann},
  date = {2000},
  series = {Working Papers in {{Scandinavian}} Syntax},
  volume = {65},
  publisher = {{Department of Scandinavian Languages, Lund University}},
  url = {https://portal.research.lu.se/portal/en/publications/the-locus-of-case-and-agreement(10f7ec31-7acf-4f87-a921-814c1dc5b140).html},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2021-03-12 11:36:31 -0500},
  howpublished = {Working paper},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@article{sigurdsson.h:2008,
  title = {Icelandic Dative Intervention: {{Person}} and Number Are Separate Probes},
  author = {Sigurðsson, Halldór Ármann and Holmberg, Anders},
  date = {2008},
  journaltitle = {Agreement restrictions},
  pages = {251--280},
  publisher = {{Mouton de Gruyter Berlin}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:26:40 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,split probe}
}

@article{simon.h:1955,
  title = {A {{Behavioral Model}} of {{Rational Choice}}},
  author = {Simon, Herbert A.},
  date = {1955-02-01},
  journaltitle = {The Quarterly Journal of Economics},
  shortjournal = {The Quarterly Journal of Economics},
  volume = {69},
  number = {1},
  pages = {99--118},
  issn = {0033-5533},
  doi = {10.2307/1884852},
  url = {https://doi.org/10.2307/1884852},
  urldate = {2022-06-13},
  abstract = {Introduction, 99. — I. Some general features of rational choice, 100.— II. The essential simplifications, 103. — III. Existence and uniqueness of solutions, 111. — IV. Further comments on dynamics, 113. — V. Conclusion, 114. — Appendix, 115.},
  file = {/Users/j/Zotero/storage/HRPK7MYQ/Simon - 1955 - A Behavioral Model of Rational Choice.pdf}
}

@article{simon.h:1956,
  title = {Rational Choice and the Structure of the Environment},
  author = {Simon, H. A.},
  date = {1956},
  journaltitle = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {129--138},
  publisher = {{American Psychological Association}},
  location = {{US}},
  issn = {1939-1471},
  doi = {10.1037/h0042769},
  abstract = {"In this paper I have attempted to identify some of the structural characteristics that are typical of the 'psychological' environments of organisms. We have seen that an organism in an environment with these characteristics requires only very simple perceptual and choice mechanisms to satisfy its several needs and to assure a high probability of its survival over extended periods of time. In particular, no 'utility function' needs to be postulated for the organism, nor does it require any elaborate procedure for calculating marginal rates of substitution among different wants." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Choice Behavior,Decision Making},
  file = {/Users/j/Zotero/storage/NXMHT5CF/Simon - 1956 - Rational choice and the structure of the environme.pdf}
}

@inproceedings{smith.n:2008,
  title = {Probabilistic Prediction and the Continuity of Language Comprehension},
  booktitle = {9th Conference on Conceptual Structure, Discourse, and Language ({{CSDL9}})},
  author = {Smith, Nathaniel J. and Levy, Roger},
  date = {2008-10-18},
  url = {https://ssrn.com/abstract=1295346},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2021-03-16 17:32:08 -0400},
  project = {syntactic embedding}
}

@inproceedings{smith.n:2008cogsci,
  title = {Optimal Processing Times in Reading: A Formal Model and Empirical Investigation},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  date = {2008-07-25},
  volume = {30},
  pages = {570--576},
  location = {{Washington, DC, USA}},
  url = {https://escholarship.org/uc/item/3mr8m3rf},
  date-added = {2021-05-31 12:59:02 -0400},
  date-modified = {2021-12-14 20:34:04 -0500},
  eventtitle = {{{CogSci}} 2008},
  file = {/Users/j/Zotero/storage/BKKHXAQN/Smith and Levy (2008) Optimal processing times in reading a formal mode.pdf}
}

@inproceedings{smith.n:2011cloze,
  title = {Cloze but No Cigar: {{The}} Complex Relationship between Cloze, Corpus, and Subjective Probabilities in Language Processing},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  date = {2011},
  number = {33},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.208.5547},
  date-added = {2021-03-16 14:39:39 -0400},
  date-modified = {2022-03-16 10:06:04 -0400}
}

@article{smith.n:2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  date = {2013},
  journaltitle = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027713000413},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability – even for differences between highly unpredictable words – and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2013.02.013},
  date-added = {2021-03-09 22:52:29 -0500},
  date-modified = {2021-11-14 23:57:40 -0500},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Zotero/storage/N767I5L3/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf}
}

@article{smith.p:1969,
  title = {Coding Strategies in Language},
  author = {Smith, Philip Twitchell},
  date = {1969},
  journaltitle = {Information and Control},
  volume = {14},
  number = {1},
  pages = {72--97},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(69)90033-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0019995869900333},
  abstract = {The problem of selecting a code to transmit four messages over the binary symmetric channel is studied in relation to two types of channel noise (“substitution≓ error and “deletion≓ error) and to two types of decoding strategy (maximum hit and minimum error). It is shown that mean Hamming distance is a good general guide to coding efficiency, except in the case of a minimum error strategy with a deletion error channel, where coding efficiency is critically dependent on noise level. An experiment in which subjects selected codes in an artificial language suggests that the process of recall from memory is similar to the process of transmitting over a deletion error channel with a minimum error strategy. A similar interpretation can be placed on the analysis of consonant systems in English, French, German and Welsh, where the sets of consonants of a given class in a given environment are considered as codes whose alphabet is the phonological distinctive feature system of Halle (1958)},
  bdsk-url-2 = {https://doi.org/10.1016/S0019-9958(69)90033-3},
  date-added = {2022-04-27 13:10:31 -0400},
  date-modified = {2022-04-27 13:11:36 -0400},
  keywords = {artificial language,hamming distance,noisy channel coding,phonology}
}

@inproceedings{snyder.b:2009,
  title = {Unsupervised Multilingual Grammar Induction},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {{ACL}} and the 4th International Joint Conference on Natural Language Processing of the {{AFNLP}}: {{Volume}} 1 - Volume 1},
  author = {Snyder, Benjamin and Naseem, Tahira and Barzilay, Regina},
  date = {2009},
  series = {{{ACL}} '09},
  pages = {73--81},
  publisher = {{Association for Computational Linguistics}},
  location = {{USA}},
  url = {http://www.aclweb.org/anthology/P09-1009},
  abstract = {We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting. Using this formalism, our model loosely binds parallel trees while allowing language-specific syntactic structure. We perform inference under this model using Markov Chain Monte Carlo and dynamic programming. Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we find substantial performance gains over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure.},
  date-added = {2022-04-04 12:46:23 -0400},
  date-modified = {2022-04-04 12:47:09 -0400},
  isbn = {978-1-932432-45-9},
  pagetotal = {9}
}

@inproceedings{socolof.m:2022,
  title = {Characterizing Idioms: {{Conventionality}} and Contingency},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Socolof, Michaela and Cheung, Jackie and Wagner, Michael and O'Donnell, Timothy},
  date = {2022-05},
  pages = {4024--4037},
  publisher = {{Association for Computational Linguistics}},
  location = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.278},
  abstract = {Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that introducing special machinery to handle idioms may not be warranted.},
  date-added = {2022-05-17 08:04:25 -0400},
  date-modified = {2022-05-17 08:05:15 -0400}
}

@misc{sohl-dickstein.j:2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {Sohl-Dickstein, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  date = {2015-11-18},
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  eprinttype = {arxiv},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2022-05-17},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,diffusion processes,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/4K59SF2J/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf;/Users/j/Zotero/storage/ZGS83AZ8/1503.html}
}

@inproceedings{song.y:2019,
  title = {Generative {{Modeling}} by {{Estimating Gradients}} of the {{Data Distribution}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  date = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
  urldate = {2022-07-07},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  file = {/Users/j/Zotero/storage/3QU2CCTZ/Song and Ermon - 2019 - Generative Modeling by Estimating Gradients of the.pdf}
}

@inproceedings{song.y:2022,
  title = {Score-{{Based Generative Modeling}} through {{Stochastic Differential Equations}}},
  author = {Song, Yang and Sohl-Dickstein, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  date = {2022-02-10},
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  urldate = {2022-07-11},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a...},
  eventtitle = {International {{Conference}} on {{Learning Representations}}},
  langid = {english},
  file = {/Users/j/Zotero/storage/DH424UAB/Song et al. - 2022 - Score-Based Generative Modeling through Stochastic.pdf}
}

@article{soskuthy.m:2021,
  title = {Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis},
  author = {Sóskuthy, Márton},
  date = {2021-01},
  journaltitle = {Journal of Phonetics},
  volume = {84},
  pages = {101017},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.wocn.2020.101017},
  url = {https://doi.org/10.1016%2Fj.wocn.2020.101017},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2020.101017},
  date-added = {2022-03-04 16:06:14 -0500},
  date-modified = {2022-03-04 16:06:18 -0500}
}

@inproceedings{stabler.e:1997,
  title = {Derivational Minimalism},
  booktitle = {Logical Aspects of Computational Linguistics},
  author = {Stabler, Edward},
  editor = {Retoré, Christian},
  date = {1997},
  pages = {68--95},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052152},
  url = {https://doi.org/10.1007/BFb0052152},
  abstract = {A basic idea of the transformational tradition is that constituents move. More recently, there has been a trend towards the view that all features are lexical features. And in recent “minimalist” grammars, structure building operations are assumed to be feature driven. A simple grammar formalism with these properties is presented here and briefly explored. Grammars in this formalism can define languages that are not in the “mildly context sensitive” class defined by Vijay-Shanker and Weir (1994).},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  isbn = {978-3-540-69631-5},
  project = {syntactic embedding}
}

@incollection{stabler.e:1997a,
  title = {Derivational Minimalism},
  booktitle = {Logical {{Aspects}} of {{Computational Linguistics}}},
  author = {Stabler, Edward},
  editor = {Retoré, Christian},
  options = {useprefix=true},
  date = {1997},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  volume = {1328},
  pages = {68--95},
  publisher = {{Springer Berlin Heidelberg}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052152},
  url = {http://link.springer.com/10.1007/BFb0052152},
  urldate = {2022-09-30},
  editorb = {Carbonell, Jaime G. and Siekmann, Jörg and Goos, G. and Hartmanis, J. and van Leeuwen, J.},
  editorbtype = {redactor},
  isbn = {978-3-540-63700-4 978-3-540-69631-5}
}

@article{stabler.e:2013,
  title = {Two {{Models}} of {{Minimalist}}, {{Incremental Syntactic Analysis}}},
  author = {Stabler, Edward P.},
  date = {2013-07},
  journaltitle = {Topics in Cognitive Science},
  shortjournal = {Top Cogn Sci},
  volume = {5},
  number = {3},
  pages = {611--633},
  issn = {17568757},
  doi = {10.1111/tops.12031},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12031},
  urldate = {2022-09-30},
  langid = {english}
}

@inproceedings{stanojevic.m:2021,
  title = {Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Stanojević, Miloš and Bhattasali, Shohini and Dunagan, Donald and Campanelli, Luca and Steedman, Mark and Brennan, Jonathan and Hale, John},
  date = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.cmcl-1.3},
  url = {https://doi.org/10.18653%2Fv1%2F2021.cmcl-1.3},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.3},
  date-added = {2022-04-14 13:33:35 -0400},
  date-modified = {2022-04-14 13:33:47 -0400}
}

@article{staub.a:2015,
  title = {The Effect of Lexical Predictability on Eye Movements in Reading: {{Critical}} Review and Theoretical Interpretation},
  author = {Staub, Adrian},
  date = {2015},
  journaltitle = {Language and Linguistics Compass},
  volume = {9},
  number = {8},
  pages = {311--327},
  publisher = {{Wiley}},
  doi = {10.1111/lnc3.12151},
  url = {https://doi.org/10.1111%2Flnc3.12151},
  bdsk-url-2 = {https://doi.org/10.1111/lnc3.12151},
  date-added = {2021-05-22 15:55:42 -0400},
  date-modified = {2021-05-22 15:55:54 -0400},
  keywords = {predictability,processing,review}
}

@article{staub.a:2015a,
  title = {The Influence of Cloze Probability and Item Constraint on Cloze Task Response Time},
  author = {Staub, Adrian and Grant, Margaret and Astheimer, Lori and Cohen, Andrew},
  date = {2015-07-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {82},
  pages = {1--17},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.02.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X15000236},
  urldate = {2022-09-07},
  abstract = {In research on the role of lexical predictability in language comprehension, predictability is generally defined as the probability that a word is provided as a sentence continuation in the cloze task (Taylor, 1953), in which subjects are asked to guess the next word of a sentence. The present experiments investigate the process by which subjects generate a cloze response, by measuring the latency to initiate a response in a version of the task in which subjects produce a spoken continuation to a visually presented sentence fragment. Higher probability responses were produced faster than lower probability responses. The latency to produce a response was also influenced by item constraint: A response at a given level of probability was issued faster when the context was more constraining, i.e., a single response was elicited with high probability. We show that these patterns are naturally produced by an activation-based race model in which potential responses independently race towards a response threshold. Implications for the interpretation of cloze probability as a measure of lexical predictability are discussed.},
  langid = {english},
  keywords = {Cloze task,Language processing,Prediction,Response time}
}

@misc{steedman.m:2000,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  date = {2000},
  publisher = {{MIT Press}},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{steedman.m:2000a,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  date = {2000},
  publisher = {{The MIT press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{steinhardt.j:2014,
  title = {Filtering with Abstract Particles},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  author = {Steinhardt, Jacob and Liang, Percy},
  editor = {Xing, Eric P. and Jebara, Tony},
  date = {2014-06-22/2014-06-24},
  series = {Proceedings of Machine Learning Research},
  volume = {32},
  number = {1},
  pages = {727--735},
  publisher = {{PMLR}},
  location = {{Bejing, China}},
  url = {https://proceedings.mlr.press/v32/steinhardt14.html},
  abstract = {Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue by using “abstract particles” that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.},
  date-added = {2022-03-25 12:02:29 -0400},
  date-modified = {2022-03-25 12:02:31 -0400},
  pdf = {http://proceedings.mlr.press/v32/steinhardt14.pdf}
}

@book{stevens.e:2020,
  title = {Deep Learning with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  date = {2020},
  publisher = {{Manning Publications Company}},
  url = {https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf},
  date-added = {2021-08-02 19:51:35 -0400},
  date-modified = {2021-08-02 19:52:08 -0400},
  isbn = {978-1-61729-526-3}
}

@inproceedings{stolcke.a:1994,
  title = {Inducing Probabilistic Grammars by {{Bayesian}} Model Merging},
  booktitle = {Grammatical {{Inference}} and {{Applications}}},
  author = {Stolcke, Andreas and Omohundro, Stephen},
  editor = {Carrasco, Rafael C. and Oncina, Jose},
  date = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {106--118},
  publisher = {{Springer}},
  location = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-58473-0_141},
  abstract = {We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (‘Occam's Razor’). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.},
  isbn = {978-3-540-48985-6},
  langid = {english},
  keywords = {Bayesian Posterior Probability,Beam Search,Hide Markov Model,Merging Algorithm,Relative Clause},
  file = {/Users/j/Zotero/storage/G9XMV94D/Stolcke and Omohundro - 1994 - Inducing probabilistic grammars by Bayesian model .pdf}
}

@article{stolcke.a:1995,
  title = {An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities},
  author = {Stolcke, Andreas},
  date = {1995},
  journaltitle = {Computational Linguistics},
  volume = {21},
  number = {2},
  pages = {165--201},
  url = {https://www.aclweb.org/anthology/J95-2002}
}

@article{stone.m:1960,
  title = {Models for Choice-Reaction Time},
  author = {Stone, Mervyn},
  date = {1960-09-01},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {25},
  number = {3},
  pages = {251--260},
  issn = {1860-0980},
  doi = {10.1007/BF02289729},
  url = {https://doi.org/10.1007/BF02289729},
  urldate = {2022-07-04},
  abstract = {In the two-choice situation, the Wald sequential probability ratio decision procedure is applied to relate the mean and variance of the decision times, for each alternative separately, to the error rates and the ratio of the frequencies of presentation of the alternatives. For situations involving more than two choices, a fixed sample decision procedure (selection of the alternative with highest likelihood) is examined, and the relation is found between the decision time (or size of sample), the error rate, and the number of alternatives.},
  langid = {english},
  keywords = {Decision Procedure,Error Rate,High Likelihood,Public Policy,Statistical Theory}
}

@inproceedings{strubell.e:2019,
  title = {Energy and Policy Considerations for Deep Learning in {{NLP}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  date = {2019},
  pages = {3645--3650},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1355},
  url = {https://www.aclweb.org/anthology/P19-1355},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1355}
}

@article{svenonius.p:2002,
  title = {Icelandic Case and the Structure of Events},
  author = {Svenonius, Peter},
  date = {2002},
  journaltitle = {The Journal of Comparative Germanic Linguistics},
  volume = {5},
  number = {1-3},
  pages = {197--225},
  publisher = {{Springer}},
  url = {https://rdcu.be/b2Dh6},
  date-added = {2020-03-06 14:58:07 -0800},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@article{szewczyk.j:2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  date = {2022-04},
  journaltitle = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2021.104311},
  url = {https://doi.org/10.1016%2Fj.jml.2021.104311},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2021.104311},
  date-added = {2022-01-24 13:49:32 -0500},
  date-modified = {2022-01-24 13:49:35 -0500}
}

@article{tabor.w:2004,
  title = {Evidence for Self-Organized Sentence Processing: {{Digging-in}} Effects.},
  author = {Tabor, Whitney and Hutchins, Sean},
  date = {2004},
  journaltitle = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {30},
  number = {2},
  pages = {431--450},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0278-7393.30.2.431},
  url = {https://doi.org/10.1037%2F0278-7393.30.2.431},
  bdsk-url-2 = {https://doi.org/10.1037/0278-7393.30.2.431},
  date-added = {2021-04-11 18:58:19 -0400},
  date-modified = {2021-04-11 18:58:36 -0400},
  keywords = {diggin in effect,reading time}
}

@inproceedings{takabatake.k:2004,
  title = {Information Geometry of {{Gibbs}} Sampler},
  booktitle = {Proc. of {{WSEAS}} Int. {{Conf}}. on Neural Networks and Applications ({{NNA}})},
  author = {Takabatake, Kazuya},
  date = {2004},
  url = {https://staff.aist.go.jp/k.takabatake/takabatake04.pdf},
  date-added = {2021-03-11 16:56:36 -0500},
  date-modified = {2021-03-11 17:08:06 -0500},
  keywords = {information theory,sampling},
  file = {/Users/j/Zotero/storage/I9SGWKEM/Takabatake - 2004 - Information geometry of Gibbs sampler.pdf}
}

@article{tanenhaus.m:1995,
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Tanenhaus, Michael K. and Spivey-Knowlton, Michael J. and Eberhard, Kathleen M. and Sedivy, Julie C.},
  date = {1995-06},
  journaltitle = {Science (New York, N.Y.)},
  shortjournal = {Science},
  volume = {268},
  number = {5217},
  pages = {1632--1634},
  publisher = {{American Association for the Advancement of Science (AAAS)}},
  doi = {10.1126/science.7777863},
  url = {https://doi.org/10.1126%2Fscience.7777863},
  abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
  bdsk-url-2 = {https://doi.org/10.1126/science.7777863},
  date-added = {2022-04-20 13:14:59 -0400},
  date-modified = {2022-05-02 14:45:52 -0400},
  keywords = {incrementality}
}

@incollection{taraldsen.k:1995,
  title = {On Agreement and Nominative Objects in {{Icelandic}}},
  booktitle = {Studies in Comparative {{Germanic}} Syntax},
  author = {Taraldsen, Knut Tarald},
  date = {1995},
  pages = {307--327},
  publisher = {{Springer}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:17:31 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,split probe}
}

@article{tax.t:2017,
  title = {The Partial Information Decomposition of Generative Neural Network Models},
  author = {Tax, Tycho M.S. and Mediano, Pedro A.M. and Shanahan, Murray},
  date = {2017},
  journaltitle = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  shortjournal = {Entropy},
  volume = {19},
  number = {9},
  issn = {1099-4300},
  doi = {10.3390/e19090474},
  url = {https://www.mdpi.com/1099-4300/19/9/474},
  abstract = {In this work we study the distributed representations learnt by generative neural network models. In particular, we investigate the properties of redundant and synergistic information that groups of hidden neurons contain about the target variable. To this end, we use an emerging branch of information theory called partial information decomposition (PID) and track the informational properties of the neurons through training. We find two differentiated phases during the training process: a first short phase in which the neurons learn redundant information about the target, and a second phase in which neurons start specialising and each of them learns unique information about the target. We also find that in smaller networks individual neurons learn more specific information about certain features of the input, suggesting that learning pressure can encourage disentangled representations.},
  article-number = {474},
  bdsk-url-2 = {https://doi.org/10.3390/e19090474},
  date-added = {2022-05-14 10:28:04 -0400},
  date-modified = {2022-05-14 10:28:21 -0400},
  keywords = {neural networks,partial information decomposition}
}

@misc{tay.y:2022,
  title = {Transformer Memory as a Differentiable Search Index},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  date = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2202.06991},
  url = {https://arxiv.org/abs/2202.06991},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2202.06991},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 11:01:29 -0400},
  date-modified = {2022-03-31 11:02:20 -0400},
  keywords = {transformer},
  file = {/Users/j/Zotero/storage/KXPFC45L/Tay et al. - 2022 - Transformer memory as a differentiable search inde.pdf}
}

@article{taylor.w:1953,
  title = {‘{{Cloze}} Procedure’: {{A}} New Tool for Measuring Readability},
  author = {Taylor, Wilson L.},
  date = {1953},
  journaltitle = {Journalism Quarterly},
  volume = {30},
  number = {4},
  pages = {415--433},
  publisher = {{SAGE Publications}},
  doi = {10.1177/107769905303000401},
  url = {https://doi.org/10.1177%2F107769905303000401},
  bdsk-url-2 = {https://doi.org/10.1177/107769905303000401},
  date-added = {2021-03-18 10:41:54 -0400},
  date-modified = {2021-03-18 11:25:18 -0400},
  keywords = {cloze,processing}
}

@inproceedings{teh.y:2006,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman-Yor}} Processes},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Computational Linguistics}} and the 44th Annual Meeting of the {{ACL}} - {{ACL}} 06},
  author = {Teh, Yee Whye},
  date = {2006},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1220175.1220299},
  url = {https://doi.org/10.3115%2F1220175.1220299},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220299},
  date-added = {2022-04-25 21:37:17 -0400},
  date-modified = {2022-04-25 21:38:48 -0400},
  keywords = {bayesian,HMM,Natural language processing,Pitman-Yor processes}
}

@report{teh.y:2006techreport,
  type = {Technical report},
  title = {A Bayesian Interpretation of Interpolated Kneser-Ney},
  author = {Teh, Yee Whye},
  date = {2006},
  number = {TRA2/06},
  institution = {{School of Computing, National University of Singapore}},
  url = {https://dl.comp.nus.edu.sg/xmlui/handle/1900.100/1911},
  abstract = {Interpolated Kneser-Ney is one of the best smoothing methods for n-gram language models. Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method. We propose a novel interpretation of interpolated Kneser-Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman-Yor processes. As opposed to past explanations, our interpretation can recover exactly the formulation of interpolated Kneser-Ney, and performs better than interpolated Kneser-Ney when a better inference procedure is used.},
  date-added = {2022-04-26 10:12:38 -0400},
  date-modified = {2022-04-26 10:16:03 -0400},
  keywords = {bayesian,Dirichlet processes,hierarchical clustering,language modeling,Pitman-Yor processes},
  file = {/Users/j/Zotero/storage/I3DNB6B8/Teh - 2006 - A bayesian interpretation of interpolated kneser-n.pdf}
}

@article{tenenbaum.j:2001,
  title = {Generalization, Similarity, and {{Bayesian}} Inference},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  date = {2001-08},
  journaltitle = {Behavioral and Brain Sciences},
  volume = {24},
  number = {4},
  pages = {629--640},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X01000061},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalization-similarity-and-bayesian-inference/595CAA321C9C56270C624057021DE77A},
  urldate = {2022-10-11},
  abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
  langid = {english},
  keywords = {additive clustering,Bayesian inference,categorization,concept learning,contrast model,features,generalization,psychological space,similarity},
  file = {/Users/j/Zotero/storage/2NBULECD/Tenenbaum and Griffiths (2001) Generalization, similarity, and Bayesian inference.pdf}
}

@inproceedings{tenney.i:2019,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  date = {2019},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=SJzSgnRcKX},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{tenney.t:2019,
  title = {{{BERT}} Rediscovers the Classical {{NLP}} Pipeline},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  date = {2019},
  pages = {4593--4601},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1452},
  url = {https://www.aclweb.org/anthology/P19-1452},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1452}
}

@inproceedings{terra.e:2003,
  title = {Frequency Estimates for Statistical Word Similarity Measures},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Terra, Egidio L. and Clarke, Charles L. A.},
  date = {2003},
  pages = {244--251},
  url = {https://www.aclweb.org/anthology/N03-1032}
}

@book{tesniere.l:1959,
  title = {Élements de Syntaxe Structurale. {{Préf}}. de Jean Fourquet},
  author = {Tesnière, Lucien},
  date = {1959},
  publisher = {{C. Klincksieck}},
  date-added = {2021-07-16 19:40:41 -0400},
  date-modified = {2021-07-16 19:40:43 -0400}
}

@book{tesniere.l:2015,
  title = {Elements of Structural Syntax},
  author = {Tesnière, Lucien},
  date = {2015},
  publisher = {{John Benjamins Publishing Company}},
  doi = {10.1075/z.185},
  url = {https://doi.org/10.1075%2Fz.185},
  bdsk-url-2 = {https://doi.org/10.1075/z.185},
  date-added = {2021-06-24 10:12:36 -0400},
  date-modified = {2021-06-24 10:13:38 -0400}
}

@inproceedings{thai.b:2020,
  title = {Fully {{Convolutional ASR}} for {{Less-Resourced Endangered Languages}}},
  booktitle = {Proceedings of the 1st {{Joint Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages ({{SLTU}}) and {{Collaboration}} and {{Computing}} for {{Under-Resourced Languages}} ({{CCURL}})},
  author = {Thai, Bao and Jimerson, Robert and Ptucha, Raymond and Prud'hommeaux, Emily},
  date = {2020-05},
  pages = {126--130},
  publisher = {{European Language Resources association}},
  location = {{Marseille, France}},
  url = {https://aclanthology.org/2020.sltu-1.17},
  urldate = {2022-06-06},
  abstract = {The application of deep learning to automatic speech recognition (ASR) has yielded dramatic accuracy increases for languages with abundant training data, but languages with limited training resources have yet to see accuracy improvements on this scale. In this paper, we compare a fully convolutional approach for acoustic modelling in ASR with a variety of established acoustic modeling approaches. We evaluate our method on Seneca, a low-resource endangered language spoken in North America. Our method yields word error rates up to 40\% lower than those reported using both standard GMM-HMM approaches and established deep neural methods, with a substantial reduction in training time. These results show particular promise for languages like Seneca that are both endangered and lack extensive documentation.},
  isbn = {979-10-95546-35-1},
  langid = {english},
  keywords = {automatic speech recognition,computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/RIFWW3C3/Thai et al. - 2020 - Fully Convolutional ASR for Less-Resourced Endange.pdf}
}

@book{thrun.s:2005book,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  date = {2005},
  publisher = {{MIT Press}},
  url = {https://mitpress.ublish.com/book/probabilistic-robotics},
  date-added = {2022-05-05 10:19:08 -0400},
  date-modified = {2022-05-05 10:21:11 -0400},
  isbn = {978-0-262-36380-8}
}

@misc{tishby.n:2000,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  date = {2000},
  eprint = {physics/0004057},
  eprinttype = {arxiv},
  archiveprefix = {arXiv},
  date-added = {2020-07-21 08:44:18 -0400},
  date-modified = {2020-07-21 08:47:35 -0400},
  project = {syntactic embedding},
  keywords = {information bottleneck,information theory,variational inference}
}

@inproceedings{titov.i:2007,
  title = {A Latent Variable Model for Generative Dependency Parsing},
  booktitle = {Proceedings of the Tenth International Conference on Parsing Technologies},
  author = {Titov, Ivan and Henderson, James},
  date = {2007-06},
  pages = {144--155},
  publisher = {{Association for Computational Linguistics}},
  location = {{Prague, Czech Republic}},
  url = {https://aclanthology.org/W07-2218},
  date-added = {2022-04-26 17:35:02 -0400},
  date-modified = {2022-04-26 17:35:31 -0400},
  keywords = {Dependency Grammar,dependency parsing,generative grammar}
}

@incollection{townsend.j:1974,
  title = {Issues and {{Models Concerning}} the {{Processing}} of a {{Finite Number}} of {{Inputs}}},
  booktitle = {Human {{Information Processing}}},
  author = {Townsend, James T.},
  date = {1974},
  publisher = {{Routledge}},
  abstract = {The broadest and most critical arena of investigation and contention with regard to these two matters has always been consciousness itself. Broadbent unveiled a theoretical structure that allowed collation of a substantial body of experimental literature and that was seminal in its influence on later developments. Hybrid models have been of limited current theoretical interest, probably due in part to the difficulty in testing them experimentally. As remarked earlier, the quite special case of seriality versus parallelism is difficult enough to discriminate experimentally. Among such hybrid models are those that represent processing as being serial part of the time and partially parallel within trials. There are many occasions in perceptual and memorial experiments where the information sufficient to make a correct response is embedded in only part of the total stimulus pattern presented to the subject. A finding of independence of total completion times, for instance, although perhaps more intuitively associated with parallel models, can be predicted by serial models.},
  isbn = {978-1-00-317668-8},
  pagetotal = {53},
  file = {/Users/j/Zotero/storage/EXDXLUI2/Townsend - 1974 - Issues and Models Concerning the Processing of a F.pdf}
}

@article{townsend.j:1990,
  title = {Serial vs. {{Parallel Processing}}: {{Sometimes They Look}} like {{Tweedledum}} and {{Tweedledee}} but They Can (and {{Should}}) Be {{Distinguished}}},
  shorttitle = {Serial vs. {{Parallel Processing}}},
  author = {Townsend, James T.},
  date = {1990-01-01},
  journaltitle = {Psychological Science},
  shortjournal = {Psychol Sci},
  volume = {1},
  number = {1},
  pages = {46--54},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1990.tb00067.x},
  url = {https://doi.org/10.1111/j.1467-9280.1990.tb00067.x},
  urldate = {2022-06-24},
  abstract = {A number of important models of information processing depend on whether processing is serial or parallel. However, many of the studies purporting to settle the case use weak experimental paradigms or results to draw conclusions. A brief history of the issue is given along with examples from the literature. Then a number of promising methods are presented from a variety of sources with some discussion of their potential. A brief discussion of the topic with regard to overall issues of model testing and applications concludes the paper.},
  langid = {english},
  file = {/Users/j/Zotero/storage/NUGB823Y/Townsend - 1990 - Serial vs. Parallel Processing Sometimes They Loo.pdf}
}

@article{townsend.j:2004,
  title = {Parallel versus Serial Processing and Individual Differences in High-Speed Search in Human Memory},
  author = {Townsend, James T. and Fifić, Mario},
  date = {2004-08-01},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {66},
  number = {6},
  pages = {953--962},
  issn = {1532-5962},
  doi = {10.3758/BF03194987},
  url = {https://doi.org/10.3758/BF03194987},
  urldate = {2022-06-24},
  abstract = {Many mental tasks that involve operations on a number of items take place within a few hundred milliseconds. In such tasks, whether the items are processed simultaneously (in parallel) or sequentially (serially) has long been of interest to psychologists. Although certain types of parallel and serial models have been ruled out, it has proven extremely difficult to entirely separate reasonable serial and limitedcapacity parallel models on the basis of typical data. Recent advances in theory-driven methodology now permit strong tests of serial versus parallel processing in such tasks, in ways that bypass the capacity issue and that are distribution and parameter free. We employ new methodologies to assess serial versus parallel processing and find strong evidence for pure serial or pure parallel processing, with some striking apparent differences across individuals and interstimulus conditions.},
  langid = {english},
  keywords = {Memory Search,Parallel Model,Parallel Processing,Serial Processing,Visual Search},
  file = {/Users/j/Zotero/storage/DXVKSQ9T/Townsend and Fifić - 2004 - Parallel versus serial processing and individual d.pdf}
}

@inproceedings{trevisan.l:2009,
  title = {Regularity, {{Boosting}}, and {{Efficiently Simulating Every High-Entropy Distribution}}},
  booktitle = {2009 24th {{Annual IEEE Conference}} on {{Computational Complexity}}},
  author = {Trevisan, Luca and Tulsiani, Madhur and Vadhan, Salil},
  date = {2009-07},
  pages = {126--136},
  issn = {1093-0159},
  doi = {10.1109/CCC.2009.41},
  abstract = {We show that every bounded function g: 0,1n rarr [0,1] admits an efficiently computable "simulator" function h: 0,1n rarr [0,1] such that every fixed polynomial size circuit has approximately the same correlation with g as with h. If g describes (up to scaling) a high min-entropy distribution D, then h can be used to efficiently sample a distribution D' of the same min-entropy that is indistinguishable from D by circuits of fixed polynomial size. We state and prove our result in a more abstract setting, in which we allow arbitrary finite domains instead of 0,1n, and arbitrary families of distinguishers, instead of fixed polynomial size circuits. Our result implies (a) the weak Szemeredi regularity Lemma of Frieze and Kannan (b) a constructive version of the dense model theorem of Green, Tao and Ziegler with better quantitative parameters (polynomial rather than exponential in the distinguishing probability), and (c) the Impagliazzo hardcore set Lemma. It appears to be the general result underlying the known connections between "regularity" results in graph theory, "decomposition" results in additive combinatorics, and the hardcore Lemma in complexity theory. We present two proofs of our result, one in the spirit of Nisan's proof of the hardcore Lemma via duality of linear programming, and one similar to Impagliazzo's "boosting" proof. A third proof by iterative partitioning, which gives the complexity of the sampler to be exponential in the distinguishing probability, is also implicit in the Green-Tao-Ziegler proofs of the dense model theorem.},
  eventtitle = {2009 24th {{Annual IEEE Conference}} on {{Computational Complexity}}},
  keywords = {additive combinatorics,average-case complexity,boosting,Boosting,Circuit simulation,Combinatorial mathematics,Complexity theory,Computational complexity,Computational modeling,Computer science,Computer simulation,Graph theory,Polynomials,pseudorandomness},
  file = {/Users/j/Zotero/storage/C7NJX889/Trevisan et al. - 2009 - Regularity, Boosting, and Efficiently Simulating E.pdf}
}

@article{tversky.a:1971,
  title = {Belief in the Law of Small Numbers.},
  author = {Tversky, Amos and Kahneman, Daniel},
  date = {1971},
  journaltitle = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/h0031322},
  url = {https://doi.org/10.1037%2Fh0031322},
  bdsk-url-2 = {https://doi.org/10.1037/h0031322},
  date-added = {2021-08-08 20:24:01 -0400},
  date-modified = {2021-08-08 20:24:02 -0400}
}

@article{upper.d:1974,
  title = {The {{Unsuccessful Self-Treatment}} of a {{Case}} of “{{Writer}}'s {{Block}}”1},
  author = {Upper, Dennis},
  date = {1974},
  journaltitle = {Journal of Applied Behavior Analysis},
  volume = {7},
  number = {3},
  pages = {497--497},
  issn = {1938-3703},
  doi = {10.1901/jaba.1974.7-497a},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1901/jaba.1974.7-497a},
  urldate = {2022-06-16},
  langid = {english},
  keywords = {humor,writer's block},
  annotation = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1901/jaba.1974.7-497a},
  file = {/Users/j/Zotero/storage/8AN7ARSP/Upper - 1974 - The Unsuccessful Self-Treatment of a Case of “Writ.pdf}
}

@article{ussery.c:2017,
  title = {Dimensions of Variation},
  author = {Ussery, Cherlon},
  date = {2017},
  journaltitle = {Syntactic variation in insular Scandinavian},
  volume = {1},
  pages = {165},
  publisher = {{John Benjamins Publishing Company}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:38:48 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,syntactic variation}
}

@article{vandemeerendonk.n:2011,
  title = {Monitoring in Language Perception: {{Electrophysiological}} and Hemodynamic Responses to Spelling Violations},
  shorttitle = {Monitoring in Language Perception},
  author = {van de Meerendonk, Nan and Indefrey, Peter and Chwilla, Dorothee J. and Kolk, Herman H. J.},
  options = {useprefix=true},
  date = {2011-02-01},
  journaltitle = {NeuroImage},
  shortjournal = {NeuroImage},
  volume = {54},
  number = {3},
  pages = {2350--2363},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.022},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811910013145},
  urldate = {2022-06-24},
  abstract = {The monitoring theory of language perception proposes that competing representations that are caused by strong expectancy violations can trigger a conflict which elicits reprocessing of the input to check for possible processing errors. This monitoring process is thought to be reflected by the P600 component in the EEG. The present study further investigated this monitoring process by comparing syntactic and spelling violations in an EEG and an fMRI experiment. To assess the effect of conflict strength, misspellings were embedded in sentences that were weakly or strongly predictive of a critical word. In support of the monitoring theory, syntactic and spelling violations elicited similarly distributed P600 effects. Furthermore, the P600 effect was larger to misspellings in the strongly compared to the weakly predictive sentences. The fMRI results showed that both syntactic and spelling violations increased activation in the left inferior frontal gyrus (lIFG), while only the misspellings activated additional areas. Conflict strength did not affect the hemodynamic response to spelling violations. These results extend the idea that the lIFG is involved in implementing cognitive control in the presence of representational conflicts in general to the processing of errors in language perception.},
  langid = {english},
  keywords = {Cognitive control,Conflict,Left inferior frontal gyrus,P600,Reprocessing}
}

@article{vandermude.a:1978,
  title = {On the Inference of Stochastic Regular Grammars},
  author = {Van Der Mude, Antony and Walker, Adrian},
  date = {1978},
  journaltitle = {Information and Control},
  volume = {38},
  number = {3},
  pages = {310--329},
  publisher = {{Elsevier}},
  doi = {10.1016/S0019-9958(78)90106-7},
  url = {https://doi.org/10.1016/S0019-9958(78)90106-7},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:33:31 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information}
}

@article{vandyke.j:2006,
  title = {Retrieval Interference in Sentence Comprehension},
  author = {Van Dyke, Julie A. and McElree, Brian},
  date = {2006-08},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {55},
  number = {2},
  pages = {157--166},
  issn = {0749596X},
  doi = {10.1016/j.jml.2006.03.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X0600043X},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/H8HJQEKM/Van Dyke and McElree - 2006 - Retrieval interference in sentence comprehension.pdf}
}

@unpublished{vanos.m:2022amlap,
  type = {Poster},
  title = {Rational {{Speech Comprehension}}: {{Interaction}} between {{Predictability}}, {{Acoustic Signal}}, and {{Noise}}},
  author = {van Os, Marjolein and Kray, Jutta and Demberg, Vera},
  options = {useprefix=true},
  date = {2022-09-07},
  url = {https://virtual.oxfordabstracts.com/#/event/3067/submission/72},
  urldate = {2022-09-09},
  eventtitle = {28th {{Architectures}} and {{Mechanisms}} for {{Language Processing}} Conference ({{AMLaP}} 28)},
  venue = {{York, England}},
  annotation = {link-abstract: https://oxford-abstracts.s3.amazonaws.com/2dbcb575-3c17-4635-a1b8-24e1e8b44d3d.pdf link-poster: https://oxford-abstracts.s3.amazonaws.com/0e28630c-2678-413b-bb42-daa834276886.pdf}
}

@article{vasishth.s:2006,
  title = {Argument-Head Distance and Processing Complexity: {{Explaining}} Both Locality and Antilocality Effects},
  author = {Vasishth, Shravan and Lewis, Richard L.},
  date = {2006},
  journaltitle = {Language},
  volume = {82},
  number = {4},
  eprint = {4490268},
  eprinttype = {jstor},
  pages = {767--794},
  publisher = {{Linguistic Society of America}},
  issn = {00978507, 15350665},
  abstract = {Although proximity between arguments and verbs (locality) is a relatively robust determinant of sentence-processing difficulty (Hawkins 1998, 2001, Gibson 2000), increasing argument-verb distance can also facilitate processing (Konieczny 2000). We present two self-paced reading (SPR) experiments involving Hindi that provide further evidence of antilocality, and a third SPR experiment which suggests that similarity-based interference can attenuate this distance-based facilitation. A unified explanation of interference, locality, and antilocality effects is proposed via an independently motivated theory of activation decay and retrieval interference (Anderson et al. 2004).},
  date-added = {2022-03-31 11:51:04 -0400},
  date-modified = {2022-03-31 11:52:05 -0400},
  keywords = {antilocality effects,Dependency locality theory,locality effects,processing,processing complexity,self-paced reading}
}

@article{vasishth.s:2010,
  title = {Short-Term Forgetting in Sentence Comprehension: {{Crosslinguistic}} Evidence from Verb-Final Structures},
  author = {Vasishth, Shravan and Suckow, Katja and Lewis, Richard L. and Kern, Sabine},
  date = {2010-05},
  journaltitle = {Language and Cognitive Processes},
  volume = {25},
  number = {4},
  pages = {533--567},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/01690960903310587},
  url = {https://doi.org/10.1080%2F01690960903310587},
  bdsk-url-2 = {https://doi.org/10.1080/01690960903310587},
  date-added = {2022-04-19 22:48:38 -0400},
  date-modified = {2022-04-19 22:48:39 -0400}
}

@article{vasishth.s:2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and Jäger, Lena A. and Gelman, Andrew},
  date = {2018-12-01},
  journaltitle = {Journal of Memory and Language},
  shortjournal = {Journal of Memory and Language},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X18300640},
  urldate = {2022-07-01},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  langid = {english},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error},
  file = {/Users/j/Zotero/storage/74GM97LG/Vasishth et al. - 2018 - The statistical significance filter leads to overo.pdf}
}

@article{vasishth.s:2019,
  title = {Computational {{Models}} of {{Retrieval Processes}} in {{Sentence Processing}}},
  author = {Vasishth, Shravan and Nicenboim, Bruno and Engelmann, Felix and Burchert, Frank},
  date = {2019-11},
  journaltitle = {Trends in Cognitive Sciences},
  shortjournal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {968--982},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.09.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319302220},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/KS5VZ6XE/Vasishth et al. - 2019 - Computational Models of Retrieval Processes in Sen.pdf}
}

@book{vasishth.s:2021,
  title = {Sentence {{Comprehension}} as a {{Cognitive Process}}: {{A Computational Approach}}},
  shorttitle = {Sentence {{Comprehension}} as a {{Cognitive Process}}},
  author = {Vasishth, Shravan and Engelmann, Felix},
  date = {2021-10-31},
  edition = {1},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781316459560},
  url = {https://www.cambridge.org/core/product/identifier/9781316459560/type/book},
  urldate = {2022-10-12},
  abstract = {Sentence comprehension - the way we process and understand spoken and written language - is a central and important area of research within psycholinguistics. This book explores the contribution of computational linguistics to the field, showing how computational models of sentence processing can help scientists in their investigation of human cognitive processes. It presents the leading computational model of retrieval processes in sentence processing, the Lewis and Vasishth cue-based retrieval mode, and develops a principled methodology for parameter estimation and model comparison/evaluation using benchmark data, to enable researchers to test their own models of retrieval against the present model. It also provides readers with an overview of the last 20 years of research on the topic of retrieval processes in sentence comprehension, along with source code that allows researchers to extend the model and carry out new research. Comprehensive in its scope, this book is essential reading for researchers in cognitive science.},
  isbn = {978-1-316-45956-0 978-1-107-13311-2 978-1-107-58977-3}
}

@incollection{vasishth.s:2021ch3,
  title = {The {{Core ACT-R-Based Model}} of {{Retrieval Processes}}},
  booktitle = {Sentence {{Comprehension}} as a {{Cognitive Process}}: {{A Computational Approach}}},
  author = {Vasishth, Shravan and Engelmann, Felix},
  date = {2021},
  pages = {49--70},
  publisher = {{Cambridge University Press}},
  location = {{Cambridge, England}},
  doi = {10.1017/9781316459560.008},
  url = {https://www.cambridge.org/core/books/sentence-comprehension-as-a-cognitive-process/core-actrbased-model-of-retrieval-processes/E932A3F6C61A70ECAC4A4D43B4CC5A60},
  urldate = {2022-10-12},
  abstract = {The core model of sentence processing used in the book is introduced and its empirical coverage relative to the existing reading time data is considered. Here, we also discuss the Approximate Bayesian Computation method for parameter estimation for model evaluation.},
  isbn = {978-1-107-13311-2},
  keywords = {ACT-R,cue-based retrieval,parsing,psycholinguistics,sentence comprehension},
  file = {/Users/j/Zotero/storage/HDWZE8LD/Vasishth and Engelmann (2021) The Core ACT-R-Based Model of Retrieval Processes.pdf}
}

@inproceedings{vaswani.a:2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, {{CA}}, {{USA}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, Isabelle and von Luxburg, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  options = {useprefix=true},
  date = {2017},
  pages = {5998--6008},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{verdu.s:1998,
  title = {Fifty Years of {{Shannon}} Theory},
  author = {Verdu, S.},
  date = {1998},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {44},
  number = {6},
  pages = {2057--2078},
  issn = {1557-9654},
  doi = {10.1109/18.720531},
  url = {https://doi.org/10.1109/18.720531},
  abstract = {A brief chronicle is given of the historical development of the central problems in the theory of fundamental limits of data compression and reliable communication.},
  date-added = {2020-08-17 11:37:37 -0400},
  date-modified = {2020-08-17 11:39:25 -0400},
  project = {information-entropy},
  keywords = {channel capacity,data compression,information theory,rate distortion theory,source coding}
}

@article{vieira.t:2017,
  title = {Learning to Prune: {{Exploring}} the Frontier of Fast and Accurate Parsing},
  author = {Vieira, Tim and Eisner, Jason},
  date = {2017-08},
  journaltitle = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {263--278},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00060},
  abstract = {Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the lols algorithm. lols training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier—i.e., parsers which are more accurate for a given runtime.},
  date-added = {2022-04-28 10:14:22 -0400},
  date-modified = {2022-04-28 10:14:42 -0400},
  keywords = {chart parsing,pruning},
  file = {/Users/j/Zotero/storage/6CJL2LF6/Vieira and Eisner - 2017 - Learning to prune Exploring the frontier of fast .pdf}
}

@inproceedings{vijayakumar.a:2018,
  title = {Diverse Beam Search for Improved Description of Complex Scenes},
  booktitle = {Proceedings of the Thirty-Second {{AAAI}} Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth {{AAAI}} Symposium on Educational Advances in Artificial Intelligence},
  author = {Vijayakumar, Ashwin K. and Cogswell, Michael and Selvaraju, Ramprasaath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  date = {2018},
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  publisher = {{AAAI Press}},
  location = {{New Orleans, Louisiana, USA}},
  url = {https://dl.acm.org/doi/abs/10.5555/3504035.3504938},
  abstract = {A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse – with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks – image captioning and visual question generation – particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.},
  articleno = {903},
  date-added = {2022-03-25 22:32:37 -0400},
  date-modified = {2022-03-25 22:34:11 -0400},
  isbn = {978-1-57735-800-8},
  pagetotal = {9}
}

@article{vincent.p:2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  date = {2010-12},
  journaltitle = {Journal of Machine Learning Research},
  shortjournal = {J. Mach. Learn. Res.},
  volume = {11},
  pages = {3371--3408},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  issue_date = {3/1/2010},
  pagetotal = {38}
}

@inproceedings{vinyals.o:2014,
  title = {Grammar as a Foreign Language},
  booktitle = {Advances in Neural Information Processing Systems 28: {{Annual}} Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey E.},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  date = {2015},
  pages = {2773--2781},
  url = {https://proceedings.neurips.cc/paper/2015/hash/277281aada22045c03945dcb2ca6f2ec-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VinyalsKKPSH15.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{voita.e:2020mdlprobing,
  title = {Information-Theoretic Probing with Minimum Description Length},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Voita, Elena and Titov, Ivan},
  date = {2020},
  pages = {183--196},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.14},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.14},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.14}
}

@article{vul.e:2014,
  title = {One and Done? {{Optimal}} Decisions from Very Few Samples},
  author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  date = {2014},
  journaltitle = {Cognitive Science},
  volume = {38},
  number = {4},
  pages = {599--637},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12101},
  url = {https://doi.org/10.1111/cogs.12101},
  date-added = {2021-03-16 23:51:30 -0400},
  date-modified = {2021-03-16 23:51:30 -0400},
  keywords = {bayesian,bounded rationality,inference algorithms,sampling}
}

@article{vulkan.n:2000,
  title = {An Economist's Perspective on Probability Matching},
  author = {Vulkan, Nir},
  date = {2000},
  journaltitle = {Journal of Economic Surveys},
  volume = {14},
  number = {1},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-6419.00106},
  pages = {101--118},
  doi = {10.1111/1467-6419.00106},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-6419.00106},
  abstract = {The experimental phenomenon known as `probability matching' is often offered as evidence in support of adaptive learning models and against the idea that people maximise their expected utility. Recent interest in dynamic-based equilibrium theories means the term re-appears in Economics. However, there seems to be conflicting views on what is actually meant by the term and about the validity of the data. The purpose of this paper is therefore threefold: First, to introduce today's readers to what is meant by probability matching, and in particular to clarify which aspects of this phenomenon challenge the utility-maximisation hypothesis. Second, to familiarise the reader with the different theoretical approaches to behaviour in such circumstances, and to focus on the differences in predictions between these theories in light of recent advances. Third, to provide a comprehensive survey of repeated, binary choice experiments.},
  bdsk-url-2 = {https://doi.org/10.1111/1467-6419.00106},
  date-added = {2021-05-31 13:57:04 -0400},
  date-modified = {2021-05-31 13:57:05 -0400},
  keywords = {Optimisation,Probability matching,Stochastic learning}
}

@article{wald.a:1939,
  title = {Contributions to the {{Theory}} of {{Statistical Estimation}} and {{Testing Hypotheses}}},
  author = {Wald, Abraham},
  date = {1939-12},
  journaltitle = {The Annals of Mathematical Statistics},
  volume = {10},
  number = {4},
  pages = {299--326},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732144},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-10/issue-4/Contributions-to-the-Theory-of-Statistical-Estimation-and-Testing-Hypotheses/10.1214/aoms/1177732144.full},
  urldate = {2022-05-21},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {decision theory}
}

@article{wald.a:1947,
  title = {Foundations of a {{General Theory}} of {{Sequential Decision Functions}}},
  author = {Wald, Abraham},
  date = {1947},
  journaltitle = {Econometrica},
  volume = {15},
  number = {4},
  eprint = {1905331},
  eprinttype = {jstor},
  pages = {279--313},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1905331}
}

@inproceedings{wang.a:2019,
  title = {{{SuperGLUE}}: {{A}} Stickier Benchmark for General-Purpose Language Understanding Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {3261--3275},
  url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@misc{wang.b:2021GPT-J-6B,
  title = {{{GPT-J-6B}}: {{A}} 6 Billion Parameter Autoregressive Language Model},
  author = {Wang, Ben and Komatsuzaki, Aran},
  date = {2021-05},
  url = {https://github.com/kingoflolz/mesh-transformer-jax},
  date-added = {2021-11-30 10:11:28 -0500},
  date-modified = {2021-12-13 19:43:33 -0500},
  howpublished = {Software}
}

@article{ward.j:1963,
  title = {Hierarchical Grouping to Optimize an Objective Function},
  author = {Ward, Jr., Joe H.},
  date = {1963},
  journaltitle = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {{Taylor \& Francis Group}},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
  bdsk-url-2 = {https://pdfs.semanticscholar.org/0430/b241bdd0b67d37e1143370f8d24fc46d83e9.pdf},
  bdsk-url-3 = {https://doi.org/10.1080/01621459.1963.10500845},
  date-added = {2019-06-15 15:57:19 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {0},
  keywords = {hierarchical clustering}
}

@article{warstadt.a:2019,
  title = {Neural Network Acceptability Judgments},
  author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
  date = {2019-11},
  volume = {7},
  pages = {625--641},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/tacl_a_00290},
  date-added = {2021-10-19 00:10:14 -0400},
  date-modified = {2021-10-19 00:10:15 -0400},
  file = {/Users/j/Zotero/storage/VTQSLFGV/Warstadt et al. - 2019 - Neural network acceptability judgments.pdf}
}

@book{watrous.j:2018,
  title = {The Theory of Quantum Information},
  author = {Watrous, John},
  date = {2018},
  publisher = {{Cambridge University Press}},
  url = {https://cs.uwaterloo.ca/ watrous/TQI/},
  date-added = {2020-02-15 11:52:26 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {information-entropy},
  keywords = {entropy,information theory,quantum information theory}
}

@article{wieling.m:2016,
  title = {Investigating Dialectal Differences Using Articulography},
  author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Bröker, Franziska and Thiele, Samuel and Wood, Simon N. and Baayen, R. Harald},
  date = {2016-11},
  journaltitle = {Journal of Phonetics},
  volume = {59},
  pages = {122--143},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.wocn.2016.09.004},
  url = {https://doi.org/10.1016%2Fj.wocn.2016.09.004},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2016.09.004},
  date-added = {2021-12-03 00:24:24 -0500},
  date-modified = {2021-12-03 00:24:40 -0500}
}

@inproceedings{wilcox.e:2020,
  title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Cognitive Science Society}}},
  author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  date = {2020},
  pages = {1707--1713},
  publisher = {{Cognitive Science Society}},
  location = {{Virtual}},
  url = {https://www.cognitivesciencesociety.org/cogsci20/papers/0375/},
  eventtitle = {{{CogSci}} 2020},
  keywords = {sentence processing},
  file = {/Users/j/Zotero/storage/XF2IV8PJ/Wilcox et al. (2020) On the predictive power of neural language models .pdf}
}

@inproceedings{wilcox.e:2021,
  title = {A Targeted Assessment of Incremental Processing in Neural Language Models and Humans},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
  date = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.76},
  url = {https://doi.org/10.18653%2Fv1%2F2021.acl-long.76},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.76},
  date-added = {2022-04-15 16:04:48 -0400},
  date-modified = {2022-04-15 16:04:50 -0400}
}

@misc{williams.p:2010,
  title = {Nonnegative Decomposition of Multivariate Information},
  author = {Williams, Paul L. and Beer, Randall D.},
  date = {2010},
  eprint = {1004.2515},
  eprinttype = {arxiv},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:31:54 -0400},
  date-modified = {2021-09-29 21:31:56 -0400}
}

@misc{williams.p:2011,
  title = {Generalized Measures of Information Transfer},
  author = {Williams, Paul L. and Beer, Randall D.},
  date = {2011},
  eprint = {1102.1507},
  eprinttype = {arxiv},
  primaryclass = {physics.data-an},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:29:14 -0400},
  date-modified = {2021-09-29 21:29:15 -0400}
}

@incollection{wilson.k:1954,
  title = {Applications of Entropy Measures to Problems of Sequential Structure},
  booktitle = {Psycholinguistics},
  author = {Wilson, Kellogg and Carroll, John B},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  date = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {103--110},
  publisher = {{Indiana University Press}},
  doi = {10.1037/h0063655},
  url = {https://doi.org/10.1037/h0063655},
  date-added = {2021-05-20 11:54:33 -0400},
  date-modified = {2022-04-14 23:51:44 -0400},
  keywords = {entropy reduction}
}

@inproceedings{wolf.t:2020transformers,
  title = {Transformers: {{State-of-the-art}} Natural Language Processing},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: {{System}} Demonstrations},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  options = {useprefix=true},
  date = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-demos.6}
}

@article{wong.c:1980,
  title = {An {{Efficient Method}} for {{Weighted Sampling}} without {{Replacement}}},
  author = {Wong, C. K. and Easton, M. C.},
  date = {1980-02},
  journaltitle = {SIAM Journal on Computing},
  shortjournal = {SIAM J. Comput.},
  volume = {9},
  number = {1},
  pages = {111--113},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0209009},
  url = {http://epubs.siam.org/doi/10.1137/0209009},
  urldate = {2022-07-10},
  langid = {english},
  file = {/Users/j/Zotero/storage/E5M3QYF9/Wong and Easton - 1980 - An Efficient Method for Weighted Sampling without .pdf}
}

@article{wood.s:2003tprs,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  date = {2003-01},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  publisher = {{Wiley}},
  doi = {10.1111/1467-9868.00374},
  url = {https://doi.org/10.1111%2F1467-9868.00374},
  bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00374},
  date-added = {2021-12-02 20:55:01 -0500},
  date-modified = {2021-12-02 21:03:21 -0500}
}

@article{wood.s:2004GAMjustGCV,
  title = {Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models},
  author = {Wood, Simon N},
  date = {2004-09},
  journaltitle = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673--686},
  publisher = {{Informa UK Limited}},
  doi = {10.1198/016214504000000980},
  url = {https://doi.org/10.1198%2F016214504000000980},
  bdsk-url-2 = {https://doi.org/10.1198/016214504000000980},
  date-added = {2021-12-02 20:52:03 -0500},
  date-modified = {2021-12-02 20:52:18 -0500}
}

@article{wood.s:2011GAMmethod,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N.},
  date = {2011},
  journaltitle = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {3--36},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  url = {https://doi.org/10.1111%2Fj.1467-9868.2010.00749.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2010.00749.x},
  date-added = {2021-12-02 20:46:39 -0500},
  date-modified = {2021-12-02 20:49:53 -0500}
}

@article{wood.s:2014bam,
  title = {Generalized Additive Models for Large Data Sets},
  author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
  date = {2014-05},
  journaltitle = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {64},
  number = {1},
  pages = {139--155},
  publisher = {{Wiley}},
  doi = {10.1111/rssc.12068},
  url = {https://doi.org/10.1111%2Frssc.12068},
  bdsk-url-2 = {https://doi.org/10.1111/rssc.12068},
  date-added = {2021-12-14 20:13:41 -0500},
  date-modified = {2021-12-14 20:21:11 -0500},
  file = {/Users/j/Zotero/storage/BFM8L3R3/Wood et al. - 2014 - Generalized additive models for large data sets.pdf}
}

@article{wood.s:2016GAMbeyondEF,
  title = {Smoothing {{Parameter}} and {{Model Selection}} for {{General Smooth Models}}},
  author = {Wood, Simon N. and Pya, Natalya and Säfken, Benjamin},
  date = {2016-10-01},
  journaltitle = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1548--1563},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1180986},
  url = {https://doi.org/10.1080/01621459.2016.1180986},
  urldate = {2022-09-27},
  abstract = {This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online.},
  keywords = {Additive model,AIC,Distributional regression,GAM,gaulss,location scale additive models,Location scale and shape model,Ordered categorical regression,Penalized regression spline,REML,Smooth Cox model,Smoothing parameter uncertainty,Statistical algorithm,Tweedie distribution.},
  annotation = {\_eprint: https://doi.org/10.1080/01621459.2016.1180986}
}

@book{wood.s:2017GAMoverview,
  title = {Generalized Additive Models},
  author = {Wood, Simon N.},
  date = {2017-05},
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315370279},
  url = {https://doi.org/10.1201%2F9781315370279},
  bdsk-url-2 = {https://doi.org/10.1201/9781315370279},
  date-added = {2021-10-05 13:01:23 -0400},
  date-modified = {2021-12-02 20:54:17 -0500}
}

@article{wu.b:1999,
  title = {Approximation and Exact Algorithms for Constructing Minimum Ultrametric Trees from Distance Matrices},
  author = {Wu, Bang Ye and Chao, Kun-Mao and Tang, Chuan Yi},
  date = {1999},
  journaltitle = {Journal of Combinatorial Optimization},
  volume = {3},
  number = {2},
  pages = {199--211},
  issn = {1573-2886},
  url = {https://doi.org/10.1023/A:1009885610075},
  abstract = {An edge-weighted tree is called ultrametric if the distances from the root to all the leaves in the tree are equal. For an n by n distance matrix M, the minimum ultrametric tree for M is an ultrametric tree T = (V, E, w) with leaf set \{1,..., n\} such that dT(i, j) ≥ M[i, j] for all i, j and \$\$\textbackslash sum \{\_\{e \textbackslash in E\} w(e)\}\$\$is minimum, where dT(i, j) is the distance between i and j on T. Constructing minimum ultrametric trees from distance matrices is an important problem in computational biology. In this paper, we examine its computational complexity and approximability. When the distances satisfy the triangle inequality, we show that the minimum ultrametric tree problem can be approximated in polynomial time with error ratio 1.5(1 + ⌈log n⌉), where n is the number of species. We also develop an efficient branch-and-bound algorithm for constructing the minimum ultrametric tree for both metric and non-metric inputs. The experimental results show that it can find an optimal solution for 25 species within reasonable time, while, to the best of our knowledge, there is no report of algorithms solving the problem even for 12 species.},
  date-added = {2019-07-17 13:39:35 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@misc{wu.m:2022,
  title = {Foundation {{Posteriors}} for {{Approximate Probabilistic Inference}}},
  author = {Wu, Mike and Goodman, Noah},
  date = {2022-05-19},
  number = {arXiv:2205.09735},
  eprint = {2205.09735},
  eprinttype = {arxiv},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09735},
  url = {http://arxiv.org/abs/2205.09735},
  urldate = {2022-08-18},
  abstract = {Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a ``foundation'' posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,inference,probabilistic programming,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/HC3WUQXV/Wu and Goodman - 2022 - Foundation Posteriors for Approximate Probabilisti.pdf}
}

@inproceedings{wu.s:2010,
  title = {Complexity {{Metrics}} in an {{Incremental Right-Corner Parser}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Stephen and Bachrach, Asaf and Cardenas, Carlos and Schuler, William},
  date = {2010-07},
  pages = {1189--1198},
  publisher = {{Association for Computational Linguistics}},
  location = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/P10-1121},
  urldate = {2022-05-31},
  eventtitle = {{{ACL}} 2010},
  file = {/Users/j/Zotero/storage/EMUTBXBZ/Wu et al. - 2010 - Complexity Metrics in an Incremental Right-Corner .pdf}
}

@inproceedings{wu.z:2021,
  title = {Perturbed Masking: {{Parameter-free}} Probing for Analyzing and Interpreting {{BERT}}},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  date = {2020},
  pages = {4166--4176},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online}},
  doi = {10.18653/v1/2020.acl-main.383},
  url = {https://www.aclweb.org/anthology/2020.acl-main.383},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.383},
  file = {/Users/j/Zotero/storage/Q6BTLBFI/Wu et al. - 2020 - Perturbed masking Parameter-free probing for anal.pdf}
}

@inproceedings{yang.k:2020,
  title = {Strongly Incremental Constituency Parsing with Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Kaiyu and Deng, Jia},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  date = {2020},
  volume = {33},
  pages = {21687--21698},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf},
  file = {/Users/j/Zotero/storage/EUFIEWAW/Yang and Deng - 2020 - Strongly incremental constituency parsing with gra.pdf}
}

@inproceedings{yang.s:2020,
  title = {Second-Order Unsupervised Neural Dependency Parsing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  author = {Yang, Songlin and Jiang, Yong and Han, Wenjuan and Tu, Kewei},
  date = {2020},
  pages = {3911--3924},
  publisher = {{International Committee on Computational Linguistics}},
  location = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.347},
  url = {https://www.aclweb.org/anthology/2020.coling-main.347},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.347},
  file = {/Users/j/Zotero/storage/SS59BRKI/Yang et al. - 2020 - Second-order unsupervised neural dependency parsin.pdf}
}

@inproceedings{yang.z:2018,
  title = {Breaking the Softmax Bottleneck: {{A}} High-Rank {{RNN}} Language Model},
  booktitle = {International Conference on Learning Representations},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
  date = {2018},
  url = {https://openreview.net/forum?id=HkwZSG-CZ}
}

@inproceedings{yang.z:2019,
  title = {{{XLNet}}: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and d'Alché- Buc, Florence and Fox, Emily B. and Garnett, Roman},
  options = {useprefix=true},
  date = {2019},
  pages = {5754--5764},
  url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
  date-modified = {2021-09-09 23:04:25 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {/Users/j/Zotero/storage/GMLGB59B/Yang et al. - 2019 - XLNet Generalized autoregressive pretraining for .pdf}
}

@article{ye.l:2020,
  title = {Monte {{Carlo}} Co-Ordinate Ascent Variational Inference},
  author = {Ye, Lifeng and Beskos, Alexandros and De Iorio, Maria and Hao, Jie},
  date = {2020-07-01},
  journaltitle = {Statistics and Computing},
  shortjournal = {Stat Comput},
  volume = {30},
  number = {4},
  pages = {887--905},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09924-y},
  url = {https://doi.org/10.1007/s11222-020-09924-y},
  urldate = {2022-06-27},
  abstract = {In variational inference (VI), coordinate-ascent and gradient-based approaches are two major types of algorithms for approximating difficult-to-compute probability densities. In real-world implementations of complex models, Monte Carlo methods are widely used to estimate expectations in coordinate-ascent approaches and gradients in derivative-driven ones. We discuss a Monte Carlo co-ordinate ascent VI (MC-CAVI) algorithm that makes use of Markov chain Monte Carlo (MCMC) methods in the calculation of expectations required within co-ordinate ascent VI (CAVI). We show that, under regularity conditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of the evidence lower bound with any given high probability. In numerical examples, the performance of MC-CAVI algorithm is compared with that of MCMC and—as a representative of derivative-based VI methods—of Black Box VI (BBVI). We discuss and demonstrate MC-CAVI’s suitability for models with hard constraints in simulated and real examples. We compare MC-CAVI’s performance with that of MCMC in an important complex model used in nuclear magnetic resonance spectroscopy data analysis—BBVI is nearly impossible to be employed in this setting due to the hard constraints involved in the model.},
  langid = {english},
  keywords = {Bayesian inference,Coordinate-ascent,Gradient-based optimisation,Markov chain Monte Carlo,Nuclear magnetic resonance,Variational inference},
  file = {/Users/j/Zotero/storage/5Q3QJVBC/Ye et al. - 2020 - Monte Carlo co-ordinate ascent variational inferen.pdf}
}

@article{yeung.r:1991,
  title = {A New Outlook on {{Shannon}}'s Information Measures},
  author = {Yeung, Raymond W.},
  date = {1991-05},
  journaltitle = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {3},
  pages = {466--474},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.79902},
  url = {https://doi.org/10.1109%2F18.79902},
  bdsk-url-2 = {https://doi.org/10.1109/18.79902},
  date-added = {2021-09-20 18:00:57 -0400},
  date-modified = {2021-09-21 18:18:41 -0400}
}

@book{yeung.r:2008,
  title = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  date = {2008},
  series = {Information Technology Transmission Processing and Storage},
  publisher = {{Springer US}},
  doi = {10.1007/978-0-387-79234-7},
  url = {https://doi.org/10.1007%2F978-0-387-79234-7},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7},
  date-added = {2021-09-21 18:16:02 -0400},
  date-modified = {2021-09-21 18:18:28 -0400}
}

@incollection{yeung.r:2008ch3,
  title = {The {{I-Measure}}},
  booktitle = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  series = {Information Technology Transmission Processing and Storage},
  pages = {51--80},
  publisher = {{Springer US}},
  doi = {10.1007/978-0-387-79234-7_3},
  url = {https://doi.org/10.1007%2F978-0-387-79234-7₃},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7₃},
  date-added = {2021-09-21 18:19:43 -0400},
  date-modified = {2021-09-21 18:22:32 -0400}
}

@inproceedings{yoshida.r:2021,
  title = {Modeling {{Human Sentence Processing}} with {{Left-Corner Recurrent Neural Network Grammars}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yoshida, Ryo and Noji, Hiroshi and Oseki, Yohei},
  date = {2021},
  pages = {2964--2973},
  publisher = {{Association for Computational Linguistics}},
  location = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.235},
  url = {https://aclanthology.org/2021.emnlp-main.235},
  urldate = {2022-08-13},
  eventtitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  langid = {english},
  file = {/Users/j/Zotero/storage/P7YV7FZM/Yoshida et al. - 2021 - Modeling Human Sentence Processing with Left-Corne.pdf}
}

@article{yun.j:2014,
  title = {Uncertainty in Processing Relative Clauses across {{East Asian}} Languages},
  author = {Yun, Jiwon and Chen, Zhong and Hunter, Tim and Whitman, John and Hale, John},
  date = {2014},
  journaltitle = {Journal of East Asian Linguistics},
  volume = {24},
  number = {2},
  pages = {113--148},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10831-014-9126-6},
  url = {https://doi.org/10.1007%2Fs10831-014-9126-6},
  bdsk-url-2 = {https://doi.org/10.1007/s10831-014-9126-6},
  date-added = {2021-03-18 10:38:55 -0400},
  date-modified = {2021-03-18 10:39:18 -0400},
  keywords = {entropy reduction,processing},
  file = {/Users/j/Zotero/storage/I5T464WJ/Yun et al. - 2014 - Uncertainty in processing relative clauses across .pdf}
}

@thesis{yuret.d:1998,
  title = {Discovery of Linguistic Relations Using Lexical Attraction},
  author = {Yuret, Deniz},
  date = {1998},
  eprint = {cmp-lg/9805009},
  eprinttype = {arxiv},
  institution = {{Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science}},
  archiveprefix = {arXiv},
  date-added = {2020-04-23 12:44:41 -0400},
  date-modified = {2020-04-24 12:35:24 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,mutual information}
}

@article{yuret.d:2006,
  title = {Lexical Attraction Models of Language},
  author = {Yuret, Deniz},
  date = {2006},
  journaltitle = {Ms., Koç University, Istanbul, Turkey,},
  url = {http://www2.denizyuret.com/pub/lex-attr/},
  date-added = {2019-09-12 19:59:44 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lexical attraction,mutual information}
}

@incollection{zaenen.a:1985a,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thráinsson, Höskuldur},
  date = {1985},
  pages = {93--136},
  publisher = {{Brill}},
  date-added = {2020-02-15 18:32:26 -0500},
  date-modified = {2020-02-15 18:38:25 -0500},
  project = {Icelandic gluttony},
  keywords = {dative subjecthood}
}

@incollection{zaenen.a:1990,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thráinsson, Höskuldur},
  date = {1990},
  pages = {93--136},
  publisher = {{Brill}},
  date-added = {2020-02-03 16:19:23 -0500},
  date-modified = {2020-02-03 16:19:39 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{zaslavsky.n:2018,
  title = {Efficient Compression in Color Naming and Its Evolution},
  author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  date = {2018},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {31},
  eprint = {https://www.pnas.org/content/115/31/7937.full.pdf},
  pages = {7937--7942},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  url = {https://www.pnas.org/content/115/31/7937},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1800521115},
  date-added = {2019-05-15 00:03:28 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory}
}

@inproceedings{zeman.d:2020,
  title = {Universal {{Dependencies}}},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Tutorial}} Abstracts},
  author = {Nivre, Joakim and Zeman, Daniel and Ginter, Filip and Tyers, Francis},
  date = {2017},
  publisher = {{Association for Computational Linguistics}},
  location = {{Valencia, Spain}},
  url = {https://www.aclweb.org/anthology/E17-5001},
  file = {/Users/j/Zotero/storage/UGAFWW2M/Nivre et al. - 2017 - Universal Dependencies.pdf}
}

@article{zhang.k:2018,
  title = {Language Modeling Teaches You More Syntax than Translation Does: {{Lessons}} Learned through Auxiliary Task Analysis},
  author = {Zhang, Kelly W. and Bowman, Samuel R.},
  date = {2018},
  journaltitle = {CoRR},
  volume = {abs/1809.10040},
  eprint = {1809.10040},
  eprinttype = {arxiv},
  url = {http://arxiv.org/abs/1809.10040},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1809-10040},
  date-added = {2019-06-16 11:01:31 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {CoVe,ELMo,LSTM,syntactic information},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  file = {/Users/j/Zotero/storage/AQID6Y65/Zhang and Bowman - 2018 - Language modeling teaches you more syntax than tra.pdf}
}

@article{zhang.t:2021,
  title = {On the Inductive Bias of Masked Language Modeling: {{From}} Statistical to Syntactic Dependencies},
  author = {Zhang, Tianyi and Hashimoto, Tatsunori},
  date = {2021},
  journaltitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.naacl-main.404},
  url = {http://dx.doi.org/10.18653/v1/2021.naacl-main.404},
  date-added = {2021-09-08 11:13:42 -0400},
  date-modified = {2021-09-08 11:13:47 -0400},
  file = {/Users/j/Zotero/storage/4ATKKNTC/Zhang and Hashimoto - 2021 - On the inductive bias of masked language modeling.pdf}
}

@inproceedings{zhang.y:2008,
  title = {A Tale of Two Parsers: {{Investigating}} and Combining Graph-Based and Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Yue and Clark, Stephen},
  date = {2008-10},
  pages = {562--571},
  publisher = {{Association for Computational Linguistics}},
  location = {{Honolulu, Hawaii}},
  url = {https://aclanthology.org/D08-1059},
  date-added = {2022-03-25 22:14:22 -0400},
  date-modified = {2022-03-25 22:14:24 -0400},
  file = {/Users/j/Zotero/storage/QPGVRH7R/Zhang and Clark - 2008 - A tale of two parsers Investigating and combining.pdf}
}

@inproceedings{zhou.j:2019,
  title = {Head-{{Driven Phrase Structure Grammar}} Parsing on {{Penn Treebank}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Zhou, Junru and Zhao, Hai},
  date = {2019},
  pages = {2396--2408},
  publisher = {{Association for Computational Linguistics}},
  location = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1230},
  url = {https://www.aclweb.org/anthology/P19-1230},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1230},
  file = {/Users/j/Zotero/storage/PATQJVNK/Zhou and Zhao - 2019 - Head-Driven Phrase Structure Grammar parsing on Pe.pdf}
}

@book{zipf.g:1935,
  title = {The {{Psycho-Biology Of Language}}},
  author = {Zipf, George Kingsley},
  date = {2013-11-05},
  origdate = {1935},
  edition = {Republished},
  publisher = {{Routledge}},
  location = {{London, United Kingdom}},
  doi = {10.4324/9781315009421},
  url = {https://www.taylorfrancis.com/books/9781136310461},
  urldate = {2022-09-28},
  isbn = {978-1-136-31046-1},
  langid = {english}
}

@book{zipf.g:1949,
  title = {Human {{Behavior}} and the {{Principle}} of {{Least Effort}}: {{An Introduction}} to {{Human Ecology}}},
  shorttitle = {Human Behavior and the Principle of Least Effort},
  author = {Zipf, George Kingsley},
  date = {2012},
  origdate = {1949},
  publisher = {{Martino Publishing}},
  location = {{Mansfield Centre, CT}},
  isbn = {978-1-61427-312-7},
  langid = {english}
}

@preamble{ "\ifdefined\DeclarePrefChars\DeclarePrefChars{'’-}\else\fi " }

