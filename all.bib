@article{aaronson.s:2014,
  title = {The Equivalence of Sampling and Searching},
  author = {Aaronson, Scott},
  year = {2014},
  month = aug,
  journal = {Theory of Computing Systems},
  volume = {55},
  number = {2},
  pages = {281--298},
  issn = {1433-0490},
  doi = {10.1007/s00224-013-9527-3},
  url = {https://doi.org/10.1007/s00224-013-9527-3},
  urldate = {2022-10-27},
  abstract = {In a sampling problem, we are given an input x{$\in\lbrace$}0,1\vphantom\{\}n, and asked to sample approximately from a probability distribution \$\textbackslash mathcal\{D\}\_\{x\}\$over \$\textbackslash operatorname\{poly\} ( n ) \$-bit strings. In a search problem, we are given an input x{$\in\lbrace$}0,1\vphantom\{\}n, and asked to find a member of a nonempty set Axwith high probability. (An example is finding a Nash equilibrium.) In this paper, we use tools from Kolmogorov complexity to show that sampling and search problems are ``essentially equivalent.'' More precisely, for any sampling problem S, there exists a search problem RSsuch that, if \$\textbackslash mathcal\{C\}\$is any ``reasonable'' complexity class, then RSis in the search version of \$\textbackslash mathcal\{C\}\$if and only if S is in the sampling version. What makes this nontrivial is that the same RSworks for every~\$\textbackslash mathcal\{C\}\$.},
  langid = {english},
  keywords = {Algorithmic information theory,Extended Church-Turing Thesis,FBQP,Function problems,Kolmogorov complexity,Quantum computing,Relational problems,Sampling problems,Search problems},
  file = {/Users/j/Zotero/storage/4H7BVT2F/Aaronson (2014) The Equivalence of Sampling and Searching.pdf}
}

@article{abney.s:1991,
  title = {Memory Requirements and Local Ambiguities of Parsing Strategies},
  author = {Abney, Steven P. and Johnson, Mark},
  year = {1991},
  month = may,
  journal = {Journal of Psycholinguistic Research},
  volume = {20},
  number = {3},
  pages = {233--250},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf01067217},
  url = {https://doi.org/10.1007%2Fbf01067217},
  abstract = {We present a method for calculating lower bounds on the space required and local ambiguities entailed by parsing strategies. A fast, compact natural language parser must implement a strategy with low space requirements and few local ambiguities. It is also widely assumed in the psycholinguistics literature that extremely limited short-term space is available to the human parser, and that sentences containing center-embedded constructions are incomprehensible because processing them requires more space than is available. However, we show that the parsing strategies most psycholinguists assume require less space for processing center-embedded constructions than for processing other perfectly comprehensible constructions. We present alternative strategies for which center-embedded constructions do require more space than other constructions.},
  bdsk-url-2 = {https://doi.org/10.1007/bf01067217},
  date-added = {2022-03-31 09:33:23 -0400},
  date-modified = {2022-03-31 09:38:15 -0400},
  keywords = {memory,parsing,space-complexity}
}

@incollection{abney.s:1991chunks,
  title = {Parsing by Chunks},
  booktitle = {Studies in Linguistics and Philosophy},
  author = {Abney, Steven P.},
  year = {1991},
  pages = {257--278},
  publisher = {{Springer Netherlands}},
  doi = {10.1007/978-94-011-3474-3_10},
  url = {https://doi.org/10.1007%2F978-94-011-3474-3_10},
  bdsk-url-2 = {https://doi.org/10.1007/978-94-011-3474-3{$_1$}0},
  date-added = {2022-03-31 09:42:19 -0400},
  date-modified = {2022-03-31 09:45:05 -0400},
  keywords = {context free grammar,parsing}
}

@inproceedings{abney.s:1999,
  title = {Relating Probabilistic Grammars and Automata},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}} -},
  author = {Abney, Steven and McAllester, David and Pereira, Fernando},
  year = {1999},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1034678.1034759},
  url = {https://doi.org/10.3115%2F1034678.1034759},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034759},
  date-added = {2022-03-31 09:45:57 -0400},
  date-modified = {2022-03-31 09:47:10 -0400},
  keywords = {automata,context free grammar,parsing,probabilistic context free grammar,push-down automata}
}

@article{adamek.j:2004,
  title = {Abstract and Concrete Categories. {{The}} Joy of Cats},
  author = {Ad{\'a}mek, Ji{\v r}{\'i} and Herrlich, Horst and Strecker, George E},
  year = {2004},
  publisher = {{Citeseer}},
  date-added = {2019-08-24 09:17:33 -0400},
  date-modified = {2019-08-24 09:18:04 -0400},
  keywords = {category theory}
}

@misc{adams.r:2013blog,
  type = {Blog},
  title = {The {{Gumbel-max}} Trick for Discrete Distributions},
  author = {Adams, Ryan},
  year = {2013},
  month = apr,
  journal = {Laboratory for Intelligent Probabilistic Systems Blog},
  url = {https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/},
  urldate = {2022-11-06},
  keywords = {gumbel-max trick}
}

@article{adger.d:2009,
  title = {Features in Minimalist Syntax},
  author = {Adger, David and Svenonius, Peter},
  year = {2009},
  journal = {The Oxford Handbook of Minimalist Syntax},
  url = {https://doi.org/10.1093/oxfordhb/9780199549368.013.0002},
  date-added = {2020-02-20 12:37:20 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,minimalist syntax,phi features}
}

@article{agapiou.s:2017,
  title = {Importance Sampling: Intrinsic Dimension and Computational Cost},
  shorttitle = {Importance Sampling},
  author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
  year = {2017},
  month = aug,
  journal = {Statistical Science},
  volume = {32},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/17-STS611},
  url = {https://projecteuclid.org/journals/statistical-science/volume-32/issue-3/Importance-Sampling-Intrinsic-Dimension-and-Computational-Cost/10.1214/17-STS611.full},
  urldate = {2022-12-21},
  keywords = {importance sampling},
  file = {/Users/j/Zotero/storage/6TSSHYMN/Agapiou et al. (2017) Importance Sampling Intrinsic Dimension and Compu.pdf;/Users/j/Zotero/storage/9E595WFN/1511.06196.pdf;/Users/j/Zotero/storage/MQ8MZYD5/10.121417-STS611SUPP.pdf}
}

@inproceedings{ait-mokhtar.s:1997,
  title = {Incremental Finite-State Parsing},
  booktitle = {Fifth Conference on Applied Natural Language Processing},
  author = {{Ait-Mokhtar}, Salah and Chanod, Jean-Pierre},
  year = {1997},
  pages = {72--79},
  publisher = {{Association for Computational Linguistics}},
  address = {{Washington, DC, USA}},
  doi = {10.3115/974557.974569},
  url = {https://www.aclweb.org/anthology/A97-1012},
  bdsk-url-2 = {https://doi.org/10.3115/974557.974569}
}

@inproceedings{alemi.a:2016,
  title = {Deep Variational Information Bottleneck},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=HyxQzBceg},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/AlemiFD017.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@article{alexiadou.a:2014,
  title = {Opaque and Transparent Datives, and How They Behave in Passives},
  author = {Alexiadou, Artemis and Anagnostopoulou, Elena and Sevdali, Christina},
  year = {2014},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {17},
  number = {1},
  pages = {1--34},
  publisher = {{Springer}},
  url = {https://doi.org/10.1007/s10828-014-9064-8},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{allen.c:2019,
  title = {Analogies Explained: {{Towards}} Understanding Word Embeddings},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning, {{ICML}} 2019, 9-15 June 2019, Long Beach, California, {{USA}}},
  author = {Allen, Carl and Hospedales, Timothy M.},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {223--231},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v97/allen19a.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/icml/AllenH19.bib},
  timestamp = {Tue, 11 Jun 2019 01:00:00 +0200}
}

@article{altmann.g:1988,
  title = {Interaction with Context during Human Sentence Processing},
  author = {Altmann, Gerry and Steedman, Mark},
  year = {1988},
  month = dec,
  journal = {Cognition},
  volume = {30},
  number = {3},
  pages = {191--238},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0277(88)90020-0},
  url = {https://doi.org/10.1016%2F0010-0277%2888%2990020-0},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0277(88)90020-0},
  date-added = {2022-04-14 13:35:33 -0400},
  date-modified = {2022-04-14 13:35:37 -0400}
}

@article{amari.s:1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  keywords = {Computer architecture,information geometry,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes},
  file = {/Users/j/Zotero/storage/ZMHDJ3PH/Amari et al. - 1992 - Information geometry of Boltzmann machines.pdf}
}

@book{anagnostopoulou.e:2003,
  title = {The Syntax of Ditransitives: {{Evidence}} from Clitics},
  author = {Anagnostopoulou, Elena},
  year = {2003},
  volume = {54},
  publisher = {{Walter de Gruyter}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:07 -0400},
  project = {Icelandic gluttony},
  keywords = {clitics,hierarchy effects}
}

@article{anagnostopoulou.e:2017,
  title = {The {{Person Case Constraint}}},
  author = {Anagnostopoulou, Elena},
  year = {2017},
  journal = {The Wiley Blackwell Companion to Syntax, Second Edition},
  pages = {1--47},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@book{anderson.j:1990,
  title = {The Adaptive Character of Thought},
  author = {Anderson, John R.},
  year = {1990},
  month = jan,
  publisher = {{Psychology Press}},
  url = {https://doi.org/10.4324%2F9780203771730},
  bdsk-url-2 = {https://doi.org/10.4324/9780203771730},
  date-added = {2022-04-04 11:54:23 -0400},
  date-modified = {2022-04-04 12:18:17 -0400},
  file = {/Users/j/Zotero/storage/DWKYXFLQ/Anderson - 1990 - The adaptive character of thought.pdf}
}

@article{anderson.j:1991,
  title = {Is Human Cognition Adaptive?},
  author = {Anderson, John R.},
  year = {1991},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {14},
  number = {3},
  pages = {471--485},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00070801},
  url = {http://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/is-human-cognition-adaptive/518FCFF303190968CF1F54D2A603C026},
  urldate = {2022-06-12},
  abstract = {Can the output of human cognition be predicted from the assumption that it is an optimal response to the information-processing demands of the environment? A methodology called rational analysis is described for deriving predictions about cognitive phenomena using optimization assumptions. The predictions flow from the statistical structure of the environment and not the assumed structure of the mind. Bayesian inference is used, assuming that people start with a weak prior model of the world which they integrate with experience to develop stronger models of specific aspects of the world. Cognitive performance maximizes the difference between the expected gain and cost of mental effort. (1) Memory performance can be predicted on the assumption that retrieval seeks a maximal trade-off between the probability of finding the relevant memories and the effort required to do so; in (2) categorization performance there is a similar trade-off between accuracy in predicting object features and the cost of hypothesis formation; in (3) casual inference the trade-off is between accuracy in predicting future events and the cost of hypothesis formation; and in (4) problem solving it is between the probability of achieving goals and the cost of both external and mental problem-solving search. The implemention of these rational prescriptions in neurally plausible architecture is also discussed.},
  langid = {english},
  keywords = {Bayes,categorization,causal inference,computation,memory,optimality,problem solving,rational analysis,rationality},
  file = {/Users/j/Zotero/storage/CFWIF6M6/Anderson - 1991 - Is human cognition adaptive.pdf}
}

@article{anderson.j:1991a,
  title = {Reflections of the Environment in Memory},
  author = {Anderson, John R. and Schooler, Lael J.},
  year = {1991},
  month = nov,
  journal = {Psychological Science},
  volume = {2},
  number = {6},
  pages = {396--408},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1991.tb00174.x},
  url = {https://doi.org/10.1111/j.1467-9280.1991.tb00174.x},
  urldate = {2022-09-08},
  abstract = {Availability of human memories for specific items shows reliable relationships to frequency, recency, and pattern of prior exposures to the item. These relationships have defied a systematic theoretical treatment. A number of environmental sources (New York Times, parental speech, electronic mail) are examined to show that the probability that a memory will be needed also shows reliable relationships to frequency, recency, and pattern of prior exposures. Moreover, the environmental relationships are the same as the memory relationships. It is argued that human memory has the form it does because it is adapted to these environmental relationships. Models for both the environment and human memory are described. Among the memory phenomena addressed are the practice function, the retention function, the effect of spacing of practice, and the relationship between degree of practice and retention.},
  langid = {english},
  file = {/Users/j/Zotero/storage/GH4QBXIU/Anderson and Schooler (1991) Reflections of the Environment in Memory.pdf}
}

@article{anderson.j:1998,
  title = {An Integrated Theory of List Memory},
  author = {Anderson, John R. and Bothell, Dan and Lebiere, Christian and Matessa, Michael},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {341--380},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2553},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X97925535},
  urldate = {2022-09-08},
  abstract = {The ACT-R theory (Anderson, 1993; Anderson \& Lebiere, 1998) is applied to the list memory paradigms of serial recall, recognition memory, free recall, and implicit memory. List memory performance in ACT-R is determined by the level of activation of declarative chunks which encode that items occur in the list. This level of activation is in turn determined by amount of rehearsal, delay, and associative fan from a list node. This theory accounts for accuracy and latency profiles in backward and forward serial recall, set size effects in the Sternberg paradigm, length\textendash strength effects in recognition memory, the Tulving\textendash Wiseman function, serial position, length and practice effects in free recall, and lexical priming in implicit memory paradigms. This wide variety of effects is predicted with minimal parameter variation. It is argued that the strength of the ACT-R theory is that it offers a completely specified processing architecture that serves to integrate many existing models in the literature.},
  langid = {english}
}

@book{anderson.j:1998atomic,
  title = {The Atomic Components of Thought},
  author = {Anderson, John R. and Lebiere, Christian},
  year = {1998},
  publisher = {{Psychology Press}},
  address = {{New York}},
  doi = {10.4324/9781315805696},
  url = {https://doi.org/10.4324/9781315805696},
  isbn = {978-1-315-80569-6},
  langid = {english},
  annotation = {OCLC: 872682504},
  file = {/Users/j/Zotero/storage/NZLCSGS6/Anderson and Lebiere (1998) The atomic components of thought.pdf}
}

@article{anderson.j:2004,
  title = {An Integrated Theory of the Mind},
  author = {Anderson, John R. and Bothell, Daniel and Byrne, Michael D. and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {4},
  pages = {1036--1060},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.111.4.1036},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.111.4.1036},
  urldate = {2022-09-17},
  langid = {english},
  file = {/Users/j/Zotero/storage/KE52MBTG/Anderson et al. (2004) An Integrated Theory of the Mind.pdf}
}

@article{andrieu.c:2010PMCMC,
  title = {Particle Markov Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  url = {https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9868.2009.00736.x},
  abstract = {Summary. Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L\'evy-driven stochastic volatility model.},
  keywords = {bayesian inference,conditional sequential monte carlo,markov chain monte Carlo,sequential monte carlo,state space models},
  file = {/Users/j/Zotero/storage/GRSNI2MM/Andrieu et al. - 2010 - Particle markov chain monte carlo methods.pdf}
}

@article{angele.b:2015,
  title = {Do Successor Effects in Reading Reflect Lexical Parafoveal Processing? {{Evidence}} from Corpus-Based and Experimental Eye Movement Data},
  author = {Angele, Bernhard and Schotter, Elizabeth R. and Slattery, Timothy J. and Tenenbaum, Tara L. and Bicknell, Klinton and Rayner, Keith},
  year = {2015},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {79--80},
  pages = {76--96},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2014.11.003},
  url = {https://doi.org/10.1016%2Fj.jml.2014.11.003},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2014.11.003},
  date-added = {2022-04-21 09:33:17 -0400},
  date-modified = {2022-04-21 09:33:18 -0400}
}

@inproceedings{arehalli.s:2022CoNLL,
  title = {Syntactic Surprisal from Neural Models Predicts, but Underestimates, Human Processing Difficulty from Syntactic Ambiguities},
  booktitle = {Proceedings of the 26th Conference on Computational Natural Language Learning ({{CoNLL}})},
  author = {Arehalli, Suhas and Dillon, Brian and Linzen, Tal},
  year = {2022},
  month = dec,
  pages = {301--313},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates (Hybrid)}},
  url = {https://aclanthology.org/2022.conll-1.20},
  abstract = {Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory (Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.}
}

@misc{arehalli.s:2022long,
  title = {Neural Networks as Cognitive Models of the Processing of Syntactic Constraints},
  author = {Arehalli, Suhas and Linzen, Tal},
  year = {2022},
  month = oct,
  abstract = {Languages are governed by syntactic constraints \textemdash{} structural rules that determine which sentences are grammatical in the language. In English, one such constraint is subject-verb agreement, which dictates that the number of a verb must match the number of its corresponding subject: ``the dogs run'', but ``the dog runs''. While this constraint appears to be simple, in practice speakers make a substantial number of agreement errors, especially if a noun phrase near the verb differs in number from the subject (for example, a speaker might produce the ungrammatical sentence ``the key to the cabinets are rusty''). This phenomenon, referred to as agreement attraction, is sensitive to a wide range of properties of the sentence; no single existing model is able to generate predictions for the wide variety of materials studied in the human experimental literature. We explore the viability of neural network language models\textemdash broad-coverage systems trained to predict the next word in a corpus\textemdash as a framework for addressing this limitation. We analyze the agreement errors made by Long Short-Term Memory (LSTM) networks and compare them to those of humans. The models successfully simulate certain results, such as the so-called number asymmetry and the difference between attraction strength in grammatical and ungrammatical sentences, but failed to simulate others, such as the effect of syntactic distance or notional (conceptual) number. We further evaluate networks trained with explicit syntactic supervision, and find that this form of supervision does not always lead to more human-like syntactic behavior. Finally, we show that the corpus used to train a network significantly affects the pattern of agreement errors produced by the network, and discuss the strengths and limitations of neural networks as a tool for understanding human syntactic processing.},
  langid = {english},
  file = {/Users/j/Zotero/storage/AYRJGQHI/Arehalli and Linzen (Neural Networks as Cognitive Models of the Process.pdf}
}

@misc{armengol-estape.j:2021,
  title = {On the Multilingual Capabilities of Very Large-Scale {{English}} Language Models},
  author = {{Armengol-Estap{\'e}}, Jordi and {de Gibert Bonet}, Ona and Melero, Maite},
  year = {2021},
  eprint = {2108.13349},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-12-13 19:48:21 -0500},
  date-modified = {2021-12-13 19:48:22 -0500}
}

@misc{arora.k:2023,
  title = {The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis and Algorithm for Robust Natural Language Generation},
  shorttitle = {The Stable Entropy Hypothesis and Entropy-Aware Decoding},
  author = {Arora, Kushal and O'Donnell, Timothy J. and Precup, Doina and Weston, Jason and Cheung, Jackie C. K.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06784},
  eprint = {2302.06784},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.06784},
  urldate = {2023-04-05},
  abstract = {State-of-the-art language generation models can degenerate when applied to open-ended generation problems such as text completion, story generation, or dialog modeling. This degeneration usually shows up in the form of incoherence, lack of vocabulary diversity, and self-repetition or copying from the context. In this paper, we postulate that ``human-like'' generations usually lie in a narrow and nearly flat entropy band, and violation of these entropy bounds correlates with degenerate behavior. Our experiments show that this stable narrow entropy zone exists across models, tasks, and domains and confirm the hypothesis that violations of this zone correlate with degeneration. We then use this insight to propose an entropy-aware decoding algorithm that respects these entropy bounds resulting in less degenerate, more contextual, and "human-like" language generation in open-ended text generation settings.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/W2CPETCV/Arora et al. (2023) The Stable Entropy Hypothesis and Entropy-Aware De.pdf}
}

@misc{arroyo-fernandez.i:2019,
  title = {On the Possibility of Rewarding Structure Learning Agents: {{Mutual}} Information on Linguistic Random Sets},
  author = {{Arroyo-Fern{\'a}ndez}, Ignacio and {Carrasco-Ru{\'i}z}, Mauricio and {Arias-Aguilar}, J. Anibal},
  year = {2019},
  eprint = {1910.04023},
  primaryclass = {cs.LG},
  archiveprefix = {arxiv},
  date-added = {2020-01-27 11:44:31 -0500},
  date-modified = {2020-01-27 11:47:03 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised grammar induction}
}

@article{atlamaz.u:2018,
  title = {On Partial Agreement and Oblique Case},
  author = {Atlamaz, {\"U}mit and Baker, Mark},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {3},
  pages = {195--237},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{attias.h:1999,
  title = {A Variational Baysian Framework for Graphical Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Attias, Hagai},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {1999},
  volume = {12},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/1999/hash/74563ba21a90da13dacf2a73e3ddefa7-Abstract.html},
  urldate = {2022-06-27},
  abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
  file = {/Users/j/Zotero/storage/RQ4I7UC6/Attias - 1999 - A Variational Baysian Framework for Graphical Mode.pdf}
}

@book{attneave.f:1959,
  title = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  shorttitle = {Applications of Information Theory to Psychology},
  author = {Attneave, Fred},
  year = {1959},
  series = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  pages = {vii, 120},
  publisher = {{Henry Holt}},
  address = {{Oxford, England}},
  abstract = {Summarizes existing informational methods used in psychological research, and illustrates the methods of calculating some of the measures. Chapter 1 develops quantitative expressions of uncertainty and redundancy from qualitative examples. Chapter 2 describes informational methods for analyzing sequences of events. Chapter 3 gives methods of describing rates of transmission of information and reviews pertinent research. Chapter 4 concerns possible applications of information measures, particularly to the study of perceptual problems of patterning. Appendices illustrate the calculation of information measures from variance statistics and provide convenient tables and a nomograph used in calculating information measures. 87 refs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {information theory,surprisal}
}

@inproceedings{aurnhammer.c:2019cogsci,
  title = {Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing},
  booktitle = {Proceedings of the 41st {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  pages = {112--118},
  url = {http://hdl.handle.net/2066/213724},
  date-added = {2021-11-29 11:40:22 -0500},
  date-modified = {2021-11-29 12:44:15 -0500},
  file = {/Users/j/Zotero/storage/HR7P8DHW/Aurnhammer and Frank (2019) Comparing gated and simple recurrent neural networ.pdf}
}

@article{aurnhammer.c:2019LIG,
  title = {Evaluating Information-Theoretic Measures of Word Prediction in Naturalistic Sentence Reading},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  month = nov,
  journal = {Neuropsychologia},
  volume = {134},
  number = {107198},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.neuropsychologia.2019.107198},
  url = {https://doi.org/10.1016%2Fj.neuropsychologia.2019.107198},
  abstract = {We review information-theoretic measures of cognitive load during sentence processing that have been used to quantify word prediction effort. Two such measures, surprisal and next-word entropy, suffer from shortcomings when employed for a predictive processing view. We propose a novel metric, lookahead information gain, that can overcome these short-comings. We estimate the different measures using probabilistic language models. Subsequently, we put them to the test by analysing how well the estimated measures predict human processing effort in three data sets of naturalistic sentence reading. Our results replicate the well known effect of surprisal on word reading effort, but do not indicate a role of next-word entropy or lookahead information gain. Our computational results suggest that, in a predictive processing system, the costs of predicting may outweigh the gains. This idea poses a potential limit to the value of a predictive mechanism for the processing of language. The result illustrates the unresolved problem of finding estimations of word-by-word prediction that, first, are truly independent of perceptual processing of the to-be-predicted words, second, are statistically reliable predictors of experimental data, and third, can be derived from more general assumptions about the cognitive processes involved.},
  bdsk-url-2 = {https://doi.org/10.1016/j.neuropsychologia.2019.107198},
  date-added = {2021-11-29 11:28:26 -0500},
  date-modified = {2022-04-21 09:12:00 -0400}
}

@book{awodey.s:2010,
  title = {Category Theory},
  author = {Awodey, Steve},
  year = {2010},
  publisher = {{Oxford University Press}},
  date-added = {2019-08-24 09:18:40 -0400},
  date-modified = {2019-08-24 09:19:06 -0400},
  keywords = {category theory}
}

@book{axler.s:2020,
  title = {Measure, {{Integration}} \& {{Real Analysis}}},
  author = {Axler, Sheldon},
  year = {2020},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {282},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-33143-6},
  url = {https://measure.axler.net/MIRA.pdf},
  urldate = {2022-06-23},
  isbn = {978-3-030-33142-9},
  langid = {english}
}

@article{aylett.m:2004,
  title = {The Smooth Signal Redundancy Hypothesis: A Functional Explanation for Relationships between Redundancy, Prosodic Prominence, and Duration in Spontaneous Speech},
  author = {Aylett, Matthew and Turk, Alice},
  year = {2004},
  journal = {Language and Speech},
  volume = {47},
  number = {1},
  eprint = {https://doi.org/10.1177/00238309040470010201},
  pages = {31--56},
  doi = {10.1177/00238309040470010201},
  url = {https://doi.org/10.1177/00238309040470010201},
  abstract = {This paper explores two related factors which influence variation in duration, prosodic structure and redundancy in spontaneous speech. We argue that the constraint of producing robust communication while efficiently expending articulatory effort leads to an inverse relationship between language redundancy and duration. The inverse relationship improves communication robustness by spreading information more evenly across the speech signal, yielding a smoother signal redundancy profile.We argue that prosodic prominence is a linguistic means of achieving smooth signal redundancy. Prosodic prominence increases syllable duration and coincides to a large extent with unpredictable sections of speech, and thus leads to a smoother signal redundancy.The results of linear regressions carried out between measures of redundancy, syllable duration and prosodic structure in a large corpus of spontaneous speech confirm: (1) an inverse relationship between language redundancy and duration, and (2) a strong relationship between prosodic prominence and duration.The fact that a large proportion of the variance predicted by language redundancy and prosodic prominence is nonunique suggests that, in English, prosodic prominence structure is the means with which constraints caused by a robust signal requirement are expressed in spontaneous speech.},
  date-added = {2022-04-27 12:19:58 -0400},
  date-modified = {2022-04-27 12:20:19 -0400},
  pmid = {15298329},
  keywords = {noisy channel coding}
}

@book{baayen.r:2001book,
  title = {Word Frequency Distributions},
  author = {Baayen, R. Harald},
  editor = {Ide, Nancy and V{\'e}ronis, Jean},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {18},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-0844-0},
  url = {http://link.springer.com/10.1007/978-94-010-0844-0},
  urldate = {2022-10-02},
  isbn = {978-1-4020-0927-3 978-94-010-0844-0},
  keywords = {best fit,corpus,Estimator},
  file = {/Users/j/Zotero/storage/NPISTVNL/Baayen (2001) Word Frequency Distributions.pdf}
}

@incollection{baayen.r:2001bookch1,
  title = {Word Frequencies},
  booktitle = {Word {{Frequency Distributions}}},
  author = {Baayen, R. Harald},
  editor = {Baayen, R. Harald},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  pages = {1--38},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-010-0844-0_1},
  url = {https://doi.org/10.1007/978-94-010-0844-0_1},
  urldate = {2022-10-02},
  abstract = {This chapter introduces two fundamental issues in lexical statistics. The first issue concerns the role of the sample size, the number of words in a text or corpus. The sample size crucially determines a great many measures that have been proposed as characteristic text constants. However, the values of these measures change systematically as a function of the sample size. Similarly, the parameters of many models for word frequency distribution are highly dependent on the sample size. This property sets lexical statistics apart from most other areas in statistics, where an increase in the sample size leads to enhanced accuracy and not to systematic changes in basic measures and parameters.},
  isbn = {978-94-010-0844-0},
  langid = {english},
  file = {/Users/j/Zotero/storage/IRF2DT4A/Baayen (2001) Word Frequencies.pdf}
}

@phdthesis{bachrach.a:2008phd,
  title = {Imaging Neural Correlates of Syntactic Complexity in a Naturalistic Context},
  author = {Bachrach, Asaf},
  year = {2008},
  url = {http://hdl.handle.net/1721.1/45900},
  date-added = {2021-06-09 09:00:47 -0400},
  date-modified = {2022-04-20 10:20:04 -0400},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Zotero/storage/7QTXG8TU/Bachrach - 2008 - Imaging neural correlates of syntactic complexity .pdf}
}

@unpublished{bachrach.a:2009,
  type = {Unpublished Manuscript},
  title = {Incremental Prediction in Naturalistic Language Processing: {{An fMRI}} Study},
  author = {Bachrach, Asaf and Roark, Brian and Marantz, Alex and {Whitfield-Gabrieli}, Susan and Cardenas, Carlos and Gabrieli, John},
  year = {2009}
}

@incollection{bader.m:1994,
  title = {German Verb-Final Clauses and Sentence Processing: {{Evidence}} for Immediate Attachment},
  shorttitle = {German Verb-Final Clauses and Sentence Processing},
  booktitle = {Perspectives on Sentence Processing},
  author = {Bader, Markus and Lasser, Ingeborg},
  editor = {Clifton, Jr., Charles and Frazier, Lyn and Rayner, Keith},
  year = {1994},
  pages = {225--242},
  publisher = {{Lawrence Erlbaum Associates, Inc}},
  address = {{Hillsdale, NJ, US}},
  abstract = {one central question in sentence processing concerns the relationship between knowledge of language and the way this knowledge is put to use / this relationship . . . has received a great deal of attention in the discussion of models of the human parsing mechanism [responsible for computing syntactic structures] / despite this attention, the issue of how linguistic knowledge is used during sentence comprehension is far from settled / goal [is] to narrow down the number of possible parsing models by introducing some on-line data from German  discuss a particular class of parsers that assumes both principles and parameters theory and a transparent grammar\textendash parser relationship / call these parsers head-driven licensing parsers / on the basis of experimental evidence from German verb-final structures, we reject the particular interpretation of grammar-parser transparency / introduce certain properties of current syntactic theory and then show how these properties have found their way into head-driven licensing parsers / report an experiment that aims to test the prediction of head-driven licensing parsers for verb-final clauses; namely, that, in these clauses, all attachments have to be delayed until the end of the clause / [Ss were 24 native German speaking university students] (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-1581-8 978-0-8058-1582-5},
  keywords = {Grammar,Psycholinguistics,Sentence Comprehension,Sentence Structure,Syntax,Verbs}
}

@misc{bahdanau.d:2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1409.0473},
  url = {http://arxiv.org/abs/1409.0473},
  urldate = {2022-05-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/S444U3WU/Bahdanau et al. - 2016 - Neural Machine Translation by Jointly Learning to .pdf}
}

@inproceedings{bailly.r:2020,
  title = {Emergence of Syntax Needs Minimal Supervision},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Bailly, Rapha{\"e}l and G{\'a}bor, Kata},
  year = {2020},
  pages = {477--487},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.46},
  url = {https://www.aclweb.org/anthology/2020.acl-main.46},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.46}
}

@article{baker.j:1979,
  title = {Trainable Grammars for Speech Recognition},
  author = {Baker, J. K.},
  year = {1979},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {65},
  number = {S1},
  pages = {S132-S132},
  publisher = {{Acoustical Society of America}},
  issn = {0001-4966},
  doi = {10.1121/1.2017061},
  url = {https://asa.scitation.org/doi/10.1121/1.2017061},
  urldate = {2022-07-04}
}

@article{balota.d:1985,
  title = {The Interaction of Contextual Constraints and Parafoveal Visual Information in Reading},
  author = {Balota, David A and Pollatsek, Alexander and Rayner, Keith},
  year = {1985},
  journal = {Cognitive Psychology},
  volume = {17},
  number = {3},
  pages = {364--390},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0285(85)90013-1},
  url = {https://doi.org/10.1016%2F0010-0285%2885%2990013-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(85)90013-1},
  date-added = {2021-05-22 15:35:25 -0400},
  date-modified = {2021-05-22 15:35:39 -0400},
  keywords = {predictability,processing}
}

@article{bar-hillel.y:1953,
  title = {A Quasi-Arithmetical Notation for Syntactic Description},
  author = {{Bar-Hillel}, Yehoshua},
  year = {1953},
  journal = {Language},
  volume = {29},
  number = {1},
  pages = {47},
  publisher = {{JSTOR}},
  doi = {10.2307/410452},
  url = {https://doi.org/10.2307%2F410452},
  bdsk-url-2 = {https://doi.org/10.2307/410452},
  date-added = {2021-06-25 00:50:06 -0400},
  date-modified = {2021-06-25 00:50:07 -0400}
}

@incollection{barber.d:2003,
  title = {The {{IM}} Algorithm: A Variational Approach to Information Maximization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Barber, David and Agakov, Felix},
  year = {2003},
  pages = {201--208},
  url = {https://papers.nips.cc/paper/2410-information-maximization-in-noisy-channels-a-variational-approach.pdf},
  bdsk-url-2 = {https://papers.nips.cc/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},
  date-added = {2019-10-08 23:33:35 -0400},
  date-modified = {2021-03-07 16:09:06 -0500},
  project = {syntactic embedding},
  keywords = {mutual information,variational inference}
}

@article{barnard.g:1946,
  title = {Sequential Tests in Industrial Statistics},
  author = {Barnard, G. A.},
  year = {1946},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {8},
  number = {1},
  pages = {1--21},
  issn = {2517-617X},
  doi = {10.2307/2983610},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.2307/2983610},
  urldate = {2022-07-04},
  abstract = {After an introductory and an historical note, an elementary problem of simple qualitative inspection of a box of components is treated by using a ``lattice diagram representation.'' This leads to the consideration of sequential tests for such cases. Procedures for determining ``Target-Handicap'' forms of inspection, and their operating and sample size properties are given. This leads to a consideration of general linear sequential tests, which are those test procedures which can be formulated in terms of a ``score.'' Such procedures are shown to be similar to classical games of chance, and to physical diffusion processes. The diffusion analogy leads to a differential equation which gives the approximate characteristics of any such linear test. In many cases, Wald's ``Probability Ratio Sequential Test'' takes the form of a linear test. The conditions for this are determined. The P.R.S. test is seen to be ``best possible linear test,'' in the sense of minimizing average sample size. The effects of deviations from normality, and general distributions are considered. Reference is made to Wald's work on tests which involve parameters other than those being estimated, and then consideration is restricted to tests for the mean of normal populations where the variance is unknown. Methods of reducing such tests to simple binomial tests are indicated. A number of procedures for use with 2 \texttimes{} 2 comparative trials, and double dichotomies, are given, and their properties discussed. Returning to general inspection problems, the paper indicates that these are not always to be identified with problems involving merely tests of statistical hypotheses. The notions of Consumer's Lot, Producer's Batch, the Lot Quality Curve, the Process Curve, are explained, and their importance indicated. A distinction is made between Acceptance Inspection schemes and Rectifying Inspection schemes, and the notions of Operating Characteristic Curve, Operating Characteristic Matrix, and the Sample Size distribution function are explained. The lattice diagram is used to bring out relationships between notions involved in general inspection, and some other uses are also indicated. Finally, some reflections on the relevance of the matters discussed to matters of current debate among statisticians are given.},
  langid = {english}
}

@inproceedings{barrett.m:2015,
  title = {The {{Dundee}} Treebank},
  booktitle = {The 14th International Workshop on Treebanks and Linguistic Theories ({{TLT}} 14)},
  author = {Barrett, Maria and Agic, {\v Z}eljko and S{\o}gaard, Anders},
  year = {2015},
  pages = {242--248},
  url = {http://tlt14.ipipan.waw.pl/files/4614/5063/3858/TLT14â‚šroceedings.pdf#page=249},
  date-added = {2021-09-16 13:19:25 -0400},
  date-modified = {2021-09-16 13:20:18 -0400}
}

@article{bashirov.a:2008,
  title = {Multiplicative Calculus and Its Applications},
  author = {Bashirov, Agamirza E. and Kurp{\i}nar, Emine M{\i}s{\i}rl{\i} and {\"O}zyap{\i}c{\i}, Ali},
  year = {2008},
  month = jan,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {337},
  number = {1},
  pages = {36--48},
  issn = {0022-247X},
  doi = {10.1016/j.jmaa.2007.03.081},
  url = {https://www.sciencedirect.com/science/article/pii/S0022247X07003824},
  urldate = {2023-04-25},
  abstract = {Two operations, differentiation and integration, are basic in calculus and analysis. In fact, they are the infinitesimal versions of the subtraction and addition operations on numbers, respectively. In the period from 1967 till 1970 Michael Grossman and Robert Katz gave definitions of a new kind of derivative and integral, moving the roles of subtraction and addition to division and multiplication, and thus established a new calculus, called multiplicative calculus. In the present paper our aim is to bring up this calculus to the attention of researchers and demonstrate its usefulness.},
  langid = {english},
  keywords = {Calculus,Calculus of variations,Derivative,Differential equation,haar measure,Integral,Limit,multiplicative integral,product integral,Semigroup}
}

@article{bates.c:2020,
  title = {Efficient Data Compression in Perception and Perceptual Memory.},
  author = {Bates, Christopher J. and Jacobs, Robert A.},
  year = {2020},
  month = oct,
  journal = {Psychological Review},
  volume = {127},
  number = {5},
  pages = {891--917},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000197},
  url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000197},
  urldate = {2022-11-28},
  abstract = {Efficient data compression is essential for capacity-limited systems, such as biological perception and perceptual memory. We hypothesize that the need for efficient compression shapes biological systems in many of the same ways that it shapes engineered systems. If true, then the tools that engineers use to analyze and design systems, namely rate-distortion theory (RDT), can profitably be used to understand human perception and memory. The first portion of this article discusses how three general principles for efficient data compression provide accounts for many important behavioral phenomena and experimental results. We also discuss how these principles are embodied in RDT. The second portion notes that exact RDT methods are computationally feasible only in low-dimensional stimulus spaces. To date, researchers have used deep neural networks to approximately implement RDT in high-dimensional spaces, but these implementations have been limited to tasks in which the sole goal is compression with respect to reconstruction error. Here, we introduce a new deep neural network architecture that approximately implements RDT. An important property of our architecture is that it is trained ``end-to-end,'' operating on raw perceptual input (e.g., pixel values) rather than intermediate levels of abstraction, as is the case with most psychological models. The article's final portion conjectures on how efficient compression can occur in memory over time, thereby providing motivations for multiple memory systems operating at different time scales, and on how efficient compression may explain some attentional phenomena such as RTs in visual search.},
  langid = {english},
  file = {/Users/j/Zotero/storage/TW4AHDBL/Bates and Jacobs (2020) Efficient data compression in perception and perce.pdf}
}

@article{baum.c:1994,
  title = {A Sequential Procedure for Multihypothesis Testing},
  author = {Baum, C.W. and Veeravalli, V.V.},
  year = {1994},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {40},
  number = {6},
  pages = {1994--2007},
  issn = {1557-9654},
  doi = {10.1109/18.340472},
  abstract = {The sequential testing of more than two hypotheses has important applications in direct-sequence spread spectrum signal acquisition, multiple-resolution-element radar, and other areas. A useful sequential test which we term the MSPRT is studied in this paper. The test is shown to be a generalization of the sequential probability ratio test. Under Bayesian assumptions, it is argued that the MSPRT approximates the much more complicated optimal test when error probabilities are small and expected stopping times are large. Bounds on error probabilities are derived, and asymptotic expressions for the stopping time and error probabilities are given. A design procedure is presented for determining the parameters of the MSPRT. Two examples involving Gaussian densities are included, and comparisons are made between simulation results and asymptotic expressions. Comparisons with Bayesian fixed sample size tests are also made, and it is found that the MSPRT requires two to three times fewer samples on average.{$<>$}},
  keywords = {Bayesian methods,Clinical trials,Error probability,Fault detection,Medical tests,Radar applications,Sequential analysis,Spread spectrum radar,Testing},
  file = {/Users/j/Zotero/storage/ANA82QN4/Baum and Veeravalli (1994) A sequential procedure for multihypothesis testing.pdf}
}

@article{baumann.s:2018,
  title = {What Makes a Word Prominent? {{Predicting}} Untrained {{German}} Listeners' Perceptual Judgments},
  author = {Baumann, Stefan and Winter, Bodo},
  year = {2018},
  journal = {Journal of Phonetics},
  volume = {70},
  pages = {20--38},
  publisher = {{Elsevier}},
  url = {https://sfb1252.uni-koeln.de/sites/sfbâ‚252/user<sub>u</sub>pload/Pdfs<sub>P</sub>ublikationen/Baumann<sub>W</sub>interâ‚‚018<sub>w</sub>hatâ‚˜akes<sub>w</sub>ord.pdf},
  bdsk-url-2 = {https://www.sciencedirect.com/science/article/pii/S0095447017301298},
  date-added = {2020-02-27 22:24:39 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {intonation,phonetics,prominence,prosody,random forests}
}

@misc{beauchene.c:2022,
  title = {Dynamic Cognitive States Predict Individual Variability in Behavior and Modulate with {{EEG}} Functional Connectivity during Working Memory},
  author = {Beauchene, Christine and Hinault, Thomas and Sarma, Sridevi V. and Courtney, Susan},
  year = {2022},
  month = jan,
  pages = {2021.08.02.454757},
  institution = {{bioRxiv}},
  doi = {10.1101/2021.08.02.454757},
  url = {https://www.biorxiv.org/content/10.1101/2021.08.02.454757v3},
  urldate = {2022-06-13},
  abstract = {Fluctuations in strategy, attention, or motivation can cause large variability in performance across task trials. Typically, this variability is treated as noise, and assumed to cancel out, leaving supposedly stable relationships among behavior, neural activity, and experimental task conditions. Those relationships, however, could change with a participant's internal cognitive states, and variability in performance may carry important information regarding those states, which cannot be directly measured. Therefore, we used a mathematical, state-space modeling framework to estimate internal states from measured behavioral data, quantifying each participant's sensitivity to factors such as past errors or distractions, to predict their reaction time fluctuations. We show how modeling these states greatly improves trial-by-trial prediction of behavior. Further, we identify EEG functional connectivity features that modulate with each state. These results illustrate the potential of this approach and how it could enable quantification of intra- and inter-individual differences and provide insight into their neural bases. Statement of Relevance Cognitive behavioral performance and its neural bases vary both across individuals and within individuals over time. Understanding this variability may be key to the success of clinical or educational interventions. Internal cognitive states reflecting differences in strategy, attention, and motivation may drive much of these inter- and intra-individual differences, but often cannot be reliably controlled or measured in cognitive neuroscience research. The mathematical modeling framework developed here uses measured data to estimate a participant's dynamic, internal cognitive states, with each state derived from specific factors hypothesized to affect attention, motivation or strategy. The results highlight potential sources of behavioral variability and reveal EEG features that modulate with each state. Our method quantifies and characterizes individual behavioral differences and highlights their underlying neural mechanisms, which could be used for future targeted training or neuromodulation therapies to improve cognitive performance.},
  chapter = {New Results},
  copyright = {\textcopyright{} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZELCMES7/Beauchene et al. - 2022 - Dynamic Cognitive States Predict Individual Variab.pdf}
}

@phdthesis{behrenfeldt.j:2009,
  title = {A Linguist's Survey of Pumping Lemmata},
  author = {Behrenfeldt, Johan},
  year = {2009},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.482.8670},
  date-added = {2020-02-12 12:04:22 -0500},
  date-modified = {2021-03-12 11:46:23 -0500},
  school = {University of Gothenburg},
  keywords = {formal languages,pumping lemmata}
}

@incollection{bejar.s:2003,
  title = {Person Licensing and the Derivation of {{PCC}} Effects},
  booktitle = {Romance Linguistics: {{Theory}} and Acquisition},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  editor = {{Perez-Leroux}, Ana Teresa and Roberg, Yves},
  year = {2003},
  pages = {49--62},
  publisher = {{J. Benjamins}},
  address = {{Amsterdam}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:56 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{bejar.s:2009,
  title = {Cyclic Agree},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  year = {2009},
  journal = {Linguistic Inquiry},
  volume = {40},
  number = {1},
  pages = {35--73},
  publisher = {{MIT Press}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:27:46 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects}
}

@inproceedings{belinkov.y:2017,
  title = {What Do Neural Machine Translation Models Learn about Morphology?},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  year = {2017},
  pages = {861--872},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, Canada}},
  doi = {10.18653/v1/P17-1080},
  url = {https://www.aclweb.org/anthology/P17-1080},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1080}
}

@inproceedings{bell.a:2003,
  title = {The Co-Information Lattice},
  booktitle = {Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation: {{ICA}}},
  author = {Bell, Anthony J},
  year = {2003},
  volume = {2003},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.320.5264},
  date-added = {2019-05-15 00:08:25 -0400},
  date-modified = {2021-07-19 22:04:55 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,synergy}
}

@misc{belrose.n:2023,
  title = {Eliciting Latent Predictions from Transformers with the Tuned Lens},
  author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08112},
  eprint = {2303.08112},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2303.08112},
  url = {http://arxiv.org/abs/2303.08112},
  urldate = {2023-03-16},
  abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the \textbackslash emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{bender.e:2021,
  title = {On the Dangers of Stochastic Parrots: {{Can}} Language Models Be Too Big? ðŸ¦œ},
  shorttitle = {On the Dangers of Stochastic Parrots},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3442188.3445922},
  url = {http://doi.org/10.1145/3442188.3445922},
  urldate = {2022-11-16},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/Users/j/Zotero/storage/NBA3RVTP/Bender et al. (2021) On the Dangers of Stochastic Parrots Can Language.pdf}
}

@article{bennett.r:2009,
  title = {English Resumptive Pronouns and the Highest-Subject Restriction: {{A}} Corpus Study},
  author = {Bennett, Ryan},
  year = {2009},
  journal = {Trilateral (TREND) Linguistics Weekend, UC Santa Cruz},
  file = {/Users/j/Zotero/storage/HYTVICIH/SynBFinalPaperFinalDraft.pdf;/Users/j/Zotero/storage/SG5BBN93/TRENDhandoutFINAL.pdf}
}

@techreport{bennett.v:2010,
  title = {Wasatch {{Solar Project}} Final Report},
  author = {Bennett, Vicki and Bowman, Kate and Wright, Sarah},
  year = {2018},
  number = {DOE-SLC-6903-1},
  address = {{Salt Lake City, UT}},
  institution = {{Salt Lake City Corporation}},
  doi = {10.2172/1474305},
  url = {https://doi.org/10.2172/1474305},
  date-added = {2021-03-12 11:39:14 -0500},
  date-modified = {2021-03-12 11:43:47 -0500}
}

@article{berwick.r:1982,
  title = {Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories},
  author = {Berwick, Robert C. and Weinberg, Amy S.},
  year = {1982},
  journal = {Linguistic Inquiry},
  volume = {13},
  number = {2},
  eprint = {4178272},
  eprinttype = {jstor},
  pages = {165--191},
  publisher = {{The MIT Press}},
  issn = {00243892, 15309150},
  url = {http://www.jstor.org/stable/4178272},
  date-added = {2022-03-29 20:33:12 -0400},
  date-modified = {2022-03-29 20:33:17 -0400}
}

@inproceedings{bicknell.k:2009,
  title = {A Model of Local Coherence Effects in Human Sentence Processing as Consequences of Updates from Bottom-up Prior to Posterior Beliefs},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the {{North American}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2009},
  month = jun,
  pages = {665--673},
  publisher = {{Association for Computational Linguistics}},
  address = {{Boulder, Colorado}},
  url = {https://aclanthology.org/N09-1075},
  date-added = {2022-04-21 10:50:28 -0400},
  date-modified = {2022-04-21 10:50:29 -0400},
  file = {/Users/j/Zotero/storage/A5IE9MQL/Bicknell and Levy - 2009 - A model of local coherence effects in human senten.pdf}
}

@inproceedings{bicknell.k:2010,
  title = {A Rational Model of Eye Movement Control in Reading},
  booktitle = {Proceedings of the 48th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2010},
  pages = {1168--1178},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  url = {https://www.aclweb.org/anthology/P10-1119}
}

@phdthesis{bicknell.k:2011,
  title = {Eye Movements in Reading as Rational Behavior},
  author = {Bicknell, Klinton},
  year = {2011},
  url = {https://www.klintonbicknell.com/dl/bicknell_diss.pdf},
  school = {University of California, San Diego},
  file = {/Users/j/Zotero/storage/MJ65ML8P/Bicknell - 2011 - Eye movements in reading as rational behavior.pdf}
}

@inproceedings{bicknell.k:2012cogsci,
  title = {Word Predictability and Frequency Effects in a Rational Model of Reading},
  booktitle = {Proceedings of the 34th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2012},
  volume = {34},
  pages = {126--131},
  publisher = {{Cognitive Science Society}},
  address = {{Sapporo, Japan}},
  url = {https://cogsci.mindmodeling.org/2012/papers/0035/},
  abstract = {This paper presents results from the first rational model of eye movement control in reading to make predictions for the full range of the eye movement record. The model identifies the text through Bayesian inference and makes eye movement de-cisions to maximize the efficiency of text identification, go-ing beyond leading approaches which select model parame-ters to maximize the fit to human data. Two simulations with the model demonstrate that it can produce effects of word pre-dictability and frequency on eye movements in reading similar to those produced by humans, providing evidence that many properties of human reading behavior may be understood as following from the nature of efficient text identification.},
  keywords = {surprisal theory},
  file = {/Users/j/Zotero/storage/37JRPKX6/Bicknell and Levy (2012) Word predictability and frequency effects in a rat.pdf}
}

@book{billingsley.p:1995,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  year = {1995},
  edition = {Third edition},
  publisher = {{Wiley}},
  url = {https://books.google.com/books?id=QyXqOXyxEeIC},
  bdsk-url-2 = {https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf},
  date-added = {2021-03-28 11:47:36 -0400},
  date-modified = {2021-08-21 16:16:00 -0400},
  keywords = {measure theory,probability theory}
}

@book{bishop.c:2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  edition = {First},
  publisher = {{Springer}},
  address = {{New York, USA}},
  url = {https://link.springer.com/book/9780387310732},
  urldate = {2022-06-10},
  isbn = {978-0-387-31073-2},
  langid = {english}
}

@article{bitzer.s:2014,
  title = {Perceptual Decision Making: Drift-Diffusion Model Is Equivalent to a {{Bayesian}} Model},
  shorttitle = {Perceptual Decision Making},
  author = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel, Stefan},
  year = {2014},
  journal = {Frontiers in Human Neuroscience},
  volume = {8},
  issn = {1662-5161},
  url = {https://www.frontiersin.org/articles/10.3389/fnhum.2014.00102},
  urldate = {2022-07-04},
  abstract = {Behavioral data obtained with perceptual decision making experiments are typically analyzed with the drift-diffusion model. This parsimonious model accumulates noisy pieces of evidence toward a decision bound to explain the accuracy and reaction times of subjects. Recently, Bayesian models have been proposed to explain how the brain extracts information from noisy input as typically presented in perceptual decision making tasks. It has long been known that the drift-diffusion model is tightly linked with such functional Bayesian models but the precise relationship of the two mechanisms was never made explicit. Using a Bayesian model, we derived the equations which relate parameter values between these models. In practice we show that this equivalence is useful when fitting multi-subject data. We further show that the Bayesian model suggests different decision variables which all predict equal responses and discuss how these may be discriminated based on neural correlates of accumulated evidence. In addition, we discuss extensions to the Bayesian model which would be difficult to derive for the drift-diffusion model. We suggest that these and other extensions may be highly useful for deriving new experiments which test novel hypotheses.},
  file = {/Users/j/Zotero/storage/TWAH7Q9N/Bitzer et al. - 2014 - Perceptual decision making drift-diffusion model .pdf}
}

@article{blachman.n:1968,
  title = {The Amount of Information That y Gives about {{X}}},
  author = {Blachman, N.},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {1},
  pages = {27--31},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tit.1968.1054094},
  url = {https://doi.org/10.1109%2Ftit.1968.1054094},
  bdsk-url-2 = {https://doi.org/10.1109/tit.1968.1054094},
  date-added = {2021-04-08 14:57:27 -0400},
  date-modified = {2021-04-08 14:57:41 -0400},
  keywords = {entropy,entropy reduction,surprisal},
  file = {/Users/j/Zotero/storage/82WR77XB/Blachman - 1968 - The amount of information that y gives about X.pdf}
}

@misc{black.s:2021GPT-Neo,
  title = {{{GPT-Neo}}: Large Scale Autoregressive Language Modeling with Mesh-Tensorflow},
  author = {Black, Sid and Leo, Gao and Wang, Phil and Leahy, Connor and Biderman, Stella},
  year = {2021},
  month = mar,
  publisher = {{Zenodo}},
  doi = {10.5281/zenodo.5297715},
  url = {https://doi.org/10.5281/zenodo.5297715},
  date-added = {2021-10-12 20:47:28 -0400},
  date-modified = {2022-05-11 20:39:00 -0400},
  version = {1.0}
}

@article{blei.d:2017,
  title = {Variational Inference: A Review for Statisticians},
  shorttitle = {Variational Inference},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  eprint = {1601.00670},
  primaryclass = {cs, stat},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  url = {http://arxiv.org/abs/1601.00670},
  urldate = {2022-06-29},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/TSYRFXUY/Blei et al. - 2017 - Variational Inference A Review for Statisticians.pdf}
}

@inproceedings{blevins.t:2018,
  title = {Deep {{RNNs}} Encode Soft Hierarchical Syntax},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  year = {2018},
  pages = {14--19},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-2003},
  url = {https://www.aclweb.org/anthology/P18-2003},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2003}
}

@book{boas.r:1997,
  title = {A Primer of Real Functions},
  author = {Boas, Ralph P. and Boas, Harold P.},
  year = {1997},
  month = jan,
  series = {The {{Carus}} Mathematical Monographs},
  edition = {4th ed},
  number = {no. 13},
  publisher = {{Mathematical Association of America}},
  address = {{Washington, D.C.}},
  isbn = {978-0-88385-029-9},
  lccn = {QA331.5 .B57 1996},
  keywords = {Functions of real variables}
}

@article{bobaljik.j:1996,
  title = {Subject Positions and the Roles of {{TP}}},
  author = {Bobaljik, Jonathan David and Jonas, Dianne},
  year = {1996},
  journal = {Linguistic Inquiry},
  volume = {27},
  number = {2},
  eprint = {4178934},
  eprinttype = {jstor},
  pages = {195--236},
  publisher = {{The MIT Press}},
  issn = {00243892, 15309150},
  url = {http://www.jstor.org/stable/4178934},
  abstract = {We propose that the specifier of a VP-external functional projection-Tense Phrase-may host subject NPs under certain conditions. We present empirical evidence that nonspecific subject NPs that have elsewhere been analyzed as remaining VP-internal occupy this position. We also offer theoretical arguments that transitive subjects may never remain internal to the VP at S-Structure in languages for which the Extended Projection Principle holds. Extending work by Bures (1992, 1993), we argue further that [Spec, TP] is implicated as a subject position in NP object shift constructions. Parametric availability of this one position accounts for a cluster of properties within the Germanic languages.},
  date-added = {2019-06-14 09:34:00 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {expletives,subject positions}
}

@article{bobaljik.j:2006,
  title = {Where's Phi},
  author = {Bobaljik, Jonathan David},
  year = {2006},
  journal = {Agreement as a postsyntactic operation. Ms. University of Connecticut},
  publisher = {{Citeseer}},
  date-added = {2020-02-02 22:23:49 -0500},
  date-modified = {2020-02-02 22:24:22 -0500},
  project = {Icelandic gluttony},
  keywords = {dative intervention,phi features}
}

@article{bock.k:1991,
  title = {Broken Agreement},
  author = {Bock, Kathryn and Miller, Carol A},
  year = {1991},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {23},
  number = {1},
  pages = {45--93},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(91)90003-7},
  url = {https://www.sciencedirect.com/science/article/pii/0010028591900037},
  urldate = {2023-04-03},
  abstract = {The subjects and verbs of English sentences agree in number. This superficially simple syntactic operation is regularly implemented by speakers, but occasionally derails in sentences such as The cost of the improvements have not yet been estimated. We examined whether the incidence of such errors was related to the presence of subject-like semantic features in the immediate preverbal nouns, in light of current questions about the semantic versus syntactic nature of sentence subjects and the interactivity of language processing. In three experiments, speakers completed sentence fragments designed to elicit erroneous agreement. We varied the number and animacy of the head noun and the immediate preverbal (local) noun, as well as the amount of material separating the head noun from the verb. The plurality of the local noun phrase had a large and reliable effect on the incidence of agreement errors, but neither its animacy nor its length affected their occurrence. The latter findings suggest, respectively, that the semantic features of sentence subjects are of minimal relevance to the syntactic and morphological processes that implement agreement, and that agreement features are specified at a point in processing where the eventual length of sentential constituents has little effect on syntactic planning. Both results follow naturally from explanations of language production that emphasize the segregation of sentence formulation processes into relatively autonomous components.},
  langid = {english},
  keywords = {agreement attraction}
}

@incollection{bod.r:2003,
  title = {Data-Oriented Parsing},
  booktitle = {Data-Oriented Parsing},
  editor = {Bod, Rens and Scha, Remko and Sima'an, Khalil},
  year = {2003},
  publisher = {{CSLI}},
  address = {{Stanford, CA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{bod.r:2006,
  title = {An All-Subtrees Approach to Unsupervised Parsing},
  booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  author = {Bod, Rens},
  year = {2006},
  pages = {865--872},
  publisher = {{Association for Computational Linguistics}},
  address = {{Sydney, Australia}},
  doi = {10.3115/1220175.1220284},
  url = {https://www.aclweb.org/anthology/P06-1109},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220284}
}

@article{boeckx.c:2000,
  title = {Quirky Agreement},
  author = {Boeckx, Cedric},
  year = {2000},
  journal = {Studia linguistica},
  volume = {54},
  number = {3},
  pages = {354--380},
  publisher = {{Wiley Online Library}},
  url = {https://doi.org/10.1111/1467-9582.00070},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{bogacz.r:2017,
  title = {A Tutorial on the Free-Energy Framework for Modelling Perception and Learning},
  author = {Bogacz, Rafal},
  year = {2017},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  series = {Model-Based {{Cognitive Neuroscience}}},
  volume = {76},
  pages = {198--211},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2015.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249615000759},
  urldate = {2022-08-07},
  abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
  langid = {english},
  file = {/Users/j/Zotero/storage/XB6DV3IH/Bogacz - 2017 - A tutorial on the free-energy framework for modell.pdf}
}

@phdthesis{bonet.m:1991,
  title = {Morphology after Syntax: {{Pronominal}} Clitics in {{Romance}}},
  author = {Bonet, M. Eul{\'a}lia},
  year = {1991},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:14:38 -0400},
  project = {Icelandic gluttony},
  school = {MIT},
  keywords = {clitics,hierarchy effects}
}

@article{boston.m:2008,
  title = {Parsing Costs as Predictors of Reading Difficulty: {{An}} Evaluation Using the {{Potsdam Sentence Corpus}}},
  shorttitle = {Parsing Costs as Predictors of Reading Difficulty},
  author = {Boston, Marisa Ferrara and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
  year = {2008},
  month = sep,
  journal = {Journal of Eye Movement Research},
  volume = {2},
  number = {1},
  issn = {1995-8692},
  doi = {10.16910/jemr.2.1.1},
  url = {https://bop.unibe.ch/JEMR/article/view/2255},
  urldate = {2022-10-13},
  abstract = {The surprisal of a word on a probabilistic grammar constitutes a promising complexity metric for human sentence comprehension difficulty. Using two different grammar types, surprisal is shown to have an effect on fixation durations and regression probabilities in a sample of German readers' eye movements, the Potsdam Sentence Corpus. A linear mixed-effects model was used to quantify the effect of surprisal while taking into account unigram frequency and bigram frequency (transitional probability), word length, and empirically-derived word predictability; the socalled ``early'' and ``late'' measures of processing difficulty both showed an effect of surprisal. Surprisal is also shown to have a small but statistically non-significant effect on empirically-derived predictability itself. This work thus demonstrates the importance of including parsing costs as a predictor of comprehension difficulty in models of reading, and suggests that a simple identification of syntactic parsing costs with early measures and late measures with durations of post-syntactic events may be difficult to uphold.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {parsing costs,parsing difficulty,potsdam sentence corpus,surprisal},
  file = {/Users/j/Zotero/storage/ECLBJ369/Boston et al. (2008) Parsing costs as predictors of reading difficulty.pdf}
}

@incollection{boston.m:2009,
  title = {Dependency Structures Derived from Minimalist Grammars},
  booktitle = {The Mathematics of Language},
  author = {Boston, Marisa Ferrara and Hale, John T. and Kuhlmann, Marco},
  year = {2009},
  pages = {1--12},
  publisher = {{Springer}},
  date-added = {2019-06-15 11:31:22 -0400},
  date-modified = {2022-04-20 13:49:51 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,minimalist grammars}
}

@inproceedings{bouchard-cote.a:2009,
  title = {Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Bouchard-C{\^o}t{\'e}}, Alexandre and Petrov, Slav and Klein, Dan},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2009/file/e515df0d202ae52fcebb14295743063b-Paper.pdf},
  date-added = {2022-03-31 10:05:14 -0400},
  date-modified = {2022-03-31 22:30:35 -0400},
  keywords = {markov chain monte carlo,pruning},
  file = {/Users/j/Zotero/storage/YX8NDKT7/Bouchard-CÃ´tÃ© et al. (2009) Randomized pruning Efficiently calculating expect.pdf}
}

@inproceedings{bouma.g:2009npmi,
  title = {Normalized (Pointwise) Mutual Information in Collocation Extraction},
  booktitle = {Von Der {{Form}} Zur {{Bedeutung}}: {{Texte}} Automatisch Verarbeiten / {{From Form}} to {{Meaning}}: {{Processing Texts Automatically}}},
  author = {Bouma, Gerlof},
  editor = {Chiarcos, Christian and {de Castilho}, Richard Eckart and Stede, Manfred},
  year = {2009},
  month = sep,
  pages = {43--53},
  publisher = {{Narr Francke Attempto Verlag GmbH + Co. KG}},
  address = {{T\"ubingen}},
  url = {https://elibrary.narr.digital/book/99.125005/9783823375111},
  abstract = {In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.},
  langid = {english},
  file = {/Users/j/Zotero/storage/Q3BFPINF/Bouma (2009) Normalized (Pointwise) Mutual Information in Collo.pdf}
}

@misc{boyce.v:2020amlap,
  type = {Talk},
  title = {A-Maze of {{Natural Stories}}: Texts Are Comprehensible Using the {{Maze}} Task},
  author = {Boyce, Veronica and Levy, Roger},
  year = {2020},
  month = sep,
  address = {{Potsdam, Germany}},
  url = {https://amlap2020.github.io/a/154.pdf},
  collaborator = {{von der Malsburg}, Titus and Vasishth, Shravan and Wartenburger, Isabell}
}

@misc{boyce.v:2020amlap_github,
  title = {Amaze-Natural-Stories},
  author = {Boyce, Veronica},
  year = {2022},
  month = mar,
  url = {https://github.com/vboyce/amaze-natural-stories},
  urldate = {2022-09-24},
  abstract = {Materials, data, code for A-maze of Natural Stories talk},
  keywords = {amlap}
}

@article{boyce.v:2020jml,
  title = {Maze Made Easy: Better and Easier Measurement of Incremental Processing Difficulty},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger},
  year = {2020},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {111},
  pages = {104082},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2019.104082},
  url = {https://doi.org/10.1016%2Fj.jml.2019.104082},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2019.104082},
  date-added = {2021-09-30 07:52:16 -0400},
  date-modified = {2022-04-20 13:48:24 -0400}
}

@misc{boyce.v:2022,
  title = {A-Maze of {{Natural Stories}}: {{Comprehension}} and Surprisal in the Maze Task},
  shorttitle = {A-Maze of Natural Stories},
  author = {Boyce, Veronica and Levy, Roger Philip},
  year = {2022},
  month = aug,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/8xkrf},
  url = {https://psyarxiv.com/8xkrf/},
  urldate = {2023-03-01},
  abstract = {Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task. Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, with little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task.},
  langid = {american},
  keywords = {A-Maze,incremental processing,Linguistics,naturalistic text,Psycholinguistics and Neurolinguistics,self-paced reading,Social and Behavioral Sciences,surprisal},
  file = {/Users/j/Zotero/storage/3DXXVM66/Boyce and Levy (2022) A-maze of Natural Stories Comprehension and surpr.pdf}
}

@book{brasoveanu.a:2020,
  title = {Computational Cognitive Modeling and Linguistic Theory},
  author = {Brasoveanu, Adrian and Dotla{\v c}il, Jakub},
  year = {2020},
  publisher = {{Springer Nature}},
  doi = {10.1007/978-3-030-31846-8},
  url = {https://library.oapen.org/handle/20.500.12657/39529},
  urldate = {2022-09-08},
  abstract = {This open access book introduces a general framework that allows natural language researchers to enhance existing competence theories with fully specified performance and processing components. Gradually developing increasingly complex and cognitively realistic competence-performance models, it provides running code for these models and shows how to fit them to real-time experimental data. This computational cognitive modeling approach opens up exciting new directions for research in formal semantics, and linguistics more generally, and offers new ways of (re)connecting semantics and the broader field of cognitive science. The approach of this book is novel in more ways than one. Assuming the mental architecture and procedural modalities of Anderson's ACT-R framework, it presents fine-grained computational models of human language processing tasks which make detailed quantitative predictions that can be checked against the results of self-paced reading and other psycho-linguistic experiments. All models are presented as computer programs that readers can run on their own computer and on inputs of their choice, thereby learning to design, program and run their own models. But even for readers who won't do all that, the book will show how such detailed, quantitatively predicting modeling of linguistic processes is possible. A methodological breakthrough and a must for anyone concerned about the future of linguistics! (Hans Kamp) This book constitutes a major step forward in linguistics and psycholinguistics. It constitutes a unique synthesis of several different research traditions: computational models of psycholinguistic processes, and formal models of semantics and discourse processing. The work also introduces a sophisticated python-based software environment for modeling linguistic processes. This book has the potential to revolutionize not only formal models of linguistics, but also models of language processing more generally. (Shravan Vasishth)},
  isbn = {978-3-030-31846-8},
  langid = {english},
  keywords = {ACT-R Based Left-corner Parser,bic Book Industry Communication::C Language::CF linguistics::CFA Philosophy of language,bic Book Industry Communication::C Language::CF linguistics::CFD Psycholinguistics,bic Book Industry Communication::C Language::CF linguistics::CFG Semantics,Cataphoric Presupposition Resolution,Cognitive Aspects of Processing Semantic Representations,discourse analysis,Enriched Semantics,etc,Incremental Dynamic Predicate Logic,Language Interpretation Processes,Linguistics,Meaning Representations in Formal Semantics,Natural Language Processing,Open Access,Philosophy of language,Philosophy of Language,Processing Enriched Logical Forms,Processing of Lexical Semantic and Syntactic Representations,Psycholinguistics,Psycholinguistics and Cognitive Lingusitics,Psycholinguistics on Incremental Interpretation,Real-time Construction of Syntactic Representations,Real-time Semantic Interpretation,Semantics,Semantics and Processing,stylistics},
  annotation = {Accepted: 2020-06-15T15:07:27Z},
  file = {/Users/j/Zotero/storage/BR4RVBDJ/Brasoveanu and DotlaÄil (2020) Computational Cognitive Modeling and Linguistic Th.pdf}
}

@article{braun.d:2014,
  title = {Information-Theoretic Bounded Rationality and {$\epsilon$}-Optimality},
  author = {Braun, Daniel A. and Ortega, Pedro A.},
  year = {2014},
  month = aug,
  journal = {Entropy},
  volume = {16},
  number = {8},
  pages = {4662--4676},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e16084662},
  url = {https://www.mdpi.com/1099-4300/16/8/4662},
  urldate = {2022-06-08},
  abstract = {Bounded rationality concerns the study of decision makers with limited information processing resources. Previously, the free energy difference functional has been suggested to model bounded rational decision making, as it provides a natural trade-off between an energy or utility function that is to be optimized and information processing costs that are measured by entropic search costs. The main question of this article is how the information-theoretic free energy model relates to simple {$\epsilon$}-optimality models of bounded rational decision making, where the decision maker is satisfied with any action in an {$\epsilon$}-neighborhood of the optimal utility. We find that the stochastic policies that optimize the free energy trade-off comply with the notion of {$\epsilon$}-optimality. Moreover, this optimality criterion even holds when the environment is adversarial. We conclude that the study of bounded rationality based on {$\epsilon$}-optimality criteria that abstract away from the particulars of the information processing constraints is compatible with the information-theoretic free energy model of bounded rationality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {ambiguity,bounded rationality,decision theory,information theory,probabilistic choice,{$\epsilon$}-optimality},
  file = {/Users/j/Zotero/storage/TID5UPAK/Braun and Ortega - 2014 - Information-Theoretic Bounded Rationality and Îµ-Op.pdf}
}

@book{bresnan.j:2001,
  title = {Lexical-Functional Syntax},
  author = {Bresnan, Joan},
  year = {2001},
  publisher = {{Wiley-Blackwell}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{brody.s:2010,
  title = {It Depends on the Translation: {{Unsupervised}} Dependency Parsing via Word Alignment},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  author = {Brody, Samuel},
  year = {2010},
  pages = {1214--1222},
  publisher = {{Association for Computational Linguistics}},
  address = {{Cambridge, MA}},
  url = {https://www.aclweb.org/anthology/D10-1118}
}

@article{brothers.t:2021,
  title = {Word Predictability Effects Are Linear, Not Logarithmic: Implications for Probabilistic Models of Sentence Comprehension},
  author = {Brothers, Trevor and Kuperberg, Gina R.},
  year = {2021},
  journal = {Journal of Memory and Language},
  volume = {116},
  pages = {104174},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104174},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X20300887},
  abstract = {During language comprehension, we routinely use information from the prior context to help identify the meaning of individual words. While measures of online processing difficulty, such as reading times, are strongly influenced by contextual predictability, there is disagreement about the mechanisms underlying this lexical predictability effect, with different models predicting different linking functions \textendash{} linear (Reichle, Rayner, \& Pollatsek, 2003) or logarithmic (Levy, 2008). To help resolve this debate, we conducted two highly-powered experiments (self-paced reading, N = 216; cross-modal picture naming, N = 36), and a meta-analysis of prior eye-tracking while reading studies (total N = 218). We observed a robust linear relationship between lexical predictability and word processing times across all three studies. Beyond their methodological implications, these findings also place important constraints on predictive processing models of language comprehension. In particular, these results directly contradict the empirical predictions of surprisal theory, while supporting a proportional pre-activation account of lexical prediction effects in comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2020.104174},
  date-added = {2021-03-09 22:52:14 -0500},
  date-modified = {2021-11-14 23:57:23 -0500},
  keywords = {Information theory,Language comprehension,Prediction,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Zotero/storage/4H9J3RQ3/Brothers and Kuperberg - 2021 - Word predictability effects are linear, not logari.pdf}
}

@article{brown.p:1993,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
  year = {1993},
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {263--311},
  url = {https://www.aclweb.org/anthology/J93-2003}
}

@inproceedings{brown.t:2020GPT3,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020},
  url = {https://proceedings.neurips.cc/paper/2020/hash/1457c0d6bfcb4967418bfb8ac142f64a-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  date-modified = {2022-04-30 13:50:00 -0400},
  keywords = {GPT,GPT3},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100}
}

@article{bruening.b:2012,
  title = {{\emph{By}} Phrases in Passives and Nominals},
  author = {Bruening, Benjamin},
  year = {2012},
  journal = {Syntax (Oxford, England)},
  volume = {16},
  number = {1},
  pages = {1--41},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9612.2012.00171.x},
  url = {https://doi.org/10.1111%2Fj.1467-9612.2012.00171.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2012.00171.x},
  date-added = {2021-03-22 13:11:47 -0400},
  date-modified = {2021-03-24 11:10:53 -0400}
}

@unpublished{bruening.b:2015,
  title = {Idioms: {{Movement}} and Non-Movement Dependencies},
  author = {Bruening, Benjamin},
  year = {2015},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {Idioms}
}

@article{buckley.c:2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  shorttitle = {The Free Energy Principle for Action and Perception},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  year = {2017},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {55--79},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0022249617300962},
  urldate = {2022-07-27},
  abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  langid = {english},
  keywords = {Action,Agent-based model,Bayesian brain,free energy principle,Free energy principle,Inference,Perception},
  file = {/Users/j/Zotero/storage/3HKHSLGE/Buckley et al. - 2017 - The free energy principle for action and perceptio.pdf}
}

@misc{burda.y:2015IWAE,
  title = {Importance Weighted Autoencoders},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2015},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1509.00519},
  url = {https://arxiv.org/abs/1509.00519},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1509.00519},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:17:42 -0400},
  date-modified = {2022-05-05 09:19:51 -0400},
  keywords = {importance weighted autoencoders}
}

@misc{burda.y:2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2016},
  month = nov,
  number = {arXiv:1509.00519},
  eprint = {1509.00519},
  primaryclass = {cs, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1509.00519},
  url = {http://arxiv.org/abs/1509.00519},
  urldate = {2022-05-18},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/KYZW47BL/Burda et al. - 2016 - Importance Weighted Autoencoders.pdf}
}

@inproceedings{buys.j:2015,
  title = {A {{Bayesian}} Model for Generative Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015)},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = aug,
  pages = {58--67},
  publisher = {{Uppsala University, Uppsala, Sweden}},
  address = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/W15-2108},
  date-added = {2022-04-25 20:09:07 -0400},
  date-modified = {2022-04-25 20:09:41 -0400},
  keywords = {bayesian,dependency parsing},
  file = {/Users/j/Zotero/storage/NG8H3XDE/Buys and Blunsom - 2015 - A Bayesian model for generative transition-based d.pdf}
}

@inproceedings{buys.j:2015short,
  title = {Generative {{Incremental Dependency Parsing}} with {{Neural Networks}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = jul,
  pages = {863--869},
  publisher = {{Association for Computational Linguistics}},
  address = {{Beijing, China}},
  doi = {10.3115/v1/P15-2142},
  url = {https://aclanthology.org/P15-2142},
  urldate = {2022-06-14},
  file = {/Users/j/Zotero/storage/GXIK587E/Buys and Blunsom - 2015 - Generative Incremental Dependency Parsing with Neu.pdf}
}

@phdthesis{buys.j:2018phd,
  title = {Incremental Generative Models for Syntactic and Semantic Natural Language Processing},
  author = {Buys, Jan},
  year = {2018},
  url = {https://ora.ox.ac.uk/objects/uuid:a9a7b5cf-3bb1-4e08-b109-de06bf387d1d},
  urldate = {2022-06-14},
  abstract = {{$<$}p{$>$}This thesis investigates the role of linguistically-motivated generative models of syntax and semantic structure in natural language processing (NLP). Syntactic well-formedness is crucial in language generation, but most statistical models do not account for the hierarchical structure of sentences. Many applications exhibiting natural language understanding rely on structured semantic representations to enable querying, inference and reasoning. Yet most semantic parsers produce domain-specific or inadequately expressive representations.{$<$}/p{$>$} {$<$}p{$>$}We propose a series of generative transition-based models for dependency syntax which can be applied as both parsers and language models while being amenable to supervised or unsupervised learning. Two models are based on Markov assumptions commonly made in NLP: The first is a Bayesian model with hierarchical smoothing, the second is parameterised by feed-forward neural networks. The Bayesian model enables careful analysis of the structure of the conditioning contexts required for generative parsers, but the neural network is more accurate. As a language model the syntactic neural model outperforms both the Bayesian model and n-gram neural networks, pointing to the complementary nature of distributed and structured representations for syntactic prediction. We propose approximate inference methods based on particle filtering. The third model is parameterised by recurrent neural networks (RNNs), dropping the Markov assumptions. Exact inference with dynamic programming is made tractable here by simplifying the structure of the conditioning contexts.{$<$}/p{$>$} {$<$}p{$>$}We then shift the focus to semantics and propose models for parsing sentences to labelled semantic graphs. We introduce a transition-based parser which incrementally predicts graph nodes (predicates) and edges (arguments). This approach is contrasted against predicting top-down graph traversals. RNNs and pointer networks are key components in approaching graph parsing as an incremental prediction problem. The RNN architecture is augmented to condition the model explicitly on the transition system configuration. We develop a robust parser for Minimal Recursion Semantics, a linguistically-expressive framework for compositional semantics which has previously been parsed only with grammar-based approaches. Our parser is much faster than the grammar-based model, while the same approach improves the accuracy of neural Abstract Meaning Representation parsing.{$<$}/p{$>$}},
  langid = {english},
  school = {University of Oxford},
  file = {/Users/j/Zotero/storage/ERI98WK2/Buys - 2018 - Incremental generative models for syntactic and se.pdf}
}

@article{cappe.o:2007,
  title = {An Overview of Existing Methods and Recent Advances in Sequential {{Monte Carlo}}},
  author = {Cappe, Olivier and Godsill, Simon J. and Moulines, Eric},
  year = {2007},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/jproc.2007.893250},
  url = {https://doi.org/10.1109%2Fjproc.2007.893250},
  bdsk-url-2 = {https://doi.org/10.1109/jproc.2007.893250},
  date-added = {2022-03-25 11:26:52 -0400},
  date-modified = {2022-03-25 11:26:52 -0400}
}

@article{carpenter.r:1995,
  title = {Neural Computation of Log Likelihood in Control of Saccadic Eye Movements},
  author = {Carpenter, R. H. S. and Williams, M. L. L.},
  year = {1995},
  month = sep,
  journal = {Nature},
  volume = {377},
  number = {6544},
  pages = {59--62},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/377059a0},
  url = {https://www.nature.com/articles/377059a0},
  urldate = {2022-06-24},
  abstract = {THE latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals1. In one such model2, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s0 until it reaches a fixed threshold 0, when a saccade is initiated. One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present. Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if s0 simply reflects the prior log likelihood of the stimulus.},
  copyright = {1995 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/j/Zotero/storage/6EA288J3/Carpenter and Williams - 1995 - Neural computation of log likelihood in control of.pdf}
}

@techreport{carroll.g:1992,
  type = {{{AAAI}} Technical Report},
  title = {Two Experiments on Learning Probabilistic Dependency Grammars from Corpora},
  author = {Carroll, Glenn and Charniak, Eugene},
  year = {1992},
  journal = {Working notes of the AAAI workshop statistically-based NLP techniques},
  number = {WS-92-01},
  pages = {1--13},
  institution = {{AAAI}},
  url = {https://aaai.org/Library/Workshops/1992/ws92-01-001.php},
  abstract = {We present a scheme for learning prohabilistic dependency grammars from positive training examples plus constraints on rules. In particular we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.},
  date-added = {2021-04-18 10:28:37 -0400},
  date-modified = {2021-04-18 10:33:11 -0400},
  project = {syntactic embedding},
  keywords = {Dependency Grammar,dependency parsing,dependency structures,mutual information,pmi,unsupervised parsing}
}

@article{casadio.c:2002,
  title = {A Tale of Four Grammars},
  author = {Casadio, Claudia and Lambek, Joachim},
  year = {2002},
  journal = {Studia Logica. An International Journal for Symbolic Logic},
  volume = {71},
  number = {3},
  pages = {315--329},
  publisher = {{Springer}},
  date-added = {2019-10-25 23:52:18 -0400},
  date-modified = {2019-10-25 23:54:15 -0400},
  keywords = {bilinear logic,category theory,pregroup grammar}
}

@article{chaloner.k:1995,
  title = {Bayesian Experimental Design: A Review},
  author = {Chaloner, Kathryn and Verdinelli, Isabella},
  year = {1995},
  journal = {Statistical Science},
  volume = {10},
  number = {3},
  eprint = {2246015},
  eprinttype = {jstor},
  pages = {273--304},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {08834237},
  url = {http://www.jstor.org/stable/2246015},
  abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision-theoretic approach. This framework casts criteria from the Bayesian literature of design as part of a single coherent approach. The decision-theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility function are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion.},
  date-added = {2021-09-15 10:23:51 -0400},
  date-modified = {2021-09-15 10:23:53 -0400}
}

@inproceedings{chang.h:2022,
  title = {Softmax {{Bottleneck Makes Language Models Unable}} to {{Represent Multi-mode Word Distributions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chang, Haw-Shiuan and McCallum, Andrew},
  year = {2022},
  month = may,
  pages = {8048--8073},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.554},
  url = {https://aclanthology.org/2022.acl-long.554},
  urldate = {2022-06-23},
  abstract = {Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.},
  file = {/Users/j/Zotero/storage/J66UL3ER/Chang and McCallum - 2022 - Softmax Bottleneck Makes Language Models Unable to.pdf}
}

@article{chang.j:1997,
  title = {Conditioning as Disintegration},
  author = {Chang, Joseph T and Pollard, David},
  year = {1997},
  journal = {Statistica Neerlandica},
  volume = {51},
  number = {3},
  pages = {287--317},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/1467-9574.00056},
  url = {https://doi.org/10.1111/1467-9574.00056},
  date-added = {2021-02-05 12:20:15 -0500},
  date-modified = {2021-02-05 12:22:28 -0500},
  keywords = {probability theory}
}

@article{chater.n:1999,
  title = {Ten Years of the Rational Analysis of Cognition},
  author = {Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, Nick and Oaksford, Mike},
  year = {1999},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {2},
  pages = {57--65},
  publisher = {{Elsevier}},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(98)01273-X},
  url = {http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(98)01273-X},
  urldate = {2022-07-07},
  langid = {english},
  pmid = {10234228},
  keywords = {Memory,Neuroscience,Rational analysis,Rationality,Reasoning},
  file = {/Users/j/Zotero/storage/JZ32KMG5/Chater et al. - 1999 - Ten years of the rational analysis of cognition.pdf}
}

@article{chater.n:2006,
  title = {Probabilistic Models of Language Processing and Acquisition},
  author = {Chater, Nick and Manning, Christopher D.},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  series = {Special Issue: {{Probabilistic}} Models of Cognition},
  volume = {10},
  number = {7},
  pages = {335--344},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.05.006},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661306001318},
  urldate = {2022-10-22},
  abstract = {Probabilistic methods are providing new explanatory approaches to fundamental cognitive science questions of how humans structure, process and acquire language. This review examines probabilistic models defined over traditional symbolic structures. Language comprehension and production involve probabilistic inference in such models; and acquisition involves choosing the best model, given innate constraints and linguistic and other input. Probabilistic models can account for the learning and processing of language, while maintaining the sophistication of symbolic models. A recent burgeoning of theoretical developments and online corpus creation has enabled large models to be tested, revealing probabilistic constraints in processing, undermining acquisition arguments based on a perceived poverty of the stimulus, and suggesting fruitful links with probabilistic theories of categorization and ambiguity resolution in perception.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ERH9LPVT/Chater and Manning (2006) Probabilistic models of language processing and ac.pdf}
}

@article{chatterjee.s:2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  year = {2018},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {28},
  number = {2},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/17-aap1326},
  url = {https://doi.org/10.1214%2F17-aap1326},
  file = {/Users/j/Zotero/storage/VMZ3QKKH/Chatterjee and Diaconis - 2018 - The sample size required in importance sampling.pdf}
}

@misc{chen.c:2023,
  title = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01318},
  eprint = {2302.01318},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2302.01318},
  urldate = {2023-02-15},
  abstract = {We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,language model decoding,rejection sampling,speculative sampling},
  file = {/Users/j/Zotero/storage/IHLM5UPW/Chen et al. (2023) Accelerating Large Language Model Decoding with Sp.pdf}
}

@inproceedings{chen.d:2014,
  title = {A Fast and Accurate Dependency Parser Using Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Chen, Danqi and Manning, Christopher},
  year = {2014},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/d14-1082},
  url = {https://doi.org/10.3115%2Fv1%2Fd14-1082},
  bdsk-url-2 = {https://doi.org/10.3115/v1/d14-1082},
  date-added = {2022-05-06 15:52:04 -0400},
  date-modified = {2022-05-06 15:53:01 -0400},
  keywords = {dependency parsing,dependency structures,parsing,stanford dependencies}
}

@inproceedings{chen.s:1995,
  title = {Bayesian Grammar Induction for Language Modeling},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Stanley F.},
  year = {1995},
  month = jun,
  series = {{{ACL}} '95},
  pages = {228--235},
  publisher = {{Association for Computational Linguistics}},
  address = {{USA}},
  doi = {10.3115/981658.981689},
  url = {http://doi.org/10.3115/981658.981689},
  urldate = {2022-07-04},
  abstract = {We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.},
  file = {/Users/j/Zotero/storage/R22E3FH4/Chen - 1995 - Bayesian grammar induction for language modeling.pdf}
}

@inproceedings{chen.x:2016,
  title = {{{InfoGAN}}: {{Interpretable}} Representation Learning by Information Maximizing Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 29: {{Annual}} Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and {von Luxburg}, Ulrike and Guyon, Isabelle and Garnett, Roman},
  year = {2016},
  pages = {2172--2180},
  url = {https://proceedings.neurips.cc/paper/2016/hash/7c9d0b1f96aebd7b5eca8c3edaa19ebb-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChenCDHSSA16.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{chen.y:2005,
  title = {Another Look at Rejection Sampling through Importance Sampling},
  author = {Chen, Yuguo},
  year = {2005},
  month = may,
  journal = {Statistics \& Probability Letters},
  volume = {72},
  number = {4},
  pages = {277--283},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2005.01.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0167715205000374},
  urldate = {2023-01-01},
  abstract = {We show that rejection sampling is inferior to the importance sampling algorithm in terms of the {$\chi$}2 distance between the proposal distribution and the target distribution. Similar conclusions are drawn for comparing rejection control with importance sampling.},
  langid = {english},
  keywords = {distance,Effective sample size,importance sampling,Importance sampling,Rejection control,rejection sampling,Rejection sampling},
  file = {/Users/j/Zotero/storage/ATCTMLNL/Chen (2005) Another look at rejection sampling through importa.pdf}
}

@article{cheyette.s:2020,
  title = {A Unified Account of Numerosity Perception},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  year = {2020},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {12},
  pages = {1265--1272},
  publisher = {{Nature Publishing Group}},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00946-0},
  url = {http://colala.berkeley.edu/papers/cheyette2020unified.pdf},
  urldate = {2022-05-19},
  abstract = {People can identify the number of objects in sets of four or fewer items with near-perfect accuracy but exhibit linearly increasing error for larger sets. Some researchers have taken this discontinuity as evidence of two distinct representational systems. Here, we present a mathematical derivation showing that this behaviour is an optimal representation of cardinalities under a limited informational capacity, indicating that this behaviour can emerge from a single system. Our derivation predicts how the amount of information accessible to viewers should influence the perception of quantity for both large and small sets. In a series of four preregistered experiments (N\,=\,100 each), we varied the amount of information accessible to participants in number estimation. We find tight alignment between the model and human performance for both small and large quantities, implicating efficient representation as the common origin behind key phenomena of human and animal numerical cognition.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational models,Human behaviour,Object vision},
  file = {/Users/j/Zotero/storage/XBEQTIC8/Cheyette and Piantadosi - 2020 - A unified account of numerosity perception.pdf}
}

@book{chierchia.g:2013,
  title = {Logic in Grammar: {{Polarity}}, Free Choice, and Intervention},
  author = {Chierchia, Gennaro},
  year = {2013},
  publisher = {{Oxford}},
  url = {https://doi.org/10.1093/acprof:oso/9780199697977.001.0001},
  date-added = {2019-05-19 22:01:50 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {logic,mathematical linguistics,semantics}
}

@inproceedings{chierchia.g:2019,
  title = {Ultrametric Fitting by Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Chierchia, Giovanni and Perret, Benjamin},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3175--3186},
  url = {https://proceedings.neurips.cc/paper/2019/hash/b865367fc4c0845c0682bd466e6ebf4c-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChierchiaP19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@unpublished{chomsky.n:1955,
  type = {Unpublished Manuscript},
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {[1975] 1955},
  url = {http://alpha-leonis.lids.mit.edu/wordpress/?page<sub>i</sub>d=466},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  keywords = {information theory,syntactic information,syntax}
}

@book{chomsky.n:1957,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {1957},
  publisher = {{Mouton and Co.}},
  address = {{The Hague}},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{chomsky.n:1965aspects,
  title = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  year = {1965},
  publisher = {{MIT Press}},
  date-added = {2022-04-14 12:41:20 -0400},
  date-modified = {2022-04-14 12:42:21 -0400},
  keywords = {generative grammar,syntax},
  file = {/Users/j/Zotero/storage/JJRGVPGC/chomsky.n.1965aspects.pdf;/Users/j/Zotero/storage/RTM36H2P/chomsky.n.1965aspects50th.pdf}
}

@book{chomsky.n:1975,
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {1975},
  publisher = {{Springer}},
  url = {http://alpha-leonis.lids.mit.edu/wordpress/?page<sub>i</sub>d=466},
  date-added = {2020-06-05 14:43:06 -0400},
  date-modified = {2020-06-05 14:44:48 -0400},
  project = {syntactic embedding},
  keywords = {syntax}
}

@book{chomsky.n:1995,
  title = {The Minimalist Program},
  author = {Chomsky, Noam},
  year = {1995},
  publisher = {{MIT Press}},
  url = {https://muse.jhu.edu/book/36980},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@incollection{chomsky.n:1995a,
  title = {Bare Phrase Structure},
  booktitle = {Government and Binding Theory and the Minimalist Program},
  author = {Chomsky, Noam},
  editor = {Webelhuth, Gerth},
  year = {1995},
  pages = {383--349},
  publisher = {{Blackwell}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {X-bar}
}

@book{chomsky.n:2002ss2e,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {2002},
  month = nov,
  edition = {Second},
  publisher = {{Mouton de Gruyter}},
  doi = {10.1515/9783110218329},
  url = {https://doi.org/10.1515%2F9783110218329},
  bdsk-url-2 = {https://doi.org/10.1515/9783110218329},
  date-added = {2021-09-24 08:27:37 -0400},
  date-modified = {2021-09-24 08:30:27 -0400},
  file = {/Users/j/Zotero/storage/R93E8RMP/Chomsky - 2002 - Syntactic structures.pdf}
}

@book{chopin.n:2020,
  title = {An Introduction to Sequential {{Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  month = oct,
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {First},
  publisher = {{Springer}},
  address = {{Cham, Switzerland}},
  doi = {10.1007/978-3-030-47845-2},
  abstract = {Offers a general and gentle introduction to all aspects of particle filtering: the algorithms, their uses in different areas, their computer implementation in Python and the supporting theory Covers both the basics and more advanced, cutting-edge developments, such as PMCMC (particle Markov chain Monte Carlo) and SQMC (Sequential quasi-Monte Carlo) Comes with a freely available Python library (particles), which implements all the algorithms discussed in the book. Each chapter ends with a ``Python corner'' that discusses how the methods covered can be implemented in Python},
  isbn = {978-3-030-47844-5},
  langid = {english},
  keywords = {monte carlo,sampling,statistics},
  file = {/Users/j/Zotero/storage/9YYK7L7C/Chopin and Papaspiliopoulos - 2020 - An introduction to sequential monte carlo.pdf}
}

@article{chow.c:1968,
  title = {Approximating Discrete Probability Distributions with Dependence Trees},
  author = {Chow, C and Liu, Cong},
  year = {1968},
  journal = {IEEE transactions on Information Theory},
  volume = {14},
  number = {3},
  pages = {462--467},
  publisher = {{IEEE}},
  date-added = {2019-10-09 20:57:36 -0400},
  date-modified = {2019-10-09 20:58:15 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,mutual information}
}

@misc{chowdhery.a:2022PaLM,
  title = {{{PaLM}}: Scaling Language Modeling with Pathways},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2204.02311},
  url = {https://arxiv.org/abs/2204.02311},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.02311},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-04-19 13:50:14 -0400},
  date-modified = {2022-04-28 12:21:29 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{chung.f:1984,
  title = {On Optimal Linear Arrangements of Trees},
  author = {Chung, F.R.K.},
  year = {1984},
  journal = {Computers and Mathematics with Applications},
  volume = {10},
  number = {1},
  pages = {43--60},
  issn = {0898-1221},
  url = {http://www.sciencedirect.com/science/article/pii/0898122184900853},
  bdsk-url-2 = {https://doi.org/10.1016/0898-1221(84)90085-3},
  date-added = {2019-05-14 23:56:39 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,linearization}
}

@inproceedings{church.k:1990,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  year = {1989},
  pages = {76--83},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, British Columbia, Canada}},
  doi = {10.3115/981623.981633},
  url = {https://www.aclweb.org/anthology/P89-1010},
  bdsk-url-2 = {https://doi.org/10.3115/981623.981633}
}

@inproceedings{clark.c:2019,
  title = {Don't Take the Easy Way out: {{Ensemble}} Based Methods for Avoiding Known Dataset Biases},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  year = {2019},
  pages = {4069--4082},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1418},
  url = {https://www.aclweb.org/anthology/D19-1418},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1418}
}

@inproceedings{clark.k:2019,
  title = {What Does {{BERT}} Look at? {{An}} Analysis of {{BERT}}'s Attention},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  pages = {276--286},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4828},
  url = {https://www.aclweb.org/anthology/W19-4828},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4828}
}

@article{clark.t:2022,
  title = {Evidence for Availability Effects on Speaker Choice in the {{Russian}} Comparative Alternation},
  author = {Clark, Thomas and Wilcox, Ethan Gotlieb and Gibson, Edward and Levy, Roger},
  year = {2022},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {44},
  url = {https://escholarship.org/uc/item/1q19f8vt},
  urldate = {2022-10-29},
  abstract = {When a language offers multiple options for expressing the same meaning, what principles govern a speaker's choice? Two well-known principles proposed for explaining wide-ranging speaker preference are Uniform Information Density and Availability-Based Production. Here we test the predictions of these theories in a previously uninvestigated case of speaker choice. Russian has two ways of expressing the comparative: an \textbackslash textsc\{explicit\} option (\textbackslash textit\{Ona bystree chem ja\}/She fast\{\textbackslash sc-comp\} than me\{\textbackslash sc-nom\}) and a \textbackslash textsc\{genitive\} option (\textbackslash textit\{Ona bystree menya/She fast\{\textbackslash sc-comp\} me\{\textbackslash sc-gen\}\}). We lay out several potential predictions of each theory for speaker choice in the Russian comparative construction, including effects of post-comparative word predictability, phrase length, syntactic complexity, and semantic association between the comparative adjective and subsequent noun. In a corpus study, we find that the explicit construction is used preferentially when the post-comparative noun phrase is longer, has a relative clause, and is less semantically associated with the comparative adjective. A follow-up production experiment using visual scene stimuli to elicit comparative sentences replicates the corpus finding that Russian native speakers prefer the explicit form when post-comparative phrases are longer. These findings offer no clear support for the predictions of Uniform Information Density, but are broadly supportive of Availability-Based Production, with the explicit option serving as an unreduced form that eases speakers' planning of complex or low-availability utterances. Code for this study is available at https://github.mit.edu/thclark/russian\_uid},
  langid = {english},
  keywords = {uniform information density}
}

@misc{coecke.b:2010,
  title = {Mathematical Foundations for a Compositional Distributional Model of Meaning},
  author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
  year = {2010},
  eprint = {1003.4394},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2019-08-06 08:49:05 +0300},
  date-modified = {2019-08-06 08:50:43 +0300},
  project = {syntactic embedding},
  keywords = {compositionality,distributional models}
}

@inproceedings{coenen.a:2019,
  title = {Visualizing and Measuring the Geometry of {{BERT}}},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Vi{\'e}gas, Fernanda B. and Coenen, Andy and Pearce, Adam and Kim, Been},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {8592--8600},
  url = {https://proceedings.neurips.cc/paper/2019/hash/159c1ffe5b61b41b3c4d8f4c2150f6c4-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ReifYWVCPK19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{cohen.j:1994,
  title = {The Earth Is Round ({{p$<$.05}}).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0003-066x.49.12.997},
  url = {https://doi.org/10.1037%2F0003-066x.49.12.997},
  bdsk-url-2 = {https://doi.org/10.1037/0003-066x.49.12.997},
  date-added = {2021-08-08 11:16:17 -0400},
  date-modified = {2021-08-08 20:26:36 -0400}
}

@phdthesis{collins.m:1999phd,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael John},
  year = {1999},
  address = {{Philadelphia, PA, USA}},
  url = {https://www.proquest.com/docview/304536592/abstract/20DAD4A135504E28PQ/1},
  urldate = {2022-10-16},
  abstract = {Statistical models for parsing natural language have recently shown considerable success in broad-coverage domains. Ambiguity often leads to an input sentence having many possible parse trees; statistical approaches assign a probability to each tree, thereby ranking competing trees in order of plausibility. The probability for each candidate tree is calculated as a product of terms, each term corresponding to some sub-structure within the tree. The choice of parameterization is the choice of how to break down the tree. There are two critical questions regarding the parameterization of the problem: (1) What linguistic objects (e.g., context-free rules, parse moves) should the model's parameters be associated with? I.e., How should trees be decomposed into smaller fragments? (2) How can this choice be instantiated in a sound probabilistic model? This thesis argues that the locality of a lexical head's influence in a tree should motivate modeling choices in the parsing problem. In the final parsing models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The goals of the work are two-fold. First, we aim to advance the state of the art. We report tests on Wall Street Journal text showing that the models give improved accuracy over other methods in the literature. The models recover richer representations than previous approaches, adding the complement/adjunct distinction and information regarding wh-movement. Second, we aim to increase understanding of statistical parsing models. Each parameter type is motivated through tree examples where it provides discriminative information. An empirical study of prepositional phrase attachment ambiguity is used to investigate the effectiveness of dependency parameters for ambiguity resolution. A number of parsing models are tested, and we give a breakdown of their performance on different types of construction. Finally, we give a detailed comparison of the models to others in the literature.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780599258747},
  langid = {english},
  school = {University of Pennsylvania},
  keywords = {Applied sciences,Head-driven,Natural language,Parameterization,Parsing,Statistical models},
  file = {/Users/j/Zotero/storage/PAPH6B6F/Collins (Head-driven statistical models for natural languag.pdf}
}

@article{collins.m:2003,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael},
  year = {2003},
  month = dec,
  journal = {Computational Linguistics},
  volume = {29},
  number = {4},
  pages = {589--637},
  issn = {0891-2017},
  doi = {10.1162/089120103322753356},
  url = {https://doi.org/10.1162/089120103322753356},
  urldate = {2022-10-16},
  abstract = {This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.},
  file = {/Users/j/Zotero/storage/CXY9YP6H/Collins (2003) Head-Driven Statistical Models for Natural Languag.pdf}
}

@inproceedings{collins.m:2004,
  title = {Incremental Parsing with the Perceptron Algorithm},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Collins, Michael and Roark, Brian},
  year = {2004},
  pages = {111--118},
  address = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1218970},
  url = {https://www.aclweb.org/anthology/P04-1015},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218970}
}

@article{collins.m:2013,
  title = {Information Density and Dependency Length as Complementary Cognitive Models},
  author = {Collins, Michael Xavier},
  year = {2013},
  month = sep,
  volume = {43},
  number = {5},
  pages = {651--681},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10936-013-9273-3},
  url = {https://doi.org/10.1007%2Fs10936-013-9273-3},
  bdsk-url-2 = {https://doi.org/10.1007/s10936-013-9273-3},
  date-added = {2021-10-18 22:13:42 -0400},
  date-modified = {2021-10-18 22:13:43 -0400}
}

@inproceedings{conneau.a:2018,
  title = {What You Can Cram into a Single \$\&!\#* Vector: {{Probing}} Sentence Embeddings for Linguistic Properties},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"i}c and Baroni, Marco},
  year = {2018},
  pages = {2126--2136},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1198},
  url = {https://www.aclweb.org/anthology/P18-1198},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1198}
}

@misc{coon.j:2018,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2018},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-02-26 09:53:56 -0500},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@misc{coon.j:2019,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2019},
  url = {https://ling.auf.net/lingbuzz/004224},
  date-added = {2020-06-16 10:17:01 -0400},
  date-modified = {2020-06-16 10:17:01 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@article{coon.j:2020,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {to appear},
  journal = {Linguistic Inquiry},
  doi = {10.1162/ling_a_00386},
  url = {https://doi.org/10.1162/lingâ‚â‚€0386},
  date-added = {2020-02-03 15:20:36 -0500},
  date-modified = {2020-06-16 10:21:03 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects,person case constraint,phi features,quirky case}
}

@article{costa.f:2003,
  title = {Towards Incremental Parsing of Natural Language Using Recursive Neural Networks},
  author = {Costa, F.},
  year = {2003},
  journal = {Applied Intelligence},
  volume = {19},
  number = {1/2},
  pages = {9--25},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1023860521975},
  url = {https://doi.org/10.1023%2Fa%3A1023860521975},
  bdsk-url-2 = {https://doi.org/10.1023/a:1023860521975},
  date-added = {2021-06-22 15:15:28 -0400},
  date-modified = {2021-06-22 15:16:11 -0400},
  file = {/Users/j/Zotero/storage/Q8KJXIDW/Costa (2003) Towards incremental parsing of natural language us.pdf}
}

@book{cover.t:2006,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  edition = {Second},
  publisher = {{Wiley}},
  doi = {10.1002/047174882x},
  url = {https://doi.org/10.1002%2F047174882x},
  bdsk-url-2 = {https://doi.org/10.1002/047174882x},
  date-added = {2022-04-28 11:28:46 -0400},
  date-modified = {2022-04-28 11:29:11 -0400}
}

@inproceedings{cramer.b:2007,
  title = {Limitations of Current Grammar Induction Algorithms},
  booktitle = {Proceedings of the {{ACL}} 2007 Student Research Workshop},
  author = {Cramer, Bart},
  year = {2007},
  pages = {43--48},
  publisher = {{Association for Computational Linguistics}},
  address = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/P07-3008}
}

@article{crameri.f:2020,
  title = {The Misuse of Colour in Science Communication},
  author = {Crameri, Fabio and Shephard, Grace E. and Heron, Philip J.},
  year = {2020},
  month = oct,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5444},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19160-7},
  url = {https://www.nature.com/articles/s41467-020-19160-7},
  urldate = {2022-10-12},
  abstract = {The accurate representation of data is essential in science communication. However, colour maps that visually distort data through uneven colour gradients or are unreadable to those with colour-vision deficiency remain prevalent in science. These include, but are not limited to, rainbow-like and red\textendash green colour maps. Here, we present a simple guide for the scientific use of colour. We show how scientifically derived colour maps report true data variations, reduce complexity, and are accessible for people with colour-vision deficiencies. We highlight ways for the scientific community to identify and prevent the misuse of colour in science, and call for a proactive step away from colour misuse among the community, publishers, and the press.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Scientific community,Software}
}

@article{crocker.m:2000,
  title = {Wide-Coverage Probabilistic Sentence Processing},
  author = {Crocker, Matthew W. and Brants, Thorsten},
  year = {2000},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {29},
  number = {6},
  pages = {647--669},
  issn = {1573-6555},
  doi = {10.1023/A:1026560822390},
  url = {https://doi.org/10.1023/A:1026560822390},
  urldate = {2022-10-13},
  abstract = {This paper describes a fully implemented, broad-coverage model of human syntactic processing. The model uses probabilistic parsing techniques, which combine phrase structure, lexical category, and limited subcategory probabilities with an incremental, left-to-right ``pruning'' mechanism based on cascaded Markov models. The parameters of the system are established through a uniform training algorithm, which determines maximum-likelihood estimates from a parsed corpus. The probabilistic parsing mechanism enables the system to achieve good accuracy on typical, ``garden-variety'' language (i.e., when tested on corpora). Furthermore, the incremental probabilistic ranking of the preferred analyses during parsing also naturally explains observed human behavior for a range of garden-path structures. We do not make strong psychological claims about the specific probabilistic mechanism discussed here, which is limited by a number of practical considerations. Rather, we argue incremental probabilistic parsing models are, in general, extremely well suited to explaining this dual nature\textemdash generally good and occasionally pathological\textemdash of human linguistic performance.},
  langid = {english},
  keywords = {frequency,Markov models,probabilistic parsing},
  file = {/Users/j/Zotero/storage/HDZW33HZ/Crocker and Brants (2000) Wide-Coverage Probabilistic Sentence Processing.pdf}
}

@book{crooks.g:2019,
  title = {Field {{Guide}} to {{Continuous Probability Distributions}}},
  author = {Crooks, Gavin E.},
  year = {2019},
  edition = {1.0.0},
  publisher = {{Berkeley Institute for Theoretical Science}},
  url = {http://threeplusone.com/pubs/fieldguide/},
  urldate = {2022-06-22},
  abstract = {Over 170 continuous univariate probability distributions (and at least as many synonyms) organized into 20 families.},
  isbn = {978-1-73393-810-5},
  langid = {english}
}

@incollection{culicover.p:1999,
  title = {Syntactic Nuts: {{Hard}} Cases in Syntax},
  booktitle = {Syntactic Nuts: {{Hard}} Cases in Syntax.},
  author = {Culicover, Peter},
  year = {1999},
  series = {Foundations of Syntax},
  publisher = {{Oxford University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{culicover.p:2005,
  title = {Simpler Syntax},
  booktitle = {Simpler Syntax},
  author = {Culicover, Peter and Jackendoff, Ray},
  year = {2005},
  publisher = {{Oxford University Press}},
  address = {{Oxford}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:43 -0400}
}

@inproceedings{cusumano-towner.m:2018,
  title = {Incremental Inference for Probabilistic Programs},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {{Cusumano-Towner}, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
  year = {2018},
  series = {{{PLDI}} 2018},
  pages = {571--585},
  publisher = {{Association for Computing Machinery}},
  address = {{New York, NY, USA}},
  doi = {10.1145/3192366.3192399},
  url = {https://doi.org/10.1145/3192366.3192399},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
  date-added = {2022-05-03 21:06:22 -0400},
  date-modified = {2022-05-03 21:07:50 -0400},
  isbn = {978-1-4503-5698-5},
  keywords = {incremental computation,probabilistic programming,sequential Monte Carlo}
}

@inproceedings{dai.z:2019,
  title = {Transformer-{{XL}}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
  year = {2019},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/p19-1285},
  url = {https://doi.org/10.18653%2Fv1%2Fp19-1285},
  bdsk-url-2 = {https://doi.org/10.18653/v1/p19-1285},
  date-added = {2021-11-30 13:59:20 -0500},
  date-modified = {2021-11-30 13:59:33 -0500}
}

@article{danon.g:2006,
  title = {Caseless Nominals and the Projection of {{DP}}},
  author = {Danon, Gabi},
  year = {2006},
  journal = {Natural Language \& Linguistic Theory},
  volume = {24},
  number = {4},
  pages = {977},
  issn = {1573-0859},
  doi = {10.1007/s11049-006-9005-6},
  url = {https://doi.org/10.1007/s11049-006-9005-6},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:43:43 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case,subject positions}
}

@article{danon.g:2011,
  title = {Agreement and {{DP-Internal}} Feature Distribution},
  author = {Danon, Gabi},
  year = {2011},
  journal = {Syntax (Oxford, England)},
  volume = {14},
  number = {4},
  pages = {297--317},
  doi = {10.1111/j.1467-9612.2011.00154.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9612.2011.00154.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2011.00154.x},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:45:11 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,subject positions}
}

@inproceedings{dasgupta.s:2016,
  title = {A Cost Function for Similarity-Based Hierarchical Clustering},
  booktitle = {Proceedings of the 48th Annual {{ACM SIGACT}} Symposium on Theory of Computing, {{STOC}} 2016, Cambridge, {{MA}}, {{USA}}, June 18-21, 2016},
  author = {Dasgupta, Sanjoy},
  editor = {Wichs, Daniel and Mansour, Yishay},
  year = {2016},
  pages = {118--127},
  publisher = {{ACM}},
  doi = {10.1145/2897518.2897527},
  url = {https://doi.org/10.1145/2897518.2897527},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/stoc/Dasgupta16.bib},
  timestamp = {Tue, 06 Nov 2018 00:00:00 +0100}
}

@inproceedings{dathathri.s:2020,
  title = {Plug and Play Language Models: {{A}} Simple Approach to Controlled Text Generation},
  shorttitle = {Plug and Play Language Models},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  year = {2020},
  month = apr,
  url = {https://openreview.net/forum?id=H1edEyBKDS},
  urldate = {2022-07-11},
  abstract = {We control the topic and sentiment of text generation (almost) without any training.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZWAD349P/Dathathri et al. - 2020 - Plug and Play Language Models A Simple Approach t.pdf}
}

@article{dawid.a:2004,
  title = {Probability, Causality and the Empirical World: {{A Bayes}}\textendash de {{Finetti}}\textendash{{Popper}}\textendash{{Borel}} Synthesis},
  author = {Dawid, A. P.},
  year = {2004},
  month = feb,
  journal = {Statistical Science},
  volume = {19},
  number = {1},
  publisher = {{Institute of Mathematical Statistics}},
  doi = {10.1214/088342304000000125},
  url = {https://doi.org/10.1214%2F088342304000000125},
  bdsk-url-2 = {https://doi.org/10.1214/088342304000000125},
  date-added = {2022-04-13 19:51:33 -0400},
  date-modified = {2022-04-13 19:51:34 -0400},
  file = {/Users/j/Zotero/storage/VQYK37KT/Dawid - 2004 - Probability, causality and the empirical world A .pdf}
}

@article{dayan.p:1995,
  title = {The {{Helmholtz}} Machine},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  journal = {Neural Computation},
  volume = {7},
  number = {5},
  pages = {889--904},
  issn = {0899-7667},
  doi = {10.1162/neco.1995.7.5.889},
  url = {https://doi.org/10.1162/neco.1995.7.5.889},
  urldate = {2022-11-30},
  abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
  file = {/Users/j/Zotero/storage/3UGNWUGT/Dayan et al. (1995) The Helmholtz Machine.pdf}
}

@inproceedings{deal.a:2015,
  title = {Interaction and Satisfaction in {{$\varphi$}}-Agreement},
  booktitle = {Proceedings of {{NELS}}},
  author = {Deal, Amy Rose},
  year = {2015},
  volume = {45},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:25 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@article{decaire.r:2017,
  title = {On Optionality in {{Mohawk}} Noun Incorporation},
  author = {DeCaire, Ryan and Johns, Alana and Ku{\v c}erov{\'a}, Ivona},
  year = {2017},
  month = dec,
  journal = {Toronto Working Papers in Linguistics},
  volume = {39},
  issn = {1718-3510},
  url = {https://twpl.library.utoronto.ca/index.php/twpl/article/view/28604},
  urldate = {2022-05-30},
  abstract = {Noun incorporation is a phenomenon much discussed within Iroquoian language literature. In this paper, we consider noun incorporation in Mohawk, a language within the Iroquoian language family, and argue that what has often been considered to be optional noun incorporation is in fact primarily determined by the information structure of the clause. We show that with the exception of lexically-determined verbs that always or never incorporate, every verb may or may not incorporate its nominal object. We analyse the incorporated version as the default structure. The non-incorporated counterpart is licensed only under particular information-structure properties. We provide evidence that noun excorporation arises whenever the verb or the object noun is focused, and in turn moves to the left periphery.},
  copyright = {Copyright (c) 2017 Ryan DeCaire, Alana Johns, Ivona Ku\v{c}erov\'a},
  langid = {english},
  keywords = {excorporation,iroquoian,Mohawk,noun incorporation},
  file = {/Users/j/Zotero/storage/VYJWIBAA/DeCaire et al. - 2017 - On optionality in Mohawk noun incorporation.pdf}
}

@article{del-moral.p:2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  url = {https://doi.org/10.1111%2Fj.1467-9868.2006.00553.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
  date-added = {2022-04-30 17:47:24 -0400},
  date-modified = {2022-04-30 22:51:54 -0400},
  keywords = {particle filtering,sampling,sequential monte carlo},
  file = {/Users/j/Zotero/storage/IVS4EKHZ/Del Moral et al. - 2006 - Sequential monte carlo samplers.pdf;/Users/j/Zotero/storage/ZCZ3LNH2/Del Moral et al. - 2006 - Sequential monte carlo samplers.pdf}
}

@misc{deletang.g:2022,
  title = {Neural Networks and the {{Chomsky}} Hierarchy},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and {Grau-Moya}, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  year = {2022},
  month = oct,
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2207.02098},
  urldate = {2022-10-11},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (10250 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {/Users/j/Zotero/storage/NN7TDKA4/DelÃ©tang et al. (2022) Neural Networks and the Chomsky Hierarchy.pdf}
}

@book{delmoral.p:2004,
  title = {Feynman-{{Kac Formulae}}},
  author = {Del Moral, Pierre},
  editor = {Gani, J. and Heyde, C. C. and Kurtz, T. G.},
  year = {2004},
  series = {Probability and Its {{Applications}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4684-9393-1},
  url = {http://link.springer.com/10.1007/978-1-4684-9393-1},
  urldate = {2023-04-22},
  isbn = {978-1-4419-1902-1 978-1-4684-9393-1},
  keywords = {algorithms,Feynman-Kac formula,filtering problem,genetic algorithms,interacting particle system,Markov chain,Markov kernel,Markov process,Monte Carlo method,Statistical Physics},
  file = {/Users/j/Zotero/storage/FFCC4GL4/Del Moral (2004) Feynman-Kac Formulae.pdf}
}

@inproceedings{demarneffe.m:2006stanforddep,
  title = {Generating Typed Dependency Parses from Phrase Structure Parses},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({{LREC}}'06)},
  author = {{de Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
  year = {2006},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Genoa, Italy}},
  url = {http://www.lrec-conf.org/proceedings/lrec2006/pdf/440â‚šdf.pdf}
}

@inproceedings{demarneffe.m:2008,
  title = {The {{Stanford}} Typed Dependencies Representation},
  booktitle = {Coling 2008: {{Proceedings}} of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher D.},
  year = {2008},
  pages = {1--8},
  publisher = {{Coling 2008 Organizing Committee}},
  address = {{Manchester, UK}},
  url = {https://www.aclweb.org/anthology/W08-1301}
}

@manual{demarneffe.m:2008sdmanual,
  type = {Manual},
  title = {Stanford Typed Dependencies Manual},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher},
  year = {2008},
  url = {https://nlp.stanford.edu/software/dependenciesâ‚˜anual.pdf},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-03-12 10:47:01 -0500},
  organization = {{Stanford NLP}},
  project = {syntactic embedding},
  version = {Stanford Parser v.3.7.0},
  keywords = {dependency structures,stanford dependencies}
}

@article{demberg.v:2008,
  title = {Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity},
  author = {Demberg, Vera and Keller, Frank},
  year = {2008},
  journal = {Cognition},
  volume = {109},
  number = {2},
  pages = {193--210},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.07.008},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027708001741},
  abstract = {We evaluate the predictions of two theories of syntactic processing complexity, dependency locality theory (DLT) and surprisal, against the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text. Our results show that DLT integration cost is not a significant predictor of reading times for arbitrary words in the corpus. However, DLT successfully predicts reading times for nouns. We also find evidence for integration cost effects at auxiliaries, not predicted by DLT. For surprisal, we demonstrate that an unlexicalized formulation of surprisal can predict reading times for arbitrary words in the corpus. Comparing DLT integration cost and surprisal, we find that the two measures are uncorrelated, which suggests that a complete theory will need to incorporate both aspects of processing complexity. We conclude that eye-tracking corpora, which provide reading time data for naturally occurring, contextualized sentences, can complement experimental evidence as a basis for theories of processing complexity.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2008.07.008},
  keywords = {Corpus data,Dependency locality theory,Eye-tracking,Processing complexity,Surprisal},
  file = {/Users/j/Zotero/storage/RD6WXH4Z/Demberg and Keller - 2008 - Data from eye-tracking corpora as evidence for the.pdf}
}

@incollection{demberg.v:2019,
  title = {Cognitive Models of Syntax and Sentence Processing},
  booktitle = {Human {{Language}}: {{From Genes}} and {{Brains}} to {{Behavior}}},
  author = {Demberg, Vera and Keller, Frank},
  editor = {Hagoort, Peter},
  year = {2019},
  month = oct,
  pages = {0},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/10841.003.0027},
  url = {https://doi.org/10.7551/mitpress/10841.003.0027},
  urldate = {2022-10-22},
  isbn = {978-0-262-35386-1},
  file = {/Users/j/Zotero/storage/GT4GJU5N/Demberg and Keller (2019) Cognitive Models of Syntax and Sentence Processing.pdf}
}

@inproceedings{devlin.j:2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1423},
  url = {https://www.aclweb.org/anthology/N19-1423},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423},
  file = {/Users/j/Zotero/storage/FZG9EPZ3/Devlin et al. - 2019 - BERT Pre-training of deep bidirectional transform.pdf}
}

@article{diaconis.p:2008,
  title = {The {{Markov}} Chain {{Monte Carlo}} Revolution},
  author = {Diaconis, Persi},
  year = {2008},
  month = nov,
  journal = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {179--205},
  publisher = {{American Mathematical Society (AMS)}},
  doi = {10.1090/s0273-0979-08-01238-x},
  url = {https://doi.org/10.1090%2Fs0273-0979-08-01238-x},
  bdsk-url-2 = {https://doi.org/10.1090/s0273-0979-08-01238-x},
  date-added = {2022-03-17 13:44:06 -0400},
  date-modified = {2022-03-17 13:44:07 -0400},
  file = {/Users/j/Zotero/storage/VMFKC8TS/Diaconis - 2008 - The markov chain monte carlo revolution.pdf}
}

@inproceedings{dieng.a:2017,
  title = {Variational Inference via {$\chi$} Upper Bound Minimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  year = {2017},
  volume = {30},
  publisher = {{Curran Associates, Inc.}},
  url = {https://papers.nips.cc/paper/2017/hash/35464c848f410e55a13bb9d78e7fddd0-Abstract.html},
  urldate = {2023-01-01},
  keywords = {chi-squared divergence,CHIVI,variational inference},
  file = {/Users/j/Zotero/storage/SBG6IH3T/Dieng et al. (2017) Variational Inference via textbackslash chi Upper.pdf}
}

@article{dotlacil.j:2021,
  title = {Parsing as a Cue-Based Retrieval Model},
  author = {Dotla{\v c}il, Jakub},
  year = {2021},
  journal = {Cognitive science},
  volume = {45},
  number = {8},
  pages = {e13020},
  issn = {1551-6709},
  doi = {10.1111/cogs.13020},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13020},
  urldate = {2022-07-01},
  abstract = {This paper develops a novel psycholinguistic parser and tests it against experimental and corpus reading data. The parser builds on the recent research into memory structures, which argues that memory retrieval is content-addressable and cue-based. It is shown that the theory of cue-based memory systems can be combined with transition-based parsing to produce a parser that, when combined with the cognitive architecture ACT-R, can model reading and predict online behavioral measures (reading times and regressions). The parser's modeling capacities are tested against self-paced reading experimental data (Grodner \& Gibson, 2005), eye-tracking experimental data (Staub, 2011), and a self-paced reading corpus (Futrell et al., 2018).},
  langid = {english},
  keywords = {ACT-R,Computational psycholinguistics,Cue-based retrieval,Memory retrieval,Modeling reading data,Processing},
  file = {/Users/j/Zotero/storage/CE7V9VPS/DotlaÄil - 2021 - Parsing as a Cue-Based Retrieval Model.pdf}
}

@article{douc.r:2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  author = {Douc, Randal and Capp{\'e}, Olivier and Moulines, Eric},
  year = {2005},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.CS/0507025},
  url = {https://arxiv.org/abs/cs/0507025},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.CS/0507025},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  date-added = {2022-03-29 20:05:42 -0400},
  date-modified = {2022-03-29 20:05:43 -0400},
  keywords = {and Science (cs.CE),Computational Engineering,Finance,FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/ECTS7DHP/Douc et al. - 2005 - Comparison of resampling schemes for particle filt.pdf}
}

@incollection{doucet.a:2001,
  title = {An Introduction to Sequential {{Monte Carlo}} Methods},
  booktitle = {Sequential Monte Carlo Methods in Practice},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  pages = {3--14},
  publisher = {{Springer New York}},
  doi = {10.1007/978-1-4757-3437-9_1},
  date-added = {2021-03-22 19:46:53 -0400},
  date-modified = {2021-03-22 19:46:54 -0400},
  file = {/Users/j/Zotero/storage/597H86JN/Doucet et al. - 2001 - An introduction to sequential monte carlo methods.pdf}
}

@book{doucet.a:2001smcbook,
  title = {Sequential {{Monte Carlo}} Methods in Practice},
  editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  series = {Statistics for Engineering and Information Science},
  publisher = {{Springer}},
  address = {{New York}},
  doi = {10.1007/978-1-4757-3437-9},
  url = {https://doi.org/10.1007%2F978-1-4757-3437-9},
  bdsk-url-2 = {https://doi.org/10.1007/978-1-4757-3437-9},
  date-added = {2022-03-25 22:05:58 -0400},
  date-modified = {2022-03-25 22:10:04 -0400},
  file = {/Users/j/Zotero/storage/E66PEYMC/Doucet et al. - 2001 - Sequential monte carlo methods in practice.pdf}
}

@incollection{doucet.a:2011,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {The {{Oxford Handbook}} of {{Nonlinear Filtering}}},
  author = {Doucet, Arnaud and Johansen, Adam M.},
  editor = {Crisan, Dan and Rozovski{\u \i}, Boris},
  year = {2011},
  month = apr,
  series = {Oxford {{Handbooks}}},
  pages = {656--704},
  publisher = {{Oxford University Press}},
  url = {http://www.stats.ox.ac.uk/~doucet/doucet_johansen_tutorialPF2011.pdf},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision,ã€€econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
  annotation = {Note: Version 1.1 \textendash{} December 2008 with typographical corrections March 2012},
  file = {/Users/j/Zotero/storage/AQ62L6MY/Doucet and Johansen - 2011 - A tutorial on particle filtering and smoothing fi.pdf;/Users/j/Zotero/storage/U3NCLMN4/doucet.a.2011scan.pdf}
}

@inproceedings{dozat.t:2016,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Dozat, Timothy and Manning, Christopher D.},
  year = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=Hk95PK9le},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@incollection{drieghe.d:2011,
  title = {Parafoveal-on-Foveal Effects on Eye Movements during Reading},
  booktitle = {The Oxford Handbook of Eye Movements},
  author = {Drieghe, Denis},
  editor = {Liversedge, Simon P. and Gilchrist, Iain and Everling, Stefan},
  year = {2011},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780199539789.013.0046},
  url = {https://doi.org/10.1093%2Foxfordhb%2F9780199539789.013.0046},
  bdsk-url-2 = {https://doi.org/10.1093/oxfordhb/9780199539789.013.0046},
  date-added = {2022-04-21 11:02:00 -0400},
  date-modified = {2022-04-21 11:03:00 -0400}
}

@inproceedings{du.w:2020,
  title = {Exploiting Syntactic Structure for Better Language Modeling: {{A}} Syntactic Distance Approach},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Du, Wenyu and Lin, Zhouhan and Shen, Yikang and O'Donnell, Timothy J. and Bengio, Yoshua and Zhang, Yue},
  year = {2020},
  pages = {6611--6628},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.591},
  url = {https://www.aclweb.org/anthology/2020.acl-main.591},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.591},
  date-modified = {2022-05-17 08:09:30 -0400}
}

@article{dupoux.e:2018,
  title = {Cognitive Science in the Era of Artificial Intelligence: A Roadmap for Reverse-Engineering the Infant Language-Learner},
  author = {Dupoux, Emmanuel},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {43--59},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.cognition.2017.11.008},
  url = {https://doi.org/10.1016%2Fj.cognition.2017.11.008},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2017.11.008},
  date-added = {2021-10-04 22:09:38 -0400},
  date-modified = {2021-10-04 22:09:56 -0400}
}

@inproceedings{dyer.c:2016,
  title = {Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
  year = {2016},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/n16-1024},
  url = {https://doi.org/10.18653%2Fv1%2Fn16-1024},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n16-1024},
  date-added = {2021-09-13 21:32:13 -0400},
  date-modified = {2021-09-13 21:32:16 -0400}
}

@article{earley.j:1970,
  title = {An Efficient Context-Free Parsing Algorithm},
  author = {Earley, Jay},
  year = {1970},
  month = feb,
  journal = {Communications of the ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  url = {https://doi.org/10.1145/362007.362035},
  urldate = {2022-06-13},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  file = {/Users/j/Zotero/storage/3W34H2B6/Earley - 1970 - An efficient context-free parsing algorithm.pdf}
}

@article{ebbinghaus.h:1885,
  title = {Memory: {{A Contribution}} to {{Experimental Psychology}}},
  shorttitle = {Memory},
  author = {Ebbinghaus, Hermann},
  translator = {Ruger, Henry A. and Bussenius, Clara E.},
  year = {2013},
  journal = {Annals of Neurosciences},
  series = {Annals {{Classics}}},
  volume = {20},
  number = {4},
  pages = {155--156},
  issn = {09727531, 09763260},
  doi = {10.5214/ans.0972.7531.200408},
  url = {http://annalsofneurosciences.org/journal/index.php/annal/article/view/540},
  urldate = {2022-06-13},
  abstract = {The language of life as well as of science in attributing a memory to the mind attempts to point out the facts and their interpretation somewhat as follows: Mental states of every kind, -- sensations, feelings, ideas, -- which were at one time present in consciousness and then have disappeared from it, have not with their disappearance absolutely ceased to exist. Although the inwardly-turned look may no longer be able to find them, nevertheless they have not been utterly destroyed and annulled, but in a certain manner they continue to exist, stored up, so to speak, in the memory. We cannot, of course, directly observe their present existence, but it is revealed by the effects which come to our knowledge with a certainty like that with which we infer the existence of the stars below the horizon. These effects are of different kinds.},
  langid = {english},
  file = {/Users/j/Zotero/storage/5EM4LARF/Ebbinghaus - 2013 - Memory A Contribution to Experimental Psychology.pdf}
}

@article{eberhard.k:1995,
  title = {Eye Movements as a Window into Real-Time Spoken Language Comprehension in Natural Contexts},
  author = {Eberhard, Kathleen M. and {Spivey-Knowlton}, Michael J. and Sedivy, Julie C. and Tanenhaus, Michael K.},
  year = {1995},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {24},
  number = {6},
  pages = {409--436},
  issn = {1573-6555},
  doi = {10.1007/BF02143160},
  url = {https://doi.org/10.1007/BF02143160},
  urldate = {2022-10-13},
  abstract = {When listeners follow spoken instructions to manipulate real objects, their eye movements to the objects are closely time locked to the referring words. We review five experiments showing that this time-locked characteristic of eye movements provides a detailed profile of the processes that underlie real-time spoken language comprehension. Together, the first four experiments showed that listerners immediately integrated lexical, sublexical, and prosodic information in the spoken input with information from the visual context to reduce the set of referents to the intended one. The fifth experiment demonstrated that a visual referential context affected the initial structuring of the linguistic input, eliminating even strong syntactic preferences that result in clear garden paths when the referential context is introduced linguistically. We argue that context affected the earliest moments of language processing because it was highly accessible and relevant to the behavioral goals of the listener.},
  langid = {english},
  keywords = {Cognitive Psychology,Initial Structure,Language Comprehension,Language Processing,Real Object},
  file = {/Users/j/Zotero/storage/TZ2DTPRA/Eberhard et al. (1995) Eye movements as a window into real-time spoken la.pdf}
}

@article{efraimidis.p:2006,
  title = {Weighted Random Sampling with a Reservoir},
  author = {Efraimidis, Pavlos S. and Spirakis, Paul G.},
  year = {2006},
  month = mar,
  journal = {Information Processing Letters},
  volume = {97},
  number = {5},
  pages = {181--185},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2005.11.003},
  url = {https://www.sciencedirect.com/science/article/pii/S002001900500298X},
  urldate = {2022-11-06},
  abstract = {In this work, a new algorithm for drawing a weighted random sample of size m from a population of n weighted items, where m{$\leqslant$}n, is presented. The algorithm can generate a weighted random sample in one-pass over unknown populations.},
  langid = {english},
  keywords = {Data streams,Parallel algorithms,Randomized algorithms,Reservoir sampling,Weighted random sampling}
}

@article{ehrlich.s:1981,
  title = {Contextual Effects on Word Perception and Eye Movements during Reading},
  author = {Ehrlich, Susan F. and Rayner, Keith},
  year = {1981},
  journal = {Journal of Memory and Language},
  volume = {20},
  number = {6},
  pages = {641},
  publisher = {{Academic Press}},
  doi = {10.1016/S0022-5371(81)90220-6},
  url = {https://doi.org/10.1016/S0022-5371(81)90220-6},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@inproceedings{eisner.j:1996,
  title = {Three New Probabilistic Models for Dependency Parsing: An Exploration},
  booktitle = {{{COLING}} 1996 Volume 1: {{The}} 16th International Conference on Computational Linguistics},
  author = {Eisner, Jason M.},
  year = {1996},
  url = {https://www.aclweb.org/anthology/C96-1058}
}

@techreport{eisner.j:1997,
  type = {Technical Report},
  title = {An Empirical Comparison of Probability Models for Dependency Grammar},
  author = {Eisner, Jason},
  year = {1997},
  number = {IRCS-96-11},
  eprint = {cmp-lg/9706004},
  institution = {{Institute for Research in Cognitive Science, University of Pennsylvania}},
  url = {https://arxiv.org/pdf/cmp-lg/9706004.pdf},
  archiveprefix = {arxiv},
  date-added = {2020-02-24 16:29:57 -0500},
  date-modified = {2021-09-09 23:03:05 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm,projective dependencies,technical report},
  file = {/Users/j/Zotero/storage/WR55UZ3E/Eisner - 1997 - An empirical comparison of probability models for .pdf}
}

@inproceedings{eisner.j:2016,
  title = {Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)},
  booktitle = {Proceedings of the {{Workshop}} on {{Structured Prediction}} for {{NLP}}},
  author = {Eisner, Jason},
  year = {2016},
  month = nov,
  pages = {1--17},
  publisher = {{Association for Computational Linguistics}},
  address = {{Austin, TX}},
  doi = {10.18653/v1/W16-5901},
  url = {https://aclanthology.org/W16-5901},
  urldate = {2022-07-05},
  file = {/Users/j/Zotero/storage/B55HLLIY/Eisner - 2016 - Inside-Outside and Forward-Backward Algorithms Are.pdf}
}

@article{elvira.v:2017,
  title = {Adapting the Number of Particles in Sequential {{Monte Carlo}} Methods through an Online Scheme for Convergence Assessment},
  author = {Elvira, V{\'i}ctor and M{\'i}guez, Joaqu{\'i}n and Djuri{\'c}, Petar M.},
  year = {2017},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {7},
  pages = {1781--1794},
  issn = {1941-0476},
  doi = {10.1109/TSP.2016.2637324},
  abstract = {Particle filters are broadly used to approximate posterior distributions of hidden states in state-space models by means of sets of weighted particles. While the convergence of the filter is guaranteed when the number of particles tends to infinity, the quality of the approximation is usually unknown but strongly dependent on the number of particles. In this paper, we propose a novel method for assessing the convergence of particle filters in an online manner, as well as a simple scheme for the online adaptation of the number of particles based on the convergence assessment. The method is based on a sequential comparison between the actual observations and their predictive probability distributions approximated by the filter. We provide a rigorous theoretical analysis of the proposed methodology and, as an example of its practical use, we present simulations of a simple algorithm for the dynamic and online adaptation of the number of particles during the operation of a particle filter on a stochastic version of the Lorenz 63 system.},
  keywords = {Adaptation models,adaptive complexity,computational complexity,Computational modeling,Convergence,convergence analysis,convergence assessment,Heuristic algorithms,Monte Carlo methods,Particle filtering,predictive distribution,Probability density function,sequential Monte Carlo,Signal processing algorithms},
  file = {/Users/j/Zotero/storage/5RGHRRU8/Elvira et al. (2017) Adapting the Number of Particles in Sequential Mon.pdf}
}

@article{elvira.v:2022,
  title = {Rethinking the Effective Sample Size},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2022},
  journal = {International Statistical Review},
  volume = {90},
  number = {3},
  pages = {525--550},
  issn = {1751-5823},
  doi = {10.1111/insr.12500},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/insr.12500},
  urldate = {2022-12-08},
  abstract = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit the approximation of the ESS in the specific context of importance sampling. The derivation of this approximation, that we will denote as ESS\^, is partially available in a 1992 foundational technical report of Augustine Kong. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of ESS\^ make it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS\^ in the multiple importance sampling setting, we display numerical examples and we discuss several avenues for developing alternative metrics. This paper does not cover the use of ESS for Markov chain Monte Carlo algorithms.},
  langid = {english},
  keywords = {Bayesian inference,effective sample size,importance sampling,Monte Carlo methods},
  file = {/Users/j/Zotero/storage/45X7C3NE/Elvira et al. (2022) Rethinking the Effective Sample Size.pdf}
}

@article{engbert.r:2005,
  title = {{{SWIFT}}: {{A}} Dynamical Model of Saccade Generation during Reading.},
  author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
  year = {2005},
  journal = {Psychological Review},
  volume = {112},
  number = {4},
  pages = {777--813},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0033-295x.112.4.777},
  url = {https://doi.org/10.1037%2F0033-295x.112.4.777},
  bdsk-url-2 = {https://doi.org/10.1037/0033-295x.112.4.777},
  date-added = {2021-06-02 14:39:17 -0400},
  date-modified = {2021-06-02 14:39:49 -0400},
  keywords = {eye-tracking,frequency effects,predictability}
}

@article{engelmann.f:2013,
  title = {A Framework for Modeling the Interaction of Syntactic Processing and Eye Movement Control},
  author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {452--474},
  issn = {17568757},
  doi = {10.1111/tops.12026},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12026},
  urldate = {2022-08-28},
  langid = {english},
  file = {/Users/j/Zotero/storage/WN3VF4TX/Engelmann et al. - 2013 - A Framework for Modeling the Interaction of Syntac.pdf}
}

@phdthesis{engelmann.f:2016PhD,
  title = {Toward an Integrated Model of Sentence Processing in Reading},
  author = {Engelmann, Felix},
  year = {2016},
  address = {{Potsdam, Germany}},
  url = {https://publishup.uni-potsdam.de/frontdoor/index/index/docId/10086},
  urldate = {2022-10-12},
  abstract = {In experiments investigating sentence processing, eye movement measures such as fixation durations and regression proportions while reading are commonly used to draw conclusions about processing difficulties. However, these measures are the result of an interaction of multiple cognitive levels and processing strategies and thus are only indirect indicators of processing difficulty. In order to properly interpret an eye movement response, one has to understand the underlying principles of adaptive processing such as trade-off mechanisms between reading speed and depth of comprehension that interact with task demands and individual differences. Therefore, it is necessary to establish explicit models of the respective mechanisms as well as their causal relationship with observable behavior. There are models of lexical processing and eye movement control on the one side and models on sentence parsing and memory processes on the other. However, no model so far combines both sides with explicitly defined linking assumptions. In this thesis, a model is developed that integrates oculomotor control with a parsing mechanism and a theory of cue-based memory retrieval. On the basis of previous empirical findings and independently motivated principles, adaptive, resource-preserving mechanisms of underspecification are proposed both on the level of memory access and on the level of syntactic parsing. The thesis first investigates the model of cue-based retrieval in sentence comprehension of Lewis \& Vasishth (2005) with a comprehensive literature review and computational modeling of retrieval interference in dependency processing. The results reveal a great variability in the data that is not explained by the theory. Therefore, two principles, 'distractor prominence' and 'cue confusion', are proposed as an extension to the theory, thus providing a more adequate description of systematic variance in empirical results as a consequence of experimental design, linguistic environment, and individual differences. In the remainder of the thesis, four interfaces between parsing and eye movement control are defined: Time Out, Reanalysis, Underspecification, and Subvocalization. By comparing computationally derived predictions with experimental results from the literature, it is investigated to what extent these four interfaces constitute an appropriate elementary set of assumptions for explaining specific eye movement patterns during sentence processing. Through simulations, it is shown how this system of in itself simple assumptions results in predictions of complex, adaptive behavior.  In conclusion, it is argued that, on all levels, the sentence comprehension mechanism seeks a balance between necessary processing effort and reading speed on the basis of experience, task demands, and resource limitations. Theories of linguistic processing therefore need to be explicitly defined and implemented, in particular with respect to linking assumptions between observable behavior and underlying cognitive processes. The comprehensive model developed here integrates multiple levels of sentence processing that hitherto have only been studied in isolation. The model is made publicly available as an expandable framework for future studies of the interactions between parsing, memory access, and eye movement control.},
  copyright = {https://rightsstatements.org/vocab/InC/1.0/},
  langid = {english},
  school = {Universit\"at Potsdam},
  keywords = {ACT-R},
  file = {/Users/j/Zotero/storage/QR59CZFZ/Engelmann (2016) Toward an integrated model of sentence processing .pdf}
}

@article{engelmann.f:2019,
  title = {The Effect of Prominence and Cue Association on Retrieval Processes: A Computational Account},
  shorttitle = {The Effect of Prominence and Cue Association on Retrieval Processes},
  author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {12},
  pages = {e12800},
  issn = {1551-6709},
  doi = {10.1111/cogs.12800},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12800},
  urldate = {2022-10-12},
  abstract = {We present a comprehensive empirical evaluation of the ACT-R\textendash based model of sentence processing developed by Lewis and Vasishth (2005) (LV05). The predictions of the model are compared with the results of a recent meta-analysis of published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject\textendash verb dependencies (J\"ager, Engelmann, \& Vasishth, 2017). The comparison shows that the model has only partial success in explaining the data; and we propose that its prediction space is restricted by oversimplifying assumptions. We then implement a revised model that takes into account differences between individual experimental designs in terms of the prominence of the target and the distractor in memory- and context-dependent cue-feature associations. The predictions of the original and the revised model are quantitatively compared with the results of the meta-analysis. Our simulations show that, compared to the original LV05 model, the revised model accounts for the data better. The results suggest that effects of prominence and variable cue-feature associations need to be considered in the interpretation of existing empirical results and in the design and planning of future experiments. With regard to retrieval interference in sentence processing and to the broader field of psycholinguistic studies, we conclude that well-specified models in tandem with high-powered experiments are needed in order to uncover the underlying cognitive processes.},
  langid = {english},
  keywords = {ACT-R,Computational modeling,Cue-based retrieval,Dependency completion,Retrieval interference,Sentence processing},
  file = {/Users/j/Zotero/storage/P3AY5IXV/Engelmann et al. (2019) The Effect of Prominence and Cue Association on Re.pdf}
}

@article{ennever.t:2017,
  title = {A Replicable Acoustic Measure of Lenition and the Nature of Variability in {{Gurindji}} Stops},
  author = {Ennever, Thomas and Meakins, Felicity and Round, Erich R.},
  year = {2017},
  month = aug,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  number = {1},
  publisher = {{Open Library of the Humanities}},
  doi = {10.5334/labphon.18},
  url = {https://doi.org/10.5334%2Flabphon.18},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.18},
  date-added = {2022-05-10 10:59:44 -0400},
  date-modified = {2022-05-10 10:59:54 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/M7JP2B7J/Ennever et al. - 2017 - A replicable acoustic measure of lenition and the .pdf}
}

@misc{erion.g:2019,
  title = {Learning Explainable Models Using Attribution Priors},
  author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  year = {2019},
  eprint = {1906.10670},
  primaryclass = {cs.LG},
  archiveprefix = {arxiv},
  date-added = {2019-07-05 11:04:57 -0400},
  date-modified = {2019-07-05 11:06:04 -0400},
  project = {syntactic embedding},
  keywords = {gradient attribution priors}
}

@article{estes.w:1959,
  title = {Foundations of Linear Models},
  author = {Estes, William K and Suppes, Patrick},
  year = {1959},
  journal = {Studies in mathematical learning theory},
  pages = {137--179},
  publisher = {{Stanford University Press Stanford}},
  date-added = {2021-05-31 13:41:13 -0400},
  date-modified = {2021-05-31 13:41:26 -0400},
  keywords = {probability matching}
}

@book{fano.r:1961,
  title = {Transmission of Information: A Statistical Theory of Communications},
  author = {Fano, Robert M},
  year = {1961},
  edition = {First},
  publisher = {{MIT Press}},
  address = {{Cambdridge, Mass}},
  url = {https://b-ok.cc/book/5577269/50d40b},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-07 09:14:32 -0400}
}

@incollection{farmer.t:2012,
  title = {Individual Differences in Sentence Processing},
  booktitle = {The {{Cambridge Handbook}} of {{Psycholinguistics}}},
  author = {Farmer, Thomas A. and Misyak, Jennifer B. and Christiansen, Morten H.},
  editor = {McRae, Ken and Joanisse, Marc and Spivey, Michael},
  year = {2012},
  series = {Cambridge {{Handbooks}} in {{Psychology}}},
  pages = {353--364},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139029377.018},
  url = {https://www.cambridge.org/core/books/cambridge-handbook-of-psycholinguistics/individual-differences-in-sentence-processing/460369A02A4D236E53C2F920F8EC43A0},
  urldate = {2022-10-13},
  isbn = {978-0-521-86064-2},
  file = {/Users/j/Zotero/storage/8BA6RBE6/Farmer et al. (2012) Individual Differences in Sentence Processing.pdf}
}

@article{fasiolo.m:2020,
  title = {Fast Calibrated Additive Quantile Regression},
  author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Rapha{\"e}l and Goude, Yannig},
  year = {2020},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {535},
  pages = {1402--1412},
  publisher = {{Informa UK Limited}},
  issn = {1537-274X},
  doi = {10.1080/01621459.2020.1725521},
  url = {http://dx.doi.org/10.1080/01621459.2020.1725521},
  date-added = {2022-03-10 22:30:30 -0500},
  date-modified = {2022-03-10 22:30:33 -0500}
}

@misc{feder.a:2021,
  title = {Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and {Wood-Doughty}, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2109.00725},
  url = {https://arxiv.org/abs/2109.00725},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2109.00725},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-10 11:31:28 -0400},
  date-modified = {2022-05-10 11:31:44 -0400},
  keywords = {causality,machine learning,natural language processing},
  file = {/Users/j/Zotero/storage/CKBHHB2T/Feder et al. - 2021 - Causal inference in natural language processing E.pdf}
}

@article{fedorenko.e:2004,
  title = {Verbal Working Memory in Sentence Comprehension},
  author = {Fedorenko, Evelina and Gibson, Edward and Rohde, Douglas},
  year = {2004},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {26},
  number = {26},
  url = {https://escholarship.org/uc/item/02z4h125},
  urldate = {2022-10-26},
  abstract = {Author(s): Fedorenko, Evelina; Gibson, Edward; Rohde, Douglas},
  langid = {english},
  keywords = {uniform information density},
  file = {/Users/j/Zotero/storage/K49GNQGR/Fedorenko et al. (2004) Verbal Working Memory in Sentence Comprehension.pdf}
}

@inproceedings{feller.w:1949,
  title = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Proceedings of the [{{First}}] Berkeley Symposium on Mathematical Statistics and Probability},
  author = {Feller, William},
  year = {1949},
  pages = {403--432},
  url = {https://digitalassets.lib.berkeley.edu/math/ucb/text/mathâ‚›1â‚rticle-21.pdf},
  date-added = {2022-04-30 11:52:57 -0400},
  date-modified = {2022-04-30 11:55:11 -0400},
  organization = {{University of California Press}},
  keywords = {diffusion processes}
}

@article{fenk.a:1980,
  title = {Konstanz Im Kurzzeitged\"achtnis - Konstanz Im Sprachlichen Informationsflu\ss?},
  author = {Fenk, August and {Fenk-Oczlon}, Gertraud},
  year = {1980},
  month = jan,
  journal = {Zeitschrift f\"ur experimentelle und angewandte Psychologie},
  volume = {27},
  number = {3},
  pages = {400--414},
  date-added = {2021-10-26 14:23:04 -0400},
  date-modified = {2021-10-27 09:20:37 -0400}
}

@inproceedings{fernandez-monsalve.i:2012,
  title = {Lexical Surprisal as a General Predictor of Reading Time},
  booktitle = {Proceedings of the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fernandez Monsalve, Irene and Frank, Stefan L. and Vigliocco, Gabriella},
  year = {2012},
  month = apr,
  pages = {398--408},
  publisher = {{Association for Computational Linguistics}},
  address = {{Avignon, France}},
  url = {https://aclanthology.org/E12-1041},
  date-added = {2021-11-29 10:29:43 -0500},
  date-modified = {2021-11-29 10:29:44 -0500}
}

@article{ferreira.f:2001,
  title = {Misinterpretations of {{Garden-Path Sentences}}: {{Implications}} for {{Models}} of {{Sentence Processing}} and {{Reanalysis}}},
  shorttitle = {Misinterpretations of {{Garden-Path Sentences}}},
  author = {Ferreira, Fernanda and Christianson, Kiel and Hollingworth, Andrew},
  year = {2001},
  month = jan,
  journal = {Journal of Psycholinguistic Research},
  volume = {30},
  number = {1},
  pages = {3--20},
  issn = {1573-6555},
  doi = {10.1023/A:1005290706460},
  url = {https://doi.org/10.1023/A:1005290706460},
  urldate = {2022-06-11},
  abstract = {Theories of sentence comprehension have addressed both initial parsing processes and mechanisms responsible for reanalysis. Three experiments are summarized that were designed to investigate the reanalysis and interpretation of relatively difficult garden-path sentences (e.g., While Anna dressed the baby spit up on the bed). After reading such sentences, participants correctly believed that the baby spit up on the bed; however, they often confidently, yet incorrectly, believed that Anna dressed the baby. These results demonstrate that garden-path reanalysis is not an all-or-nothing process and that thematic roles initially assigned for the subordinate clause verb are not consistently revised. The implications of the partial reanalysis phenomenon for Fodor and Inoue's (1998) model of reanalysis and sentence processing are discussed. In addition, we discuss the possibility that language processing often creates ``good enough'' structures rather than ideal structures.},
  langid = {english},
  keywords = {parsing,reanalysis,semantics,syntactic ambiguity},
  file = {/Users/j/Zotero/storage/AQCBQ96B/Ferreira et al. - 2001 - Misinterpretations of Garden-Path Sentences Impli.pdf}
}

@article{ferreira.f:2002,
  title = {Good-Enough Representations in Language Comprehension},
  author = {Ferreira, Fernanda and Bailey, Karl G.D. and Ferraro, Vittoria},
  year = {2002},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {11},
  number = {1},
  pages = {11--15},
  publisher = {{SAGE Publications Inc}},
  issn = {0963-7214},
  doi = {10.1111/1467-8721.00158},
  url = {https://doi.org/10.1111/1467-8721.00158},
  urldate = {2022-06-11},
  abstract = {People comprehend utterances rapidly and without conscious effort. Traditional theories assume that sentence processing is algorithmic and that meaning is derived compositionally. The language processor is believed to generate representations of the linguistic input that are complete, detailed, and accurate. However, recent findings challenge these assumptions. Investigations of the misinterpretation of both garden-path and passive sentences have yielded support for the idea that the meaning people obtain for a sentence is often not a reflection of its true content. Moreover, incorrect interpretations may persist even after syntactic reanalysis has taken place. Our good-enough approach to language comprehension holds that language processing is sometimes only partial and that semantic representations are often incomplete. Future work will elucidate the conditions under which sentence processing is simply good enough.},
  langid = {english},
  keywords = {language comprehension,linguistic ambiguity,satisficing,syntax},
  file = {/Users/j/Zotero/storage/5D9WINXM/Ferreira et al. - 2002 - Good-Enough Representations in Language Comprehens.pdf}
}

@incollection{ferreira.f:2016,
  title = {Chapter {{Six}} - {{Prediction}}, {{Information Structure}}, and {{Good-Enough Language Processing}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Ferreira, Fernanda and Lowder, Matthew W.},
  editor = {Ross, Brian H.},
  year = {2016},
  month = jan,
  volume = {65},
  pages = {217--247},
  publisher = {{Academic Press}},
  doi = {10.1016/bs.plm.2016.04.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0079742116300020},
  urldate = {2022-06-14},
  abstract = {The good-enough language processing approach emphasizes people's tendency to generate superficial and even inaccurate interpretations of sentences. At the same time, a number of researchers have argued that prediction plays a key role in comprehension, allowing people to anticipate features of the input and even specific upcoming words based on sentential constraint. In this chapter, we review evidence from our lab supporting both approaches, even though at least superficially these two perspectives seem incompatible. We then argue that what allows us to link good-enough processing and prediction is the concept of information structure, which states that sentences are organized to convey both given or presupposed information, and new or focused information. Our fundamental proposal is that given or presupposed information is processed in a good-enough manner, while new or focused information is the target of the comprehender's prediction efforts. The result is a theory that brings together three different literatures that have been treated almost entirely independently, and which can be evaluated using a combination of behavioral, computational, and neural methods.},
  langid = {english},
  keywords = {Comprehension,Good-enough processing,Information structure,Language processing,Prediction}
}

@article{ferrer-i-cancho.r:2015,
  title = {The Placement of the Head That Minimizes Online Memory: A Complex Systems Approach},
  author = {{Ferrer-i-Cancho}, Ramon},
  year = {2015},
  journal = {Language Dynamics and Change},
  volume = {5},
  number = {1},
  pages = {114--137},
  publisher = {{Brill}},
  date-added = {2019-05-15 00:10:35 -0400},
  date-modified = {2019-06-17 21:57:08 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,memory}
}

@inproceedings{finkel.j:2008,
  title = {Efficient, Feature-Based, Conditional Random Field Parsing},
  booktitle = {Proceedings of {{ACL-08}}: {{HLT}}},
  author = {Finkel, Jenny Rose and Kleeman, Alex and Manning, Christopher D.},
  year = {2008},
  month = jun,
  pages = {959--967},
  publisher = {{Association for Computational Linguistics}},
  address = {{Columbus, Ohio}},
  url = {https://aclanthology.org/P08-1109},
  keywords = {pruning}
}

@article{floridi.l:2020,
  title = {{{GPT-3}}: {{Its}} Nature, Scope, Limits, and Consequences},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  year = {2020},
  journal = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s11023-020-09548-1},
  url = {https://doi.org/10.1007%2Fs11023-020-09548-1},
  bdsk-url-2 = {https://doi.org/10.1007/s11023-020-09548-1},
  date-added = {2021-06-05 22:29:51 -0400},
  date-modified = {2021-06-05 22:29:53 -0400}
}

@article{fodor.j:1978,
  title = {Parsing Strategies and Constraints on Transformations},
  author = {Fodor, Janet Dean},
  year = {1978},
  journal = {Linguistic Inquiry},
  volume = {9},
  number = {3},
  eprint = {4178071},
  eprinttype = {jstor},
  pages = {427--473},
  publisher = {{The MIT Press}},
  issn = {0024-3892},
  url = {https://www.jstor.org/stable/4178071},
  urldate = {2023-02-22},
  file = {/Users/j/Zotero/storage/DLVKPXHK/Fodor (1978) Parsing Strategies and Constraints on Transformati.pdf}
}

@book{folland.g:1999,
  title = {Real {{Analysis}}: {{Modern Techniques}} and {{Their Applications}}, 2nd {{Edition}} | {{Wiley}}},
  shorttitle = {Real {{Analysis}}},
  author = {Folland, Gerald B.},
  year = {1999},
  month = apr,
  series = {Pure and {{Applied Mathematics}}: {{A Wiley Series}} of {{Texts}}, {{Monographs}} and {{Tracts}}},
  edition = {2nd Edition},
  url = {https://www.wiley.com/en-us/Real+Analysis%3A+Modern+Techniques+and+Their+Applications%2C+2nd+Edition-p-9780471317166},
  urldate = {2022-06-23},
  abstract = {An in-depth look at real analysis and its applications-now expanded and revised. This new edition of the widely used analysis book continues to cover real analysis in greater detail and at a more advanced level than most books on the subject. Encompassing several subjects that underlie much of modern analysis, the book focuses on measure and integration theory, point set topology, and the basics of functional analysis. It illustrates the use of the general theories and introduces readers to other branches of analysis such as Fourier analysis, distribution theory, and probability theory. This edition is bolstered in content as well as in scope-extending its usefulness to students outside of pure analysis as well as those interested in dynamical systems. The numerous exercises, extensive bibliography, and review chapter on sets and metric spaces make Real Analysis: Modern Techniques and Their Applications, Second Edition invaluable for students in graduate-level analysis courses. New features include: * Revised material on the n-dimensional Lebesgue integral. * An improved proof of Tychonoffs theorem. * Expanded material on Fourier analysis. * A newly written chapter devoted to distributions and differential equations. * Updated material on Hausdorff dimension and fractal dimension.},
  isbn = {978-0-471-31716-6},
  langid = {english},
  file = {/Users/j/Zotero/storage/NYLUISAM/folland.g.1999RealAnalysis.djvu}
}

@inproceedings{fossum.v:2012,
  title = {Sequential vs. {{Hierarchical}} Syntactic Models of Human Incremental Sentence Processing},
  booktitle = {Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({{CMCL}} 2012)},
  author = {Fossum, Victoria and Levy, Roger},
  year = {2012},
  month = jun,
  pages = {61--69},
  publisher = {{Association for Computational Linguistics}},
  address = {{Montr\'eal, Canada}},
  url = {https://aclanthology.org/W12-1706},
  date-added = {2021-11-29 10:02:26 -0500},
  date-modified = {2021-11-29 10:02:27 -0500}
}

@inproceedings{foster.a:2019,
  title = {Variational Bayesian Optimal Experimental Design},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah D.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {14036--14047},
  url = {https://proceedings.neurips.cc/paper/2019/hash/d55cbf210f175f4a37916eafe6c04f0d-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/FosterJBHTRG19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{fox.c:2012,
  title = {A Tutorial on Variational {{Bayesian}} Inference},
  author = {Fox, Charles W. and Roberts, Stephen J.},
  year = {2012},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {38},
  number = {2},
  pages = {85--95},
  issn = {1573-7462},
  doi = {10.1007/s10462-011-9236-8},
  url = {https://doi.org/10.1007/s10462-011-9236-8},
  urldate = {2022-06-27},
  abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  langid = {english},
  keywords = {Mean-field,Tutorial,Variational Bayes},
  file = {/Users/j/Zotero/storage/W2PZQNV6/Fox and Roberts - 2012 - A tutorial on variational Bayesian inference.pdf}
}

@article{fox.d:2003,
  title = {Adapting the Sample Size in Particle Filters through {{KLD-Sampling}}},
  author = {Fox, Dieter},
  year = {2003},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {22},
  number = {12},
  pages = {985--1003},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0278364903022012001},
  url = {https://doi.org/10.1177%2F0278364903022012001},
  bdsk-url-2 = {https://doi.org/10.1177/0278364903022012001},
  date-added = {2022-05-05 09:45:16 -0400},
  date-modified = {2022-05-05 09:46:04 -0400},
  keywords = {bayes filtering,KLD-sampling,particle filtering}
}

@article{frank.s:2009cogsci,
  title = {Surprisal-Based Comparison between a Symbolic and a Connectionist Model of Sentence Processing},
  author = {Frank, Stefan L.},
  year = {2009},
  journal = {Proceedings of the 31st Annual Meeting of the Cognitive Science Society},
  url = {https://escholarship.org/uc/item/02v5m1hf},
  urldate = {2022-10-12},
  langid = {english},
  file = {/Users/j/Zotero/storage/UZHHW2J6/Frank (2009) Surprisal-based comparison between a symbolic and .pdf}
}

@article{frank.s:2011,
  title = {Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author = {Frank, Stefan L. and Bod, Rens},
  year = {2011},
  month = may,
  journal = {Psychological Science},
  volume = {22},
  number = {6},
  pages = {829--834},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0956797611409589},
  url = {https://doi.org/10.1177%2F0956797611409589},
  bdsk-url-2 = {https://doi.org/10.1177/0956797611409589},
  date-added = {2021-11-29 10:04:00 -0500},
  date-modified = {2021-11-29 10:04:02 -0500}
}

@article{frank.s:2013corpus,
  title = {Reading Time Data for Evaluating Broad-Coverage Models of {{English}} Sentence Processing},
  author = {Frank, Stefan L. and Monsalve, Irene Fernandez and Thompson, Robin L. and Vigliocco, Gabriella},
  year = {2013},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {4},
  pages = {1182--1190},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-012-0313-y},
  url = {https://doi.org/10.3758%2Fs13428-012-0313-y},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-012-0313-y},
  date-added = {2021-09-16 13:31:43 -0400},
  date-modified = {2021-12-15 09:07:19 -0500}
}

@inproceedings{frank.s:2013surp,
  title = {Word Surprisal Predicts {{N400}} Amplitude during Reading},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2013},
  pages = {878--883},
  publisher = {{Association for Computational Linguistics}},
  address = {{Sofia, Bulgaria}},
  url = {https://www.aclweb.org/anthology/P13-2152},
  date-added = {2021-12-15 09:07:40 -0500},
  date-modified = {2021-12-15 09:07:44 -0500}
}

@article{frank.s:2021,
  title = {Toward Computational Models of Multilingual Sentence Processing},
  author = {Frank, Stefan L.},
  year = {2021},
  journal = {Language Learning},
  volume = {71},
  number = {S1},
  pages = {193--218},
  issn = {1467-9922},
  doi = {10.1111/lang.12406},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/lang.12406},
  urldate = {2022-10-22},
  abstract = {Although computational models can simulate aspects of human sentence processing, research on this topic has remained almost exclusively limited to the single language case. The current review presents an overview of the state of the art in computational cognitive models of sentence processing, and discusses how recent sentence-processing models can be used to study bi- and multilingualism. Recent results from cognitive modeling and computational linguistics suggest that phenomena specific to bilingualism can emerge from systems that have no dedicated components for handling multiple languages. Hence, accounting for human bi-/multilingualism may not require models that are much more sophisticated than those for the monolingual case.},
  langid = {english},
  keywords = {computational models,multilingualism,neural networks,probabilistic grammars,sentence processing},
  file = {/Users/j/Zotero/storage/DR5BTWQX/Frank (2021) Toward Computational Models of Multilingual Senten.pdf}
}

@article{frazier.l:1978,
  title = {The Sausage Machine: {{A}} New Two-Stage Parsing Model},
  shorttitle = {The Sausage Machine},
  author = {Frazier, Lyn and Fodor, Janet Dean},
  year = {1978},
  month = jan,
  journal = {Cognition},
  volume = {6},
  number = {4},
  pages = {291--325},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(78)90002-1},
  url = {https://www.sciencedirect.com/science/article/pii/0010027778900021},
  urldate = {2022-06-12},
  abstract = {It is proposed that the human sentence parsing device assigns phrase structure to word strings in two steps. The first stage parser assigns lexical and phrasal nodes to substrings of roughly six words. The second stage parser then adds higher nodes to link these phrasal packages together into a complete phrase marker. This model of the parser is compared with ATN models, and with the two-stage models of Kimball (1973) and Fodor, Bever and Garrett (1974). Our assumption that the units which are shunted from the first stage to the second stage are defined by their length, rather than by their syntactic type, explains the effects of constituent length on perceptual complexity in center embedded sentences and in sentences of the kind that fall under Kimball's principle of Right Association. The particular division of labor between the two parsing units allows us to explain, without appeal to any ad hoc parsing strategies, why the parser makes certain `shortsighted' errors even though, in general, it is able to make intelligent use of all the information that is available to it. R\'esum\'e Dans cet article on propose un m\'ecanisme de segmentation des \'enonc\'es qui assigne en deux \'etapes une structure syntagmatique aux suites de mots. La premi\`ere m\'ethode de segmentation assigne des noeuds lexicaux et syntagmatiques \`a des suites de 6 mots environ. La seconde ajoute des noeuds \`a un niveau sup\'erieur pour lier ces blocs syntagmatiques et obtenir ainsi un marqueur syntagmatique complet. Ce mod\`ele de segmentation est compar\'e d'une part aux mod\`eles ATN et d'autre part au mod\`ele en deux \'etapes de Kimball (1973) et Fodor, Bever et Garrett (1974). Nous pensons que les unit\'es qui passent du ler au 2\`e niveau sont caract\'eris\'ees par leur longueur plut\^ot que par leur forme syntaxique. Ceci expliquerait les effects de la longueur des constituants sur la complexit\'e perceptuelle des phrases enclass\'ees et des phrases du type de celles qui tombent sous le principe de l'association \`a droite de Kimball. La distinction sp\'ecifique du travail entre les deux unit\'es de segmentation permet d'expliquer, sans faire intervenir des strat\'egies ad hoc, certaines erreurs de segmentation m\^eme si, en g\'en\'eral, il est possible de faire un usage intelligent de toutes les informations disponibles.},
  langid = {english}
}

@article{frazier.l:1982,
  title = {Making and Correcting Errors during Sentence Comprehension: Eye Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  year = {1982},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {14},
  number = {2},
  pages = {178--210},
  publisher = {{Elsevier BV}},
  doi = {10.1016/0010-0285(82)90008-1},
  url = {https://doi.org/10.1016%2F0010-0285%2882%2990008-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(82)90008-1},
  date-added = {2022-05-06 15:36:26 -0400},
  date-modified = {2022-05-06 15:36:46 -0400},
  keywords = {eye-tracking,regressions}
}

@article{frazier.l:1987,
  title = {Syntactic Processing: {{Evidence}} from {{Dutch}}},
  shorttitle = {Syntactic Processing},
  author = {Frazier, Lyn},
  year = {1987},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  volume = {5},
  number = {4},
  pages = {519--559},
  issn = {1573-0859},
  doi = {10.1007/BF00138988},
  url = {https://doi.org/10.1007/BF00138988},
  urldate = {2022-10-13},
  langid = {english},
  keywords = {Artificial Intelligence,eager processing,Syntactic Processing},
  file = {/Users/j/Zotero/storage/SYEKGJMU/Frazier (1987) Syntactic processing Evidence from dutch.pdf}
}

@inproceedings{freer.f:2010,
  title = {When Are Probabilistic Programs Probably Computationally Tractable?},
  booktitle = {{{NIPS}} Workshop on {{Monte Carlo}} Methods for Modern Applications},
  author = {Freer, Cameron and Mansinghka, Vikash K. and Roy, Daniel},
  year = {2010},
  month = dec,
  address = {{Whistler, British Columbia, Canada}},
  url = {http://montecarlo.wdfiles.com/local--files/contributed-abstracts/nipsmc2010_freer_etal.pdf},
  date-added = {2021-03-09 22:52:16 -0500},
  date-modified = {2022-04-14 10:57:16 -0400},
  keywords = {probabilistic programming,rejection sampling,sampling,surprisal},
  file = {/Users/j/Zotero/storage/U7AFW2PV/Freer et al. (2010) When are probabilistic programs probably computati.pdf}
}

@article{friston.k:2012,
  title = {A {{Free Energy Principle}} for {{Biological Systems}}},
  author = {Friston, Karl},
  year = {2012},
  month = nov,
  journal = {Entropy},
  volume = {14},
  number = {11},
  pages = {2100--2121},
  publisher = {{Multidisciplinary Digital Publishing Institute}},
  issn = {1099-4300},
  doi = {10.3390/e14112100},
  url = {https://www.mdpi.com/1099-4300/14/11/2100},
  urldate = {2022-06-10},
  abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian,ergodicity,free energy,random dynamical system,self-organization,surprise},
  file = {/Users/j/Zotero/storage/929TWEGN/Karl - 2012 - A Free Energy Principle for Biological Systems.pdf}
}

@article{fromkin.v:1971,
  title = {The Non-Anomalous Nature of Anomalous Utterances},
  author = {Fromkin, Victoria A.},
  year = {1971},
  journal = {Language},
  volume = {47},
  number = {1},
  eprint = {412187},
  eprinttype = {jstor},
  pages = {27--52},
  publisher = {{Linguistic Society of America}},
  issn = {0097-8507},
  doi = {10.2307/412187},
  url = {https://www.jstor.org/stable/412187},
  urldate = {2022-06-24},
  abstract = {An analysis of speech errors provides evidence for the psychological reality of theoretical linguistic concepts such as distinctive features, morpheme structure constraints, abstract underlying forms, phonological rules, and syntactic and semantic features. Furthermore, such errors reveal that linguistic performance is highly rule-governed, and that in many cases it is grammatical rules which constrain or monitor actual speech production. While a model of linguistic competence is independent of temporal constraints, a model of linguistic performance must provide information as to the sequencing of events in real time. To explain the occurrence of particular kinds of errors, a specific ordering of rules is posited, which ordering may or may not coincide with the organization of a grammar.},
  keywords = {speech errors}
}

@inproceedings{futrell.r:2017,
  title = {Noisy-Context Surprisal as a Human Sentence Processing Cost Model},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Volume}} 1, Long Papers},
  author = {Futrell, Richard and Levy, Roger},
  year = {2017},
  pages = {688--698},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  url = {https://www.aclweb.org/anthology/E17-1065}
}

@phdthesis{futrell.r:2017phd,
  title = {Memory and Locality in Natural Language},
  author = {Futrell, Richard},
  year = {2017},
  url = {http://hdl.handle.net/1721.1/114075},
  abstract = {I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning.},
  date-added = {2022-04-11 23:46:38 -0400},
  date-modified = {2022-04-11 23:50:11 -0400},
  school = {Massachusetts Institute of Technology / Massachusetts Institute of Technology. Department of Brain and Cognitive Sciences},
  keywords = {information theory,noisy channel coding},
  file = {/Users/j/Zotero/storage/63RJV3GR/Futrell - 2017 - Memory and locality in natural language.pdf}
}

@inproceedings{futrell.r:2018,
  title = {The {{Natural Stories}} Corpus},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven and Fedorenko, Evelina},
  year = {2018},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Miyazaki, Japan}},
  url = {https://www.aclweb.org/anthology/L18-1012},
  date-modified = {2021-12-02 00:12:56 -0500}
}

@inproceedings{futrell.r:2019,
  title = {Syntactic Dependencies Correspond to Word Pairs with High Mutual Information},
  booktitle = {Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, {{SyntaxFest}} 2019)},
  author = {Futrell, Richard and Qian, Peng and Gibson, Edward and Fedorenko, Evelina and Blank, Idan},
  year = {2019},
  pages = {3--13},
  publisher = {{Association for Computational Linguistics}},
  address = {{Paris, France}},
  doi = {10.18653/v1/W19-7703},
  url = {https://www.aclweb.org/anthology/W19-7703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-7703}
}

@article{futrell.r:2020,
  title = {Lossy-Context Surprisal: {{An}} Information-Theoretic Model of Memory Effects in Sentence Processing},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {3},
  pages = {e12814},
  publisher = {{Wiley Online Library}},
  doi = {10.1111/cogs.12814},
  url = {https://doi.org/10.1111/cogs.12814},
  date-added = {2020-03-27 16:35:14 -0400},
  date-modified = {2022-04-20 13:48:30 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lossy context surprisal,mutual information,processing},
  file = {/Users/j/Zotero/storage/5YHHLIBG/supplementary C.pdf;/Users/j/Zotero/storage/F4RL26FY/Futrell et al. - 2020 - Lossy-context surprisal An information-theoretic .pdf;/Users/j/Zotero/storage/GLQTQS4Z/supplementary A.pdf;/Users/j/Zotero/storage/KLZP5Z9B/supplementary B.pdf}
}

@article{futrell.r:2021,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2021},
  month = mar,
  journal = {Language Resources and Evaluation},
  volume = {55},
  number = {1},
  pages = {63--77},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09503-7},
  url = {https://doi.org/10.1007/s10579-020-09503-7},
  urldate = {2022-06-09},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  keywords = {cognitive modeling,psycholinguistics,reading time,self-paced reading time},
  file = {/Users/j/Zotero/storage/3GDJYJRE/Futrell et al. - 2021 - The Natural Stories corpus a reading-time corpus .pdf}
}

@incollection{gamut.l:1991,
  title = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  booktitle = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  author = {Gamut, L. T. F.},
  year = {1991},
  publisher = {{University of Chicago Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@misc{gao.l:2020ThePile,
  title = {The Pile: {{An 800GB}} Dataset of Diverse Text for Language Modeling},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  eprint = {2101.00027},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-11-30 10:16:24 -0500},
  date-modified = {2021-11-30 10:19:58 -0500}
}

@misc{gao.l:2021blogGPT3sizes,
  title = {On the Sizes of {{OpenAI API}} Models},
  author = {Gao, Leo},
  year = {2021},
  month = may,
  journal = {EleutherAI Blog},
  url = {https://blog.eleuther.ai/gpt3-model-sizes/},
  urldate = {2021-12-13},
  date-added = {2021-12-13 19:27:28 -0500},
  date-modified = {2021-12-13 19:29:08 -0500},
  howpublished = {Blog post}
}

@inproceedings{gauthier.j:2020syntaxgym,
  title = {{{SyntaxGym}}: {{An}} Online Platform for Targeted Evaluation of Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: {{System}} Demonstrations},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {70--76},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-demos.10},
  url = {https://www.aclweb.org/anthology/2020.acl-demos.10},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-demos.10}
}

@incollection{gazdar.g:1985,
  title = {Generalized Phrase Structure Grammar},
  booktitle = {Generalized Phrase Structure Grammar},
  author = {Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K. and Sag, Ivan A.},
  year = {1985},
  publisher = {{Harvard University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:14 -0400},
  keywords = {GPSG}
}

@article{geman.s:1984a,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  date-added = {2021-03-17 15:08:02 -0400},
  date-modified = {2021-03-17 15:09:10 -0400},
  keywords = {Additive noise,Annealing,bayesian,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,gibbs sampling,image restoration,Image restoration,line process,MAP estimate,Markov random field,markov random fields,Markov random fields,relaxation,scene modeling,spatial degradation,stochastic processes,Stochastic processes,Temperature distribution},
  file = {/Users/j/Zotero/storage/X8ULM23N/Geman and Geman - 1984 - Stochastic Relaxation, Gibbs Distributions, and th.pdf}
}

@misc{genewein.t:2013,
  title = {Abstraction in Decision-Makers with Limited Information Processing Capabilities},
  author = {Genewein, Tim and Braun, Daniel A.},
  year = {2013},
  month = dec,
  number = {arXiv:1312.4353},
  eprint = {1312.4353},
  primaryclass = {cs, math, stat},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.1312.4353},
  url = {http://arxiv.org/abs/1312.4353},
  urldate = {2022-06-09},
  abstract = {A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Statistics - Machine Learning},
  annotation = {note: Presented at the NIPS 2013 Workshop on Planning with Information Constraints},
  file = {/Users/j/Zotero/storage/EY79F5ND/Genewein and Braun - 2013 - Abstraction in decision-makers with limited inform.pdf}
}

@article{genewein.t:2015,
  title = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making: An Information-Theoretic Optimality Principle},
  shorttitle = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making},
  author = {Genewein, Tim and Leibfried, Felix and {Grau-Moya}, Jordi and Braun, Daniel Alexander},
  year = {2015},
  journal = {Frontiers in Robotics and AI},
  volume = {2},
  issn = {2296-9144},
  url = {https://www.frontiersin.org/article/10.3389/frobt.2015.00027},
  urldate = {2022-06-07},
  abstract = {Abstraction and hierarchical information processing are hallmarks of human and animal intelligence underlying the unrivaled flexibility of behavior in biological systems. Achieving such flexibility in artificial systems is challenging, even with more and more computational power. Here, we investigate the hypothesis that abstraction and hierarchical information processing might in fact be the consequence of limitations in information-processing power. In particular, we study an information-theoretic framework of bounded rational decision-making that trades off utility maximization against information-processing costs. We apply the basic principle of this framework to perception-action systems with multiple information-processing nodes and derive bounded-optimal solutions. We show how the formation of abstractions and decision-making hierarchies depends on information-processing costs. We illustrate the theoretical ideas with example simulations and conclude by formalizing a mathematically unifying optimization principle that could potentially be extended to more complex systems.},
  keywords = {bounded rationality,information theory,lossy compression,rate-distortion theory},
  file = {/Users/j/Zotero/storage/AS9HLP5H/Genewein et al. - 2015 - Bounded Rationality, Abstraction, and Hierarchical.pdf;/Users/j/Zotero/storage/NYI26A99/Genewein et al. - 2015 - Bounded Rationality, Abstraction, and Hierarchical.pdf}
}

@inproceedings{georgi.d:2011,
  title = {Deriving the Distribution of Person Portmanteaux by Relativized Probing},
  booktitle = {Proceedings of the North-Eastern Linguistic Society},
  author = {Georgi, Doreen},
  year = {2011},
  volume = {42},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:19 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@incollection{gershman.s:2012,
  title = {Perception, Action and Utility: {{The}} Tangled Skein},
  booktitle = {Principles of Brain Dynamics: {{Global}} State Interactions},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  series = {Computational {{Neuroscience Series}}},
  pages = {293--312},
  publisher = {{MIT Press}},
  doi = {10.7551/mitpress/9108.003.0015},
  url = {https://doi.org/10.7551/mitpress/9108.003.0015},
  isbn = {978-0-262-01764-0},
  file = {/Users/j/Zotero/storage/B9IW77BL/Gershman and Daw - 2012 - Perception, action and utility The tangled skein.pdf}
}

@article{gershman.s:2019,
  title = {What Does the Free Energy Principle Tell Us about the Brain?},
  author = {Gershman, Samuel J.},
  year = {2019},
  month = jan,
  doi = {10.48550/arXiv.1901.07945},
  url = {https://arxiv.org/abs/1901.07945v5},
  urldate = {2022-07-27},
  abstract = {The free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
  langid = {english},
  keywords = {Bayesian brain,free energy principle,inference,predictive coding},
  file = {/Users/j/Zotero/storage/JP945JX9/Gershman - 2019 - What does the free energy principle tell us about .pdf}
}

@unpublished{gershman.s:2021draft,
  type = {Draft for the {{Oxford}} Handbook of Human Memory},
  title = {The Rational Analysis of Memory},
  author = {Gershman, Samuel J.},
  year = {2021},
  abstract = {This chapter surveys rational models of memory, which posit that memory is optimized to store information that will be needed in the future, subject to the constraint that information can only be stored with a limited amount of precision. This optimization problem can be formalized using the framework of rate-distortion theory, which addresses the trade-off between memory precision and task performance. The design principles that emerge from this framework shed light on numerous regularities of memory, as well as how cognitive and environmental factors shape these regularities.},
  langid = {english},
  file = {/Users/j/Zotero/storage/KKCRCGIR/Gershman (The rational analysis of memory.pdf}
}

@article{gibbs.a:2002,
  title = {On Choosing and Bounding Probability Metrics},
  author = {Gibbs, Alison L. and Su, Francis Edward},
  year = {2002},
  journal = {International Statistical Review},
  volume = {70},
  number = {3},
  pages = {419--435},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2002.tb00178.x},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1751-5823.2002.tb00178.x},
  urldate = {2022-12-22},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
  langid = {english},
  keywords = {Discrepancy,Hellinger distance,probability metrics,Probability metrics,Prokhorov metric,Rates of convergence,relative entropy,Relative entropy,Wasserstein distance},
  file = {/Users/j/Zotero/storage/XCES8JA3/Gibbs and Su (2002) On Choosing and Bounding Probability Metrics.pdf}
}

@phdthesis{gibson.e:1991phd,
  title = {A Computational Theory of Human Linguistic Processing: {{Memory}} Limitations and Processing Breakdown},
  shorttitle = {A Computational Theory of Human Linguistic Processing},
  author = {Gibson, Edward},
  year = {1991},
  address = {{United States -- Pennsylvania}},
  url = {https://www.proquest.com/docview/303922361/abstract/D91794B7808B4DE7PQ/1},
  urldate = {2022-06-14},
  abstract = {This thesis gives a theory of sentence comprehension that attempts to explain a number of linguistic performance effects, including garden-path effects, preferred readings for ambiguous input and processing overload effects. It is hypothesized that the human parser heuristically determines its options based upon evaluation of possible representations with respect to lexical, syntactic, semantic and pragmatic properties, each of which is associated with a weight. Processing overload effects are explained by the assumption of the existence of a maximum load corresponding to the limited capacity of short term memory: a structure becomes unacceptable at a particular parse state if the combination of the processing weights associated with its properties at that state is greater than the available capacity. Furthermore, it is assumed that the language processor is an automatic device that maintains only the best of the set of all compatible representations for an input string. This thesis assumes a formulation of representational evaluation within a parallel framework: one structure is preferred over another if the processing load associated with the first structure is markedly lower than the processing load associated with the second. Thus a garden path effect results if the unpreferred structure is necessary for a successful parse of the input. Four properties of linguistic representations are presented within this framework. The first two--the Properties of Thematic Reception and Transmission--derivable from the \$\textbackslash theta\$-Criterion from Government-Binding (GB) Theory (Chomsky (1981)); the third--the Property of Lexical Requirement--derivable from the Projection Principle of GB Theory; and the fourth--the Property of Recency Preference--prefers local attachments over more distant attachments (cf. Kimball (1973), Frazier (1979)). This thesis shows how these properties interact to give a partially unified theory of many performance effects.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798617014688},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {Applied sciences,computational linguistics,Language,literature and linguistics,memory,parsing,processing,psycholinguistics},
  file = {/Users/j/Zotero/storage/YBKJ9FUN/Gibson - A computational theory of human linguistic process.pdf}
}

@article{gibson.e:1998,
  title = {Linguistic Complexity: Locality of Syntactic Dependencies},
  author = {Gibson, Edward},
  year = {1998},
  month = aug,
  journal = {Cognition},
  volume = {68},
  number = {1},
  pages = {1--76},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00034-1},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027798000341},
  abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory \textendash{} the Syntactic Prediction Locality Theory (SPLT) \textendash{} has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
  keywords = {Computational resources,Linguistic complexity,Sentence processing,Syntactic dependency}
}

@article{gibson.e:1999,
  title = {Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical},
  author = {Gibson, Edward and Thomas, James},
  year = {1999},
  month = jun,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {3},
  pages = {225--248},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/016909699386293},
  url = {https://doi.org/10.1080%2F016909699386293},
  bdsk-url-2 = {https://doi.org/10.1080/016909699386293},
  date-added = {2022-04-19 22:36:47 -0400},
  date-modified = {2022-04-19 22:36:48 -0400}
}

@incollection{gibson.e:2000,
  title = {The Dependency Locality Theory: {{A}} Distance-Based Theory of Linguistic Complexity},
  shorttitle = {The Dependency Locality Theory},
  booktitle = {Image, Language, Brain:  {{Papers}} from the First Mind Articulation Project Symposium},
  author = {Gibson, Edward},
  editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  year = {2000},
  pages = {94--126},
  publisher = {{The MIT Press}},
  address = {{Cambridge, MA, US}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.592.5833&rank=1&q=The%20dependency%20locality%20theory:%20A%20distance-based%20theory%20of%20linguistic%20complexity.&osm=&ossid=},
  abstract = {Discusses the dependency locality theory (DLT) of human computational resources in sentence parsing that relies on 2 kinds of resource use. One of the key ideas underlying the theory is locality, such that the cost of integrating 2 elements (such as a head and a dependent, or a pronominal referent to its antecedent) depends on the distance between the 2. The remainder of the chapter reviews some empirical observations regarding the proceeding difficulty associated with unambiguous structures. It is shown that the DLT accounts for the complexity effect in these structures as well as preferences in ambiguous structures.},
  isbn = {978-0-262-13371-5},
  keywords = {Linguistics,Psychological Theories,Sentence Comprehension,Sentence Structure,Syntax}
}

@article{gibson.e:2013,
  title = {A Noisy-Channel Account of Crosslinguistic Word-Order Variation},
  author = {Gibson, Edward and Piantadosi, Steven T. and Brink, Kimberly and Bergen, Leon and Lim, Eunice and Saxe, Rebecca},
  year = {2013},
  month = may,
  journal = {Psychological Science},
  volume = {24},
  number = {7},
  pages = {1079--1088},
  publisher = {{SAGE Publications}},
  doi = {10.1177/0956797612463705},
  url = {https://doi.org/10.1177%2F0956797612463705},
  bdsk-url-2 = {https://doi.org/10.1177/0956797612463705},
  date-added = {2022-04-19 22:26:57 -0400},
  date-modified = {2022-05-02 14:46:14 -0400},
  keywords = {noisy channel coding}
}

@article{gibson.e:2013pnas,
  title = {Rational Integration of Noisy Evidence and Prior Semantic Expectations in Sentence Interpretation},
  author = {Gibson, Edward and Bergen, Leon and Piantadosi, Steven T.},
  year = {2013},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {20},
  pages = {8051--8056},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1216438110},
  url = {https://doi.org/10.1073%2Fpnas.1216438110},
  keywords = {noisy channel coding},
  file = {/Users/j/Zotero/storage/E6DQDNMG/Gibson et al. - 2013 - Rational integration of noisy evidence and prior s.pdf}
}

@inproceedings{gildea.d:2007,
  title = {Optimizing Grammars for Minimum Dependency Length},
  booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  author = {Gildea, Daniel and Temperley, David},
  year = {2007},
  pages = {184--191},
  publisher = {{Association for Computational Linguistics}},
  address = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/P07-1024}
}

@article{gildea.d:2010,
  title = {Do Grammars Minimize Dependency Length?},
  author = {Gildea, Daniel and Temperley, David},
  year = {2010},
  journal = {Cognitive Science},
  volume = {34},
  number = {2},
  pages = {286--310},
  publisher = {{Wiley Online Library}},
  date-added = {2019-05-14 23:50:31 -0400},
  date-modified = {2019-06-17 21:56:52 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,projectivity}
}

@article{gleitman.l:2019,
  title = {The Impossibility of Language Acquisition (and How They Do It)},
  author = {Gleitman, Lila R. and Liberman, Mark Y. and McLemore, Cynthia A. and Partee, Barbara H.},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {1--24},
  publisher = {{Annual Reviews}},
  doi = {10.1146/annurev-linguistics-011718-011640},
  url = {https://doi.org/10.1146%2Fannurev-linguistics-011718-011640},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011640},
  date-added = {2021-08-17 09:52:01 -0400},
  date-modified = {2021-08-17 09:52:03 -0400}
}

@misc{godfrey.j:1993switchboard,
  title = {Switchboard-1 Release 2},
  author = {Godfrey, John J. and Holliman, Edward},
  year = {1993},
  number = {LDC97S62},
  publisher = {{Linguistic Data Consortium}},
  doi = {10.35111/SW3H-RW02},
  url = {https://catalog.ldc.upenn.edu/LDC97S62},
  bdsk-url-2 = {https://doi.org/10.35111/SW3H-RW02},
  date-added = {2022-05-06 14:21:03 -0400},
  date-modified = {2022-05-06 14:23:45 -0400},
  howpublished = {Web Download},
  keywords = {dataset,speech errors}
}

@inproceedings{gogate.v:2007,
  title = {{{SampleSearch}}: A Scheme That Searches for Consistent Samples},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  author = {Gogate, Vibhav and Dechter, Rina},
  editor = {Meila, Marina and Shen, Xiaotong},
  year = {2007},
  month = mar,
  series = {Proceedings of Machine Learning Research},
  volume = {2},
  pages = {147--154},
  publisher = {{PMLR}},
  address = {{San Juan, Puerto Rico}},
  url = {https://proceedings.mlr.press/v2/gogate07a.html},
  abstract = {Sampling from belief networks which have a substantial number of zero probabilities is problematic. MCMC algorithms like Gibbs sampling do not converge and importance sampling schemes generate many zero weight samples that are rejected, yielding an inefficient sampling process (the rejection problem). In this paper, we propose to augment importance sampling with systematic constraint-satisfaction search in order to overcome the rejection problem. The resulting SampleSearch scheme can be made unbiased by using a computationally expensive weighting scheme. To overcome this an approximation is proposed such that the resulting estimator is asymptotically unbiased. Our empirical results demonstrate the potential of our new scheme.},
  date-added = {2022-05-05 09:35:36 -0400},
  date-modified = {2022-05-05 09:37:37 -0400},
  pdf = {http://proceedings.mlr.press/v2/gogate07a/gogate07a.pdf},
  keywords = {sample search}
}

@book{goldberg.y:2017,
  title = {Neural Network Methods for Natural Language Processing},
  author = {Goldberg, Yoav},
  year = {2017},
  publisher = {{Morgan and Claypool Publishers}},
  date-added = {2019-05-17 21:08:13 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  keywords = {machine learning,neural networks,recurrent neural networks,sequence to sequence models,word embeddings}
}

@misc{goldberg.y:2019,
  title = {Assessing {{BERT}}'s Syntactic Abilities},
  author = {Goldberg, Yoav},
  year = {2019},
  eprint = {1901.05287},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{goldstein.a:2021,
  title = {Thinking Ahead: Spontaneous Prediction in Context as a Keystone of Language in Humans and Machines},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  year = {2021},
  journal = {bioRxiv : the preprint server for biology},
  eprint = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477.full.pdf},
  publisher = {{Cold Spring Harbor Laboratory}},
  doi = {10.1101/2020.12.02.403477},
  url = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM's next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM's contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.Competing Interest StatementThe authors have declared no competing interest.},
  bdsk-url-2 = {https://doi.org/10.1101/2020.12.02.403477},
  date-added = {2021-06-09 15:53:23 -0400},
  date-modified = {2021-06-09 15:53:24 -0400},
  elocation-id = {2020.12.02.403477},
  file = {/Users/j/Zotero/storage/YG4G4JNR/Goldstein et al. - 2021 - Thinking ahead spontaneous prediction in context .pdf}
}

@inproceedings{goodkind.a:2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}} ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/w18-0102},
  url = {https://doi.org/10.18653%2Fv1%2Fw18-0102},
  bdsk-url-2 = {https://doi.org/10.18653/v1/w18-0102},
  date-added = {2021-11-29 10:00:16 -0500},
  date-modified = {2021-11-29 10:00:18 -0500}
}

@misc{goodkind.a:2021,
  title = {Local Word Statistics Affect Reading Times Independently of Surprisal},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2103.04469},
  url = {https://arxiv.org/abs/2103.04469},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2103.04469},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-09 17:20:07 -0400},
  date-modified = {2022-05-09 17:21:18 -0400},
  keywords = {causal bottleneck},
  file = {/Users/j/Zotero/storage/H3U3VVSI/Goodkind and Bicknell - 2021 - Local word statistics affect reading times indepen.pdf}
}

@article{goodman.j:1999,
  title = {Semiring Parsing},
  author = {Goodman, Joshua},
  year = {1999},
  journal = {Computational Linguistics},
  volume = {25},
  number = {4},
  pages = {573--606},
  url = {https://www.aclweb.org/anthology/J99-4004}
}

@article{goodman.j:2001,
  title = {A Bit of Progress in Language Modeling},
  author = {Goodman, Joshua T.},
  year = {2001},
  journal = {Computer Speech \& Language},
  volume = {15},
  number = {4},
  pages = {403--434},
  issn = {0885-2308},
  doi = {10.1006/csla.2001.0174},
  url = {https://doi.org/10.1006/csla.2001.0174},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-09 15:53:09 -0400},
  opturl = {http://www.sciencedirect.com/science/article/pii/S0885230801901743}
}

@inproceedings{goodwin.e:2020,
  title = {Probing {{Linguistic Systematicity}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  year = {2020},
  month = jul,
  pages = {1958--1969},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.177},
  url = {https://aclanthology.org/2020.acl-main.177},
  urldate = {2022-05-19},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.},
  keywords = {natural language inference,natural logic,systematicity}
}

@inproceedings{gorla.j:2007,
  title = {Two Approaches for Building an Unsupervised Dependency Parser and Their Other Applications},
  booktitle = {{{PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE}}},
  author = {Gorla, Jagadeesh and Goyal, Amit and Sangal, Rajeev},
  year = {2007},
  volume = {22},
  pages = {1860},
  url = {https://www.aaai.org/Papers/AAAI/2007/AAAI07-305.pdf},
  date-added = {2020-04-23 11:09:55 -0400},
  date-modified = {2020-04-23 11:10:41 -0400},
  organization = {{Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999}},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised parsing},
  file = {/Users/j/Zotero/storage/3KCRD7XK/Gorla et al. - 2007 - Two approaches for building an unsupervised depend.pdf}
}

@article{gottwald.s:2020,
  title = {The Two Kinds of Free Energy and the {{Bayesian}} Revolution},
  author = {Gottwald, Sebastian and Braun, Daniel A.},
  year = {2020},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008420},
  publisher = {{Public Library of Science}},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008420},
  url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008420},
  urldate = {2022-07-11},
  abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information processing of perception.},
  langid = {english},
  keywords = {Decision making,Entropy,Free energy,Helmholtz free energy,Information processing,Kullback Leibler divergence,Optimization,Probability distribution},
  file = {/Users/j/Zotero/storage/C39AKFN2/Gottwald and Braun - 2020 - The two kinds of free energy and the Bayesian revo.pdf}
}

@misc{goyal.k:2021,
  title = {Exposing the Implicit Energy Networks behind Masked Language Models via {{Metropolis}}\textendash{{Hastings}}},
  author = {Goyal, Kartik and Dyer, Chris and {Berg-Kirkpatrick}, Taylor},
  year = {2021},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2106.02736},
  url = {https://arxiv.org/abs/2106.02736},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.02736},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 12:27:57 -0400},
  date-modified = {2022-04-09 00:57:41 -0400},
  keywords = {energy networks,masked language models,metropolis hastings},
  file = {/Users/j/Zotero/storage/JY2YPR3J/Goyal et al. - 2021 - Exposing the implicit energy networks behind maske.pdf}
}

@article{graf.t:2017,
  title = {Relative Clauses as a Benchmark for Minimalist Parsing},
  author = {Graf, Thomas and Monette, James and Zhang, Chong},
  year = {2017},
  month = jul,
  journal = {Journal of Language Modelling},
  volume = {5},
  number = {1},
  issn = {2299-8470, 2299-856X},
  doi = {10.15398/jlm.v5i1.157},
  url = {https://jlm.ipipan.waw.pl/index.php/JLM/article/view/157},
  urldate = {2022-09-30},
  abstract = {Minimalist grammars have been used recently in a series of papers to explain well-known contrasts in human sentence processing in terms of subtle structural differences. These proposals combine a top-down parser with complexity metrics that relate parsing difficulty to memory usage. So far, though, there has been no large-scale exploration of the space of viable metrics. Building on this earlier work, we compare the ability of 1600 metrics to derive several processing effects observed with relative clauses, many of which have been proven difficult to unify. We show that among those 1600 candidates, a few metrics (and only a few) can provide a unified account of all these contrasts. This is a welcome result for two reasons: First, it provides a novel account of extensively studied psycholinguistic data. Second, it significantly limits the number of viable metrics that may be applied to other phenomena, thus reducing theoretical indeterminacy.},
  file = {/Users/j/Zotero/storage/FBS723CR/Graf et al. (2017) Relative clauses as a benchmark for Minimalist par.pdf}
}

@incollection{grice.h:1975,
  title = {Logic and Conversation},
  booktitle = {Speech {{Acts}}},
  author = {Grice, H. P.},
  editor = {Cole, Peter and Morgan, Jerry L.},
  year = {1975},
  month = dec,
  pages = {41--58},
  publisher = {{BRILL}},
  doi = {10.1163/9789004368811_003},
  url = {https://brill.com/view/book/edcoll/9789004368811/BP000003.xml},
  urldate = {2022-09-30},
  isbn = {978-90-04-36881-1 978-90-04-36857-6}
}

@misc{griffith.v:2012,
  title = {Quantifying Synergistic Mutual Information},
  author = {Griffith, Virgil and Koch, Christof},
  year = {2012},
  eprint = {1205.4265},
  primaryclass = {cs.IT},
  archiveprefix = {arxiv},
  date-added = {2020-07-06 08:48:33 -0400},
  date-modified = {2020-07-06 08:49:07 -0400},
  project = {information-compositionality}
}

@article{grodner.d:2003,
  title = {Against Repair-Based Reanalysis in Sentence Comprehension},
  author = {Grodner, Daniel and Gibson, Edward and Argaman, Vered and Babyonyshev, Maria},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {141--166},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1022496223965},
  url = {https://doi.org/10.1023%2Fa%3A1022496223965},
  bdsk-url-2 = {https://doi.org/10.1023/a:1022496223965},
  date-added = {2021-03-18 11:28:51 -0400},
  date-modified = {2021-03-18 11:30:16 -0400},
  keywords = {reading time,self-paced reading}
}

@book{grune.d:2008,
  title = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  year = {2008},
  publisher = {{Springer New York}},
  doi = {10.1007/978-0-387-68954-8},
  url = {https://doi.org/10.1007%2F978-0-387-68954-8},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-68954-8},
  date-added = {2022-03-31 10:23:54 -0400},
  date-modified = {2022-03-31 10:24:43 -0400},
  keywords = {book,parsing}
}

@misc{grunwald.p:2004,
  title = {Shannon Information and Kolmogorov Complexity},
  author = {Grunwald, Peter and Vitanyi, Paul},
  year = {2004},
  eprint = {cs/0410002},
  archiveprefix = {arxiv},
  date-added = {2019-09-13 08:19:19 -0400},
  date-modified = {2019-09-13 08:20:10 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity,mutual information,rate-distortion theory,shannon entropy}
}

@misc{grunwald.p:2004a,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  year = {2004},
  eprint = {math/0406077},
  url = {https://arxiv.org/pdf/math/0406077.pdf},
  archiveprefix = {arxiv},
  date-added = {2020-02-20 11:40:38 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {minimum description length,mutual information},
  file = {/Users/j/Zotero/storage/Y9C8S7ZU/Grunwald - 2004 - A tutorial introduction to the minimum description.pdf}
}

@inproceedings{gulordava.k:2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1108},
  url = {https://www.aclweb.org/anthology/N18-1108},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1108}
}

@article{gutknecht.a:2021,
  title = {Bits and Pieces: Understanding Information Decomposition from Part-Whole Relationships and Formal Logic},
  author = {Gutknecht, A. J. and Wibral, M. and Makkeh, A.},
  year = {2021},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {477},
  number = {2251},
  pages = {20210110},
  publisher = {{The Royal Society}},
  doi = {10.1098/rspa.2021.0110},
  url = {https://doi.org/10.1098%2Frspa.2021.0110},
  bdsk-url-2 = {https://doi.org/10.1098/rspa.2021.0110},
  date-added = {2022-04-18 11:14:51 -0400},
  date-modified = {2022-04-18 11:15:12 -0400},
  keywords = {partial information decomposition}
}

@book{hacking.i:2006,
  title = {The Emergence of Probability: {{A}} Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference},
  author = {Hacking, Ian},
  year = {2006},
  edition = {Second},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9780511817557},
  url = {https://doi.org/10.1017/CBO9780511817557},
  date-added = {2021-02-16 15:11:17 -0500},
  date-modified = {2021-02-16 15:11:20 -0500}
}

@inproceedings{hahn.m:2018cogsci,
  title = {An Information-Theoretic Explanation of Adjective Ordering Preferences},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Hahn, Michael and Degen, Judith and Goodman, Noah and Jurafsky, Dan and Futrell, Richard},
  editor = {Kalish, Charles and Rau, Martina and Zhu, Jerry and Rogers, Timothy},
  year = {2018},
  pages = {1766--1772},
  publisher = {{Cognitive Science Society}},
  address = {{Madison, Wisconsin, USA}},
  url = {https://mindmodeling.org/cogsci2018/papers/0339/index.html},
  abstract = {Across languages, adjectives are subject to ordering restrictions. Recent research shows that these are predicted by adjective subjectivity, but the question remains open why this is the case. We first conduct a corpus study and not only replicate the subjectivity effect, but also find a previously undocumented effect of mutual information between adjectives and nouns. We then describe a rational model of adjective use in which listeners explicitly reason about judgments made by different speakers, formalizing the notion of subjectivity as agreement between speakers. We show that, once incremental processing is combined with memory limitations, our model predicts effects both of subjectivity and mutual information. We confirm the adequacy of our model by evaluating it on corpus data, finding that it correctly predicts ordering in unseen data with an accuracy of 96.2\%. This suggests that adjective ordering can be explained by general principles of human communication and language processing.},
  langid = {english},
  file = {/Users/j/Zotero/storage/MVSH6AA3/Hahn et al. (An Information-Theoretic Explanation of Adjective .pdf}
}

@unpublished{hahn.m:2019,
  type = {Unpublished Manuscript},
  title = {Estimating Predictive Rate-Distortion Curves via Neural Variational Inference},
  author = {Hahn, Michael and Futrell, Richard},
  year = {2019},
  date-added = {2019-06-11 14:15:47 -0400},
  date-modified = {2019-06-17 21:56:11 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory,variational inference}
}

@article{hahn.m:2021,
  title = {Modeling Word and Morpheme Order in Natural Language as an Efficient Trade-off of Memory and Surprisal.},
  author = {Hahn, Michael and Degen, Judith and Futrell, Richard},
  year = {2021},
  month = apr,
  journal = {Psychological Review},
  volume = {128},
  number = {4},
  pages = {726},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/rev0000269},
  url = {https://psycnet.apa.org/fulltext/2021-31510-001.pdf},
  urldate = {2023-03-27},
  file = {/Users/j/Zotero/storage/JQ5YQYZG/2021-31510-001.pdf;/Users/j/Zotero/storage/VF7BTJSP/Hahn et al. (Modeling word and morpheme order in natural langua.pdf}
}

@article{hahn.m:2022,
  title = {Morpheme Ordering across Languages Reflects Optimization for Processing Efficiency},
  author = {Hahn, Michael and Mathew, Rebecca and Degen, Judith},
  year = {2022},
  month = feb,
  journal = {Open Mind},
  volume = {5},
  pages = {208--232},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00051},
  url = {https://doi.org/10.1162/opmi_a_00051},
  urldate = {2023-03-27},
  abstract = {The ordering of morphemes in a word displays well-documented regularities across languages. Previous work has explained these in terms of notions such as semantic scope, relevance, and productivity. Here, we test a recently formulated processing theory of the ordering of linguistic units, the efficient tradeoff hypothesis (Hahn et al., 2021). The claim of the theory is that morpheme ordering can partly be explained by the optimization of a tradeoff between memory and surprisal. This claim has received initial empirical support from two languages. In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology, and by considering nouns in addition to verbs. We find that the efficient tradeoff hypothesis predicts ordering in most cases with high accuracy, and accounts for cross-linguistic regularities in noun and verb inflection. Our work adds to a growing body of work suggesting that many ordering properties of language arise from a pressure for efficient language processing.},
  file = {/Users/j/Zotero/storage/5HSKRPVL/Hahn et al. (2022) Morpheme Ordering Across Languages Reflects Optimi.pdf}
}

@article{hahn.m:2022PNAS,
  title = {A Resource-Rational Model of Human Processing of Recursive Linguistic Structure},
  author = {Hahn, Michael and Futrell, Richard and Levy, Roger and Gibson, Edward},
  year = {2022},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {43},
  pages = {e2122602119},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2122602119},
  url = {https://www.pnas.org/doi/10.1073/pnas.2122602119},
  urldate = {2022-11-28},
  abstract = {A major goal of psycholinguistic theory is to account for the cognitive constraints limiting the speed and ease of language comprehension and production. Wide-ranging evidence demonstrates a key role for linguistic expectations: A word's predictability, as measured by the information-theoretic quantity of surprisal, is a major determinant of processing difficulty. But surprisal, under standard theories, fails to predict the difficulty profile of an important class of linguistic patterns: the nested hierarchical structures made possible by recursion in human language. These nested structures are better accounted for by psycholinguistic theories of constrained working memory capacity. However, progress on theory unifying expectation-based and memory-based accounts has been limited. Here we present a unified theory of a rational trade-off between precision of memory representations with ease of prediction, a scaled-up computational implementation using contemporary machine learning methods, and experimental evidence in support of the theory's distinctive predictions. We show that the theory makes nuanced and distinctive predictions for difficulty patterns in nested recursive structures predicted by neither expectation-based nor memory-based theories alone. These predictions are confirmed 1) in two language comprehension experiments in English, and 2) in sentence completions in English, Spanish, and German. More generally, our framework offers computationally explicit theory and methods for understanding how memory constraints and prediction interact in human language comprehension and production.},
  file = {/Users/j/Zotero/storage/37EFXUNC/Appendix01.pdf;/Users/j/Zotero/storage/U9LRCM9S/Hahn et al. (2022) A resource-rational model of human processing of r.pdf}
}

@misc{hahn.m:2023arxiv,
  title = {A Theory of Emergent In-Context Learning as Implicit Structure Induction},
  author = {Hahn, Michael and Goyal, Navin},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07971},
  eprint = {2303.07971},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2303.07971},
  urldate = {2023-03-23},
  abstract = {Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{hahn.m:2023cognition,
  title = {Modeling Task Effects in Human Reading with Neural Network-Based Attention},
  author = {Hahn, Michael and Keller, Frank},
  year = {2023},
  month = jan,
  journal = {Cognition},
  volume = {230},
  pages = {105289},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105289},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027722002773},
  urldate = {2023-03-23},
  abstract = {Research on human reading has long documented that reading behavior shows task-specific effects, but it has been challenging to build general models predicting what reading behavior humans will show in a given task. We introduce NEAT, a computational model of the allocation of attention in human reading, based on the hypothesis that human reading optimizes a tradeoff between economy of attention and success at a task. Our model is implemented using contemporary neural network modeling techniques, and makes explicit and testable predictions about how the allocation of attention varies across different tasks. We test this in an eyetracking study comparing two versions of a reading comprehension task, finding that our model successfully accounts for reading behavior across the tasks. Our work thus provides evidence that task effects can be modeled as optimal adaptation to task demands.},
  langid = {english},
  keywords = {Computational modeling,Reading,Task effects},
  file = {/Users/j/Zotero/storage/HKBMNXUA/Hahn and Keller (2023) Modeling task effects in human reading with neural.pdf}
}

@inproceedings{hale.j:2001,
  title = {A Probabilistic {{Earley}} Parser as a Psycholinguistic Model},
  booktitle = {Second Meeting of the {{North American}} Chapter of the Association for Computational Linguistics},
  author = {Hale, John T.},
  year = {2001},
  url = {https://www.aclweb.org/anthology/N01-1021},
  date-modified = {2022-04-20 13:49:59 -0400},
  file = {/Users/j/Zotero/storage/JVG9SXWX/Hale - 2001 - A probabilistic Earley parser as a psycholinguisti.pdf}
}

@article{hale.j:2003,
  title = {The Information Conveyed by Words in Sentences},
  author = {Hale, John T.},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {101--123},
  publisher = {{Springer}},
  doi = {10.1023/A:1022492123056},
  url = {https://doi.org/10.1023/A:1022492123056},
  date-added = {2021-04-08 14:24:37 -0400},
  date-modified = {2022-04-20 13:49:43 -0400},
  keywords = {entropy reduction}
}

@phdthesis{hale.j:2003phd,
  title = {Grammar, Uncertainty and Sentence Processing},
  author = {Hale, John T.},
  year = {2003},
  address = {{Baltimore, Maryland}},
  url = {https://www.proquest.com/docview/288510490},
  abstract = {Toward a probabilistic theory of human sentence processing, this dissertation proposes a definition of computational work done in the course of analyzing sentences generated by formal grammars. It applies the idea of entropy from information theory to the set of derivations compatible with an initial substring of a sentence. Given a probabilistic grammar, this permits the set of such compatible derivations to be viewed as a random variable, and the change in uncertainty about the outcomes to be calculated. This definition of computational work is examined as a cognitive model of human sentence processing difficulty. To apply the model, a variety of existing syntactic proposals for English sentences are cast as probabilistic Generalized Phrase Structure Grammars (Gazdar et al., 1985) and probabilistic Minimalist Grammars (Stabler, 1997). It is shown that the amount of predicted processing effort in relative clauses correlates with the Accessibility Hierarchy of relativized grammatical relations (Keenan and Comrie, 1977) on a Kaynian (1994) view of relative clause structure. Results from three new on-line sentence reading experiments suggest that while genitivity has the role suggested by the Accessibility Hierarchy, extraction from oblique does not. Evidence is also found for a direct object/indirect object processing asymmetry, which can be derived from the proposed cognitive model under the assumption of a lexicalized probabilistic grammar.},
  date-added = {2022-04-14 15:27:00 -0400},
  date-modified = {2022-04-14 15:31:29 -0400},
  isbn = {978-0-496-55064-7},
  school = {Johns Hopkins University},
  file = {/Users/j/Zotero/storage/KGA6L2HA/Hale - 2003 - Grammar, uncertainty and sentence processing.pdf}
}

@inproceedings{hale.j:2004,
  title = {The Information-Processing Difficulty of Incremental Parsing},
  booktitle = {Proceedings of the Workshop on Incremental Parsing: {{Bringing}} Engineering and Cognition Together},
  author = {Hale, John T.},
  year = {2004},
  month = jul,
  pages = {58--65},
  publisher = {{Association for Computational Linguistics}},
  address = {{Barcelona, Spain}},
  url = {https://aclanthology.org/W04-0309},
  date-added = {2022-04-14 13:31:29 -0400},
  date-modified = {2022-04-20 13:50:05 -0400}
}

@article{hale.j:2006,
  title = {Uncertainty about the Rest of the Sentence},
  author = {Hale, John T.},
  year = {2006},
  journal = {Cognitive Science},
  volume = {30},
  number = {4},
  pages = {643--672},
  publisher = {{Wiley}},
  doi = {10.1207/s15516709cog0000_64},
  url = {https://doi.org/10.1207%2Fs15516709cog0000â‚†4},
  bdsk-url-2 = {https://doi.org/10.1207/s15516709cog0000{$_6$}4},
  date-added = {2021-03-18 10:37:45 -0400},
  date-modified = {2022-04-20 13:50:10 -0400},
  keywords = {entropy reduction,processing}
}

@article{hale.j:2011,
  title = {What a Rational Parser Would Do},
  author = {Hale, John T.},
  year = {2011},
  month = apr,
  journal = {Cognitive Science},
  volume = {35},
  number = {3},
  pages = {399--443},
  issn = {03640213},
  doi = {10.1111/j.1551-6709.2010.01145.x},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1551-6709.2010.01145.x},
  urldate = {2022-10-24},
  abstract = {This article examines cognitive process models of human sentence comprehension based on the idea of informed search. These models are rational in the sense that they strive to find a good syntactic analysis quickly. Informed search derives a new account of garden pathing that handles traditional counterexamples. It supports a symbolic explanation for local coherence as well as an algorithmic account of entropy reduction. The models are expressed in a broad framework for theories of human sentence comprehension.},
  langid = {english},
  keywords = {entropy reduction,parsing algorithm,rational analysis,sentence processing}
}

@book{hale.j:2014,
  title = {Automaton Theories of Human Sentence Comprehension},
  author = {Hale, John T.},
  year = {2014},
  month = sep,
  series = {{{CSLI Studies}} in {{Computational Linguistics}}},
  publisher = {{CSLI Publications, Center for the Study of Language and Information}},
  address = {{Stanford, CA}},
  url = {https://csli.sites.stanford.edu/publications/csli-studies-computational-linguistics/automaton-theories-human-sentence-comprehension},
  urldate = {2022-07-01},
  abstract = {Different kinds of grammars may actually be used in models of perceptual processing. By relating grammars to cognitive architecture, John T. Hale shows step-by-step how incremental parsing works and how specific learning rules might lead to frequency-sensitive preferences. Along the way, this book reconsiders garden-pathing, the parallel/serial distinction and information-theoretical complexity metrics, such as surprisal. This book is a must for cognitive scientists of language.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZKXUQCX8/Hale - 2014 - Automaton Theories of Human Sentence Comprehension.pdf}
}

@inproceedings{hall.d:2014,
  title = {On Substance in Phonology},
  booktitle = {Proceedings of the 2014 Annual Conference of the {{Canadian Linguistic Association}}},
  author = {Hall, Daniel Currie},
  year = {2014},
  date-added = {2019-06-17 08:36:30 -0400},
  date-modified = {2019-06-17 08:37:21 -0400},
  keywords = {substance free phonology}
}

@article{halle.m:1994,
  title = {Some Key Features of {{Distributed Morphology}}},
  author = {Halle, Morris and Marantz, Alec},
  year = {1994},
  journal = {MIT working papers in linguistics},
  volume = {21},
  number = {275},
  pages = {88},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@misc{hamilton.w:2017,
  title = {Representation Learning on Graphs: {{Methods}} and Applications},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2017},
  eprint = {1709.05584},
  primaryclass = {cs.SI},
  archiveprefix = {arxiv},
  date-added = {2021-08-03 10:09:17 -0400},
  date-modified = {2021-08-03 10:10:59 -0400},
  project = {syntactic embedding},
  keywords = {graph embedding}
}

@inproceedings{hao.y:2020,
  title = {Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling},
  shorttitle = {Probabilistic Predictions of People Perusing},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Hao, Yiding and Mendelsohn, Simon and Sterneck, Rachel and Martinez, Randi and Frank, Robert},
  year = {2020},
  month = nov,
  pages = {75--86},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.cmcl-1.10},
  url = {https://aclanthology.org/2020.cmcl-1.10},
  urldate = {2022-10-13},
  abstract = {By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model's ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell's analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.},
  keywords = {psychometrics},
  file = {/Users/j/Zotero/storage/5QAKLKT3/Hao et al. (2020) Probabilistic Predictions of People Perusing Eval.pdf}
}

@incollection{harb.b:2005,
  title = {Approximating the Best-Fit Tree under {{L}} p Norms},
  booktitle = {Approximation, Randomization and Combinatorial Optimization. {{Algorithms}} and Techniques},
  author = {Harb, Boulos and Kannan, Sampath and McGregor, Andrew},
  year = {2005},
  pages = {123--133},
  publisher = {{Springer}},
  date-added = {2019-07-17 17:56:30 -0400},
  date-modified = {2019-07-17 17:56:53 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{harley.h:2002,
  title = {Person and Number in Pronouns: {{A}} Feature-Geometric Analysis},
  author = {Harley, Heidi and Ritter, Elizabeth},
  year = {2002},
  journal = {Language},
  volume = {78},
  number = {3},
  pages = {482--526},
  publisher = {{Linguistic Society of America}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:21:59 -0400},
  project = {Icelandic gluttony},
  keywords = {feature geometry,phi features,pronouns}
}

@inproceedings{harremoes.p:2007,
  title = {The Information Bottleneck Revisited or How to Choose a Good Distortion Measure},
  booktitle = {2007 {{IEEE}} International Symposium on Information Theory},
  author = {Harremo{\"e}s, Peter and Tishby, Naftali},
  year = {2007},
  pages = {566--570},
  doi = {10.1109/ISIT.2007.4557285},
  url = {https://doi.org/10.1109/ISIT.2007.4557285},
  date-added = {2019-05-15 00:02:01 -0400},
  date-modified = {2020-07-22 11:17:35 -0400},
  organization = {{IEEE}},
  project = {syntactic embedding},
  keywords = {information bottleneck,KL divergence},
  file = {/Users/j/Zotero/storage/8XQF8FIW/HarremoÃ«s and Tishby - 2007 - The information bottleneck revisited or how to cho.pdf}
}

@inproceedings{hartmann.j:2016,
  title = {Evading Agreement: {{A}} New Perspective on Low Nominative Agreement in {{Icelandic}}},
  booktitle = {Proceedings of the 46th Annual Meeting of the North East Linguistic Society ({{NELS}})},
  author = {Hartmann, Jutta M and Heycock, Caroline},
  editor = {Hammerly, Christopher and Prickett, Brandon},
  year = {2016},
  volume = {2},
  pages = {67--80},
  publisher = {{GLSA Publications}},
  date-added = {2020-01-22 18:01:44 -0500},
  date-modified = {2020-02-01 19:41:52 -0500},
  project = {Icelandic gluttony},
  keywords = {invisible dative,low nominative}
}

@article{hartmann.j:2022,
  title = {Person Effects in Agreement with {{Icelandic}} Low Nominatives: {{An}} Experimental Investigation},
  shorttitle = {Person Effects in Agreement with {{Icelandic}} Low Nominatives},
  author = {Hartmann, Jutta M. and Heycock, Caroline},
  year = {2022},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  issn = {1573-0859},
  doi = {10.1007/s11049-022-09564-z},
  url = {https://doi.org/10.1007/s11049-022-09564-z},
  urldate = {2023-01-12},
  abstract = {This paper investigates agreement\textemdash in particular person agreement\textemdash in two configurations in Icelandic where there are two potential controllers of agreement and where at least in some cases agreement is with the lower of the two (a~``low nominative''). One case is the Dative-Nominative construction, where there is a dative subject and a lower nominative argument. The other is the Specificational Copular Clause (SCC) construction, where there are two nominative arguments. A much-discussed aspect of agreement in the former case is that agreement in number with the low nominative is generally possible, but agreement in person is at best highly restricted, leading in some cases to ineffability. This person effect has been claimed to be ameliorated by syncretism in the agreement paradigm, but there is limited data available substantiating this effect, which is however crucial to deciding between two recent types of account. This paper reports on a pair of experimental rating studies on the Dat-Nom and SCC configurations in Icelandic. We show that, taken together, the two sets of data provide evidence against the Person Licensing Condition and in favour of an account of the Dat-Nom construction in terms of morphological conflict arising from double agreement, although we show that the ameliorating effect of morphological syncretism, while real, is limited. Further, we show that there is no evidence of double agreement in the copular clauses investigated. We argue that full agreement with the low nominative here arises if the first nominal can move out of the domain of agreement entirely. The possibility of agreement with the initial nominal we suggest indicates that nominatives, unlike datives, cause the search of the agreement probe to terminate.},
  langid = {english},
  keywords = {Agreement,Copula,hierarchy effects,icelandic,Icelandic,Person Case Constraint,Person Licensing Condition,Syncretism},
  file = {/Users/j/Zotero/storage/9TLPWE8I/Hartmann and Heycock (2022) Person effects in agreement with Icelandic low nom.pdf}
}

@book{hastie.t:1990,
  title = {Generalized Additive Models},
  author = {Hastie, T.J. and Tibshirani, R.J.},
  year = {1990},
  month = oct,
  publisher = {{Routledge}},
  doi = {10.1201/9780203753781},
  url = {https://doi.org/10.1201%2F9780203753781},
  bdsk-url-2 = {https://doi.org/10.1201/9780203753781},
  date-added = {2022-01-05 22:06:57 -0500},
  date-modified = {2022-01-05 22:10:25 -0500}
}

@article{havelka.j:2007,
  title = {Mathematical Properties of Dependency Trees and Their Application to Natural Language Syntax},
  author = {Havelka, Ji{\v r}{\'i}},
  year = {2007},
  publisher = {{Univerzita Karlova, Matematicko-fyzik\'aln\'i fakulta}},
  date-added = {2020-02-26 18:33:58 -0500},
  date-modified = {2020-02-26 18:36:25 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projectivity}
}

@inproceedings{heafield.k:2011,
  title = {{{KenLM}}: Faster and Smaller Language Model Queries},
  shorttitle = {Kenlm},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{Statistical Machine Translation}}},
  author = {Heafield, Kenneth},
  year = {2011},
  month = jul,
  pages = {187--197},
  publisher = {{Association for Computational Linguistics}},
  address = {{Edinburgh, Scotland}},
  url = {https://aclanthology.org/W11-2123},
  urldate = {2023-03-03}
}

@article{heathcote.a:2012,
  title = {Linear Deterministic Accumulator Models of Simple Choice},
  author = {Heathcote, Andrew and Love, Jonathon},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00292},
  url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2012.00292/abstract},
  urldate = {2022-08-13},
  file = {/Users/j/Zotero/storage/APYGQJTQ/Heathcote and Love - 2012 - Linear Deterministic Accumulator Models of Simple .pdf}
}

@article{heilbron.m:2022,
  title = {A Hierarchy of Linguistic Predictions during Natural Language Comprehension},
  author = {Heilbron, Micha and Armeni, Kristijan and Schoffelen, Jan-Mathijs and Hagoort, Peter and {de Lange}, Floris P.},
  year = {2022},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {32},
  pages = {e2201968119},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2201968119},
  url = {http://www.pnas.org/doi/full/10.1073/pnas.2201968119},
  urldate = {2022-10-29},
  abstract = {Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analyzing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable neural signatures of predictions about syntactic category (parts of speech), phonemes, and semantics. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.}
}

@book{heim.i:1998,
  title = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {1998},
  publisher = {{Blackwell}},
  date-added = {2019-05-19 21:59:01 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {0-631-19712-5},
  keywords = {semantics}
}

@incollection{heim.i:2000,
  title = {Semantics in Generative Grammar},
  booktitle = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {2000},
  publisher = {{Blackwell Publishing}},
  address = {{Malden, MA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:16:57 -0400},
  keywords = {Semantics}
}

@inproceedings{hewitt.j:2019,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  pages = {4129--4138},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1419},
  url = {https://www.aclweb.org/anthology/N19-1419},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1419}
}

@inproceedings{hewitt.j:2019selectivity,
  title = {Designing and Interpreting Probes with Control Tasks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Hewitt, John and Liang, Percy},
  year = {2019},
  pages = {2733--2743},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1275},
  url = {https://www.aclweb.org/anthology/D19-1275},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1275}
}

@article{hidaka.s:2013,
  title = {A Computational Model Associating Learning Process, Word Attributes, and Age of Acquisition},
  author = {Hidaka, Shohei},
  year = {2013},
  month = nov,
  journal = {PLOS ONE},
  volume = {8},
  number = {11},
  pages = {e76242},
  publisher = {{Public Library of Science}},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076242},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0076242},
  urldate = {2022-09-28},
  abstract = {We propose a new model-based approach linking word learning to the age of acquisition (AoA) of words; a new computational tool for understanding the relationships among word learning processes, psychological attributes, and word AoAs as measures of vocabulary growth. The computational model developed describes the distinct statistical relationships between three theoretical factors underpinning word learning and AoA distributions. Simply put, this model formulates how different learning processes, characterized by change in learning rate over time and/or by the number of exposures required to acquire a word, likely result in different AoA distributions depending on word type. We tested the model in three respects. The first analysis showed that the proposed model accounts for empirical AoA distributions better than a standard alternative. The second analysis demonstrated that the estimated learning parameters well predicted the psychological attributes, such as frequency and imageability, of words. The third analysis illustrated that the developmental trend predicted by our estimated learning parameters was consistent with relevant findings in the developmental literature on word learning in children. We further discuss the theoretical implications of our model-based approach.},
  langid = {english},
  keywords = {Children,Learning,Learning curves,Linguistic morphology,Semantics,Social psychology,Statistical distributions,Vocabulary},
  file = {/Users/j/Zotero/storage/3SSR2NIZ/Hidaka (2013) A Computational Model Associating Learning Process.pdf}
}

@inproceedings{ho.j:2020,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/hash/4c5bcfec8584af0d967f1ab10179ca4b-Abstract.html},
  urldate = {2022-07-07},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/Users/j/Zotero/storage/JKXPLCHC/Ho et al. - 2020 - Denoising Diffusion Probabilistic Models.pdf}
}

@article{hochreiter.s:1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/neco.1997.9.8.1735},
  url = {https://doi.org/10.1162%2Fneco.1997.9.8.1735},
  bdsk-url-2 = {https://doi.org/10.1162/neco.1997.9.8.1735},
  date-added = {2021-12-01 18:30:30 -0500},
  date-modified = {2021-12-01 18:30:32 -0500}
}

@book{hodges.w:2008,
  title = {Model Theory},
  author = {Hodges, Wilfrid},
  year = {2008},
  month = jun,
  edition = {1st edition},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  isbn = {978-0-521-06636-5},
  langid = {english}
}

@article{hoffman.m:2013,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {40},
  pages = {1303--1347},
  issn = {1533-7928},
  url = {http://jmlr.org/papers/v14/hoffman13a.html},
  urldate = {2022-06-30},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {/Users/j/Zotero/storage/N8K45R6U/Hoffman et al. - 2013 - Stochastic Variational Inference.pdf}
}

@incollection{hofmann.m:2017,
  title = {Benchmarking N-Grams, Topic Models and Recurrent Neural Networks by Cloze Completions, Eegs and Eye Movements},
  booktitle = {Cognitive {{Approach}} to {{Natural Language Processing}}},
  author = {Hofmann, Markus J. and Biemann, Chris and Remus, Steffen},
  year = {2017},
  pages = {197--215},
  publisher = {{Elsevier}},
  doi = {10.1016/B978-1-78548-253-3.50010-X},
  url = {https://linkinghub.elsevier.com/retrieve/pii/B978178548253350010X},
  urldate = {2022-08-28},
  isbn = {978-1-78548-253-3},
  langid = {english}
}

@article{hofmann.m:2022,
  title = {Language Models Explain Word Reading Times Better than Empirical Predictability},
  author = {Hofmann, Markus J. and Remus, Steffen and Biemann, Chris and Radach, Ralph and Kuchinke, Lars},
  year = {2022},
  month = feb,
  journal = {Frontiers in Artificial Intelligence},
  volume = {4},
  pages = {730570},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.730570},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2021.730570/full},
  urldate = {2022-08-28},
  abstract = {Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.},
  file = {/Users/j/Zotero/storage/VW34WVNF/Hofmann et al. - 2022 - Language Models Explain Word Reading Times Better .pdf}
}

@article{holly.j:2001,
  title = {Pictures of Ultrametric Spaces, the p-Adic Numbers, and Valued Fields},
  author = {Holly, Jan E},
  year = {2001},
  journal = {The American Mathematical Monthly},
  volume = {108},
  number = {8},
  pages = {721--728},
  publisher = {{Taylor \& Francis}},
  date-added = {2019-06-11 08:49:52 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry,ultrametric}
}

@article{holmberg.a:2003,
  title = {Agreement and Movement in {{Icelandic}} Raising Constructions},
  author = {Holmberg, Anders and Hr{\'o}arsd{\'o}ttir, {\TH}orbj{\"o}rg},
  year = {2003},
  journal = {Lingua. International review of general linguistics. Revue internationale de linguistique g\'en\'erale},
  volume = {113},
  number = {10},
  pages = {997--1019},
  publisher = {{Elsevier}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:24:18 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@inproceedings{hoogeboom.e:2021,
  title = {Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
  year = {2021},
  url = {https://openreview.net/forum?id=6nbpPqUCIi7},
  date-added = {2022-03-31 10:59:19 -0400},
  date-modified = {2022-03-31 11:00:13 -0400},
  keywords = {denoising}
}

@phdthesis{hoover.i:2022phd,
  title = {Effective Equidistribution on {{Hilbert}} Modular Varieties},
  author = {Hoover, Ian},
  year = {2022},
  month = aug,
  doi = {2345/bc-ir:109520},
  url = {http://dlib.bc.edu/islandora/object/bc-ir:109520},
  urldate = {2022-09-03},
  abstract = {We compute effective error rates for the equidistribution of translates of diagonal orbits on Hilbert modular varieties. The translation is determined by \$n\$ real parameters and our results require the assumption that all parameters are non-zero. The error rate is given in explicit polynomial terms of the translation parameters and Sobolev type norms of the test functions. The effective equidistribution is applied to give counting estimates for binary quadratic forms of square discriminant over real number rings.},
  langid = {english},
  school = {Boston College},
  file = {/Users/j/Zotero/storage/MIHBUTMM/Hoover - 2022 - Effective Equidistribution on Hilbert Modular Vari.pdf}
}

@misc{hoover.j:2020wccflhandout,
  type = {Handout},
  title = {Accounting for Variation in Number Agreement in {{Icelandic DAT-NOM}} Constructions},
  author = {Hoover, Jacob Louis},
  year = {2020},
  month = mar,
  doi = {10.14288/1.0389856},
  url = {https://doi.library.ubc.ca/10.14288/1.0389856},
  howpublished = {WCCFL2020 Talk handout},
  peerreview = {no},
  project = {Icelandic gluttony},
  keywords = {agreement,feature geometry}
}

@misc{hoover.j:2021arxiv,
  title = {Linguistic Dependencies and Statistical Dependence},
  author = {Hoover, Jacob Louis and Sordoni, Alessandro and Du, Wenyu and O'Donnell, Timothy J.},
  year = {2021},
  eprint = {2104.08685},
  primaryclass = {cs, math},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2104.08685},
  url = {http://arxiv.org/abs/2104.08685},
  urldate = {2022-05-18},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \$\textbackslash approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  archiveprefix = {arxiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {/Users/j/Zotero/storage/UXJMS7PY/Hoover et al. - 2021 - Linguistic Dependencies and Statistical Dependence.pdf}
}

@inproceedings{hoover.j:2021emnlp,
  title = {Linguistic Dependencies and Statistical Dependence},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hoover, Jacob Louis and Du, Wenyu and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2021},
  month = nov,
  pages = {2941--2963},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.234},
  url = {https://aclanthology.org/2021.emnlp-main.234},
  urldate = {2022-05-19},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \$\textbackslash approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {db},
  keywords = {dependency grammar,linguistic dependencies},
  file = {/Users/j/Zotero/storage/CR64LXWY/Hoover et al. (2021) Linguistic dependencies and statistical dependence.pdf}
}

@inproceedings{hoover.j:2021wccfl,
  title = {Accounting for Variation in Number Agreement in Icelandic Dative-Nominative Constructions},
  booktitle = {Proceedings of the 38th {{West Coast Conference}} on {{Formal Linguistics}}},
  author = {Hoover, Jacob Louis},
  editor = {Soo, Rachel and Chow, Una Y. and Nederveen, Sander},
  year = {2021},
  month = jun,
  pages = {231--241},
  publisher = {{Cascadilla Proceedings Project}},
  address = {{Somerville, Mass., USA}},
  url = {http://www.lingref.com/cpp/wccfl/38/abstract3568.html},
  abstract = {Icelandic dative-nominative constructions exhibit a syntactic hierarchy effect known as the Person Restriction: only third person nominatives may control agreement. In these constructions, there is variation between speakers in the extent to which the verb agrees with the nominative for number. Sigur\dh sson \& Holmberg (2008) explain this variation as arising due to differences between varieties in the timing of subject raising, using a split phi-probe. This paper revises their approach, using the feature gluttony mechanism for Agree developed in Coon \& Keine (2020), and a split phi-probe in which person probing precedes number probing. Within this framework, the observed variation can be captured by allowing variability two independent parameters: the timing of EPP subject raising, and the visibility of a number feature on dative DPs. The proposed mechanism describes the variation, including predicting the observed optional agreement in certain cases that previous literature had struggled to account for, and makes additional predictions about the differences between varieties in cases of syncretism within the verbal paradigm. An investigation into these predictions should allow this already well-studied area of Icelandic grammar to continue to be a useful test-case for crosslinguistic assumptions about the mechanism of Agree, and the status of dative arguments.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {no},
  annotation = {Note: www.lingref.com, document 3568},
  file = {/Users/j/Zotero/storage/XA3QY83Q/Hoover (2021) Accounting for variation in number agreement in Ic.pdf}
}

@misc{hoover.j:2022amlap,
  type = {Poster},
  title = {With Better Language Models, Processing Time Is Superlinear in Surprisal},
  author = {Hoover, Jacob Louis},
  year = {2022},
  month = sep,
  address = {{York, England}},
  url = {https://virtual.oxfordabstracts.com/#/event/3067/submission/297},
  collaborator = {Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  copyright = {All rights reserved},
  peerreview = {no}
}

@misc{hoover.j:2022psyarxiv,
  title = {The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/qjnpv},
  abstract = {Words that are more surprising given context take longer to process. However, no incremental parsing algorithm has been shown to directly predict this phenomenon. In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisal---sampling algorithms. Our first contribution is to show that simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase. These two predictions stand in contrast with literature on surprisal theory (Hale, 2001; Levy, 2008), which assumes that the expected processing cost increases linearly with surprisal, and makes no prediction about variance. In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal.  We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.},
  copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
  langid = {american},
  peerreview = {no},
  keywords = {Linguistics,parsing algorithms,sampling,sentence processing,Social and Behavioral Sciences,surprisal},
  file = {/Users/j/Zotero/storage/M5U2JW3U/2022-11-15-psyarxiv-v2.pdf;/Users/j/Zotero/storage/VJCZIBRE/2023-03-22-psyarxiv-v3.pdf}
}

@article{hoyer.p:2004,
  title = {Non-Negative Matrix Factorization with Sparseness Constraints},
  author = {Hoyer, Patrik O},
  year = {2004},
  journal = {Journal of machine learning research},
  volume = {5},
  number = {Nov},
  pages = {1457--1469},
  url = {http://www.jmlr.org/papers/v5/hoyer04a.html},
  date-added = {2020-07-26 15:09:12 -0400},
  date-modified = {2020-07-26 15:10:47 -0400},
  project = {syntactic embedding},
  keywords = {sparseness}
}

@misc{htut.p:2019,
  title = {Do Attention Heads in {{BERT}} Track Syntactic Dependencies?},
  author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
  year = {2019},
  eprint = {1911.12246},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{hu.j:2020systematic,
  title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
  year = {2020},
  pages = {1725--1744},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.158},
  url = {https://www.aclweb.org/anthology/2020.acl-main.158},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.158}
}

@misc{hu.j:2023arxiv,
  title = {Expectations over Unspoken Alternatives Predict Pragmatic Inferences},
  author = {Hu, Jennifer and Levy, Roger and Degen, Judith and Schuster, Sebastian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04758},
  eprint = {2304.04758},
  primaryclass = {cs},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2304.04758},
  url = {http://arxiv.org/abs/2304.04758},
  urldate = {2023-04-28},
  abstract = {Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable -- both within instances of a single scale, and across different scales -- there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/INX5G4Y6/Hu et al. (2023) Expectations over Unspoken Alternatives Predict Pr.pdf}
}

@inproceedings{hu.x:2021,
  title = {{{R2D2}}: Recursive Transformer Based on Differentiable Tree for Interpretable Hierarchical Language Modeling},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and {de Melo}, Gerard},
  year = {2021},
  month = aug,
  pages = {4897--4908},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.379},
  url = {https://aclanthology.org/2021.acl-long.379},
  abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.379},
  date-added = {2022-04-28 10:19:40 -0400},
  date-modified = {2022-04-28 10:19:58 -0400},
  keywords = {chart parsing,pruning}
}

@misc{hu.x:2022,
  title = {Fast-{{R2D2}}: A Pretrained Recursive Neural Network Based on Pruned {{CKY}} for Grammar Induction and Text Representation},
  author = {Hu, Xiang and Mi, Haitao and Li, Liang and {de Melo}, Gerard},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2203.00281},
  url = {https://arxiv.org/abs/2203.00281},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.00281},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-28 10:20:35 -0400},
  date-modified = {2022-04-28 10:20:59 -0400},
  keywords = {chart parsing,pruning,sampling},
  file = {/Users/j/Zotero/storage/DQPTD2F6/Hu et al. - 2022 - Fast-R2D2 A pretrained recursive neural network b.pdf}
}

@inproceedings{huang.c:2021,
  title = {A Variational Perspective on Diffusion-Based Generative Models and Score Matching},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron C},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {22863--22876},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2021/hash/c11abfd29e4d9b4d4b566b01114d8486-Abstract.html},
  file = {/Users/j/Zotero/storage/25BH7NUL/Huang et al. - 2021 - A variational perspective on diffusion-based gener.pdf}
}

@misc{huang.k:2022HSP,
  type = {Talk},
  title = {{{SPR}} Mega-Benchmark Shows Surprisal Tracks Construction- but Not Item-Level Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2022},
  address = {{Santa Cruz, California, USA}},
  keywords = {eye-tracking,surprisal},
  file = {/Users/j/Zotero/storage/EJF69NL4/Huang et al. (2022) SPR mega-benchmark shows surprisal tracks construc.pdf;/Users/j/Zotero/storage/HQ2EBQ6S/Huang et al. (2022) SPR mega-benchmark shows surprisal tracks construc.pdf}
}

@misc{huang.k:2023psyarxiv,
  title = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty: Evidence from a Large-Scale Benchmark},
  shorttitle = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2023},
  month = apr,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/z38u6},
  url = {https://psyarxiv.com/z38u6/},
  urldate = {2023-04-22},
  abstract = {Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences - one of the major concerns of psycholinguistics - be explained by predictability, as estimated using computational language models? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. We find that the predictions of language models with two different architectures sharply diverge from the reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that prediction is most likely insufficient on its own to explain human syntactic processing.},
  langid = {american},
  keywords = {Language models,Linguistics,Prediction,Psycholinguistics and Neurolinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {/Users/j/Zotero/storage/7XSTH6PP/Huang et al. (2023) Surprisal does not explain syntactic disambiguatio.pdf}
}

@phdthesis{huang.l:2008phd,
  title = {Forest-Based Algorithms in Natural Language Processing},
  author = {Huang, Liang},
  year = {2008},
  url = {https://www.proquest.com/docview/304495357},
  date-added = {2022-03-31 09:58:59 -0400},
  date-modified = {2022-04-26 21:20:55 -0400},
  school = {University of Pennsylvania},
  keywords = {parsing},
  file = {/Users/j/Zotero/storage/FI56RTQ3/Huang - 2008 - Forest-based algorithms in natural language proces.pdf}
}

@inproceedings{huang.l:2010,
  title = {Dynamic Programming for Linear-Time Incremental Parsing},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Huang, Liang and Sagae, Kenji},
  year = {2010},
  month = jul,
  pages = {1077--1086},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/P10-1110},
  date-added = {2022-04-26 21:01:34 -0400},
  date-modified = {2022-04-26 21:02:08 -0400},
  keywords = {dependency parsing,dynamic programming}
}

@article{huddleston.r:2002,
  title = {The Cambridge Grammar of English},
  author = {Huddleston, Rodney and Pullum, Geoffrey K and others},
  year = {2002},
  journal = {Language. Cambridge: Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{hudson.r:1984,
  title = {Word Grammar},
  author = {Hudson, Richard A},
  year = {1984},
  publisher = {{Blackwell Oxford}},
  url = {https://archive.org/details/wordgrammar0000huds},
  date-added = {2021-07-17 10:26:33 -0400},
  date-modified = {2021-07-17 10:48:55 -0400}
}

@article{hughes.b:2004,
  title = {Trees and Ultrametric Spaces: A Categorical Equivalence},
  author = {Hughes, Bruce},
  year = {2004},
  journal = {Advances in Mathematics},
  volume = {189},
  number = {1},
  pages = {148--191},
  publisher = {{Elsevier}},
  url = {https://doi.org/10.1016/j.aim.2003.11.008},
  date-added = {2019-07-10 18:13:04 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{huijben.i:2022,
  title = {A Review of the {{Gumbel-max}} Trick and Its Extensions for Discrete Stochasticity in Machine Learning},
  author = {Huijben, Iris A.M. and Kool, Wouter and Paulus, Max Benedikt and Sloun, Ruud JG Van},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tpami.2022.3157042},
  url = {https://doi.org/10.1109%2Ftpami.2022.3157042},
  bdsk-url-2 = {https://doi.org/10.1109/tpami.2022.3157042},
  date-added = {2022-04-10 19:20:21 -0400},
  date-modified = {2022-04-10 19:26:14 -0400},
  file = {/Users/j/Zotero/storage/PGXVRCZU/Huijben et al. - 2022 - A review of the gumbel-max trick and its extension.pdf}
}

@article{hupkes.d:2018,
  title = {Visualisation and'diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure},
  author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  year = {2018},
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {907--926},
  date-added = {2019-06-17 18:51:15 -0400},
  date-modified = {2019-06-17 18:52:12 -0400},
  project = {syntactic embedding},
  keywords = {implicit information probing}
}

@inproceedings{hwa.r:1999,
  title = {Supervised Grammar Induction Using Training Data with Limited Constituent Information},
  booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics},
  author = {Hwa, Rebecca},
  year = {1999},
  pages = {73--79},
  publisher = {{Association for Computational Linguistics}},
  address = {{College Park, Maryland, USA}},
  doi = {10.3115/1034678.1034699},
  url = {https://www.aclweb.org/anthology/P99-1010},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034699}
}

@article{itti.l:2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  year = {2009},
  month = jun,
  journal = {Vision Research},
  series = {Visual {{Attention}}: {{Psychophysics}}, Electrophysiology and Neuroimaging},
  volume = {49},
  number = {10},
  pages = {1295--1306},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2008.09.007},
  url = {https://www.sciencedirect.com/science/article/pii/S0042698908004380},
  urldate = {2022-08-07},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  langid = {english},
  keywords = {Attention,Bayes theorem,Eye movements,Free viewing,Information theory,Natural vision,Novelty,Saliency,Surprise},
  file = {/Users/j/Zotero/storage/PBY9SBAG/Itti and Baldi - 2009 - Bayesian surprise attracts human attention.pdf}
}

@incollection{jackendoff.r:2002,
  title = {Foundations of Language},
  booktitle = {Foundations of Language},
  author = {Jackendoff, Ray},
  year = {2002},
  publisher = {{Oxford University Press}},
  address = {{New York}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:52 -0400},
  group = {Cognitive Science, Language},
  readinglist = {Thesis},
  keywords = {Parallel Architecture}
}

@phdthesis{jaeger.t:2006,
  title = {Redundancy and Syntactic Reduction in Spontaneous Speech},
  author = {Jaeger, Tim Florian},
  year = {2006},
  school = {Stanford University Stanford, CA},
  file = {/Users/j/Zotero/storage/YDQHTXHN/Jaeger (2006) Redundancy and syntactic reduction in spontaneous .pdf}
}

@article{jager.g:2012,
  title = {Formal Language Theory: Refining the {{Chomsky}} Hierarchy},
  shorttitle = {Formal Language Theory},
  author = {J{\"a}ger, Gerhard and Rogers, James},
  year = {2012},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  number = {1598},
  pages = {1956--1970},
  publisher = {{Royal Society}},
  doi = {10.1098/rstb.2012.0077},
  url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0077},
  urldate = {2023-03-09},
  abstract = {The first part of this article gives a brief overview of the four levels of the Chomsky hierarchy, with a special emphasis on context-free and regular languages. It then recapitulates the arguments why neither regular nor context-free grammar is sufficiently expressive to capture all phenomena in the natural language syntax. In the second part, two refinements of the Chomsky hierarchy are reviewed, which are both relevant to the extant research in cognitive science: the mildly context-sensitive languages (which are located between context-free and context-sensitive languages), and the sub-regular hierarchy (which distinguishes several levels of complexity within the class of regular languages).},
  keywords = {artificial grammar learning,complexity,formal language theory},
  file = {/Users/j/Zotero/storage/8YWPUE6I/JÃ¤ger and Rogers (2012) Formal language theory refining the Chomsky hiera.pdf}
}

@article{jager.l:2015,
  title = {Retrieval Interference in Reflexive Processing: Experimental Evidence from {{Mandarin}}, and Computational Modeling},
  shorttitle = {Retrieval Interference in Reflexive Processing},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2015.00617},
  urldate = {2022-10-12},
  abstract = {We conducted two eye-tracking experiments investigating the processing of the Mandarin reflexive ziji in order to tease apart structurally constrained accounts from standard cue-based accounts of memory retrieval. In both experiments, we tested whether structurally inaccessible distractors that fulfill the animacy requirement of ziji influence processing times at the reflexive. In Experiment 1, we manipulated animacy of the antecedent and a structurally inaccessible distractor intervening between the antecedent and the reflexive. In conditions where the accessible antecedent mismatched the animacy cue, we found inhibitory interference whereas in antecedent-match conditions, no effect of the distractor was observed. In Experiment 2, we tested only antecedent-match configurations and manipulated locality of the reflexive-antecedent binding (Mandarin allows non-local binding). Participants were asked to hold three distractors (animate vs. inanimate nouns) in memory while reading the target sentence. We found slower reading times when animate distractors were held in memory (inhibitory interference). Moreover, we replicated the locality effect reported in previous studies. These results are incompatible with structure-based accounts. However, the cue-based ACT-R model of Lewis and Vasishth (2005) cannot explain the observed pattern either. We therefore extend the original ACT-R model and show how this model not only explains the data presented in this article, but is also able to account for previously unexplained patterns in the literature on reflexive processing.},
  keywords = {ACT-R},
  file = {/Users/j/Zotero/storage/W9IFD7Q4/JÃ¤ger et al. (2015) Retrieval interference in reflexive processing ex.pdf}
}

@article{jager.l:2017,
  title = {Similarity-Based Interference in Sentence Comprehension: {{Literature}} Review and {{Bayesian}} Meta-Analysis},
  shorttitle = {Similarity-Based Interference in Sentence Comprehension},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2017},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {94},
  pages = {316--339},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X17300049},
  urldate = {2023-01-08},
  abstract = {We report a comprehensive review of the published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies. We also provide a quantitative random-effects meta-analysis of eyetracking and self-paced reading studies. We show that the empirical evidence is only partly consistent with cue-based retrieval as implemented in the ACT-R-based model of sentence processing by Lewis and Vasishth (2005) (LV05) and that there are important differences between the reviewed dependency types. In non-agreement subject-verb dependencies, there is evidence for inhibitory interference in configurations where the correct dependent fully matches the retrieval cues. This is consistent with the LV05 cue-based retrieval account. By contrast, in subject-verb agreement as well as in reflexive-/reciprocal-antecedent dependencies, no evidence for inhibitory interference is found in configurations with a fully cue-matching subject/antecedent. In configurations with only a partially cue-matching subject or antecedent, the meta-analysis reveals facilitatory interference in subject-verb agreement and inhibitory interference in reflexives/reciprocals. The former is consistent with the LV05 account, but the latter is not. Moreover, the meta-analysis reveals that (i) interference type (proactive versus retroactive) leads to different effects in the reviewed dependency types and (ii) the prominence of the distractor strongly influences the interference effect. In sum, the meta-analysis suggests that the LV05 needs important modifications to account for the unexplained interference patterns and the differences between the dependency types. More generally, the meta-analysis provides a quantitative empirical basis for comparing the predictions of competing accounts of retrieval processes in sentence comprehension.},
  langid = {english},
  keywords = {Agreement,Bayesian meta-analysis,Cue-based retrieval,Interference,Reflexives,Syntactic dependency processing},
  file = {/Users/j/Zotero/storage/R8S5G867/JÃ¤ger et al. (2017) Similarity-based interference in sentence comprehe.pdf}
}

@misc{jakulin.a:2003,
  title = {Quantifying and Visualizing Attribute Interactions},
  author = {Jakulin, Aleks and Bratko, Ivan},
  year = {2003},
  eprint = {cs/0308002},
  archiveprefix = {arxiv},
  date-added = {2021-07-19 22:11:58 -0400},
  date-modified = {2021-07-19 22:12:25 -0400},
  keywords = {co-information,interaction information,multivariate mututal information}
}

@inproceedings{jang.e:2017,
  title = {Categorical Reparameterization with {{Gumbel-softmax}}},
  booktitle = {5th International Conference on Learning Representations, Conference Track Proceedings},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  month = apr,
  publisher = {{OpenReview.net}},
  address = {{Toulon, France}},
  url = {https://openreview.net/forum?id=rkE3y85ee},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  date-added = {2022-04-10 19:10:47 -0400},
  date-modified = {2022-04-10 19:20:19 -0400},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200}
}

@article{jarnik.v:1930,
  title = {{O jist\'em probl\'emu minim\'aln\'im. (Z dopisu panu O. Bor\u{u}vkovi) [On a certain problem of minimization. (From a letter to Mr. O. Bor\u{u}vka).]}},
  author = {Jarn{\'i}k, Vojt{\v e}ch},
  year = {1930},
  journal = {Pr\'ace moravsk\'e p\v{r}\'irodov\v{e}deck\'e spole\v{c}nosti},
  volume = {6},
  number = {4},
  pages = {57--63},
  doi = {10338.dmlcz/500726},
  urldate = {2021-02-05},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  langid = {czech}
}

@misc{jasra.a:2013,
  title = {The Alive Particle Filter},
  author = {Jasra, Ajay and Lee, Anthony and Yau, Christopher and Zhang, Xiaole},
  year = {2013},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1304.0151},
  url = {https://arxiv.org/abs/1304.0151},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1304.0151},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:49:28 -0400},
  date-modified = {2022-05-05 09:49:46 -0400},
  keywords = {particle filtering},
  file = {/Users/j/Zotero/storage/VLKBETLS/Jasra et al. - 2013 - The alive particle filter.pdf}
}

@article{jelinek.f:1976,
  title = {Continuous Speech Recognition by Statistical Methods},
  author = {Jelinek, F.},
  year = {1976},
  month = apr,
  journal = {Proceedings of the IEEE},
  volume = {64},
  number = {4},
  pages = {532--556},
  issn = {1558-2256},
  doi = {10.1109/PROC.1976.10159},
  abstract = {Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.},
  keywords = {Acoustic devices,Automatic speech recognition,Decoding,Loudspeakers,Natural languages,noisy-channel,Signal processing,Speech processing,Speech recognition,Statistical analysis,Statistics}
}

@inproceedings{jin.l:2020,
  title = {Memory-Bounded Neural Incremental Parsing for Psycholinguistic Prediction},
  booktitle = {Proceedings of the 16th International Conference on Parsing Technologies and the {{IWPT}} 2020 Shared Task on Parsing into Enhanced Universal Dependencies},
  author = {Jin, Lifeng and Schuler, William},
  year = {2020},
  month = jul,
  pages = {48--61},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.iwpt-1.6},
  url = {https://aclanthology.org/2020.iwpt-1.6},
  abstract = {Syntactic surprisal has been shown to have an effect on human sentence processing, and can be predicted from prefix probabilities of generative incremental parsers. Recent state-of-the-art incremental generative neural parsers are able to produce accurate parses and surprisal values but have unbounded stack memory, which may be used by the neural parser to maintain explicit in-order representations of all previously parsed words, inconsistent with results of human memory experiments. In contrast, humans seem to have a bounded working memory, demonstrated by inhibited performance on word recall in multi-clause sentences (Bransford and Franks, 1971), and on center-embedded sentences (Miller and Isard,1964). Bounded statistical parsers exist, but are less accurate than neural parsers in predict-ing reading times. This paper describes a neural incremental generative parser that is able to provide accurate surprisal estimates and can be constrained to use a bounded stack. Results show that the accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to un-bounded working memory.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.iwpt-1.6},
  date-added = {2021-09-13 19:25:38 -0400},
  date-modified = {2021-09-13 19:25:40 -0400}
}

@book{johnson.d:1980,
  title = {Arc Pair Grammar},
  author = {Johnson, David E. and Postal, Paul M.},
  year = {1980},
  publisher = {{Princeton University Press}},
  address = {{Princeton, New Jersey}},
  area = {Linguistics, Syntax},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{johnson.m:2004,
  title = {A {{TAG-based}} Noisy-Channel Model of Speech Repairs},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Johnson, Mark and Charniak, Eugene},
  year = {2004},
  month = jul,
  pages = {33--39},
  address = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1218960},
  url = {https://aclanthology.org/P04-1005},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218960},
  date-added = {2022-05-03 15:15:11 -0400},
  date-modified = {2022-05-03 15:18:08 -0400},
  keywords = {noisy channel coding,tree adjoining grammars,tree transducers}
}

@phdthesis{johnson.m:2014phd,
  type = {Thesis},
  title = {Bayesian Time Series Models and Scalable Inference},
  author = {Johnson, Matthew James},
  year = {2014},
  url = {https://dspace.mit.edu/handle/1721.1/89993},
  urldate = {2022-07-05},
  abstract = {With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and their Bayesian nonparametric extensions. The HMM is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model (HDP-HMM), have been applied in many settings. HSMMs and HDP-HSMMs extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which HSMM message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference (SVI) framework to develop scalable inference for the HMM, HSMM, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models (LDA), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2014-09-19T21:33:09Z},
  file = {/Users/j/Zotero/storage/7N5NAEDX/Johnson - 2014 - Bayesian time series models and scalable inference.pdf;/Users/j/Zotero/storage/X34XRVGE/Johnson - 2014 - Bayesian time series models and scalable inference.pdf}
}

@article{johnson.s:1967,
  title = {Hierarchical Clustering Schemes},
  author = {Johnson, Stephen C},
  year = {1967},
  journal = {Psychometrika},
  volume = {32},
  number = {3},
  pages = {241--254},
  publisher = {{Springer-Verlag}},
  url = {https://doi.org/10.1007/BF02289588},
  date-added = {2019-06-15 10:38:05 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {1},
  keywords = {hierarchical clustering,ultrametric}
}

@article{jozefowicz.r:2016,
  title = {Exploring the Limits of Language Modeling},
  author = {J{\'o}zefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1602.02410},
  eprint = {1602.02410},
  url = {http://arxiv.org/abs/1602.02410},
  archiveprefix = {arxiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/JozefowiczVSSW16},
  date-added = {2019-06-23 21:20:27 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {convolutional,language modeling,LSTM},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  file = {/Users/j/Zotero/storage/STG35LBG/JÃ³zefowicz et al. - 2016 - Exploring the limits of language modeling.pdf}
}

@article{jurafsky.d:1996,
  title = {A Probabilistic Model of Lexical and Syntactic Access and Disambiguation},
  author = {Jurafsky, Daniel},
  year = {1996},
  journal = {Cognitive Science},
  volume = {20},
  number = {2},
  pages = {137--194},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2002_1},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog2002_1},
  urldate = {2022-07-23},
  abstract = {The problems of access\textemdash retrieving linguistic structure from some mental grammar \textemdash and disambiguation\textemdash choosing among these structures to correctly parse ambiguous linguistic input\textemdash are fundamental to language understanding. The literature abounds with psychological results on lexical access, the access of idioms, syntactic rule access, parsing preferences, syntactic disambiguation, and the processing of garden-path sentences. Unfortunately, it has been difficult to combine models which account for these results to build a general, uniform model of access and disambiguation at the lexical, idiomatic, and syntactic levels. For example, psycholinguistic theories of lexical access and idiom access and parsing theories of syntactic rule access have almost no commonality in methodology or coverage of psycholinguistic data. This article presents a single probabilistic algorithm which models both the access and disambiguation of linguistic knowledge. The algorithm is based on a parallel parser which ranks constructions for access, and interpretations for disambiguation, by their conditional probability. Low-ranked constructions and interpretations are pruned through beam-search; this pruning accounts, among other things, for the garden-path effect. I show that this motivated probabilistic treatment accounts for a wide variety of psycholinguistic results, arguing for a more uniform representation of linguistic knowledge and for the use of probabilistically-enriched grammars and interpreters as models of human knowledge of and processing of language.},
  langid = {english},
  file = {/Users/j/Zotero/storage/ZARYLTGK/Jurafsky - 1996 - A Probabilistic Model of Lexical and Syntactic Acc.pdf}
}

@book{jurafsky.d:2009slp2,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2009},
  edition = {Second},
  publisher = {{Pearson Prentice Hall}},
  url = {https://home.cs.colorado.edu/ martin/slp.html},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kahane.s:1997,
  title = {Bubble Trees and Syntactic Representations},
  booktitle = {Proceedings of Mathematics of Language (Mol5) Meeting},
  author = {Kahane, Sylvain},
  year = {1997},
  pages = {70--76},
  url = {https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.23.7125&rep=rep1&type=pdf},
  date-added = {2021-07-17 10:51:30 -0400},
  date-modified = {2021-07-17 10:52:15 -0400},
  file = {/Users/j/Zotero/storage/UEJTS85H/Kahane - 1997 - Bubble trees and syntactic representations.pdf}
}

@misc{kahardipraja.p:2021,
  title = {Towards {{Incremental Transformers}}: {{An Empirical Analysis}} of {{Transformer Models}} for {{Incremental NLU}}},
  shorttitle = {Towards {{Incremental Transformers}}},
  author = {Kahardipraja, Patrick and Madureira, Brielen and Schlangen, David},
  year = {2021},
  month = sep,
  number = {arXiv:2109.07364},
  eprint = {2109.07364},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2109.07364},
  url = {http://arxiv.org/abs/2109.07364},
  urldate = {2022-05-18},
  abstract = {Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5JDRJ39Y/Kahardipraja et al. - 2021 - Towards Incremental Transformers An Empirical Ana.pdf}
}

@article{kalin.l:2018,
  title = {Licensing and {{Differential Object Marking}}: {{The}} View from {{Neo-Aramaic}}},
  author = {Kalin, Laura},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {2},
  pages = {112--159},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case}
}

@article{kamide.y:1999,
  title = {Incremental Pre-Head Attachment in {{Japanese}} Parsing},
  author = {Kamide, Yuki and Mitchell, Don C.},
  year = {1999},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {5-6},
  pages = {631--662},
  publisher = {{Routledge}},
  issn = {0169-0965},
  doi = {10.1080/016909699386211},
  url = {https://doi.org/10.1080/016909699386211},
  urldate = {2022-10-13},
  abstract = {The present study addresses the question of whether structural analyses of verb-arguments are postponed up until the head verb has been processed (head-driven parsing accounts) or initiated prior to the appearance of the verb (pre-head attachment accounts). To explore this question in relation to a head-final language, a Japanese dative argument attachment ambiguity was examined in both a questionnaire study (Experiment 1) and a self-paced reading test (Experiment 2). The data suggested that the dative argument attachment ambiguity is resolved in the manner predicted by pre-head attachment accounts. The results were incompatible with most variants of the head-driven parsing model, and were not of the form currently predicted by constraint-satisfaction models. We end by discussing the general theoretical implications of the findings.},
  keywords = {eager processing},
  file = {/Users/j/Zotero/storage/RVVYX9B6/Kamide and Mitchell (1999) Incremental Pre-head Attachment in Japanese Parsin.pdf}
}

@article{kamide.y:2008,
  title = {Anticipatory {{Processes}} in {{Sentence Processing}}},
  author = {Kamide, Yuki},
  year = {2008},
  journal = {Language and Linguistics Compass},
  volume = {2},
  number = {4},
  pages = {647--670},
  issn = {1749-818X},
  doi = {10.1111/j.1749-818X.2008.00072.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1749-818X.2008.00072.x},
  urldate = {2022-06-11},
  abstract = {Anticipation is an essential ability for the human cognitive system to survive in its surrounding environment. The present article will review previous research on anticipatory processes in sentence processing (comprehension). I start by pointing out past research carried out with inadequate methods, then move on to reviewing recent research with relatively new, more appropriate methods, specifically, the so-called `visual-world' eye-tracking paradigm, and neuropsychological techniques. I then discuss remaining unresolved issues, both methodological and theoretical.},
  langid = {english},
  file = {/Users/j/Zotero/storage/G4V9J49D/Kamide - 2008 - Anticipatory Processes in Sentence Processing.pdf}
}

@article{kartsaklis.d:2019,
  title = {Linguistic Matrix Theory},
  author = {Kartsaklis, Dimitrios and Ramgoolam, Sanjaye and Sadrzadeh, Mehrnoosh},
  year = {2019},
  journal = {Annales de l'Institut Henri Poincar\'e D},
  publisher = {{European Mathematical Publishing House}},
  issn = {2308-5827},
  url = {http://dx.doi.org/10.4171/aihpd/75},
  date-added = {2019-08-06 08:52:19 +0300},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {physics}
}

@article{katz.j:2019,
  title = {The Phonetics and Phonology of Lenition: A {{Campidanese Sardinian}} Case Study},
  author = {Katz, Jonah and Pitzanti, Gianmarco},
  year = {2019},
  month = sep,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {10},
  number = {1},
  pages = {16},
  publisher = {{Open Library of the Humanities}},
  doi = {10.5334/labphon.184},
  url = {https://doi.org/10.5334%2Flabphon.184},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.184},
  date-added = {2022-05-10 10:57:54 -0400},
  date-modified = {2022-05-10 10:58:06 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/JAUFQPV3/Katz and Pitzanti - 2019 - The phonetics and phonology of lenition A Campida.pdf}
}

@article{kawabata.t:1992,
  title = {The Structure of the {{I-measure}} of a {{Markov}} Chain},
  author = {Kawabata, T. and Yeung, R.W.},
  year = {1992},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {3},
  pages = {1146--1149},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.135658},
  url = {https://doi.org/10.1109%2F18.135658},
  bdsk-url-2 = {https://doi.org/10.1109/18.135658},
  date-added = {2021-09-21 17:47:00 -0400},
  date-modified = {2021-09-21 17:47:01 -0400}
}

@incollection{kay.p:2005,
  title = {Argument Structure Constructions and the {{Argument}}\textendash{{Adjunct}} Distinction},
  booktitle = {Grammatical {{Constructions}}: {{Back}} to the Roots},
  author = {Kay, Paul},
  editor = {Fried, Mirjam and Boas, Hans C.},
  year = {2005},
  volume = {4},
  pages = {71--98},
  publisher = {{John Benjamins Publishing Company}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:32 -0400},
  readinglist = {Adjunction},
  keywords = {Argument/Modifier}
}

@inproceedings{kazantseva.a:2018,
  title = {Kawenn\'on:Nis: The {{Wordmaker}} for {{Kanyen}}'k\'eha},
  shorttitle = {Kawenn\'on:Nis},
  booktitle = {Proceedings of the {{Workshop}} on {{Computational Modeling}} of {{Polysynthetic Languages}}},
  author = {Kazantseva, Anna and Maracle, Owennatekha Brian and Maracle, Ronkwe'tiy{\'o}hstha Josiah and Pine, Aidan},
  year = {2018},
  month = aug,
  pages = {53--64},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico, USA}},
  url = {https://aclanthology.org/W18-4806},
  urldate = {2022-06-04},
  abstract = {In this paper we describe preliminary work on Kawenn\'on:nis, a verb conjugator for Kanyen'k\'eha (Ohsweken dialect). The project is the result of a collaboration between Onkwawenna Kentyohkwa Kanyen'k\'eha immersion school and the Canadian National Research Council's Indigenous Language Technology lab. The purpose of Kawenn\'on:nis is to build on the educational successes of the Onkwawenna Kentyohkwa school and develop a tool that assists students in learning how to conjugate verbs in Kanyen'k\'eha; a skill that is essential to mastering the language. Kawenn\'on:nis is implemented with both web and mobile front-ends that communicate with an application programming interface that in turn communicates with a symbolic language model implemented as a finite state transducer. Eventually, it will serve as a foundation for several other applications for both Kanyen'k\'eha and other Iroquoian languages.},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/CDLZ5N9N/Kazantseva et al. - 2018 - KawennÃ³nnis the Wordmaker for Kanyen'kÃ©ha.pdf}
}

@phdthesis{kelley.p:2018,
  title = {More People Understand {{Eschers}} than the Linguist Does: {{The}} Causes and Effects of Grammatical Illusions},
  shorttitle = {More People Understand Eschers than the Linguist Does},
  author = {Kelley, Patrick},
  year = {2018},
  address = {{East Lansing, Michigan}},
  url = {https://www.proquest.com/docview/2041968142/abstract/42208A941C8E47AFPQ/1},
  urldate = {2023-02-22},
  abstract = {A grammatical illusion can be defined as a sentence that seems acceptable, but structurally, the sentence is ungrammatical. Grammatical illusions provide a challenge for linguists to understand why we do not immediately reject illusions like we do for most ungrammatical sentences. One type of illusion that has stirred several ongoing debates is the Escher Sentence. This dissertation focuses on the source of the illusory effect, or the reason why people fail to consistently reject these sentences. This dissertation explores the properties of Escher Sentences, the reason why they are illusory in nature, and what this contributes to our understanding of the parser. Six Experiments were designed to test the acceptability judgments, interpretations, and neurophysiological responses to these sentences. I conclude that Escher Sentences are recognized by the parser as ungrammatical, but because of the structure of these sentences, the parser is tricked into using a coercive operation to force Escher Sentences to have an acceptable interpretation. Escher Sentences gives us potential insight into the constraints of the parser in processing language while at the same time highlighting the parser's strategies in resolving computations that are ungrammatical.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780355930733},
  langid = {english},
  school = {Michigan State University},
  keywords = {Escher,grammatical illusions,Illusion,Language,literature and linguistics,Parser,Processing,Psychology},
  file = {/Users/j/Zotero/storage/TFD7EIVN/Kelley (More People Understand Eschers than the Linguist D.pdf}
}

@inproceedings{kennedy.a:2003,
  title = {The {{Dundee}} Corpus},
  booktitle = {Proceedings of the 12th {{European}} Conference on Eye Movement},
  author = {Kennedy, Alan and Hill, Robin and Pynte, Jo{\"e}l},
  year = {2003},
  date-added = {2021-06-02 17:24:08 -0400},
  date-modified = {2021-06-02 17:25:50 -0400}
}

@article{kennedy.a:2013,
  title = {Frequency and Predictability Effects in the {{Dundee Corpus}}: {{An}} Eye Movement Analysis},
  author = {Kennedy, Alan and Pynte, Jo{\"e}l and Murray, Wayne S. and Paul, Shirley-Anne},
  year = {2013},
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {3},
  pages = {601--618},
  publisher = {{SAGE Publications}},
  doi = {10.1080/17470218.2012.676054},
  url = {https://doi.org/10.1080%2F17470218.2012.676054},
  bdsk-url-2 = {https://doi.org/10.1080/17470218.2012.676054},
  date-added = {2021-06-02 17:10:01 -0400},
  date-modified = {2021-06-02 17:10:34 -0400}
}

@misc{kim.t:2020chartbased,
  title = {Chart-Based Zero-Shot Constituency Parsing on Multiple Languages},
  author = {Kim, Taeuk and Li, Bowen and Lee, Sang-goo},
  year = {2020},
  eprint = {2004.13805},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kim.t:2020pretrained,
  title = {Are Pre-Trained Language Models Aware of Phrases? {{Simple}} but Strong Baselines for Grammar Induction},
  booktitle = {8th International Conference on Learning Representations, {{ICLR}} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  author = {Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  year = {2020},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=H1xPR3NtPB},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KimCEL20.bib},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200}
}

@inproceedings{kim.y:2015,
  title = {Character-Aware Neural Language Models},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {{USA}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David A. and Rush, Alexander M.},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  year = {2016},
  pages = {2741--2749},
  publisher = {{AAAI Press}},
  url = {http://www.aaai.org/ocs/index.php/AAAI/AAAI16/paper/view/12489},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aaai/KimJSR16.bib},
  timestamp = {Fri, 15 Nov 2019 00:00:00 +0100}
}

@inproceedings{kingma.d:2013,
  title = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Welling, Max},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  url = {http://arxiv.org/abs/1312.6114},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
  file = {/Users/j/Zotero/storage/F8H45BMD/Kingma and Welling - 2014 - Auto-encoding variational bayes.pdf}
}

@phdthesis{kingma.d:2017,
  title = {Variational Inference \& Deep Learning: {{A}} New Synthesis},
  author = {Kingma, Diederik P},
  year = {2017},
  url = {https://pure.uva.nl/ws/files/17891313/Thesis.pdf},
  date-added = {2019-10-08 21:58:23 -0400},
  date-modified = {2021-03-12 11:48:12 -0500},
  project = {syntactic embedding},
  school = {University of Amsterdam},
  keywords = {autoencoders,variational inference},
  file = {/Users/j/Zotero/storage/CZSPTAWT/Kingma - 2017 - Variational inference & deep learning A new synth.pdf}
}

@inproceedings{kipf.t:2017,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=SJU4ayYgl},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{kitaev.n:2018,
  title = {Constituency Parsing with a Self-Attentive Encoder},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kitaev, Nikita and Klein, Dan},
  year = {2018},
  pages = {2676--2686},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1249},
  url = {https://www.aclweb.org/anthology/P18-1249},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1249},
  file = {/Users/j/Zotero/storage/MJEFTKPU/Kitaev and Klein - 2018 - Constituency parsing with a self-attentive encoder.pdf}
}

@inproceedings{kitaev.n:2019,
  title = {Multilingual {{Constituency Parsing}} with {{Self-Attention}} and {{Pre-Training}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
  year = {2019},
  month = jul,
  pages = {3499--3505},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1340},
  url = {https://aclanthology.org/P19-1340},
  urldate = {2022-05-18},
  abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2\% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
  keywords = {parsing},
  file = {/Users/j/Zotero/storage/ETEMZZ8N/Kitaev et al. - 2019 - Multilingual Constituency Parsing with Self-Attent.pdf}
}

@inproceedings{kitaev.n:2022,
  title = {Learned {{Incremental Representations}} for {{Parsing}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kitaev, Nikita and Lu, Thomas and Klein, Dan},
  year = {2022},
  month = may,
  pages = {3086--3095},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.220},
  urldate = {2022-05-18},
  abstract = {We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.},
  file = {/Users/j/Zotero/storage/Y5TAURC3/Kitaev et al. - 2022 - Learned Incremental Representations for Parsing.pdf}
}

@inproceedings{klein.d:2002parserFactored,
  title = {Fast Exact Inference with a Factored Model for Natural Language Parsing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Klein, Dan and Manning, Christopher D},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2002},
  volume = {15},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2002/file/6c97cd07663b099253bc569fe8d342bb-Paper.pdf},
  date-added = {2022-05-06 15:57:44 -0400},
  date-modified = {2022-05-06 16:01:12 -0400},
  keywords = {stanford dependencies,stanford parser},
  file = {/Users/j/Zotero/storage/NSJVN9IE/Klein and Manning - 2002 - Fast exact inference with a factored model for nat.pdf}
}

@inproceedings{klein.d:2003parserPCFG,
  title = {Accurate Unlexicalized Parsing},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  author = {Klein, Dan and Manning, Christopher D.},
  year = {2003},
  month = jul,
  pages = {423--430},
  publisher = {{Association for Computational Linguistics}},
  address = {{Sapporo, Japan}},
  doi = {10.3115/1075096.1075150},
  url = {https://aclanthology.org/P03-1054},
  bdsk-url-2 = {https://doi.org/10.3115/1075096.1075150},
  date-added = {2022-05-06 16:00:23 -0400},
  date-modified = {2022-05-06 16:00:53 -0400}
}

@inproceedings{klein.d:2004induction,
  title = {Corpus-Based Induction of Syntactic Structure: {{Models}} of Dependency and Constituency},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Klein, Dan and Manning, Christopher},
  year = {2004},
  pages = {478--485},
  address = {{Barcelona, Spain}},
  doi = {10.3115/1218955.1219016},
  url = {https://www.aclweb.org/anthology/P04-1061},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1219016}
}

@article{kliegl.r:2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  year = {2004},
  journal = {European Journal of Cognitive Psychology},
  volume = {16},
  number = {1-2},
  pages = {262--284},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/09541440340000213},
  url = {https://doi.org/10.1080%2F09541440340000213},
  bdsk-url-2 = {https://doi.org/10.1080/09541440340000213},
  date-added = {2021-06-02 17:15:22 -0400},
  date-modified = {2021-06-02 17:15:24 -0400}
}

@article{kollar.t:2017,
  title = {Generalized Grounding Graphs: {{A}} Probabilistic Framework for Understanding Grounded Commands},
  author = {Kollar, Thomas and Tellex, Stefanie and Walter, Matthew and Huang, Albert and Bachrach, Abraham and Hemachandra, Sachi and Brunskill, Emma and Banerjee, Ashis and Roy, Deb and Teller, Seth and others},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.01097},
  eprint = {1712.01097},
  archiveprefix = {arxiv},
  date-added = {2020-07-28 16:14:45 -0400},
  date-modified = {2020-07-28 16:15:35 -0400},
  project = {syntactic embedding},
  keywords = {robotics,semantics}
}

@article{kolmogorov.a:1968,
  title = {Logical Basis for Information Theory and Probability Theory},
  author = {Kolmogorov, Andrei},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {5},
  pages = {662--664},
  publisher = {{IEEE}},
  date-added = {2019-09-13 08:11:08 -0400},
  date-modified = {2019-09-13 08:11:46 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@article{kolmogorov.a:1968a,
  title = {Three Approaches to the Quantitative Definition of Information},
  author = {Kolmogorov, Andrei Nikolaevich},
  year = {1968},
  journal = {International journal of computer mathematics},
  volume = {2},
  number = {1-4},
  pages = {157--168},
  publisher = {{Taylor \& Francis}},
  url = {https://www.tandfonline.com/doi/pdf/10.1080/00207166808803030},
  date-added = {2019-09-13 08:14:37 -0400},
  date-modified = {2019-09-13 08:15:41 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@techreport{kong.a:1992,
  type = {Technical Report},
  title = {A Note on Importance Sampling Using Standardized Weights},
  author = {Kong, Augustine},
  year = {1992},
  month = jul,
  number = {348},
  institution = {{Department of Statistics, University of Chicago}},
  file = {/Users/j/Zotero/storage/QQIUZ4E5/Kong (1992) A note on importance sampling using standardized w.pdf}
}

@article{kong.a:1994,
  title = {Sequential Imputations and {{Bayesian}} Missing Data Problems},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1994.10476469},
  pages = {278--288},
  publisher = {{Taylor \& Francis}},
  doi = {10.1080/01621459.1994.10476469},
  url = {https://www.tandfonline.com/doi/abs/10.1080/01621459.1994.10476469},
  abstract = {Abstract For missing data problems, Tanner and Wong have described a data augmentation procedure that approximates the actual posterior distribution of the parameter vector by a mixture of complete data posteriors. Their method of constructing the complete data sets is closely related to the Gibbs sampler. Both required iterations, and, similar to the EM algorithm, convergence can be slow. We introduce in this article an alternative procedure that involves imputing the missing data sequentially and computing appropriate importance sampling weights. In many applications this new procedure works very well without the need for iterations. Sensitivity analysis, influence analysis, and updating with new data can be performed cheaply. Bayesian prediction and model selection can also be incorporated. Examples taken from a wide range of applications are used for illustration.},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1994.10476469},
  date-added = {2022-05-07 10:36:04 -0400},
  date-modified = {2022-05-07 10:36:22 -0400},
  keywords = {sequential importance sampling,sequential imputation,sequential Monte Carlo}
}

@inproceedings{konieczny.l:2003,
  title = {Anticipation of Clause-Final Heads. {{Evidence}} from Eye-Tracking and {{SRNs}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Cognitive Science}}},
  author = {Konieczny, Lars and D{\"o}ring, Philipp},
  year = {2003},
  month = jun,
  pages = {13--17},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Melbourne, Australia and St. Petersburg, Russia}},
  url = {https://www.researchgate.net/publication/235711358},
  abstract = {In a Simple Recurrent Network simulation and an eye- tracking study, we investigated the processing of clause- final verbs. Following the integration cost hypothesis (Gibson, 1998), processing verbs should be the harder, the more complement integrations have to take place. In contrast, probabilistic prediction-based models, like Simple Recurrent Networks (SRNs, Elman, 1990), might anticipate verbs the better, the more dependents have been encountered beforehand. We trained SRNs with a subset of the German language to establish basic dependency relationships between verbs and their arguments in both verb-second and verb-final constructions. The test results established a clear anticipation hypothesis: the more arguments precede the verb, the lower the prediction error and hence, predicted reading times. The data from an eye-tracking experiment confirm the anticipation hypothesis: Clause final verbs are read faster when an additional Dative, instead of a noun-modifying Genitive, is read beforehand. Adverbial PP-adjuncts, in contrast to Noun-modifying PPs, however, did not affect reading times. In general, the results support a restricted anticipation hypothesis.}
}

@inproceedings{koo.t:2007,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  year = {2007},
  pages = {141--150},
  publisher = {{Association for Computational Linguistics}},
  address = {{Prague, Czech Republic}},
  url = {https://www.aclweb.org/anthology/D07-1015}
}

@inproceedings{kool.w:2019,
  title = {Stochastic Beams and Where to Find Them: {{The Gumbel-top-k}} Trick for Sampling Sequences without Replacement},
  shorttitle = {Stochastic Beams and Where to Find Them},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kool, Wouter and Hoof, Herke Van and Welling, Max},
  year = {2019},
  month = may,
  pages = {3499--3508},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v97/kool19a.html},
  urldate = {2022-11-06},
  abstract = {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample {$\mathsl{k}$}kk elements without replacement. We show how to implicitly apply this 'Gumbel-Top-{$\mathsl{k}$}kk' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in {$\mathsl{k}$}kk and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.},
  langid = {english},
  file = {/Users/j/Zotero/storage/CH7ER5BN/Kool et al. (2019) Stochastic Beams and Where To Find Them The Gumbe.pdf;/Users/j/Zotero/storage/S9XG9WQE/Kool et al. (2019) Stochastic Beams and Where To Find Them The Gumbe.pdf}
}

@book{kozen.d:1997,
  title = {Automata and Computability},
  author = {Kozen, Dexter C.},
  year = {1997},
  publisher = {{Springer New York}},
  address = {{New York, NY}},
  doi = {10.1007/978-1-4612-1844-9},
  url = {http://link.springer.com/10.1007/978-1-4612-1844-9},
  urldate = {2023-01-24},
  isbn = {978-1-4612-7309-7 978-1-4612-1844-9},
  langid = {english},
  file = {/Users/j/Zotero/storage/PD3237HJ/Kozen (1997) Automata and Computability.pdf}
}

@book{kracht.m:2003,
  title = {The Mathematics of Language},
  author = {Kracht, Marcus},
  year = {2003},
  series = {Studies in Generative Grammar},
  number = {63},
  publisher = {{Mouton De Gruyter}},
  date-added = {2019-05-19 21:51:49 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {3-11-017620-3 978-3-11-017620-9},
  keywords = {automata,complexity,formal languages,mathematical linguistics,model theory}
}

@article{kubler.s:2009,
  title = {Dependency Parsing},
  author = {K{\"u}bler, Sandra and McDonald, Ryan and Nivre, Joakim},
  year = {2009},
  journal = {Synthesis lectures on human language technologies},
  volume = {1},
  number = {1},
  pages = {1--127},
  publisher = {{Morgan \& Claypool Publishers}},
  date-added = {2020-02-26 14:44:36 -0500},
  date-modified = {2020-02-26 14:45:01 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm}
}

@article{kucerova.i:2016,
  title = {Long-Distance Agreement in {{Icelandic}}: Locality Restored},
  author = {Ku{\v c}erov{\'a}, Ivona},
  year = {2016},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {19},
  number = {1},
  pages = {49--74},
  publisher = {{Springer}},
  url = {http://ling.auf.net/lingbuzz/002237/current.pdf},
  date-added = {2020-02-26 09:11:49 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,object shift,phi features},
  file = {/Users/j/Zotero/storage/9PYVFJSG/KuÄerovÃ¡ - 2016 - Long-distance agreement in Icelandic locality res.pdf}
}

@book{kuhlmann.m:2010,
  title = {Dependency Structures and Lexicalized Grammars: {{An}} Algebraic Approach},
  author = {Kuhlmann, Marco},
  year = {2010},
  volume = {6270},
  publisher = {{Springer}},
  url = {https://www.ida.liu.se/ marku61/pdf/kuhlmann2010dependency.pdf},
  date-added = {2020-02-26 18:37:01 -0500},
  date-modified = {2021-07-16 11:22:03 -0400},
  isbn = {978-3-642-14568-1},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projective dependencies,projectivity}
}

@article{kullback.s:1951,
  title = {On Information and Sufficiency},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-22/issue-1/On-Information-and-Sufficiency/10.1214/aoms/1177729694.full},
  urldate = {2022-10-11},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/j/Zotero/storage/G42NQYWW/Kullback and Leibler (1951) On Information and Sufficiency.pdf}
}

@book{kullback.s:1959,
  title = {Information Theory and Statistics},
  author = {Kullback, Solomon},
  year = {[1968] 1959},
  edition = {1968 Dover republication of 1959 (Wiley) first edition},
  publisher = {{Peter Smith}},
  address = {{New York, NY, USA}},
  isbn = {978-0-8446-5625-0},
  langid = {english},
  keywords = {Information theory},
  file = {/Users/j/Zotero/storage/JLNQK6GX/kullback.s.1959.djvu}
}

@inproceedings{kuncoro.a:2018,
  title = {{{LSTMs}} Can Learn Syntax-Sensitive Dependencies Well, but Modeling Structure Makes Them Better},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John T. and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  year = {2018},
  pages = {1426--1436},
  publisher = {{Association for Computational Linguistics}},
  address = {{Melbourne, Australia}},
  doi = {10.18653/v1/P18-1132},
  url = {https://www.aclweb.org/anthology/P18-1132},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1132},
  date-modified = {2022-04-20 13:50:17 -0400}
}

@article{kuperberg.g:2016,
  title = {What Do We Mean by Prediction in Language Comprehension?},
  author = {Kuperberg, Gina R. and Jaeger, T. Florian},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  eprint = {https://doi.org/10.1080/23273798.2015.1102299},
  pages = {32--59},
  publisher = {{Routledge}},
  doi = {10.1080/23273798.2015.1102299},
  url = {https://doi.org/10.1080/23273798.2015.1102299},
  date-modified = {2021-06-05 22:29:28 -0400}
}

@inproceedings{kuribayashi.t:2021,
  title = {Lower Perplexity Is Not Always Human-Like},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  year = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.405},
  url = {https://doi.org/10.18653%2Fv1%2F2021.acl-long.405},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.405},
  date-added = {2021-12-02 19:40:09 -0500},
  date-modified = {2021-12-02 19:40:25 -0500}
}

@inproceedings{kuribayashi.t:2022,
  title = {Context Limitations Make Neural Language Models More Human-Like},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Brassard, Ana and Inui, Kentaro},
  year = {2022},
  month = dec,
  pages = {10421--10436},
  publisher = {{Association for Computational Linguistics}},
  address = {{Abu Dhabi, United Arab Emirates}},
  url = {https://aclanthology.org/2022.emnlp-main.712},
  urldate = {2023-04-30},
  abstract = {Language models (LMs) have been used in cognitive modeling as well as engineering studies\textemdash they compute information-theoretic complexity metrics that simulate humans' cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.Our results showed that constraining the LMs' context access improved their simulation of human reading behavior.We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs' context access might enhance their cognitive plausibility.},
  file = {/Users/j/Zotero/storage/HQW8295D/Kuribayashi et al. (2022) Context Limitations Make Neural Language Models Mo.pdf}
}

@inproceedings{kurihara.k:2004,
  title = {An Application of the Variational {{Bayesian}} Approach to Probabilistic Context-Free Grammars},
  booktitle = {In {{International Joint Conference}} on {{Natural Language Processing Workshop Beyond Shallow Analyses}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  year = {2004},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.75.4186},
  abstract = {We present an efficient learning algorithm for probabilistic context-free grammars based on the variational Bayesian approach. Although the maximum likelihood method has traditionally been used for learning probabilistic language models, Bayesian learning is, in principle, less likely to cause overfitting problems than the maximum likelihood method. We show that the computational complexity of our algorithm is equal to that of the Inside-Outside algorithm. We also report results of experiments to compare precisions of the Inside-Outside algorithm and our algorithm. 1},
  file = {/Users/j/Zotero/storage/GIIQM9EA/Kurihara - 2004 - 2004. An application of the variational Bayesian a.pdf}
}

@incollection{kurihara.k:2006,
  title = {Variational {{Bayesian}} Grammar Induction for Natural Language},
  booktitle = {Grammatical {{Inference}}: {{Algorithms}} and {{Applications}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  editor = {Sakakibara, Yasubumi and Kobayashi, Satoshi and Sato, Kengo and Nishino, Tetsuro and Tomita, Etsuji},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--96},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/11872436_8},
  abstract = {This paper presents a new grammar induction algorithm for probabilistic context-free grammars (PCFGs). There is an approach to PCFG induction that is based on parameter estimation. Following this approach, we apply the variational Bayes to PCFGs. The variational Bayes (VB) is an approximation of Bayesian learning. It has been empirically shown that VB is less likely to cause overfitting. Moreover, the free energy of VB has been successfully used in model selection. Our algorithm can be seen as a generalization of PCFG induction algorithms proposed before. In the experiments, we empirically show that induced grammars achieve better parsing results than those of other PCFG induction algorithms. Based on the better parsing results, we give examples of recursive grammatical structures found by the proposed algorithm.},
  isbn = {978-3-540-45265-2},
  langid = {english},
  keywords = {Bayesian Learning,Noun Phrase,Parse Tree,Training Corpus,Wall Street Journal},
  file = {/Users/j/Zotero/storage/LNILI62V/Kurihara and Sato - 2006 - Variational Bayesian Grammar Induction for Natural.pdf}
}

@incollection{lai.l:2021,
  title = {Policy Compression: {{An}} Information Bottleneck in Action Selection},
  shorttitle = {Chapter {{Five}} - {{Policy}} Compression},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Lai, Lucy and Gershman, Samuel J.},
  editor = {Federmeier, Kara D.},
  year = {2021},
  month = jan,
  series = {The {{Psychology}} of {{Learning}} and {{Motivation}}},
  volume = {74},
  pages = {195--232},
  publisher = {{Academic Press}},
  doi = {10.1016/bs.plm.2021.02.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0079742121000049},
  urldate = {2022-11-30},
  abstract = {The brain has evolved to produce a diversity of behaviors under stringent computational resource constraints. Given this limited capacity, how do biological agents balance reward maximization against the costs of representing complex action policies? In this chapter, we examine behavioral evidence for this reward-complexity trade-off. First, we introduce a theoretical framework that formalizes the idea of policy compression, or the reduction in cognitive cost of representing action policies by making them simpler. We then describe how a wide range of behavioral phenomena, including stochasticity, perseveration, response time, state and action chunking, and navigation are brought together under this framework. Finally, we discuss how our model can be used to probe the neural underpinnings of policy compression and their dysfunction in psychiatric illness.},
  langid = {english},
  keywords = {Action selection,Rational behavior,Reinforcement learning,Resource-rationality}
}

@article{lambek.j:1958,
  title = {The Mathematics of Sentence Structure},
  author = {Lambek, Joachim},
  year = {1958},
  journal = {The American Mathematical Monthly},
  volume = {65},
  number = {3},
  eprint = {2310058},
  eprinttype = {jstor},
  pages = {154--170},
  publisher = {{Taylor \& Francis, Ltd. on behalf of the Mathematical Association of America}},
  doi = {10.1080/00029890.1958.11989160},
  url = {https://www.jstor.org/stable/2310058},
  bdsk-url-2 = {https://doi.org/10.1080/00029890.1958.11989160},
  date-added = {2019-08-26 14:46:48 -0400},
  date-modified = {2021-06-25 00:48:42 -0400},
  keywords = {category theory,pregroup grammar}
}

@inproceedings{lambek.j:1999,
  title = {Type Grammar Revisited},
  booktitle = {International Conference on Logical Aspects of Computational Linguistics},
  author = {Lambek, Joachim},
  year = {1999},
  pages = {1--27},
  date-added = {2019-08-26 22:09:20 -0400},
  date-modified = {2019-08-26 22:09:55 -0400},
  organization = {{Springer}},
  keywords = {pregroup grammar}
}

@article{lambek.j:2001,
  title = {Type Grammars as Pregroups},
  author = {Lambek, Joachim},
  year = {2001},
  journal = {Grammars},
  volume = {4},
  pages = {21--39},
  date-added = {2019-08-26 21:51:00 -0400},
  date-modified = {2019-08-26 21:51:54 -0400},
  keywords = {pregroup grammar}
}

@article{lambek.j:2012,
  title = {Logic and Grammar},
  author = {Lambek, Joachim},
  year = {2012},
  journal = {Studia Logica: An International Journal for Symbolic Logic},
  volume = {100},
  number = {4},
  eprint = {23262129},
  eprinttype = {jstor},
  pages = {667--681},
  publisher = {{Springer}},
  issn = {00393215, 15728730},
  url = {http://www.jstor.org/stable/23262129},
  abstract = {Grammar can be formulated as a kind of substructural propositional logic. In support of this claim, we survey bare Gentzen style deductive systems and two kinds of non-commutative linear logic: intuitionistic and compact bilinear logic. We also glance at their categorical refinements.},
  date-added = {2019-08-26 21:59:20 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {pregroup grammar}
}

@inproceedings{lample.g:2019,
  title = {Cross-Lingual Language Model Pretraining},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Conneau, Alexis and Lample, Guillaume},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {7057--7067},
  url = {https://proceedings.neurips.cc/paper/2019/hash/c04c19c2c2474dbf5f7ac4372c5b9af1-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ConneauL19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@book{lanchier.n:2017,
  title = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  publisher = {{Springer International}},
  doi = {10.1007/978-3-319-50038-6},
  url = {https://doi.org/10.1007%2F978-3-319-50038-6},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6},
  date-added = {2022-04-07 10:01:23 -0400},
  date-modified = {2022-04-14 10:21:03 -0400}
}

@incollection{lanchier.n:2017ch1,
  title = {Basics of Measure and Probability Theory},
  booktitle = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  pages = {3--24},
  publisher = {{Springer International}},
  doi = {10.1007/978-3-319-50038-6_1},
  url = {https://doi.org/10.1007%2F978-3-319-50038-6_1},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6{$_{1}$}},
  date-added = {2022-04-07 10:02:11 -0400},
  date-modified = {2022-04-14 10:20:52 -0400}
}

@article{lari.k:1991,
  title = {Applications of Stochastic Context-Free Grammars Using the inside-Outside Algorithm},
  author = {Lari, K. and Young, S. J.},
  year = {1991},
  month = jul,
  journal = {Computer Speech \& Language},
  volume = {5},
  number = {3},
  pages = {237--257},
  issn = {0885-2308},
  doi = {10.1016/0885-2308(91)90009-F},
  url = {https://www.sciencedirect.com/science/article/pii/088523089190009F},
  urldate = {2022-07-04},
  abstract = {This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.},
  langid = {english},
  file = {/Users/j/Zotero/storage/SGHR95TR/Lari and Young - 1991 - Applications of stochastic context-free grammars u.pdf}
}

@article{lau.j:2016,
  title = {Grammaticality, Acceptability, and Probability: {{A}} Probabilistic View of Linguistic Knowledge},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2016},
  month = oct,
  volume = {41},
  number = {5},
  pages = {1202--1241},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12414},
  url = {https://doi.org/10.1111%2Fcogs.12414},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12414},
  date-added = {2021-10-19 00:11:36 -0400},
  date-modified = {2021-10-19 00:11:37 -0400}
}

@inproceedings{laverghetta.a:2022,
  title = {Predicting Human Psychometric Properties Using Computational Language Models},
  booktitle = {Quantitative {{Psychology}}},
  author = {Laverghetta, Antonio and Nighojkar, Animesh and Mirzakhalov, Jamshidbek and Licato, John},
  editor = {Wiberg, Marie and Molenaar, Dylan and Gonz{\'a}lez, Jorge and Kim, Jee-Seon and Hwang, Heungsun},
  year = {2022},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  pages = {151--169},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-031-04572-1_12},
  abstract = {Transformer-based language models (LMs) continue to achieve state-of-the-art performance on natural language processing (NLP) benchmarks, including tasks designed to mimic human-inspired ``commonsense'' competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts from psychometrics. But to what extent can benefits flow in the other direction? In other words, can LMs be of use in predicting the psychometric properties of test items, when those items are given to human participants? If so, the benefit for psychometric practitioners is enormous, as it can reduce the need for multiple rounds of empirical testing. We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the human responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions correlate. We find that transformer-based LMs predict the human psychometric data consistently well across most categories, suggesting that they can be used to gather human-like psychometric data without the need for extensive human trials.},
  isbn = {978-3-031-04572-1},
  langid = {english},
  keywords = {Classical test theory,Item response theory,Natural language processing,psychometrics},
  file = {/Users/j/Zotero/storage/67XZY8SF/Laverghetta et al. (2022) Predicting Human Psychometric Properties Using Com.pdf}
}

@book{lazore.d:1993,
  title = {The {{Mohawk}} Language Standardisation Project Conference Report, Aug. 9-10, 1993},
  author = {Lazore, Dorothy Karihw{\'e}nhawe},
  editor = {Jacobs, Annette Kaia'tit{\'a}hkhe and Thompson, Nancy Kahawin{\'o}nkie and Leaf, Minnie Kai{\`a}:khons},
  year = {1993},
  month = aug,
  publisher = {{Literacy and Basic Skills Section, Ministry of Education and Training}},
  address = {{Toronto}},
  url = {http://kanienkeha.net/the-mohawk-language-standardisation-project/},
  date-added = {2022-05-03 17:07:15 -0400},
  date-modified = {2022-05-03 17:17:25 -0400},
  isbn = {0-7778-6105-4},
  langid = {english},
  organization = {{Mohawk Language Standardisation Conference (1993 : Tyendinaga Indian Reserve)}},
  keywords = {kanien'keha,mohawk language,standardization}
}

@article{lebesgue.h:1902,
  title = {{Int\'egrale, Longueur, Aire}},
  author = {Lebesgue, H.},
  year = {1902},
  month = dec,
  journal = {Annali di Matematica Pura ed Applicata (1898-1922)},
  volume = {7},
  number = {1},
  pages = {231--359},
  issn = {0373-3114},
  doi = {10.1007/BF02420592},
  url = {https://doi.org/10.1007/BF02420592},
  urldate = {2022-06-22},
  langid = {french}
}

@inproceedings{lebrun.b:2022,
  title = {Evaluating Distributional Distortion in Neural Language Modeling},
  booktitle = {International Conference on Learning Representations},
  author = {LeBrun, Benjamin and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022},
  url = {https://openreview.net/forum?id=bTteFbU99ye}
}

@article{leemis.l:2008,
  title = {Univariate {{Distribution Relationships}}},
  author = {Leemis, Lawrence M. and McQueston, Jacquelyn T.},
  year = {2008},
  month = feb,
  journal = {The American Statistician},
  volume = {62},
  number = {1},
  pages = {45--53},
  publisher = {{Taylor \& Francis}},
  issn = {0003-1305},
  doi = {10.1198/000313008X270448},
  url = {https://doi.org/10.1198/000313008X270448},
  urldate = {2022-06-20},
  abstract = {Probability distributions are traditionally treated separately in introductory mathematical statistics textbooks. A figure is presented here that shows properties that individual distributions possess and many of the relationships between these distributions.},
  keywords = {Asymptotic relationships,Distribution properties,Limiting distributions,Stochastic parameters,Transformations},
  file = {/Users/j/Zotero/storage/T9J6S7Z7/Leemis and McQueston - 2008 - Univariate Distribution Relationships.pdf}
}

@book{legate.j:2014,
  title = {Voice and v: {{Lessons}} from Acehnese},
  author = {Legate, Julie Anne},
  year = {2014},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262028141.001.0001},
  url = {https://doi.org/10.7551%2Fmitpress%2F9780262028141.001.0001},
  bdsk-url-2 = {https://doi.org/10.7551/mitpress/9780262028141.001.0001},
  date-added = {2021-03-22 00:34:12 -0400},
  date-modified = {2021-03-22 13:11:36 -0400},
  keywords = {argument structure,voice}
}

@article{legate.j:2020,
  title = {On Passives of Passives},
  author = {Legate, Julie Anne and Akku{\c s}, Faruk and {\v S}ereikait{\.e}, Milena and Ringe, Don},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {4},
  pages = {771--818},
  publisher = {{Project Muse}},
  doi = {10.1353/lan.2020.0062},
  url = {https://doi.org/10.1353%2Flan.2020.0062},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0062},
  date-added = {2021-03-20 12:21:19 -0400},
  date-modified = {2021-03-20 12:21:35 -0400},
  keywords = {argument structure,passives}
}

@book{levelt.w:1974,
  title = {Formal Grammars in Linguistics and Psycholinguistics: {{Volume}} 3: {{Psycholinguistic}} Applications},
  author = {Levelt, Willem JM},
  year = {1974},
  series = {{{JANUA LINGUARUM}}},
  volume = {192},
  publisher = {{Mouton}},
  address = {{The Hague}},
  date-added = {2019-06-11 14:51:49 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@incollection{levin.b:2005,
  title = {Argument Realization: {{Research}} Surveys in Linguistics},
  booktitle = {Argument Realization: {{Research}} Surveys in Linguistics},
  author = {Levin, Beth and Rappaport Hovav, Malka},
  year = {2005},
  publisher = {{Cambridge University Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:20 -0400},
  readinglist = {Thesis}
}

@inproceedings{levshina.n:2017,
  title = {Communicative Efficiency and Syntactic Predictability: {{A}} Cross-Linguistic Study Based on the {{Universal Dependencies}} Corpora},
  booktitle = {Proceedings of the {{NoDaLiDa}} 2017 Workshop on Universal Dependencies, 22 May, Gothenburg Sweden},
  author = {Levshina, Natalia},
  year = {2017},
  number = {135},
  pages = {72--78},
  url = {http://www.ep.liu.se/ecp/article.asp?issue=135&article=009&volume=},
  date-added = {2020-04-05 12:14:22 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  organization = {{Link\"oping University Electronic Press}},
  project = {syntactic embedding},
  keywords = {dependency structures,information theory,random forests}
}

@inproceedings{levy.o:2014,
  title = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27: {{Annual}} Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  year = {2014},
  pages = {2177--2185},
  url = {https://proceedings.neurips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyG14.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{levy.o:2014dependency,
  title = {Dependency-Based Word Embeddings},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Levy, Omer and Goldberg, Yoav},
  year = {2014},
  pages = {302--308},
  publisher = {{Association for Computational Linguistics}},
  address = {{Baltimore, Maryland}},
  doi = {10.3115/v1/P14-2050},
  url = {https://www.aclweb.org/anthology/P14-2050},
  bdsk-url-2 = {https://doi.org/10.3115/v1/P14-2050}
}

@phdthesis{levy.r:2005phd,
  title = {Probabilistic Models of Word Order and Syntactic Discontinuity},
  author = {Levy, Roger},
  year = {2005},
  url = {https://www.proquest.com/dissertations-theses/probabilistic-models-word-order-syntactic/docview/305432573/se-2?accountid=12339},
  abstract = {This thesis takes up the problem of syntactic comprehension, or parsing\textemdash how an agent (human or machine) with knowledge of a specific language goes about inferring the hierarchical structural relationships underlying a surface string in the language. I take the position that probabilistic models of combining evidential information are cognitively plausible and practically useful for syntactic comprehension. In particular, the thesis applies probabilistic methods in investigating the relationship between word order and psycholinguistic models of comprehension; and in the practical problems of accuracy and efficiency in parsing sentences with syntactic discontinuity. On the psychological side, the thesis proposes a theory of expectation-based processing difficulty as a consequence of probabilistic syntactic disambiguation: the ease of processing a word during comprehension is determined primarily by the degree to which that word is expected. I identify a class of syntactic phenomena, associated primarily with verb-final clause order, where the predictions of expectation-based processing diverge most sharply from more established locality-based theories of processing difficulty. Using existing probabilistic parsing algorithms and syntactically annotated data sources, I show that the expectation-based theory matches a range of established experimental psycholinguistic results better than locality-based theories. The comparison of probabilistic- and locality-driven processing theories is a crucial area of psycholinguistic research due to its implications for the relationship between linguistic production and comprehension, and more generally for theories of modularity in cognitive science. The thesis also takes up the problem of probabilistic models for discontinuous constituency, when phrases do not consist of continuous substrings of a sentence. Discontinuity poses a computational challenge in parsing, because it expands the set of possible substructures in a sentence beyond the bound, quadratic in sentence length, on the set of possible continuous constituents. For discontinuous constituency, I investigate the problem of accuracy employing discriminative classifiers organized on principles of syntactic theory and used to introduce discontinuous relationships into otherwise strictly context-free phrase structure trees; and the problem of efficiency in joint inference over both continuous and discontinuous structures, using probabilistic instantiations of mildly context-sensitive grammatical formalisms and factorizing grammatical generalizations into probabilistic components of dominance and linear order.},
  date-added = {2021-09-18 22:16:45 -0400},
  date-modified = {2022-04-04 13:25:27 -0400},
  isbn = {978-0-542-28638-4},
  school = {Stanford University},
  keywords = {Applied sciences,Cognitive psychology,Cognitive therapy,Computer science,Language,Linguistics,literature and linguistics,Natural language processing,Parsing,Probabilistic,Psychology,Syntactic discontinuity,Word order},
  file = {/Users/j/Zotero/storage/9EWYEL2I/Levy - 2005 - Probabilistic models of word order and syntactic d.pdf}
}

@inproceedings{levy.r:2006,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems},
  author = {Levy, Roger and Jaeger, T. Florian},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John C. and Hofmann, Thomas},
  year = {2006},
  pages = {849--856},
  publisher = {{MIT Press}},
  address = {{Vancouver, British Columbia, Canada}},
  url = {https://proceedings.neurips.cc/paper/2006/hash/c6a01432c8138d46ba39957a8250e027-Abstract.html},
  biburl = {https://dblp.org/rec/conf/nips/LevyJ06.bib},
  keywords = {uniform information density},
  file = {/Users/j/Zotero/storage/X66UJUGA/Levy and Jaeger (2006) Speakers optimize information density through synt.pdf}
}

@article{levy.r:2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  url = {http://www.sciencedirect.com/science/article/pii/S0010027707001436},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159\textendash 166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2007.05.006},
  date-added = {2021-01-14 13:02:24 -0500},
  date-modified = {2021-03-09 22:53:26 -0500},
  keywords = {Frequency,Information theory,Parsing,Prediction,processing,Sentence processing,surprisal,Syntactic complexity,Syntax,Word order},
  file = {/Users/j/Zotero/storage/CTRQQCHF/Levy (2008) Expectation-based syntactic comprehension.pdf}
}

@inproceedings{levy.r:2008noisy,
  title = {A Noisy-Channel Model of Human Sentence Comprehension under Uncertain Input},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Levy, Roger},
  year = {2008},
  month = oct,
  pages = {234--243},
  publisher = {{Association for Computational Linguistics}},
  address = {{Honolulu, Hawaii}},
  url = {https://aclanthology.org/D08-1025},
  date-added = {2022-04-11 23:17:10 -0400},
  date-modified = {2022-04-11 23:17:30 -0400},
  file = {/Users/j/Zotero/storage/9QI3PFHK/Levy (2008) A noisy-channel model of human sentence comprehens.pdf}
}

@inproceedings{levy.r:2008particle,
  title = {Modeling the Effects of Memory on Human Online Sentence Processing with Particle Filters},
  booktitle = {Advances in Neural Information Processing Systems 21, Proceedings of the Twenty-Second Annual Conference on Neural Information Processing Systems, Vancouver, British Columbia, Canada, December 8-11, 2008},
  author = {Levy, Roger and Reali, Florencia and Griffiths, Thomas L.},
  editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, L{\'e}on},
  year = {2008},
  pages = {937--944},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2008/hash/a02ffd91ece5e7efeb46db8f10a74059-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyRG08.bib},
  date-modified = {2022-05-12 19:43:45 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {/Users/j/Zotero/storage/N2SFBHNE/Levy et al. (2008) Modeling the effects of memory on human online sen.pdf}
}

@article{levy.r:2009pnas,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  year = {2009},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {50},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0907664106},
  pages = {21086--21090},
  doi = {10.1073/pnas.0907664106},
  url = {https://www.pnas.org/doi/abs/10.1073/pnas.0907664106},
  abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.0907664106},
  date-added = {2022-04-27 22:17:38 -0400},
  date-modified = {2022-04-27 22:18:16 -0400},
  keywords = {memory,noisy channel coding}
}

@inproceedings{levy.r:2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Levy, Roger},
  year = {2011},
  month = jun,
  pages = {1055--1065},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1106},
  date-added = {2022-04-27 08:47:55 -0400},
  date-modified = {2022-04-27 08:49:23 -0400},
  keywords = {noisy channel coding},
  file = {/Users/j/Zotero/storage/TTPQFXEQ/Levy (2011) Integrating surprisal and uncertain-input models i.pdf}
}

@article{levy.r:2012,
  title = {The Processing of Extraposed Structures in {{English}}},
  author = {Levy, Roger and Fedorenko, Evelina and Breen, Mara and Gibson, Edward},
  year = {2012},
  month = jan,
  journal = {Cognition},
  volume = {122},
  number = {1},
  pages = {12--36},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2011.07.012},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027711002010},
  urldate = {2022-10-24},
  abstract = {In most languages, most of the syntactic dependency relations found in any given sentence are projective: the word\textendash word dependencies in the sentence do not cross each other. Some syntactic dependency relations, however, are non-projective: some of their word\textendash word dependencies cross each other. Non-projective dependencies are both rarer and more computationally complex than projective dependencies; hence, it is of natural interest to investigate whether there are any processing costs specific to non-projective dependencies, and whether factors known to influence processing of projective dependencies also affect non-projective dependency processing. We report three self-paced reading studies, together with corpus and sentence completion studies, investigating the comprehension difficulty associated with the non-projective dependencies created by the extraposition of relative clauses in English. We find that extraposition over either verbs or prepositional phrases creates comprehension difficulty, and that this difficulty is consistent with probabilistic syntactic expectations estimated from corpora. Furthermore, we find that manipulating the expectation that a given noun will have a postmodifying relative clause can modulate and even neutralize the difficulty associated with extraposition. Our experiments rule out accounts based purely on derivational complexity and/or dependency locality in terms of linear positioning. Our results demonstrate that comprehenders maintain probabilistic syntactic expectations that persist beyond projective-dependency structures, and suggest that it may be possible to explain observed patterns of comprehension difficulty associated with extraposition entirely through probabilistic expectations.},
  langid = {english},
  keywords = {Frequency,Memory and language,Parsing,Prediction,Self-paced reading,Sentence comprehension,surprisal,surprisal theory,Syntactic complexity,Word order}
}

@incollection{levy.r:2013,
  title = {Memory and Surprisal in Human Sentence Comprehension},
  booktitle = {Sentence Processing},
  author = {Levy, Roger},
  editor = {{van Gompel}, Roger P. G.},
  year = {2013},
  pages = {78--114},
  publisher = {{Psychology Press}},
  url = {https://www.mit.edu/ rplevy/papers/levy-2013-memory-and-surprisal-corrected.pdf},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-05-03 14:37:38 -0400},
  file = {/Users/j/Zotero/storage/6LQVKMGP/Levy - 2013 - Memory and surprisal in human sentence comprehensi.pdf}
}

@article{levy.r:2013jml,
  title = {The Syntactic Complexity of {{Russian}} Relative Clauses},
  author = {Levy, Roger and Fedorenko, Evelina and Gibson, Edward},
  year = {2013},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {69},
  number = {4},
  pages = {461--495},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.10.005},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X12001209},
  urldate = {2023-03-09},
  abstract = {Although syntactic complexity has been investigated across dozens of studies, the available data still greatly underdetermine relevant theories of processing difficulty. Memory-based and expectation-based theories make opposite predictions regarding fine-grained time course of processing difficulty in syntactically constrained contexts, and each class of theory receives support from results on some constructions in some languages. Here we report four self-paced reading experiments on the online comprehension of Russian relative clauses together with related corpus studies, taking advantage of Russian's flexible word order to disentangle predictions of competing theories. We find support for key predictions of memory-based theories in reading times at RC verbs, and for key predictions of expectation-based theories in processing difficulty at RC-initial accusative noun phrase (NP) objects, which corpus data suggest should be highly unexpected. These results suggest that a complete theory of syntactic complexity must integrate insights from both expectation-based and memory-based theories.},
  langid = {english},
  keywords = {Expectation-based processing,Memory limitations in language processing,Parsing,Russian,Sentence comprehension,Syntax},
  file = {/Users/j/Zotero/storage/YN4BPVVD/Levy et al. (2013) The syntactic complexity of Russian relative claus.pdf}
}

@article{levy.r:2013opinion,
  title = {Surprisal, the {{PDC}}, and the Primary Locus of Processing Difficulty in Relative Clauses},
  author = {Levy, Roger and Gibson, Edward},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2013.00229},
  urldate = {2023-03-08},
  file = {/Users/j/Zotero/storage/RQZ4D7T6/Levy and Gibson (2013) Surprisal, the PDC, and the primary locus of proce.pdf}
}

@inproceedings{levy.r:2018cogsci,
  title = {Communicative Efficiency, Uniform Information Density, and the Rational Speech Act Theory},
  booktitle = {Proceedings of the 40th Annual Meeting of the Cognitive Science Society},
  author = {Levy, Roger},
  editor = {Kalish, Chuck and Martina Rau, Jerry Zhu and Rogers, Timothy},
  year = {2018},
  month = jul,
  pages = {684--689},
  address = {{Madison, Wisconsin, USA}},
  url = {https://cogsci.mindmodeling.org/2018/papers/0146/},
  file = {/Users/j/Zotero/storage/I5AH32WJ/Levy (2018) Communicative efficiency, uniform information dens.pdf}
}

@misc{lew.a:2022RAVI,
  title = {Recursive {{Monte Carlo}} and Variational Inference with Auxiliary Variables},
  author = {Lew, Alexander K. and {Cusumano-Towner}, Marco and Mansinghka, Vikash K.},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2203.02836},
  url = {https://arxiv.org/abs/2203.02836},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2203.02836},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-05 09:09:41 -0400},
  date-modified = {2022-05-05 09:12:32 -0400},
  keywords = {monte carlo,recursive auxiliary-variable inference,variational inference},
  file = {/Users/j/Zotero/storage/UHYKBG9B/Lew et al. - 2022 - Recursive monte carlo and variational inference wi.pdf}
}

@misc{lew.a:2023SMCP3,
  title = {{{SMCP3}}: {{SMC}} with Probabilistic Program Proposals},
  shorttitle = {Smcp3},
  author = {Lew, Alexander K. and Matheos, George and Ghavamizadeh, Matin and Gothoskar, Nishad and Russell, Stuart and Mansinghka, Vikash K.},
  year = {2023},
  url = {https://george.matheos.com/publication/23-smcp3/},
  abstract = {There is a widespread need for sound, flexible frameworks for Monte Carlo inference. This paper introduces SMCP3, a sequential Monte Carlo framework that broadens the class of strategies practitioners can employ to update particles from iteration to iteration, relative to existing frameworks like resample-move SMC (Gilks \& Berzuini, 2001) and SMC samplers (Del Moral et al., 2006). In SMCP3, proposal kernels can be general probabilistic programs, which differ from traditional proposal densities in that they may sample many auxiliary variables, and may apply deterministic post-processing to calculate a proposed update. We have implemented our framework in the Gen probabilistic programming platform: given probabilistic programs that specify target distributions, forward kernels, and reverse kernels, our implementation fully automates the sound computation of incremental importance weights. To illustrate the effectiveness of SMCP3 algorithms, we apply our framework in two domains. First, we use it for online state-estimation, using proposal programs based on Langevin ascent to reduce the bias in log marginal likelihood estimates relative to resample-move SMC with Langevin rejuvenation. Second, we demonstrate an SMCP3 algorithm that yields more robust online clustering in Dirichlet process mixture models than strong SMC baselines.},
  file = {/Users/j/Zotero/storage/K5JTMIMS/SMCP3.pdf;/Users/j/Zotero/storage/MLRWLMF3/Lew et al. (2023) SMCP3 SMC with Probabilistic Program Proposals.pdf}
}

@article{lewis.m:2014,
  title = {Combined Distributional and Logical Semantics},
  author = {Lewis, Mike and Steedman, Mark},
  year = {2013},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {179--192},
  doi = {10.1162/tacl_a_00219},
  url = {https://www.aclweb.org/anthology/Q13-1015},
  file = {/Users/j/Zotero/storage/TDDALVLI/Lewis and Steedman - 2013 - Combined distributional and logical semantics.pdf}
}

@inproceedings{lewis.m:2019bart,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  pages = {7871--7880},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.703},
  url = {https://www.aclweb.org/anthology/2020.acl-main.703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.703}
}

@article{lewis.r:2005,
  title = {An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval},
  author = {Lewis, Richard L. and Vasishth, Shravan},
  year = {2005},
  journal = {Cognitive Science},
  volume = {29},
  number = {3},
  pages = {375--419},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_25},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog0000_25},
  urldate = {2022-07-15},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated control structure that subserve sentence comprehension. The theory is derived from the application of independently motivated principles of memory and cognitive skill to the specialized task of sentence parsing. The resulting theory construes sentence processing as a series of skilled associative memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive principles are formalized in computational form in the Adaptive Control of Thought\textendash Rational (ACT\textendash R) architecture, and our process model is realized in ACT\textendash R. We present the results of 6 sets of simulations: 5 simulation sets provide quantitative accounts of the effects of length and structural interference on both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-embedding difficulty is a novel one that derives from the model' complete reliance on discriminating retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT\textendash R defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference are the key factors shaping working memory in sentence processing. We contrast the theory and empirical predictions with several related accounts of sentence-processing complexity.},
  langid = {english},
  keywords = {ACT-R,Activation,Cognitive architectures,Cognitive modeling,Decay,Interference,Parsing,Sentence processing,Syntax,Working memory},
  file = {/Users/j/Zotero/storage/MBNQSLBP/Lewis and Vasishth (2005) An Activation-Based Model of Sentence Processing a.pdf}
}

@article{lewis.r:2006,
  title = {Computational Principles of Working Memory in Sentence Comprehension},
  author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
  year = {2006},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {10},
  pages = {447--454},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.08.007},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661306002142},
  urldate = {2022-09-08},
  abstract = {Understanding a sentence requires a working memory of the partial products of comprehension, so that linguistic relations between temporally distal parts of the sentence can be rapidly computed. We describe an emerging theoretical framework for this working memory system that incorporates several independently motivated principles of memory: a sharply limited attentional focus, rapid retrieval of item (but not order) information subject to interference from similar items, and activation decay (forgetting over time). A computational model embodying these principles provides an explanation of the functional capacities and severe limitations of human processing, as well as accounts of reading times. The broad implication is that the detailed nature of crosslinguistic sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension.},
  langid = {english},
  keywords = {ACT-R,comprehension,memory,sentence processing},
  file = {/Users/j/Zotero/storage/JESGWVM3/Lewis et al. (2006) Computational principles of working memory in sent.pdf}
}

@inproceedings{li.b:2020headsup,
  title = {Heads-up! {{Unsupervised}} Constituency Parsing via Self-Attention Heads},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  author = {Li, Bowen and Kim, Taeuk and Amplayo, Reinald Kim and Keller, Frank},
  year = {2020},
  pages = {409--424},
  publisher = {{Association for Computational Linguistics}},
  address = {{Suzhou, China}},
  url = {https://www.aclweb.org/anthology/2020.aacl-main.43}
}

@phdthesis{li.b:2022PhD,
  title = {Integrating Linguistic Theory and Neural Language Models},
  author = {Li, Bai},
  year = {2022},
  month = jul,
  eprint = {2207.09643},
  primaryclass = {cs},
  address = {{Toronto}},
  url = {http://arxiv.org/abs/2207.09643},
  urldate = {2022-07-22},
  abstract = {Transformer-based language models have recently achieved remarkable results in many natural language tasks. However, performance on leaderboards is generally achieved by leveraging massive amounts of training data, and rarely by encoding explicit linguistic knowledge into neural models. This has led many to question the relevance of linguistics for modern natural language processing. In this dissertation, I present several case studies to illustrate how theoretical linguistics and neural language models are still relevant to each other. First, language models are useful to linguists by providing an objective tool to measure semantic distance, which is difficult to do using traditional methods. On the other hand, linguistic theory contributes to language modelling research by providing frameworks and sources of data to probe our language models for specific aspects of language understanding. This thesis contributes three studies that explore different aspects of the syntax-semantics interface in language models. In the first part of my thesis, I apply language models to the problem of word class flexibility. Using mBERT as a source of semantic distance measurements, I present evidence in favour of analyzing word class flexibility as a directional process. In the second part of my thesis, I propose a method to measure surprisal at intermediate layers of language models. My experiments show that sentences containing morphosyntactic anomalies trigger surprisals earlier in language models than semantic and commonsense anomalies. Finally, in the third part of my thesis, I adapt several psycholinguistic studies to show that language models contain knowledge of argument structure constructions. In summary, my thesis develops new connections between natural language processing, linguistic theory, and psycholinguistics to provide fresh perspectives for the interpretation of language models.},
  archiveprefix = {arxiv},
  school = {University of Toronto},
  keywords = {Computer Science - Computation and Language}
}

@misc{li.j:2016a,
  title = {Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author = {Li, Jiwei and Jurafsky, Dan},
  year = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1601.00372},
  url = {https://arxiv.org/abs/1601.00372},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1601.00372},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:37:17 -0400},
  date-modified = {2022-05-15 15:40:00 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Zotero/storage/JWLLWW5N/Li and Jurafsky - 2016 - Mutual information and diverse decoding improve ne.pdf}
}

@misc{li.j:2016b,
  title = {A Simple, Fast Diverse Decoding Algorithm for Neural Generation},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2016},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1611.08562},
  url = {https://arxiv.org/abs/1611.08562},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.08562},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:39:25 -0400},
  date-modified = {2022-05-15 15:40:09 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Zotero/storage/NMKZMPD4/Li et al. - 2016 - A simple, fast diverse decoding algorithm for neur.pdf}
}

@article{li.m:2004,
  title = {The Similarity Metric},
  author = {Li, M. and Chen, X. and Li, X. and Ma, B. and Vitanyi, P.M.B.},
  year = {2004},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {50},
  number = {12},
  pages = {3250--3264},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.838101},
  url = {http://ieeexplore.ieee.org/document/1362909/},
  urldate = {2023-01-19},
  langid = {english},
  file = {/Users/j/Zotero/storage/9XC5RSHL/Li et al. (2004) The Similarity Metric.pdf}
}

@book{li.m:2008,
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  author = {Li, Ming and Vit{\'a}nyi, Paul and others},
  year = {2008},
  volume = {3},
  publisher = {{Springer}},
  date-added = {2019-09-13 08:17:22 -0400},
  date-modified = {2019-09-13 08:17:36 -0400},
  project = {information-entropy},
  keywords = {kolmogorov complexity}
}

@inproceedings{li.x:2019,
  title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Li, Xiang Lisa and Eisner, Jason},
  year = {2019},
  pages = {2744--2754},
  publisher = {{Association for Computational Linguistics}},
  address = {{Hong Kong, China}},
  doi = {10.18653/v1/D19-1276},
  url = {https://www.aclweb.org/anthology/D19-1276},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1276}
}

@misc{li.x:2022,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {2205.14217},
  primaryclass = {cs},
  institution = {{arXiv}},
  doi = {10.48550/arXiv.2205.14217},
  url = {http://arxiv.org/abs/2205.14217},
  urldate = {2022-06-13},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Zotero/storage/L4E57MR4/Li et al. - 2022 - Diffusion-LM Improves Controllable Text Generation.pdf}
}

@inproceedings{liang.d:2018,
  title = {Variational Autoencoders for Collaborative Filtering},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  year = {2018},
  month = apr,
  series = {{{WWW}} '18},
  pages = {689--698},
  publisher = {{International World Wide Web Conferences Steering Committee}},
  address = {{Republic and Canton of Geneva, CHE}},
  doi = {10.1145/3178876.3186150},
  url = {http://doi.org/10.1145/3178876.3186150},
  urldate = {2022-11-29},
  abstract = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  isbn = {978-1-4503-5639-8},
  keywords = {bayesian models,collaborative filtering,implicit feedback,recommender systems,variational autoencoder},
  file = {/Users/j/Zotero/storage/YNASJBYC/Liang et al. (2018) Variational Autoencoders for Collaborative Filteri.pdf}
}

@article{liberti.l:2016,
  title = {Six Mathematical Gems from the History of Distance Geometry},
  author = {Liberti, Leo and Lavor, Carlile},
  year = {2016},
  journal = {International Transactions in Operational Research},
  volume = {23},
  number = {5},
  pages = {897--920},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-11 11:26:58 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry}
}

@article{lieder.f:2020,
  title = {Resource-Rational Analysis: {{Understanding}} Human Cognition as the Optimal Use of Limited Computational Resources},
  shorttitle = {Resource-Rational Analysis},
  author = {Lieder, Falk and Griffiths, Thomas L.},
  year = {2020},
  journal = {Behavioral and Brain Sciences},
  volume = {43},
  pages = {e1},
  publisher = {{Cambridge University Press}},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1900061X},
  url = {http://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/resourcerational-analysis-understanding-human-cognition-as-the-optimal-use-of-limited-computational-resources/586866D9AD1D1EA7A1EECE217D392F4A},
  urldate = {2022-11-28},
  abstract = {Modeling human cognition is challenging because there are infinitely many mechanisms that can generate any given observation. Some researchers address this by constraining the hypothesis space through assumptions about what the human mind can and cannot do, while others constrain it through principles of rationality and adaptation. Recent work in economics, psychology, neuroscience, and linguistics has begun to integrate both approaches by augmenting rational models with cognitive constraints, incorporating rational principles into cognitive architectures, and applying optimality principles to understanding neural representations. We identify the rational use of limited resources as a unifying principle underlying these diverse approaches, expressing it in a new cognitive modeling paradigm called resource-rational analysis. The integration of rational principles with realistic cognitive constraints makes resource-rational analysis a promising framework for reverse-engineering cognitive mechanisms and representations. It has already shed new light on the debate about human rationality and can be leveraged to revisit classic questions of cognitive psychology within a principled computational framework. We demonstrate that resource-rational models can reconcile the mind's most impressive cognitive skills with people's ostensive irrationality. Resource-rational analysis also provides a new way to connect psychological theory more deeply with artificial intelligence, economics, neuroscience, and linguistics.},
  langid = {english},
  keywords = {bounded rationality,cognitive biases,cognitive mechanisms,cognitive modeling,representations,resource rationality},
  file = {/Users/j/Zotero/storage/ALPJMWCN/Lieder and Griffiths (2020) Resource-rational analysis Understanding human co.pdf}
}

@inproceedings{lin.c:2018,
  title = {Neural Particle Smoothing for Sampling from Conditional Sequence Models},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Lin, Chu-Cheng and Eisner, Jason},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/n18-1085},
  url = {https://doi.org/10.18653%2Fv1%2Fn18-1085},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n18-1085},
  date-added = {2022-03-31 10:36:15 -0400},
  date-modified = {2022-03-31 10:36:38 -0400},
  keywords = {parsing,sampling}
}

@article{linzen.t:2015,
  title = {Uncertainty and Expectation in Sentence Processing: {{Evidence}} from Subcategorization Distributions},
  author = {Linzen, Tal and Jaeger, T. Florian},
  year = {2015},
  journal = {Cognitive Science},
  volume = {40},
  number = {6},
  pages = {1382--1411},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12274},
  url = {https://doi.org/10.1111%2Fcogs.12274},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12274},
  date-added = {2021-03-18 10:32:01 -0400},
  date-modified = {2021-03-18 10:37:45 -0400},
  keywords = {expectation,processing}
}

@article{linzen.t:2016,
  title = {Assessing the Ability of {{LSTMs}} to Learn Syntax-Sensitive Dependencies},
  author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  year = {2016},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {521--535},
  doi = {10.1162/tacl_a_00115},
  url = {https://www.aclweb.org/anthology/Q16-1037},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0115},
  file = {/Users/j/Zotero/storage/6XDCR75F/Linzen et al. - 2016 - Assessing the ability of LSTMs to learn syntax-sen.pdf}
}

@article{linzen.t:2018,
  title = {What Can Linguistics and Deep Learning Contribute to Each Other?},
  author = {Linzen, Tal},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.04179},
  eprint = {1809.04179},
  archiveprefix = {arxiv},
  date-added = {2019-06-13 08:03:15 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {recurrent neural networks}
}

@article{lipman.b:1995,
  title = {Information {{Processing}} and {{Bounded Rationality}}: {{A Survey}}},
  shorttitle = {Information {{Processing}} and {{Bounded Rationality}}},
  author = {Lipman, Barton L.},
  year = {1995},
  journal = {The Canadian Journal of Economics / Revue canadienne d'Economique},
  volume = {28},
  number = {1},
  eprint = {136022},
  eprinttype = {jstor},
  pages = {42--67},
  publisher = {{[Wiley, Canadian Economics Association]}},
  issn = {0008-4085},
  doi = {10.2307/136022},
  url = {https://www.jstor.org/stable/136022},
  urldate = {2022-06-14},
  abstract = {This paper surveys recent attempts to formulate a plausible and tractable model of bounded rationality. I focus in particular on models that view bounded rationality as stemming from limited information processing. I discuss partitional models (such as computability, automata, perceptrons, and optimal networks), non-partitional models, and axiomatic approaches. /// Transformation de l'information et rationalit\'e limit\'ee: une revue de la litt\'erature. Ce m\'emoire examine certaines tentatives r\'ecentes pour formuler un mod\`ele plausible et utilisable de la rationalit\'e limit\'ee. L'auteur s'attache en particulier aux mod\`eles qui pr\'esentent la rationalit\'e limit\'ee comme un ph\'enom\`ene \'emanant de la limitation dans la capacit\'e \`a transformer l'information. L'auteur discute les mod\`eles qu'on appelle `partitionnels' (computabilit\'e, automates, perceptrons, r\'eseaux optimaux), les mod\`eles `non partitionnels' ainsi que les approches axiomatiques.},
  file = {/Users/j/Zotero/storage/ZDL2QRR9/Lipman - 1995 - Information Processing and Bounded Rationality A .pdf}
}

@article{liu.j:1998,
  title = {Rejection Control and Sequential Importance Sampling},
  author = {Liu, Jun S. and Chen, Rong and Wong, Wing Hung},
  year = {1998},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {443},
  pages = {1022--1031},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/01621459.1998.10473764},
  url = {https://doi.org/10.1080%2F01621459.1998.10473764},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1998.10473764},
  date-added = {2022-05-05 09:40:36 -0400},
  date-modified = {2022-05-05 09:42:57 -0400},
  keywords = {importance sampling,rejection controlled sequential importance sampling,sequential importance sampling,sequential monte carlo}
}

@book{liu.j:2004,
  title = {Monte {{Carlo}} Strategies in Scientific Computing},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {{Springer}},
  address = {{New York, NY}},
  doi = {10.1007/978-0-387-76371-2},
  url = {http://link.springer.com/10.1007/978-0-387-76371-2},
  urldate = {2022-12-07},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  keywords = {convergence of random variables,Excel,Markov chain,Markov Chains,mathematical statistics,modeling,Monte Carlo Method,optimization,Potential,Probability theory,Random variable,Scientific Computing,statistics},
  file = {/Users/j/Zotero/storage/C6EKXWN8/Liu (2004) Monte Carlo Strategies in Scientific Computing.pdf}
}

@article{liu.j:2017,
  title = {In-Order Transition-Based Constituent Parsing},
  author = {Liu, Jiangming and Zhang, Yue},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {413--424},
  doi = {10.1162/tacl_a_00070},
  url = {https://www.aclweb.org/anthology/Q17-1029},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0070},
  file = {/Users/j/Zotero/storage/5T8Q2YHA/Liu and Zhang - 2017 - In-order transition-based constituent parsing.pdf}
}

@misc{liu.q:2020,
  title = {A Survey on Contextual Embeddings},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  year = {2020},
  eprint = {2003.07278},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {word embeddings}
}

@inproceedings{liu.z:2021,
  title = {Morphological {{Segmentation}} for {{Seneca}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}}},
  author = {Liu, Zoey and Jimerson, Robert and Prud'hommeaux, Emily},
  year = {2021},
  month = jun,
  pages = {90--101},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.americasnlp-1.10},
  url = {https://aclanthology.org/2021.americasnlp-1.10},
  urldate = {2022-06-06},
  abstract = {This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning.},
  keywords = {computational revitalization,iroquoian,morphology},
  file = {/Users/j/Zotero/storage/XPP5CDYA/Liu et al. - 2021 - Morphological Segmentation for Seneca.pdf}
}

@article{lo.s:2015,
  title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
  author = {Lo, Steson and Andrews, Sally},
  year = {2015},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {{Frontiers Media SA}},
  doi = {10.3389/fpsyg.2015.01171},
  url = {https://doi.org/10.3389%2Ffpsyg.2015.01171},
  bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2015.01171},
  date-added = {2022-02-23 22:30:34 -0500},
  date-modified = {2022-02-23 22:30:36 -0500},
  file = {/Users/j/Zotero/storage/ZK6XN5P6/Lo and Andrews - 2015 - To transform or not to transform using generalize.pdf}
}

@article{lomashvili.l:2011,
  title = {Phases and Templates in {{Georgian}} Agreement},
  author = {Lomashvili, Leila and Harley, Heidi},
  year = {2011},
  journal = {Studia Linguistica},
  volume = {65},
  number = {3},
  pages = {233--267},
  publisher = {{Wiley Online Library}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:50 -0400},
  project = {Icelandic gluttony},
  keywords = {phase theory,phi features}
}

@misc{lou.p:2018,
  title = {Disfluency Detection Using a Noisy Channel Model and a Deep Neural Language Model},
  author = {Lou, Paria Jamshid and Johnson, Mark},
  year = {2018},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1808.09091},
  url = {https://arxiv.org/abs/1808.09091},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.09091},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-27 10:29:36 -0400},
  date-modified = {2022-04-27 10:29:37 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/367YWNTS/Lou and Johnson - 2018 - Disfluency detection using a noisy channel model a.pdf}
}

@incollection{lounsbury.f:1954,
  title = {Transitional Probability, Linguistic Structure, and Systems of Habit-Family Hierarchies},
  booktitle = {Psycholinguistics},
  author = {Lounsbury, Floyd G},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {93--101},
  publisher = {{Waverly Press Baltimore}},
  url = {https://publish.iupress.indiana.edu/read/db9a6002-fd15-44c0-a60e-65b6af037d2b/section/cebab226-583c-4176-a54c-0c8461c45bbc#fn47},
  chapter = {5.1},
  date-added = {2022-04-14 23:38:02 -0400},
  date-modified = {2022-04-14 23:51:56 -0400},
  keywords = {entropy reduction}
}

@article{lowder.m:2018,
  title = {Lexical Predictability during Natural Reading: {{Effects}} of Surprisal and Entropy Reduction},
  shorttitle = {Lexical Predictability during Natural Reading},
  author = {Lowder, Matthew W. and Choi, Wonil and Ferreira, Fernanda and Henderson, John M.},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1166--1183},
  issn = {1551-6709},
  doi = {10.1111/cogs.12597},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12597},
  urldate = {2022-10-13},
  abstract = {What are the effects of word-by-word predictability on sentence processing times during the natural reading of a text? Although information complexity metrics such as surprisal and entropy reduction have been useful in addressing this question, these metrics tend to be estimated using computational language models, which require some degree of commitment to a particular theory of language processing. Taking a different approach, this study implemented a large-scale cumulative cloze task to collect word-by-word predictability data for 40 passages and compute surprisal and entropy reduction values in a theory-neutral manner. A separate group of participants read the same texts while their eye movements were recorded. Results showed that increases in surprisal and entropy reduction were both associated with increases in reading times. Furthermore, these effects did not depend on the global difficulty of the text. The findings suggest that surprisal and entropy reduction independently contribute to variation in reading times, as these metrics seem to capture different aspects of lexical predictability.},
  langid = {english},
  keywords = {entropy reduction,Entropy reduction,Eyetracking,Prediction,Sentence processing,Surprisal,surprisal theory},
  file = {/Users/j/Zotero/storage/L6Z7H5NY/Lowder et al. (2018) Lexical Predictability During Natural Reading Eff.pdf}
}

@inproceedings{lueckmann.j:2021,
  title = {Benchmarking Simulation-Based Inference},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {{AISTATS}} 2021, April 13-15, 2021, Virtual Event},
  author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David S. and Gon{\c c}alves, Pedro J. and Macke, Jakob H.},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021},
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {343--351},
  publisher = {{PMLR}},
  url = {http://proceedings.mlr.press/v130/lueckmann21a.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aistats/LueckmannBGGM21.bib},
  timestamp = {Wed, 14 Apr 2021 01:00:00 +0200}
}

@misc{lugosch.l:2020,
  title = {Surprisal-Triggered Conditional Computation with Neural Networks},
  author = {Lugosch, Loren and Nowrouzezahrai, Derek and Meyer, Brett H.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.01659},
  eprint = {2006.01659},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/2006.01659},
  urldate = {2023-03-19},
  abstract = {Autoregressive neural network models have been used successfully for sequence generation, feature extraction, and hypothesis scoring. This paper presents yet another use for these models: allocating more computation to more difficult inputs. In our model, an autoregressive model is used both to extract features and to predict observations in a stream of input observations. The surprisal of the input, measured as the negative log-likelihood of the current observation according to the autoregressive model, is used as a measure of input difficulty. This in turn determines whether a small, fast network, or a big, slow network, is used. Experiments on two speech recognition tasks show that our model can match the performance of a baseline in which the big network is always used with 15\% fewer FLOPs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{luke.s:2017,
  title = {The {{Provo Corpus}}: {{A}} Large Eye-Tracking Corpus with Predictability Norms},
  author = {Luke, Steven G. and Christianson, Kiel},
  year = {2017},
  month = may,
  volume = {50},
  number = {2},
  pages = {826--833},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-017-0908-4},
  url = {https://doi.org/10.3758%2Fs13428-017-0908-4},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-017-0908-4},
  date-added = {2021-10-19 00:07:37 -0400},
  date-modified = {2021-10-19 00:07:38 -0400}
}

@inproceedings{luong.t:2015,
  title = {Evaluating Models of Computation and Storage in Human Sentence Processing},
  booktitle = {Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning},
  author = {Luong, Thang and O'Donnell, Timothy and Goodman, Noah},
  year = {2015},
  month = sep,
  pages = {14--21},
  publisher = {{Association for Computational Linguistics}},
  address = {{Lisbon, Portugal}},
  doi = {10.18653/v1/W15-2403},
  url = {https://aclanthology.org/W15-2403},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W15-2403},
  date-added = {2022-05-02 11:30:20 -0400},
  date-modified = {2022-05-17 08:07:37 -0400},
  keywords = {fragment grammars,incrementality,parsing}
}

@book{lurie.j:2009,
  title = {Higher Topos Theory (Preprint)},
  author = {Lurie, Jacob},
  year = {2009},
  publisher = {{Princeton University Press}},
  date-added = {2019-08-24 09:21:19 -0400},
  date-modified = {2019-08-24 09:23:18 -0400},
  keywords = {category theory,topos theory}
}

@book{mackay.d:2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  publisher = {{Cambridge university press}},
  url = {https://www.inference.org.uk/itila/},
  date-added = {2020-02-16 21:05:41 -0500},
  date-modified = {2020-04-29 12:55:23 -0400},
  project = {information-entropy},
  keywords = {information theory}
}

@inproceedings{madureira.b:2020,
  title = {Incremental Processing in the Age of Non-Incremental Encoders: {{An}} Empirical Assessment of Bidirectional Models for Incremental {{NLU}}},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Madureira, Brielen and Schlangen, David},
  year = {2020},
  pages = {357--374},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.26},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.26},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.26}
}

@article{maehara.h:2013,
  title = {Euclidean Embeddings of Finite Metric Spaces},
  author = {Maehara, Hiroshi},
  year = {2013},
  journal = {Discrete Mathematics},
  volume = {313},
  number = {23},
  pages = {2848--2856},
  publisher = {{Elsevier}},
  date-added = {2019-06-13 07:52:44 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {euclidean space,geometry}
}

@inproceedings{magerman.d:1990,
  title = {Parsing a Natural Language Using Mutual Information Statistics.},
  booktitle = {{{AAAI}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1990},
  volume = {90},
  pages = {984--989},
  url = {https://www.aaai.org/Library/AAAI/1990/aaai90-147.php},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:27:23 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,word association}
}

@inproceedings{magerman.d:1991,
  title = {{\emph{P}}earl: A Probabilistic Chart Parser},
  shorttitle = {{\emph{P}}earl},
  booktitle = {Proceedings of the Fifth Conference on {{European}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1991},
  month = apr,
  series = {{{EACL}} '91},
  pages = {15--20},
  publisher = {{Association for Computational Linguistics}},
  address = {{USA}},
  doi = {10.3115/977180.977184},
  url = {http://doi.org/10.3115/977180.977184},
  urldate = {2022-06-13},
  abstract = {This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.},
  file = {/Users/j/Zotero/storage/AF9TTST8/Magerman and Marcus - 1991 - Pearl a probabilistic chart parser.pdf}
}

@article{makkeh.a:2021,
  title = {Introducing a Differentiable Measure of Pointwise Shared Information},
  author = {Makkeh, Abdullah and Gutknecht, Aaron J. and Wibral, Michael},
  year = {2021},
  month = mar,
  journal = {Physical Review E},
  volume = {103},
  number = {3},
  publisher = {{American Physical Society (APS)}},
  doi = {10.1103/physreve.103.032149},
  url = {https://doi.org/10.1103%2Fphysreve.103.032149},
  bdsk-url-2 = {https://doi.org/10.1103/physreve.103.032149},
  date-added = {2022-04-18 11:15:53 -0400},
  date-modified = {2022-04-18 11:16:04 -0400},
  keywords = {partial information decomposition}
}

@book{malchukov.a:2012,
  title = {The Oxford Handbook of Case},
  author = {Malchukov, Andrej L. and Spencer, Andrew},
  year = {2012},
  publisher = {{Oxford University Press}},
  url = {https://www.oxfordhandbooks.com/view/10.1093/oxfordhb/9780199206476.001.0001/oxfordhb-9780199206476},
  date-added = {2020-02-03 16:08:35 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  isbn = {978-0-19-920647-6},
  project = {Icelandic gluttony},
  keywords = {case}
}

@article{manning.c:2020,
  title = {Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision},
  author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30046--30054},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1907367117},
  url = {https://doi.org/10.1073%2Fpnas.1907367117},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1907367117},
  date-added = {2021-07-16 19:46:55 -0400},
  date-modified = {2021-07-16 19:46:57 -0400}
}

@inproceedings{mansinghka.v:2009,
  title = {Exact and Approximate Sampling by Systematic Stochastic Search},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  author = {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},
  editor = {{van Dyk}, David and Welling, Max},
  year = {2009},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {5},
  pages = {400--407},
  publisher = {{PMLR}},
  address = {{Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA}},
  url = {https://proceedings.mlr.press/v5/mansinghka09a.html},
  abstract = {We introduce \textsubscript{a}daptive sequential rejection sampling\textsubscript{,} an algorithm for generating exact samples from high-dimensional, discrete distributions, building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions, sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation, and a randomized version of the directed arc consistency algorithm with backtracking when applied to deterministic problems. In this paper, we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on ferromagnetic Isings and other probabilistic graphical models, obtaining exact and approximate samples in a range of situations.},
  date-added = {2022-05-05 09:38:21 -0400},
  date-modified = {2022-05-05 09:39:35 -0400},
  pdf = {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},
  keywords = {adaptive sequential rejection sampling}
}

@phdthesis{marcken.c:1996,
  title = {Unsupervised Language Acquisition},
  author = {{de Marcken}, Carl},
  year = {1996},
  url = {http://hdl.handle.net/1721.1/10640},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/phd/ndltd/Marcken96},
  date-added = {2020-01-27 11:53:05 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Cambridge, MA, USA},
  keywords = {information theory,unsupervised grammar induction},
  timestamp = {Mon, 08 May 2017 16:29:45 +0200}
}

@incollection{marcken.c:1999,
  title = {On the Unsupervised Induction of Phrase-Structure Grammars},
  booktitle = {Natural Language Processing Using Very Large Corpora},
  author = {{de Marcken}, C.},
  editor = {Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  year = {1999},
  pages = {191--208},
  publisher = {{Springer Netherlands}},
  address = {{Dordrecht}},
  doi = {10.1007/978-94-017-2390-9_12},
  url = {https://doi.org/10.1007/978-94-017-2390-9â‚2},
  abstract = {Researchers investigating the acquisition of phrase-structure grammars from raw text have had only mixed success. In particular, unsupervised learning techniques, such as the inside-outside algorithm (Baker, 1979) for estimating the parameters of stochastic context-free grammars (SCFGs), tend to produce grammars that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to constrain the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint, and currently the best performing parsers are trained on treebanks (Black et al., 1992; Magerman, 1995).},
  date-added = {2020-01-27 11:50:06 -0500},
  date-modified = {2021-07-16 11:27:47 -0400},
  isbn = {978-94-017-2390-9},
  project = {syntactic embedding},
  keywords = {information theory,unsupervised grammar induction}
}

@phdthesis{marcus.m:1978phd,
  title = {A Theory of Syntactic Recognition for Natural Language},
  author = {Marcus, Mitchell P.},
  year = {1978},
  url = {http://hdl.handle.net/1721.1/16176},
  date-added = {2022-03-31 11:14:02 -0400},
  date-modified = {2022-04-26 21:21:13 -0400},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Zotero/storage/CQ6QYUBY/Marcus - 1978 - A theory of syntactic recognition for natural lang.pdf}
}

@book{marcus.m:1980phdbook,
  title = {Theory of Syntactic Recognition for Natural Languages},
  author = {Marcus, Mitchell P.},
  year = {1980},
  publisher = {{MIT Press}},
  address = {{Cambridge, MA, USA}},
  date-added = {2022-03-31 11:15:48 -0400},
  date-modified = {2022-04-26 21:21:21 -0400},
  isbn = {0-262-13149-8}
}

@inproceedings{marcus.m:1994,
  title = {The {{Penn Treebank}}: {{Annotating}} Predicate Argument Structure},
  booktitle = {Human {{Language Technology}}: {{Proceedings}} of a {{Workshop}} Held at {{Plainsboro}}, {{New Jersey}}, {{March}} 8-11, 1994},
  author = {Marcus, Mitchell P. and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  year = {1994},
  url = {https://www.aclweb.org/anthology/H94-1020}
}

@phdthesis{marecek.d:2012,
  title = {Unsupervised Dependency Parsing},
  author = {Mare{\v c}ek, David},
  year = {2012},
  address = {{Prague, Czech Republic}},
  url = {http://ufal.mff.cuni.cz/biblio/attachments/2012-marecek-m1481417340536440366.pdf},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-09-07 16:53:21 -0400},
  project = {syntactic embedding},
  school = {Charles University},
  keywords = {dependency parsing,unsupervised parsing},
  file = {/Users/j/Zotero/storage/32CIN6HJ/MareÄek - 2012 - Unsupervised dependency parsing.pdf}
}

@inproceedings{marecek.d:2018,
  title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
  booktitle = {Proceedings of the 2018 {{EMNLP}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2018},
  month = nov,
  pages = {347--349},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/W18-5444},
  url = {https://aclanthology.org/W18-5444},
  abstract = {This is a work in progress about extracting the sentence tree structures from the encoder's self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W18-5444},
  date-added = {2021-09-08 00:32:46 -0400},
  date-modified = {2021-09-08 00:32:47 -0400}
}

@inproceedings{marecek.d:2019,
  title = {From Balustrades to Pierre Vinken: {{Looking}} for Syntax in Transformer Self-Attentions},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2019},
  pages = {263--275},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/W19-4827},
  url = {https://www.aclweb.org/anthology/W19-4827},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4827}
}

@article{marneffe.m:2019,
  title = {Dependency Grammar},
  author = {{de Marneffe}, Marie-Catherine and Nivre, Joakim},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {197--218},
  publisher = {{Annual Reviews}},
  doi = {10.1146/annurev-linguistics-011718-011842},
  url = {https://doi.org/10.1146%2Fannurev-linguistics-011718-011842},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011842},
  date-added = {2021-07-16 19:34:18 -0400},
  date-modified = {2021-07-16 19:34:19 -0400}
}

@article{marr.d:1976,
  title = {From Understanding Computation to Understanding Neural Circuitry},
  author = {Marr, D. and Poggio, T.},
  year = {1976},
  month = may,
  url = {https://dspace.mit.edu/handle/1721.1/5782},
  urldate = {2022-06-06},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  langid = {american},
  annotation = {Accepted: 2004-10-01T20:36:50Z},
  file = {/Users/j/Zotero/storage/6D4ZPKH7/Marr and Poggio - 1976 - From Understanding Computation to Understanding Ne.pdf}
}

@book{marr.d:1982,
  title = {Vision: {{A}} Computational Investigation into the Human Representation and Processing of Visual Information},
  author = {Marr, David},
  year = {1982},
  publisher = {{W. H. Freeman}},
  address = {{San Francisco, CA}},
  date-added = {2021-12-01 19:20:38 -0500},
  date-modified = {2021-12-01 19:21:38 -0500}
}

@article{marslen-wilson.w:1973,
  title = {Linguistic Structure and Speech Shadowing at Very Short Latencies},
  author = {{Marslen-Wilson}, William D.},
  year = {1973},
  month = aug,
  journal = {Nature},
  volume = {244},
  number = {5417},
  pages = {522--523},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/244522a0},
  url = {https://doi.org/10.1038%2F244522a0},
  bdsk-url-2 = {https://doi.org/10.1038/244522a0},
  date-added = {2022-04-14 13:38:15 -0400},
  date-modified = {2022-05-02 14:45:30 -0400},
  keywords = {incrementality},
  file = {/Users/j/Zotero/storage/XDKKSWUI/Marslen-Wilson - 1973 - Linguistic structure and speech shadowing at very .pdf}
}

@article{marslen-wilson.w:1975,
  title = {Sentence Perception as an Interactive Parallel Process},
  author = {{Marslen-Wilson}, William D.},
  year = {1975},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {189},
  number = {4198},
  pages = {226--228},
  publisher = {{American Association for the Advancement of Science (AAAS)}},
  doi = {10.1126/science.189.4198.226},
  url = {https://doi.org/10.1126%2Fscience.189.4198.226},
  bdsk-url-2 = {https://doi.org/10.1126/science.189.4198.226},
  date-added = {2022-04-14 13:38:57 -0400},
  date-modified = {2022-05-02 14:45:37 -0400},
  keywords = {incrementality},
  file = {/Users/j/Zotero/storage/AZY7WBPS/Marslen-Wilson - 1975 - Sentence perception as an interactive parallel pro.pdf}
}

@article{martino.l:2017,
  title = {Effective Sample Size for Importance Sampling Based on Discrepancy Measures},
  author = {Martino, Luca and Elvira, V{\'i}ctor and Louzada, Francisco},
  year = {2017},
  month = feb,
  journal = {Signal Processing},
  volume = {131},
  pages = {386--401},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2016.08.025},
  url = {https://www.sciencedirect.com/science/article/pii/S0165168416302110},
  urldate = {2022-12-16},
  abstract = {The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation ESS\^ of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, ESS\^, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression ESS\^ is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.},
  langid = {english},
  keywords = {Bayesian Inference,Effective Sample Size,Importance Sampling,Particle Filtering,Perplexity,Sequential Monte Carlo},
  file = {/Users/j/Zotero/storage/PX9BU9RG/Martino et al. (2017) Effective sample size for importance sampling base.pdf}
}

@incollection{martino.l:2018,
  title = {Adaptive Rejection Sampling Methods},
  booktitle = {Independent {{Random Sampling Methods}}},
  author = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  editor = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  year = {2018},
  series = {Statistics and {{Computing}}},
  pages = {115--157},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-72634-2_4},
  url = {https://doi.org/10.1007/978-3-319-72634-2_4},
  urldate = {2022-07-05},
  abstract = {This chapter is devoted to describing the class of the adaptive rejection sampling (ARS) schemes. These (theoretically) universal methods are very efficient samplers that update the proposal density whenever a generated sample is rejected in the RS test. In this way, they can produce i.i.d. samples from the target with an increasing acceptance rate that can converge to 1. As a by-product, these techniques also generate a sequence of proposal pdfs converging to the true shape of the target density. Another advantage of the ARS samplers is that, when they can be applied, the user only has to select a set of initial conditions. After the initialization, they are completely automatic, self-tuning algorithms (i.e., no parameters need to be adjusted by the user) regardless of the specific target density. However, the need to construct a suitable sequence of proposal densities restricts the practical applicability of this methodology. As a consequence, ARS schemes are often tailored to specific classes of target distributions. Indeed, the construction of the proposal is particularly hard in multidimensional spaces. Hence, ARS algorithms are usually designed only for drawing from univariate densities.},
  isbn = {978-3-319-72634-2},
  langid = {english},
  file = {/Users/j/Zotero/storage/LF75KLR6/Martino et al. - 2018 - Adaptive Rejection Sampling Methods.pdf}
}

@inproceedings{marvin.r:2018,
  title = {Targeted Syntactic Evaluation of Language Models},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Marvin, Rebecca and Linzen, Tal},
  year = {2018},
  pages = {1192--1202},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1151},
  url = {https://www.aclweb.org/anthology/D18-1151},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1151}
}

@article{marvin.r:2018a,
  title = {Targeted Syntactic Evaluation of Language Models},
  author = {Marvin, Rebecca and Linzen, Tal},
  year = {2018},
  month = aug,
  journal = {arXiv:1808.09031 [cs]},
  eprint = {1808.09031},
  primaryclass = {cs},
  url = {http://arxiv.org/abs/1808.09031},
  urldate = {2020-02-16},
  abstract = {We present a dataset for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5NMKJ8NZ/Marvin and Linzen - 2018 - Targeted Syntactic Evaluation of Language Models.pdf}
}

@article{mazur.b:2008,
  title = {When Is One Thing Equalto Some Other Thing?},
  author = {Mazur, Barry},
  year = {2008},
  journal = {Proof and other dilemmas: Mathematics and philosophy},
  volume = {59},
  pages = {221},
  publisher = {{MAA}},
  date-added = {2019-08-24 09:29:47 -0400},
  date-modified = {2019-08-24 09:29:57 -0400},
  keywords = {category theory}
}

@inproceedings{mccann.b:2017,
  title = {Learned in Translation: {{Contextualized}} Word Vectors},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, {{CA}}, {{USA}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {6294--6305},
  url = {https://proceedings.neurips.cc/paper/2017/hash/20c86a628232a67e7bd46f76fba7ce12-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/McCannBXS17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@incollection{mccloskey.j:2017,
  title = {Resumption},
  booktitle = {The {{Wiley Blackwell Companion}} to {{Syntax}}, {{Second Edition}}},
  author = {McCloskey, James},
  year = {2017},
  pages = {1--30},
  publisher = {{John Wiley \& Sons, Ltd}},
  doi = {10.1002/9781118358733.wbsyncom105},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/9781118358733.wbsyncom105},
  urldate = {2023-04-10},
  abstract = {In many languages and in a range of circumstances, a pronominal element may appear in a position in which one might have expected to see a gap bound by a clause-peripheral element (in relative clauses, constituent questions, cleft constructions, and the like). Such elements (often, but not exclusively, personal pronouns) are known as resumptive elements. Since they are formally pronouns but serve many of the core semantic functions associated with movement constructions, the study of resumptive elements raises fundamental questions about the nature of movement, about the nature of anaphoric elements and relationships, and about the nature of the interaction between these two spheres. These questions have been the focus of a great deal of work, beginning especially in the middle 1970s; this chapter surveys that work \textendash{} the questions that have been asked, the phenomena that have been discovered, and the current state of thinking about the relevant questions. Consideration of the theoretical questions is organized around the asking of three questions: (i) what are the properties of resumptive elements; (ii) in what ways do they share, or fail to share, properties of movement constructions (island phenomena, reconstruction effects); and (iii) in what ways do they share, or fail to share, properties of anaphoric elements and interactions? In recent years especially, the study of resumption has been enriched by a great deal of experimental work, aimed not only at establishing some basic properties of resumptive elements, but also at better understanding the mechanisms involved in their production and comprehension. The chapter ends with a consideration of that work and its implications. Here, the central question that emerges quickly is that of how theories of competence and theories of performance interact with one another.},
  isbn = {978-1-118-35873-3},
  langid = {english},
  keywords = {anaphora,Chomsky,Noam,production,resumptive pronouns,syntax,theoretical linguistics},
  file = {/Users/j/Zotero/storage/YYNQZRA9/McCloskey (2017) Resumption.pdf}
}

@incollection{mcconnell-ginet.s:2000,
  title = {Meaning and Grammar: {{An}} Introduction to Semantics},
  booktitle = {Meaning and Grammar: {{An}} Introduction to Semantics},
  author = {{McConnell-Ginet}, Sally and Chierchia, Gennaro},
  year = {2000},
  publisher = {{MIT Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{mcdonald.m:1977,
  title = {Iontenwennaweienstahkhwa' {{Mohawk}} Spelling Dictionary},
  author = {McDonald, Mary and Barnes, Ann and Cook, Louise and Herne, Jean and Jacobs, Rita and Jock, Louise and LaFrance, Harriett and Ransom, Elaine and Sinclair, Winnie and Tarbell, Elizabeth},
  editor = {Mithun, Marianne},
  year = {1977},
  month = sep,
  number = {Bulletin 429},
  publisher = {{The University of the State of New York, State Education Department}},
  address = {{Albany, N.Y.}},
  url = {http://kanienkeha.net/wp-content/uploads/2015/10/Mohawk-Spelling-Dictionary.pdf},
  date-added = {2022-05-11 11:20:30 -0400},
  date-modified = {2022-05-11 11:26:42 -0400},
  keywords = {kanien'keha},
  file = {/Users/j/Zotero/storage/C5Q2JARF/McDonald et al. - 1977 - Iontenwennaweienstahkhwa' mohawk spelling dictiona.pdf}
}

@inproceedings{mcdonald.r:2005,
  title = {Non-Projective Dependency Parsing Using Spanning Tree Algorithms},
  booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Haji{\v c}, Jan},
  year = {2005},
  pages = {523--530},
  publisher = {{Association for Computational Linguistics}},
  address = {{Vancouver, British Columbia, Canada}},
  url = {https://www.aclweb.org/anthology/H05-1066}
}

@inproceedings{mcdonald.r:2005a,
  title = {Online Large-Margin Training of Dependency Parsers},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({{ACL}}'05)},
  author = {McDonald, Ryan and Crammer, Koby and Pereira, Fernando},
  year = {2005},
  pages = {91--98},
  publisher = {{Association for Computational Linguistics}},
  address = {{Ann Arbor, Michigan}},
  doi = {10.3115/1219840.1219852},
  url = {https://www.aclweb.org/anthology/P05-1012},
  bdsk-url-2 = {https://doi.org/10.3115/1219840.1219852}
}

@article{mcdonald.s:2003,
  title = {Low-Level Predictive Inference in Reading: The Influence of Transitional Probabilities on Eye Movements},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = jul,
  journal = {Vision Research},
  volume = {43},
  number = {16},
  pages = {1735--1751},
  publisher = {{Elsevier BV}},
  doi = {10.1016/s0042-6989(03)00237-2},
  url = {https://doi.org/10.1016%2Fs0042-6989%2803%2900237-2}
}

@article{mcdonald.s:2003a,
  title = {Eye Movements Reveal the On-Line Computation of Lexical Probabilities during Reading},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = nov,
  journal = {Psychological Science},
  volume = {14},
  number = {6},
  pages = {648--652},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1046/j.0956-7976.2003.psci_1480.x},
  url = {https://doi.org/10.1046/j.0956-7976.2003.psci_1480.x},
  urldate = {2022-10-13},
  abstract = {Skilled readers are able to derive meaning from a stream of visual input with remarkable efficiency. In this article, we present the first evidence that statistical information latent in the linguistic environment can contribute to an account of reading behavior. In two eye-tracking studies, we demonstrate that the transitional probabilities between words have a measurable influence on fixation durations, and using a simple Bayesian statistical model, we show that lexical probabilities derived by combining transitional probability with the prior probability of a word's occurrence provide the most parsimonious account of the eye movement data. We suggest that the brain is able to draw upon statistical information in order to rapidly estimate the lexical probabilities of upcoming words: a computationally inexpensive mechanism that may underlie proficient reading.},
  langid = {english},
  file = {/Users/j/Zotero/storage/G3EF6LJG/McDonald and Shillcock (2003) Eye Movements Reveal the On-Line Computation of Le.pdf}
}

@article{mcfadden.t:2018,
  title = {What the {{EPP}} and Comp-Trace Effects Have in Common: {{Constraining}} Silent Elements at the Edge},
  author = {McFadden, Thomas and Sundaresan, Sandhya},
  year = {2018},
  journal = {Glossa: a journal of general linguistics},
  volume = {3},
  number = {1},
  publisher = {{Ubiquity Press}},
  date-added = {2020-02-02 08:05:04 -0500},
  date-modified = {2020-02-02 08:05:55 -0500},
  keywords = {EPP}
}

@misc{mcguffie.k:2020,
  title = {The Radicalization Risks of {{GPT-3}} and Advanced Neural Language Models},
  author = {McGuffie, Kris and Newhouse, Alex},
  year = {2020},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2009.06807},
  url = {https://arxiv.org/abs/2009.06807},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.06807},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-15 11:01:44 -0400},
  date-modified = {2022-03-15 11:01:46 -0400},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences},
  file = {/Users/j/Zotero/storage/Q8BIE2E9/McGuffie and Newhouse - 2020 - The radicalization risks of GPT-3 and advanced neu.pdf}
}

@article{mehta.p:2019,
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  year = {2019},
  month = may,
  journal = {Physics Reports},
  series = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  volume = {810},
  pages = {1--124},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.03.001},
  url = {https://www.sciencedirect.com/science/article/pii/S0370157319300766},
  urldate = {2022-07-11},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias\textendash variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton\textendash proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
  langid = {english},
  keywords = {boltzmann machines,energy models},
  file = {/Users/j/Zotero/storage/9B8KKHJB/Mehta et al. - 2019 - A high-bias, low-variance introduction to Machine .pdf}
}

@inproceedings{meister.c:2020,
  title = {If Beam Search Is the Answer, What Was the Question?},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
  year = {2020},
  pages = {2173--2185},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.170},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.170},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.170},
  date-modified = {2022-03-31 09:40:23 -0400},
  keywords = {beam search,parsing}
}

@article{meister.c:2020tacl,
  title = {Best-First Beam Search},
  author = {Meister, Clara and Vieira, Tim and Cotterell, Ryan},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {795--809},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/tacl_a_00346},
  date-added = {2022-03-31 09:48:44 -0400},
  date-modified = {2022-03-31 09:55:48 -0400},
  keywords = {beam search,memory,parsing,space-complexity,time-complexity},
  file = {/Users/j/Zotero/storage/CBY2CCF3/Meister et al. - 2020 - Best-first beam search.pdf}
}

@inproceedings{meister.c:2021,
  title = {Revisiting the Uniform Information Density Hypothesis},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Meister, Clara and Pimentel, Tiago and Haller, Patrick and J{\"a}ger, Lena and Cotterell, Ryan and Levy, Roger},
  year = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.emnlp-main.74},
  url = {https://doi.org/10.18653%2Fv1%2F2021.emnlp-main.74},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.74},
  date-added = {2022-04-15 16:06:48 -0400},
  date-modified = {2022-04-15 16:06:50 -0400},
  file = {/Users/j/Zotero/storage/AYPCPFMZ/Meister et al. (2021) Revisiting the uniform information density hypothe.pdf}
}

@book{melcuk.i:1988,
  title = {Dependency Syntax : {{Theory}} and Practice},
  author = {Mel'{\v c}uk, Igor A.},
  year = {1988},
  series = {{{SUNY}} Series in Linguistics},
  publisher = {{State University of New York Press}},
  address = {{Albany, N.Y.}},
  url = {http://www.sunypress.edu/p-164-dependency-syntax.aspx},
  abstract = {This work presents the first sustained examination of Dependency Syntax. In clear and stimulating analyses Mel'cuk promotes syntactic description in terms of dependency rather than in terms of more familiar phrase-structure. The notions of dependency relations and dependency structure are introduced and substantiated, and the advantages of dependency representation are demonstrated by applying it to a number of popular linguistic problems, e.g. grammatical subject and ergative construction. A wide array of linguistic data is used \textendash{} the well-known (Dyirbal), the less known (Lezgian), and the more recent (Alutor). Several "exotic" cases of Russian are discussed to show how dependency can be used to solve difficult technical problems. The book is not only formal and rigorous, but also strongly theory-oriented and data-based. Special attention is paid to linguistic terminology, specifically to its logical consistency. The dependency formalism is presented within the framework of a new semantics-oriented general linguistic theory, Meaning-Text theory.},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-17 10:33:46 -0400},
  isbn = {978-0-88706-450-0},
  keywords = {Dependency Grammar}
}

@article{menne.m:2012,
  title = {An Overview of the Global Historical Climatology Network-Daily Database},
  author = {Menne, Matthew J. and Durre, Imke and Vose, Russell S. and Gleason, Byron E. and Houston, Tamara G.},
  year = {2012},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {29},
  number = {7},
  pages = {897--910},
  publisher = {{American Meteorological Society}},
  doi = {10.1175/jtech-d-11-00103.1},
  url = {https://doi.org/10.1175%2Fjtech-d-11-00103.1},
  bdsk-url-2 = {https://doi.org/10.1175/jtech-d-11-00103.1},
  date-added = {2021-12-03 19:02:07 -0500},
  date-modified = {2021-12-03 19:02:09 -0500}
}

@inproceedings{merkx.d:2021,
  title = {Human Sentence Processing: {{Recurrence}} or Attention?},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Merkx, Danny and Frank, Stefan L.},
  year = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.cmcl-1.2},
  url = {https://doi.org/10.18653%2Fv1%2F2021.cmcl-1.2},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.2},
  date-added = {2021-11-29 10:15:11 -0500},
  date-modified = {2021-11-29 10:15:12 -0500}
}

@article{meylan.s:2021,
  title = {The Challenges of Large-Scale, Web-Based Language Datasets: {{Word}} Length and Predictability Revisited},
  author = {Meylan, Stephan C. and Griffiths, Thomas L.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12983},
  url = {https://doi.org/10.1111%2Fcogs.12983},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12983},
  date-added = {2021-07-25 10:58:38 -0400},
  date-modified = {2021-07-25 10:58:39 -0400}
}

@book{michelson.k:2016,
  title = {Iroquoian {{Languages}}},
  author = {Michelson, Karin},
  year = {2016},
  month = aug,
  publisher = {{Oxford University Press}},
  doi = {10.1093/acrefore/9780199384655.013.47},
  url = {https://oxfordre.com/linguistics/view/10.1093/acrefore/9780199384655.001.0001/acrefore-9780199384655-e-47},
  urldate = {2022-05-30},
  abstract = {The Iroquoian languages are spoken today in New York State, Ontario, Quebec, Wisconsin, North Carolina, and Oklahoma. The languages share a relatively small segment inventory, a challenging accentual system, polysynthetic morphology, a complex system of pronominal affixes, an unusual kinship terminology, and a syntax that functions almost exclusively to combine the meaning of two expressions. Some of the languages have been documented since contact with Europeans in the 16th century. There exists substantial scholarly linguistic work on most of the languages, and solid teaching materials continue to be developed.},
  isbn = {978-0-19-938465-5},
  langid = {english},
  keywords = {iroquoian}
}

@inproceedings{mikolov.t:2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. {{Proceedings}} of a Meeting Held December 5-8, 2013, Lake Tahoe, Nevada, United States},
  author = {Mikolov, Tom{\'a}s and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
  editor = {Burges, Christopher J. C. and Bottou, L{\'e}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  year = {2013},
  pages = {3111--3119},
  url = {https://proceedings.neurips.cc/paper/2013/hash/9aa42b31882ec039965f3c4923ce901b-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{mikolov.t:2013a,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Mikolov, Tomas and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  pages = {746--751},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia}},
  url = {https://www.aclweb.org/anthology/N13-1090}
}

@article{miller.g:1957,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information.},
  shorttitle = {The Magical Number Seven, plus or Minus Two},
  author = {Miller, George A.},
  year = {1957},
  month = feb,
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {81},
  publisher = {{US: American Psychological Association}},
  issn = {1939-1471},
  doi = {10.1037/h0043158},
  url = {https://psycnet.apa.org/fulltext/1957-02914-001.pdf},
  urldate = {2022-09-25},
  file = {/Users/j/Zotero/storage/2F7M8P8S/Miller (The magical number seven, plus or minus two Some .pdf}
}

@incollection{miller.g:1963,
  title = {Finitary Models of Language Users},
  booktitle = {Handbook of Mathematical Psychology},
  author = {Miller, George A. and Chomsky, Noam},
  editor = {Luce, D.},
  year = {1963},
  pages = {2--419},
  publisher = {{John Wiley \& Sons.}},
  url = {https://www.semanticscholar.org/paper/Finitary-models-of-language-users-Miller-Chomsky/4f3695d5dd36bb0abd91c02d2725463fca556f46},
  date-added = {2022-03-31 11:48:29 -0400},
  date-modified = {2022-03-31 11:48:31 -0400}
}

@inproceedings{milward.d:1995,
  title = {Incremental Interpretation of Categorial Grammar},
  booktitle = {Seventh Conference of the {{European}} Chapter of the Association for Computational Linguistics},
  author = {Milward, David},
  year = {1995},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  url = {https://www.aclweb.org/anthology/E95-1017}
}

@inproceedings{min.s:2022,
  title = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  pages = {5316--5330},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.365},
  url = {https://aclanthology.org/2022.acl-long.365},
  urldate = {2022-10-25},
  abstract = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  file = {/Users/j/Zotero/storage/QJG5GKQI/Min et al. (2022) Noisy Channel Language Model Prompting for Few-Sho.pdf}
}

@inproceedings{minka.t:2001,
  title = {Expectation Propagation for Approximate {{Bayesian}} Inference},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas Peter},
  year = {2001},
  month = aug,
  series = {{{UAI}}'01},
  pages = {362--369},
  publisher = {{Morgan Kaufmann Publishers Inc., San Francisco, CA}},
  address = {{Seattle, WA}},
  url = {https://dl.acm.org/doi/10.5555/2074022.2074067},
  urldate = {2023-03-30},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation," unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  isbn = {978-1-55860-800-9},
  file = {/Users/j/Zotero/storage/P2XJP8N6/minka-ep-uai.pdf}
}

@phdthesis{minka.t:2001phd,
  type = {Thesis},
  title = {A Family of Algorithms for Approximate {{Bayesian}} Inference},
  author = {Minka, Thomas Peter},
  year = {2001},
  address = {{Cambridge, MA}},
  url = {https://dspace.mit.edu/handle/1721.1/86583},
  urldate = {2023-03-30},
  abstract = {Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {expectation propogation},
  annotation = {Accepted: 2014-05-07T16:51:05Z},
  file = {/Users/j/Zotero/storage/T8UFB7XE/Minka (2001) A family of algorithms for approximate Bayesian in.pdf}
}

@incollection{mitchell.d:1984,
  title = {An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading                      1},
  booktitle = {New {{Methods}} in {{Reading Comprehension Research}}},
  author = {Mitchell, Don C.},
  year = {1984},
  publisher = {{Routledge}},
  abstract = {Over the last 5-6 years my colleagues and I have made use of three main experimental techniques for investigating immediate processing. In chronological order these were the Rapid Sequential Visual Presentation (RSVP) task, the Subject-Paced Reading Task and various types of priming tasks.},
  isbn = {978-0-429-50537-9},
  file = {/Users/j/Zotero/storage/ZYZEEASG/Mitchell (1984) An Evaluation of Subject-Paced Reading Tasks and O.pdf}
}

@inproceedings{mitchell.j:2010,
  title = {Syntactic and Semantic Factors in Processing Difficulty: {{An}} Integrated Measure},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Mitchell, Jeff and Lapata, Mirella and Demberg, Vera and Keller, Frank},
  year = {2010},
  pages = {196--206},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  url = {https://www.aclweb.org/anthology/P10-1021},
  file = {/Users/j/Zotero/storage/QDP8RXL5/Mitchell et al. (2010) Syntactic and semantic factors in processing diffi.pdf}
}

@incollection{mithun.m:2014,
  title = {Syntactic and Prosodic Structures: {{Segmentation}}, Integration, and in Between},
  booktitle = {Spoken {{Corpora}} and {{Linguistic Studies}}},
  author = {Mithun, Marianne},
  editor = {Raso, Tommaso and Mello, Heliana},
  year = {2014},
  series = {Studies in {{Corpus Linguistics}}},
  volume = {61},
  pages = {297--330},
  publisher = {{John Benjamins Publishing Company}},
  url = {http://mithun.faculty.linguistics.ucsb.edu/pdfs/Mithun%202014%20Syntactic%20and%20prosodic%20structures.pdf},
  abstract = {In this paper the focus is on syntactic and prosodic structures in a language that is typologically quite different from the majority languages of Europe and Asia. Mohawk, a language of the Iroquoian family, is indigenous to northeastern North America. Examples cited here are drawn from unscripted conversations. Though much of the grammatical structure of Mohawk differs substantially from that of European languages, many of the devices exploited by speakers to shape the flow of information converge.},
  langid = {english},
  keywords = {iroquoian},
  file = {/Users/j/Zotero/storage/IZGCP3WT/Mithun - Syntactic and prosodic structures.pdf}
}

@incollection{mithun.m:2020,
  title = {Discourse Particle Position and Information Structure},
  booktitle = {Information-{{Structural Perspectives}} on {{Discourse Particles}}},
  author = {Mithun, Marianne},
  editor = {Modicom, Pierre-Yves and Dupl{\^a}tre, Olivier},
  year = {2020},
  month = mar,
  series = {Studies in {{Language Companion Series}}},
  number = {213},
  pages = {27--46},
  publisher = {{John Benjamins Publishing Company}},
  doi = {10.1075/slcs.213.01mit},
  url = {https://benjamins.com/catalog/slcs.213.01mit},
  urldate = {2022-05-30},
  abstract = {Discourse markers differ cross-linguistically not only in their functions but also in their positions within the sentence. Some are sentence-initial, some are sentence-final, and some occur in what has been termed the `middle-field'. But many appear simply in second position in the sentence. In many cases the positions of the markers can be explained in terms of the source constructions from which they emerged. Here one likely pathway of development is traced in Mohawk, indigenous to North America, illustrated with a pervasive marker of discourse coherence. Patterns in the modern language suggest that it and others emerged from marked information structures, which, over time, evolved into basic clause structures via familiar mechanisms of grammaticalization.},
  langid = {english},
  keywords = {iroquoian}
}

@article{mollica.f:2017,
  title = {How Data Drive Early Word Learning: A Cross-Linguistic Waiting Time Analysis},
  author = {Mollica, Francis and Piantadosi, Steven T.},
  year = {2017},
  month = sep,
  journal = {Open Mind},
  volume = {1},
  number = {2},
  pages = {67--77},
  issn = {2470-2986},
  doi = {10.1162/OPMI_a_00006},
  url = {https://doi.org/10.1162/OPMI_a_00006},
  urldate = {2022-09-28},
  abstract = {The extent to which word learning is delayed by maturation as opposed to accumulating data is a longstanding question in language acquisition. Further, the precise way in which data influence learning on a large scale is unknown\textemdash experimental results reveal that children can rapidly learn words from single instances as well as by aggregating ambiguous information across multiple situations. We analyze Wordbank, a large cross-linguistic dataset of word acquisition norms, using a statistical waiting time model to quantify the role of data in early language learning, building off Hidaka (2013). We find that the model both fits and accurately predicts the shape of children's growth curves. Further analyses of model parameters suggest a primarily data-driven account of early word learning. The parameters of the model directly characterize both the amount of data required and the rate at which informative data occurs. With high statistical certainty, words require on the order of {$\sim$} 10 learning instances, which occur on average once every two months. Our method is extremely simple, statistically principled, and broadly applicable to modeling data-driven learning effects in development.},
  file = {/Users/j/Zotero/storage/PLAJ32R8/Mollica and Piantadosi (2017) How Data Drive Early Word Learning A Cross-Lingui.pdf}
}

@article{montague.r:1970,
  title = {Universal Grammar},
  author = {Montague, Richard},
  year = {1970},
  journal = {Theoria: a Swedish journal of philosophy and psychology},
  volume = {36},
  number = {3},
  pages = {373--398},
  doi = {10.1111/j.1755-2567.1970.tb00434.x},
  url = {https://doi.org/10.1111/j.1755-2567.1970.tb00434.x},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@incollection{moortgat.m:1997,
  title = {Categorial Type Logics},
  booktitle = {Handbook of Logic and Language},
  author = {Moortgat, M.},
  year = {1997},
  pages = {93--177},
  publisher = {{Elsevier}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:01 -0400}
}

@incollection{muller.h:2020,
  title = {Negative Polarity Illusions},
  booktitle = {The {{Oxford Handbook}} of {{Negation}}},
  author = {Muller, Hanna and Phillips, Colin},
  editor = {D{\'e}prez, Viviane and Espinal, M. Teresa},
  year = {2020},
  month = mar,
  pages = {0},
  publisher = {{Oxford University Press}},
  doi = {10.1093/oxfordhb/9780198830528.013.42},
  url = {https://doi.org/10.1093/oxfordhb/9780198830528.013.42},
  urldate = {2023-02-22},
  abstract = {Although decades of research have illuminated the licensing requirements, both syntactic and semantic, of negative polarity items, the matter of how these licensing requirements are satisfied in real time, as a sentence is being processed, remains an ill-understood problem. Grammatical illusions\textemdash cases where native speakers, as they comprehend an ungrammatical sentence, experience a fleeting perception of acceptability\textemdash offer a window into online computations like NPI licensing. This chapter reviews the findings on negative polarity illusions, their parallels (and, in some cases, the lack of parallels) with other grammaticality illusions, and the implications of this line of research for understanding the incremental processing of negative sentences as well as negative polarity phenomena more broadly.},
  isbn = {978-0-19-883052-8},
  keywords = {grammaticality illusions,negative polarity illusions},
  file = {/Users/j/Zotero/storage/MJXGRLS6/Muller and Phillips (2020) Negative Polarity Illusions.pdf}
}

@article{murray.w:2004,
  title = {Serial Mechanisms in Lexical Access: The Rank Hypothesis.},
  author = {Murray, Wayne S and Forster, Kenneth I},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {3},
  pages = {721},
  publisher = {{American Psychological Association}},
  doi = {10.1037/0033-295X.111.3.721},
  url = {https://doi.org/10.1037/0033-295X.111.3.721},
  date-added = {2021-02-16 16:15:20 -0500},
  date-modified = {2021-02-16 16:16:45 -0500},
  keywords = {frequency effects,psycholinguistics,rank hypothesis,word access},
  file = {/Users/j/Zotero/storage/UQ9RCT8E/Murray and Forster - 2004 - Serial mechanisms in lexical access the rank hypo.pdf}
}

@misc{naesseth.c:2017VSMC,
  title = {Variational Sequential {{Monte Carlo}}},
  author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
  year = {2017},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.1705.11140},
  url = {https://arxiv.org/abs/1705.11140},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.11140},
  date-added = {2022-05-03 21:09:24 -0400},
  date-modified = {2022-05-05 09:15:25 -0400},
  keywords = {sequential monte carlo,variational inference,variational sequential monte carlo},
  file = {/Users/j/Zotero/storage/6U8M48YV/Naesseth et al. - 2017 - Variational sequential monte carlo.pdf}
}

@inproceedings{narayanan.s:1998,
  title = {Bayesian Models of Human Sentence Processing},
  booktitle = {Procedings of Twentieth Annual Conference of the Cognitive Science Society: {{University}} of Wisconsin-Madison},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  editor = {Gernsbacher, Morton Ann and Derry, Sharon J.},
  year = {1998},
  pages = {752--757},
  publisher = {{Lawrence Erlbaum Associates}},
  address = {{Mahwah, NJ}},
  url = {https://web.stanford.edu/~jurafsky/srini2.pdf},
  date-added = {2021-03-09 22:52:27 -0500},
  date-modified = {2021-03-09 22:52:27 -0500},
  keywords = {bayesian,processing}
}

@inproceedings{narayanan.s:2001,
  title = {A {{Bayesian}} Model Predicts Human Parse Preference and Reading Times in Sentence Processing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Narayanan, S. and Jurafsky, Daniel},
  year = {2001},
  volume = {14},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2001/hash/f15d337c70078947cfe1b5d6f0ed3f13-Abstract.html},
  urldate = {2022-06-28},
  abstract = {Narayanan and Jurafsky (1998) proposed that human language compre- hension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian de- cision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.},
  file = {/Users/j/Zotero/storage/AT6RBGTU/Narayanan and Jurafsky - 2001 - A Bayesian Model Predicts Human Parse Preference a.pdf}
}

@unpublished{narayanan.s:2004,
  type = {Unpublished Manuscript},
  title = {A {{Bayesian}} Model of Human Sentence Processing},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  year = {2004},
  month = nov,
  url = {https://web.stanford.edu/~jurafsky/narayananjurafsky04.pdf},
  note = {Unpublished manuscript},
  file = {/Users/j/Zotero/storage/ADJW32X6/Narayanan and Jurafsky (2004) A Bayesian model of human sentence processing.pdf}
}

@inproceedings{naseem.t:2012,
  title = {Selective Sharing for Multilingual Dependency Parsing},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Naseem, Tahira and Barzilay, Regina and Globerson, Amir},
  year = {2012},
  month = jul,
  pages = {629--637},
  publisher = {{Association for Computational Linguistics}},
  address = {{Jeju Island, Korea}},
  url = {https://aclanthology.org/P12-1066},
  date-added = {2022-04-04 12:41:51 -0400},
  date-modified = {2022-04-04 12:41:52 -0400}
}

@article{neal.r:2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2022-06-10},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation,rejection sampling,sampling,slice sampling},
  file = {/Users/j/Zotero/storage/XAHUUUPG/Neal - 2003 - Slice sampling.pdf}
}

@article{nevins.a:2011,
  title = {Multiple Agree with Clitics: {{Person}} Complementarity vs. Omnivorous Number},
  author = {Nevins, Andrew},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {939--971},
  publisher = {{Springer}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:19 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,omnivorous agree}
}

@book{newell.a:1972,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert Alexander},
  year = {1972},
  volume = {104},
  publisher = {{Prentice-hall Englewood Cliffs, NJ}}
}

@incollection{newell.a:1973,
  title = {Production Systems: Models of Control Structures},
  shorttitle = {Production Systems},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  editor = {Chase, William G.},
  year = {1973},
  month = jan,
  pages = {463--526},
  publisher = {{Academic Press}},
  doi = {10.1016/B978-0-12-170150-5.50016-0},
  url = {https://www.sciencedirect.com/science/article/pii/B9780121701505500160},
  urldate = {2022-07-15},
  abstract = {This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works\textemdash on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.},
  isbn = {978-0-12-170150-5},
  langid = {english}
}

@incollection{newell.a:1981,
  title = {Mechanisms of Skill Acquisition and the Law of Practice},
  shorttitle = {Mechanisms of Skill Acquisition and the Law of Practice},
  booktitle = {Cognitive {{Skills}} and {{Their Acquisition}}},
  author = {Newell, Allen and Paul, Rosenbloom},
  editor = {Anderson, John R.},
  year = {1981},
  publisher = {{Psychology Press}},
  doi = {10.4324/9780203728178-6},
  url = {https://www.taylorfrancis.com/books/9781135830885/chapters/10.4324/9780203728178-6},
  abstract = {Practice makes perfect. Correcting the overstatement of a maxim: Almost always, practice brings improvement, and more practice brings more improvement. We all expect improvement with practice to be ubiquitous, though obviously limits exist both in scope and extent. Take only the experimental laboratory: We do not expect people to perform an experimental task correctly without at least some practice; and we design all our psychology experiments with one eye to the confounding influence of practice effects.},
  isbn = {978-0-203-72817-8}
}

@book{newell.a:1994,
  title = {Unified Theories of Cognition},
  author = {Newell, Allen},
  year = {1994},
  publisher = {{Harvard University Press}},
  abstract = {Psychology is now ready for unified theories of cognition--so says Allen Newell, a leading investigator in computer science and cognitive psychology. Not everyone will agree on a single set of mechanisms that will explain the full range of human cognition, but such theories are within reach and we should strive to articulate them.In this book, Newell makes the case for unified theories by setting forth a candidate. After reviewing the foundational concepts of cognitive science--knowledge, representation, computation, symbols, architecture, intelligence, and search--Newell introduces Soar, an architecture for general cognition. A pioneer system in artificial intelligence, Soar is the first problem solver to create its own subgoals and learn continuously from its own experience.Newell shows how Soar's ability to operate within the real-time constraints of intelligent behavior, such as immediate-response and item-recognition tasks, illustrates important characteristics of the human cognitive structure. Throughout, Soar remains an exemplar: we know only enough to work toward a fully developed theory of cognition, but Soar's success so far establishes the viability of the enterprise.Given its integrative approach, Unified Theories of Cognition will be of tremendous interest to researchers in a variety of fields, including cognitive science, artificial intelligence, psychology, and computer science. This exploration of the nature of mind, one of the great problems of philosophy, should also transcend disciplines and attract a large scientific audience.},
  googlebooks = {1lbY14DmV2cC},
  isbn = {978-0-674-92101-6},
  langid = {english},
  keywords = {Psychology / General}
}

@inproceedings{nguyen.l:2012,
  title = {Accurate Unbounded Dependency Recovery Using Generalized Categorial Grammars},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Nguyen, Luan and {van Schijndel}, Marten and Schuler, William},
  year = {2012},
  pages = {2125--2140},
  publisher = {{The COLING 2012 Organizing Committee}},
  address = {{Mumbai, India}},
  url = {https://www.aclweb.org/anthology/C12-1130}
}

@article{nicenboim.b:2016,
  title = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed up: Working Memory and Locality Effects},
  shorttitle = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed Up},
  author = {Nicenboim, Bruno and Loga{\v c}ev, Pavel and Gattei, Carolina and Vasishth, Shravan},
  year = {2016},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00280},
  url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.00280/abstract},
  urldate = {2022-08-13},
  file = {/Users/j/Zotero/storage/4F2YFMHQ/Nicenboim et al. - 2016 - When High-Capacity Readers Slow Down and Low-Capac.pdf}
}

@article{nicenboim.b:2018,
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  author = {Nicenboim, Bruno and Vasishth, Shravan},
  year = {2018},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {99},
  pages = {1--34},
  issn = {0749596X},
  doi = {10.1016/j.jml.2017.08.004},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X16301577},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/MWGYVZ7N/Nicenboim and Vasishth - 2018 - Models of retrieval in sentence comprehension A c.pdf}
}

@inproceedings{nichol.a:2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  month = jul,
  pages = {8162--8171},
  publisher = {{PMLR}},
  issn = {2640-3498},
  url = {https://proceedings.mlr.press/v139/nichol21a.html},
  urldate = {2022-07-07},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  langid = {english},
  keywords = {diffusion processes},
  file = {/Users/j/Zotero/storage/5R9MDK85/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf;/Users/j/Zotero/storage/N2AKD33G/Nichol and Dhariwal - 2021 - Improved Denoising Diffusion Probabilistic Models.pdf}
}

@article{nivre.j:2008,
  title = {Algorithms for {{Deterministic Incremental Dependency Parsing}}},
  author = {Nivre, Joakim},
  year = {2008},
  month = dec,
  journal = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {513--553},
  issn = {0891-2017},
  doi = {10.1162/coli.07-056-R1-07-027},
  url = {https://aclanthology.org/J08-4003},
  urldate = {2022-06-19},
  abstract = {Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.},
  file = {/Users/j/Zotero/storage/YK68V25N/Nivre - 2008 - Algorithms for Deterministic Incremental Dependenc.pdf}
}

@inproceedings{nivre.j:2016universal-dependencies,
  title = {Universal {{Dependencies}} v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({{LREC}}'16)},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Haji{\v c}, Jan and Manning, Christopher D. and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
  year = {2016},
  pages = {1659--1666},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Portoro\v{z}, Slovenia}},
  url = {https://www.aclweb.org/anthology/L16-1262}
}

@misc{nivre.j:2017,
  title = {Universal Dependencies 2.0 \textendash{} {{CoNLL}} 2017 Shared Task Development and Test Data},
  author = {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Burchardt, Aljoscha and Candito, Marie and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Cinkov{\'a}, Silvie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and {de Marneffe}, Marie-Catherine and {de Paiva}, Valeria and {Diaz de Ilarraza}, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Eli, Marhaba and Elkahky, Ali and Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'i}na and Galbraith, Daniel and Garcia, Marcos and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni, Matias and Gr{\=u}ztis, Normunds and Guillaume, Bruno and Habash, Nizar and Haji{\v c}, Jan and {Haji{\v c} jr.}, Jan and H{\`a} M{\~y}, Linh and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hohle, Petter and Ion, Radu and Irimia, Elena and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and L{\^e} Ho\^\`ng, PhÆ°Æ¡ng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'i}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and McDonald, Ryan and Mendon{\c c}a, Gustavo and Missil{\"a}, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shunsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Mustafina, Nina and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Nedoluzhko, Anna and Nguye\^\~n Th{\d i}, LÆ°Æ¡ng and Nguye\^\~n Th{\d i} Minh, Huye\^\`n and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and {\O}vrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Popel, Martin and Pretkalni{\c n}a, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Real, Livy and Reddy, Siva and Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and Rosa, Rudolf and Rovati, Davide and Saleh, Shadi and Sanguinetti, Manuela and Saulte, Baiba and Sawanakunanon, Yanin and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shakurova, Lena and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Stella, Antonio and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and {van Noord}, Gertjan and Varga, Viktor and Vincze, Veronika and Washington, Jonathan North and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeman, Daniel and Zhu, Hanzhi},
  year = {2017},
  url = {http://hdl.handle.net/11234/1-2184},
  copyright = {Licence Universal Dependencies v2.0},
  date-added = {2021-04-30 13:00:05 -0400},
  date-modified = {2021-04-30 13:02:24 -0400},
  keywords = {dependency parsing,dependency structures,universal dependencies}
}

@inproceedings{nivre.j:2020,
  title = {Universal {{Dependencies}} v2: {{An}} Evergrowing Multilingual Treebank Collection},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Haji{\v c}, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
  year = {2020},
  pages = {4034--4043},
  publisher = {{European Language Resources Association}},
  address = {{Marseille, France}},
  url = {https://www.aclweb.org/anthology/2020.lrec-1.497},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@article{norris.d:2006,
  title = {The {{Bayesian}} Reader: {{Explaining}} Word Recognition as an Optimal {{Bayesian}} Decision Process},
  shorttitle = {The {{Bayesian}} Reader},
  author = {Norris, Dennis},
  year = {2006},
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {327--357},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.113.2.327},
  abstract = {This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. This leads to the development of a computational model of word recognition, the Bayesian reader. The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Lexical Decision,Models,Reading,Semantics,Statistical Probability,Word Recognition},
  file = {/Users/j/Zotero/storage/99ILH9NN/Norris - 2006 - The Bayesian reader Explaining word recognition a.pdf}
}

@article{norris.d:2009,
  title = {Putting It All Together: {{A}} Unified Account of Word Recognition and Reaction-Time Distributions},
  shorttitle = {Putting It All Together},
  author = {Norris, Dennis},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {1},
  pages = {207--219},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/a0014259},
  abstract = {R. Ratcliff, P. Gomez, and G. McKoon (2004) suggested much of what goes on in lexical decision is attributable to decision processes and may not be particularly informative about word recognition. They proposed that lexical decision should be characterized by a decision process, taking the form of a drift-diffusion model (R. Ratcliff, 1978), that operates on the output of lexical model. The present article argues that the distinction between perception and decision making is unnecessary and that it is possible to give a unified account of both lexical processing and decision making. This claim is supported by formal arguments and reinforced by simulations showing how the Bayesian Reader model (D. Norris, 2006) can be extended to fit the data on reaction time distributions collected by Ratcliff, Gomez, and McKoon simply by adding extra sources of noise. The Bayesian Reader gives an integrated explanation of both word recognition and decision making, using fewer parameters than the diffusion model. It can be thought of as a Bayesian diffusion model, which subsumes Ratcliff's drift-diffusion model as a special case. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Lexical Decision,Models,Reaction Time,Word Recognition}
}

@techreport{odonnell.t:2009,
  type = {Technical Report},
  title = {Fragment {{Grammars}}: {{Exploring Computation}} and {{Reuse}} in {{Language}}},
  shorttitle = {Fragment {{Grammars}}},
  author = {O'Donnell, Timothy J. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {2009},
  month = mar,
  number = {MIT-CSAIL-TR-2009-013},
  institution = {{MIT Computer Science and Artificial Intelligence Laboratory}},
  url = {https://dspace.mit.edu/handle/1721.1/44963},
  urldate = {2022-06-15},
  abstract = {Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP) and in particular using a probabilistic variant of Lisp known as the Church programming language (Goodman, Mansinghka, Roy, Bonawitz, \& Tenenbaum, 2008). We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them---adaptor grammars (Johnson, Griffiths, \& Goldwater, 2007). We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.},
  langid = {english},
  annotation = {Accepted: 2009-03-31T05:00:03Z},
  file = {/Users/j/Zotero/storage/YNH3B97V/O'Donnell et al. - 2009 - Fragment Grammars Exploring Computation and Reuse.pdf}
}

@article{odonnell.t:2011cogsci,
  title = {Productivity and {{Reuse}} in {{Language}}},
  author = {O'Donnell, Timothy and Snedeker, Jesse and Tenenbaum, Joshua and Goodman, Noah},
  year = {2011},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {33},
  number = {33},
  url = {https://escholarship.org/uc/item/4t2312gv},
  urldate = {2022-06-15},
  abstract = {Author(s): O'Donnell, Timothy; Snedeker, Jesse; Tenenbaum, Joshua; Goodman, Noah},
  langid = {english},
  file = {/Users/j/Zotero/storage/N7ZT4BP6/O'Donnell et al. - 2011 - Productivity and Reuse in Language.pdf}
}

@book{odonnell.t:2015phdbook,
  title = {Productivity and {{Reuse}} in {{Language}}: {{A Theory}} of {{Linguistic Computation}} and {{Storage}}},
  author = {O'Donnell, Timothy J.},
  year = {2015},
  month = aug,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9780262028844.001.0001},
  url = {https://doi.org/10.7551/mitpress/9780262028844.001.0001},
  urldate = {2022-06-16},
  abstract = {A proposal for a formal model, Fragment Grammars, that treats productivity and reuse as the target of inference in a probabilistic framework.Language allows us to express and comprehend an unbounded number of thoughts. This fundamental and much-celebrated property is made possible by a division of labor between a large inventory of stored items (e.g., affixes, words, idioms) and a computational system that productively combines these stored units on the fly to create a potentially unlimited array of new expressions. A language learner must discover a language's productive, reusable units and determine which computational processes can give rise to new expressions. But how does the learner differentiate between the reusable, generalizable units (for example, the affix -ness, as in coolness, orderliness, cheapness) and apparent units that do not actually generalize in practice (for example, -th, as in warmth but not coolth)? In this book, Timothy O'Donnell proposes a formal computational model, Fragment Grammars, to answer these questions. This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distribution of productive word-formation processes and reusable units in a given language.O'Donnell compares this model to a number of other theoretical and mathematical models, applying them to the English past tense and English derivational morphology, and showing that Fragment Grammars unifies a number of superficially distinct empirical phenomena in these domains and justifies certain seemingly ad hoc assumptions in earlier theories.},
  isbn = {978-0-262-32680-3}
}

@inproceedings{oh.b:2021,
  title = {Surprisal Estimators for Human Reading Times Need Character Models},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2021},
  month = aug,
  pages = {3746--3757},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.290},
  url = {https://aclanthology.org/2021.acl-long.290},
  urldate = {2023-05-02},
  abstract = {While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.},
  file = {/Users/j/Zotero/storage/RP9S5D9A/Oh et al. (2021) Surprisal Estimators for Human Reading Times Need .pdf}
}

@article{oh.b:2022,
  title = {Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  issn = {2624-8212},
  url = {https://www.frontiersin.org/articles/10.3389/frai.2022.777963},
  urldate = {2023-05-02},
  abstract = {Expectation-based theories of sentence processing posit that processing difficulty is determined by predictability in context. While predictability quantified via surprisal has gained empirical support, this representation-agnostic measure leaves open the question of how to best approximate the human comprehender's latent probability model. This article first describes an incremental left-corner parser that incorporates information about common linguistic abstractions such as syntactic categories, predicate-argument structure, and morphological rules as a computational-level model of sentence processing. The article then evaluates a variety of structural parsers and deep neural language models as cognitive models of sentence processing by comparing the predictive power of their surprisal estimates on self-paced reading, eye-tracking, and fMRI data collected during real-time language processing. The results show that surprisal estimates from the proposed left-corner processing model deliver comparable and often superior fits to self-paced reading and eye-tracking data when compared to those from neural language models trained on much more data. This may suggest that the strong linguistic generalizations made by the proposed processing model may help predict humanlike processing costs that manifest in latency-based measures, even when the amount of training data is limited. Additionally, experiments using Transformer-based language models sharing the same primary architecture and training data show a surprising negative correlation between parameter count and fit to self-paced reading and eye-tracking data. These findings suggest that large-scale neural language models are making weaker generalizations based on patterns of lexical items rather than stronger, more humanlike generalizations based on linguistic structure.},
  file = {/Users/j/Zotero/storage/NEIAFNEN/Oh et al. (2022) Comparison of Structural Parsers and Neural Langua.pdf}
}

@article{oh.b:2023,
  title = {Why Does Surprisal from Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {336--350},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00548},
  url = {https://doi.org/10.1162/tacl_a_00548},
  urldate = {2023-04-30},
  abstract = {This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to `memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.},
  file = {/Users/j/Zotero/storage/STK9C9MP/Oh and Schuler (2023) Why Does Surprisal From Larger Transformer-Based L.pdf}
}

@book{ontario:2011,
  title = {Native {{Languages}}: {{A Support Document}} for the {{Teaching}} of {{Language Patterns}}: {{Oneida}}, {{Cayuga}}, and {{Mohawk}}},
  author = {{Ontario Ministry of Education}},
  year = {2011},
  series = {The {{Ontario Curriculum}}: {{Grades}} 1 to 12},
  publisher = {{Ontario: Queen's Printer for Ontario}},
  langid = {english},
  file = {/Users/j/Zotero/storage/3UCUY6DE/Native Languages A Support Document for the Teach.pdf}
}

@misc{oord.a:2018,
  title = {Representation Learning with Contrastive Predictive Coding},
  author = {{van den Oord}, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  eprint = {1807.03748},
  primaryclass = {cs.LG},
  archiveprefix = {arxiv},
  date-added = {2019-07-05 11:07:24 -0400},
  date-modified = {2019-07-05 11:08:29 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,representation learning}
}

@misc{openai:2023GPT4-post,
  title = {{{GPT-4}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  url = {https://openai.com/research/gpt-4},
  urldate = {2023-03-15},
  abstract = {We've created GPT-4, the latest milestone in OpenAI's effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.},
  langid = {american}
}

@techreport{openai:2023GPT4techreport,
  type = {Technical Report},
  title = {{{GPT-4}} Technical Report},
  author = {OpenAI},
  year = {2023},
  month = mar,
  institution = {{OpenAI}},
  url = {https://cdn.openai.com/papers/gpt-4.pdf},
  urldate = {2023-03-15},
  file = {/Users/j/Zotero/storage/2HU7VBAT/gpt-4.pdf}
}

@article{ortega.p:2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information-Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  year = {2013},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {469},
  number = {2153},
  pages = {20120683},
  publisher = {{Royal Society}},
  doi = {10.1098/rspa.2012.0683},
  url = {https://royalsocietypublishing.org/doi/10.1098/rspa.2012.0683},
  urldate = {2022-06-09},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
  keywords = {bounded rationality,decision-making,information processing},
  file = {/Users/j/Zotero/storage/KRCB8XUT/Ortega and Braun - 2013 - Thermodynamics as a theory of decision-making with.pdf}
}

@article{oxford.w:2019,
  title = {Inverse Marking and Multiple Agree in Algonquin},
  author = {Oxford, Will},
  year = {2019},
  journal = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {3},
  pages = {955--996},
  doi = {10.1007/s11049-018-9428-x},
  url = {https://doi.org/10.1007/s11049-018-9428-x},
  abstract = {This paper shows that inverse marking and portmanteau agreement are in complementary distribution in Algonquin: inverse marking is possible only in contexts where portmanteau agreement is not. This correlation holds despite intralanguage variation in both phenomena. The paper proposes that the two phenomena pattern together because both are determined by the outcome of the Agree operation on Infl. When Infl enters a Multiple Agree relation with both arguments, the realization of portmanteau agreement morphology is possible. When Infl agrees only with the object, it duplicates the result of an earlier object agreement operation on Voice. The presence of identical features on Infl and Voice triggers an impoverishment operation that deletes the features of Voice, resulting in its spellout as an underspecified elsewhere form\textemdash which is the exponent that we know descriptively as the inverse marker. This analysis explains why inverse marking and portmanteau agreement never co-occur in Algonquin: the two phenomena are determined by alternative outcomes of the Agree operation on Infl. The analysis also enables a simple account of the intralanguage variation in the patterning of the two phenomena, which is shown to follow from variation in the specification of the probe on Infl.},
  da = {2019/08/01},
  date-added = {2020-06-16 10:51:08 -0400},
  date-modified = {2020-06-16 10:52:50 -0400},
  isbn = {1573-0859},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@inproceedings{paiva-alves.e:1996,
  title = {The Selection of the Most Probable Dependency Structure in {{Japanese}} Using Mutual Information},
  booktitle = {34th Annual Meeting of the Association for Computational Linguistics},
  author = {{de Paiva Alves}, Eduardo},
  year = {1996},
  pages = {372--374},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Cruz, California, USA}},
  doi = {10.3115/981863.981919},
  url = {https://www.aclweb.org/anthology/P96-1055},
  bdsk-url-2 = {https://doi.org/10.3115/981863.981919}
}

@inproceedings{pal.c:2006,
  title = {Sparse Forward-Backward Using Minimum Divergence Beams for Fast Training of Conditional Random Fields},
  booktitle = {{{IEEE}} International Conference on Acoustics Speed and Signal Processing Proceedings},
  author = {Pal, C. and Sutton, C. and McCallum, A.},
  year = {2006},
  volume = {V},
  pages = {581--584},
  publisher = {{IEEE}},
  doi = {10.1109/icassp.2006.1661342},
  url = {https://doi.org/10.1109%2Ficassp.2006.1661342},
  bdsk-url-2 = {https://doi.org/10.1109/icassp.2006.1661342},
  date-added = {2022-03-25 11:41:07 -0400},
  date-modified = {2022-03-25 11:45:04 -0400}
}

@book{palermo.d:1964,
  title = {Word Association Norms: {{Grade}} School through College},
  author = {Palermo, David and Jenkins, James},
  year = {1964},
  publisher = {{U. Minnesota Press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@inproceedings{park.y:2009,
  title = {Minimal-Length Linearizations for Mildly Context-Sensitive Dependency Trees},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2009},
  pages = {335--343},
  publisher = {{Association for Computational Linguistics}},
  address = {{Boulder, Colorado}},
  url = {https://www.aclweb.org/anthology/N09-1038}
}

@inproceedings{park.y:2011,
  title = {Automated Whole Sentence Grammar Correction Using a Noisy Channel Model},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2011},
  month = jun,
  pages = {934--944},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1094},
  date-added = {2022-04-11 23:09:22 -0400},
  date-modified = {2022-04-11 23:09:27 -0400}
}

@article{parker.d:2016,
  title = {Negative Polarity Illusions and the Format of Hierarchical Encodings in Memory},
  author = {Parker, Dan and Phillips, Colin},
  year = {2016},
  month = dec,
  journal = {Cognition},
  volume = {157},
  pages = {321--339},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2016.08.016},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027716302062},
  urldate = {2023-02-22},
  abstract = {Linguistic illusions have provided valuable insights into how we mentally navigate complex representations in memory during language comprehension. Two notable cases involve illusory licensing of agreement and negative polarity items (NPIs), where comprehenders fleetingly accept sentences with unlicensed agreement or an unlicensed NPI, but judge those same sentences as unacceptable after more reflection. Existing accounts have argued that illusions are a consequence of faulty memory access processes, and make the additional assumption that the encoding of the sentence remains fixed over time. This paper challenges the predictions made by these accounts, which assume that illusions should generalize to a broader set of structural environments and a wider range of syntactic and semantic phenomena. We show across seven reading-time and acceptability judgment experiments that NPI illusions can be reliably switched ``on'' and ``off'', depending on the amount of time from when the potential licensor is processed until the NPI is encountered. But we also find that the same profile does not extend to agreement illusions. This contrast suggests that the mechanisms responsible for switching the NPI illusion on and off are not shared across all illusions. We argue that the contrast reflects changes over time in the encoding of the semantic/pragmatic representations that can license NPIs. Just as optical illusions have been informative about the visual system, selective linguistic illusions are informative not only about the nature of the access mechanisms, but also about the nature of the encoding mechanisms.},
  langid = {english},
  keywords = {Agreement,Binding,Linguistic illusions,Memory,Negative polarity,Representation,Sentence processing}
}

@article{partee.b:1990,
  title = {Mathematical Methods in Linguistics},
  author = {Manaster Ramer, Alexis},
  year = {1992},
  journal = {Computational Linguistics},
  volume = {18},
  number = {1},
  url = {https://www.aclweb.org/anthology/J92-1009}
}

@inproceedings{paskin.m:2002,
  title = {Grammatical Bigrams},
  booktitle = {Advances in Neural Information Processing Systems 14 [{{Neural}} Information Processing Systems: {{Natural}} and Synthetic, {{NIPS}} 2001, December 3-8, 2001, Vancouver, British Columbia, Canada]},
  author = {Paskin, Mark A.},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  year = {2001},
  pages = {91--97},
  publisher = {{MIT Press}},
  url = {https://proceedings.neurips.cc/paper/2001/hash/89885ff2c83a10305ee08bd507c1049c-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/Paskin01.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@phdthesis{pentangelo.j:2020,
  title = {360\textdegree{} {{Video}} and {{Language Documentation}}: {{Towards}} a {{Corpus}} of {{Kanien}}'k\'eha ({{Mohawk}})},
  shorttitle = {360\textdegree{} {{Video}} and {{Language Documentation}}},
  author = {Pentangelo, Joseph and link will open in a new window {Link to external site}, this},
  year = {2020},
  address = {{United States -- New York}},
  url = {https://www.proquest.com/docview/2459231889/abstract/2D8FDD4353574D95PQ/1},
  urldate = {2022-05-31},
  abstract = {Robust documentation is a major goal of documentary linguistics. Recognizing spoken language as a multimodal phenomenon, researchers working in this field broadly agree that video is an improvement over audio-only recording. At the same time, video is limited by the format's frame, which permits only a relatively small portion of the visual field to be recorded at any given time. This results in much data being lost, as the documenter must decide where to aim their camera, necessarily leaving out more than they record. In this dissertation, I apply 360\textordmasculine{} video to language documentation for the first time. 360\textordmasculine{} video, which is one variety of virtual reality, improves upon traditional video by drastically expanding the frame, recording in all directions surrounding the camera. In this way, a maximum of visual data is recorded, and there is no need for the camera to be redirected as participants take turns speaking or move around the space. I recorded over 10 hours of 360\textordmasculine{} video with ambisonic audio, containing mostly naturalistic conversation in the Akwesasne variety of Kanien'k\'eha (Mohawk), an endangered Northern Iroquoian language spoken in New York State, Ontario, and Quebec. Most of the existing documentation of Kanien'k\'eha outside of this corpus is formal or non-naturalistic. The resulting corpus thus serves a dual purpose: it is both a demonstration of the capabilities of 360\textordmasculine{} video for language documentation, and a contribution to the documentation of Kanien'k\'eha. This dissertation includes a brief grammatical description of Kanien'k\'eha phonology and morphology, a discussion of the interplay between technology and language documentation throughout North American history, an exploration of the significance of 360\textordmasculine{} video to documentary linguistics, a brief analysis of gesture and intonation in the present corpus, and an assessment of the suitability of ambisonic audio for linguistic analysis. Directions for potential future research are indicated throughout.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798678110015},
  langid = {english},
  school = {City University of New York},
  keywords = {Akwesasne,Documentary linguistics,Iroquoian,Language documentation,Virtual reality},
  file = {/Users/j/Zotero/storage/QBSYW5RY/Pentangelo and Link to external site - 360Â° Video and Language Documentation Towards a C.pdf}
}

@article{pereira.f:2000,
  title = {Formal Grammar and Information Theory: {{Together}} Again?},
  author = {Pereira, Fernando C. N.},
  year = {2000},
  journal = {Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences},
  volume = {358},
  number = {1769},
  pages = {1239--1253},
  doi = {10.1098/rsta.2000.0583},
  url = {https://doi.org/10.1098/rsta.2000.0583},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {information theory},
  file = {/Users/j/Zotero/storage/KZIT4MEK/Pereira - 2000 - Formal grammar and information theory Together ag.pdf}
}

@article{perfors2011learnability,
  title = {The Learnability of Abstract Syntactic Principles},
  author = {Perfors, Amy and Tenenbaum, Joshua B and Regier, Terry},
  year = {2011},
  journal = {Cognition},
  volume = {118},
  number = {3},
  pages = {306--338},
  publisher = {{Elsevier}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{peters.m:2018,
  title = {Deep Contextualized Word Representations},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {2227--2237},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1202},
  url = {https://www.aclweb.org/anthology/N18-1202},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1202}
}

@inproceedings{peters.m:2018a,
  title = {Dissecting Contextual Word Embeddings: {{Architecture}} and Representation},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  pages = {1499--1509},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1179},
  url = {https://www.aclweb.org/anthology/D18-1179},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1179}
}

@inproceedings{petrov.s:2007,
  title = {Discriminative Log-Linear Grammars with Latent Variables},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Petrov, Slav and Klein, Dan},
  year = {2007},
  volume = {20},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2007/hash/9cc138f8dc04cbf16240daa92d8d50e2-Abstract.html},
  urldate = {2022-10-16},
  abstract = {We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be effi- ciently approximated in a gradient-based procedure. We compare L1 and L2 reg- ularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing exper- iments, the discriminative latent models outperform both the comparable genera- tive latent models as well as the discriminative non-latent baselines.},
  keywords = {pruning},
  file = {/Users/j/Zotero/storage/PJW4YGA3/Petrov and Klein (2007) Discriminative Log-Linear Grammars with Latent Var.pdf}
}

@article{piantadosi.s:2011,
  title = {Word Lengths Are Optimized for Efficient Communication},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = {2011},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3526--3529},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.1012551108},
  url = {https://doi.org/10.1073%2Fpnas.1012551108},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1012551108},
  date-added = {2021-07-25 11:06:49 -0400},
  date-modified = {2021-07-25 11:06:50 -0400}
}

@article{piantadosi.s:2014,
  title = {Zipf's Word Frequency Law in Natural Language: {{A}} Critical Review and Future Directions},
  shorttitle = {Zipf's Word Frequency Law in Natural Language},
  author = {Piantadosi, Steven T.},
  year = {2014},
  month = oct,
  journal = {Psychonomic bulletin \& review},
  volume = {21},
  number = {5},
  pages = {1112--1130},
  issn = {1069-9384},
  doi = {10.3758/s13423-014-0585-6},
  url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4176592/},
  urldate = {2022-09-27},
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf ' s law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  pmcid = {PMC4176592},
  pmid = {24664880},
  file = {/Users/j/Zotero/storage/4JZT9KFV/Piantadosi (2014) Zipfâ€™s word frequency law in natural language A c.pdf}
}

@article{pickering.m:2013,
  title = {An Integrated Theory of Language Production and Comprehension},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2013},
  journal = {Behavioral and Brain Sciences},
  volume = {36},
  number = {4},
  pages = {329--347},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/S0140525X12001495},
  url = {https://doi.org/10.1017/S0140525X12001495},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding},
  file = {/Users/j/Zotero/storage/YWM6TEES/Pickering and Garrod - 2013 - An integrated theory of language production and co.pdf}
}

@article{pickering.m:2018predicting,
  title = {Predicting While Comprehending Language: {{A}} Theory and Review.},
  author = {Pickering, Martin J and Gambi, Chiara},
  year = {2018},
  journal = {Psychological Bulletin},
  volume = {144},
  number = {10},
  pages = {1002},
  publisher = {{American Psychological Association}},
  doi = {10.1037/bul0000158},
  url = {https://doi.org/10.1037/bul0000158},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@inproceedings{pine.a:2022,
  title = {Requirements and {{Motivations}} of {{Low-Resource Speech Synthesis}} for {{Language Revitalization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pine, Aidan and Wells, Dan and Brinklow, Nathan and Littell, Patrick and Richmond, Korin},
  year = {2022},
  month = may,
  pages = {7346--7359},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.507},
  url = {https://aclanthology.org/2022.acl-long.507},
  urldate = {2022-06-04},
  abstract = {This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien'k\'eha, Gitksan \& SEN\'CO{\fontencoding{LELA}\selectfont\char47}EN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/LMSBKQLF/Pine et al. - 2022 - Requirements and Motivations of Low-Resource Speec.pdf}
}

@incollection{polinsky.m:2013,
  title = {Resumption in {{English}}},
  booktitle = {Experimental {{Syntax}} and {{Island Effects}}},
  author = {Polinsky, Maria and Eby Clemens, Lauren and Milton Morgan, Adam and Xiang, Ming and Heestand, Dustin},
  editor = {Sprouse, Jon and Hornstein, Norbert},
  year = {2013},
  pages = {341--359},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge}},
  doi = {10.1017/CBO9781139035309.017},
  url = {https://www.cambridge.org/core/books/experimental-syntax-and-island-effects/resumption-in-english/B621CF205AAE58A54C26239D193AE9DD},
  urldate = {2023-04-05},
  isbn = {978-1-107-00870-0},
  keywords = {resumptive pronouns},
  file = {/Users/j/Zotero/storage/8XBTF5P7/Polinsky et al. (2013) Resumption in English.pdf}
}

@article{poole.e:2016,
  title = {Deconstructing Subjecthood},
  author = {Poole, Ethan},
  year = {2016},
  journal = {Ms., UMass Amherst.},
  url = {http://ling.auf.net/lingbuzz/003197},
  date-added = {2019-06-14 09:32:06 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,subject positions}
}

@inproceedings{poppels.t:2016,
  title = {Structure-Sensitive {{Noise Inference}}: {{Comprehenders Expect Exchange Errors}}},
  booktitle = {Proceedings of the 38th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Poppels, Till and Levy, Roger},
  year = {2016},
  pages = {378--383},
  publisher = {{Cognitive Science Society}},
  address = {{Austin, TX}},
  url = {https://cogsci.mindmodeling.org/2016/papers/0077/},
  abstract = {Previous research has found that comprehenders are willing to adopt non-literal interpretations of sentences whose literal reading is unlikely. Several studies found evidence that comprehenders decide whether a given utterance should be taken at face value in accordance with principles of Bayesian rationality, by weighing the prior probability of potential interpretations against the degree to which they are (in)consistent with the literal form of the utterance. While all of these results are consistent with string-edit noise models, many error processes are known to be sensitive to the underlying linguistic structure of the intended utterance. Here, we explore the case of exchange errors and provide experimental evidence that comprehenders' noise model is structure-sensitive. Our results add further support to the noisy-channel theory of language comprehension, extend the set of known noise operations to include positional exchanges, and show that comprehenders' noise models are well-adapted to structure-sensitive sources of signal corruption.},
  file = {/Users/j/Zotero/storage/RK2HV63N/Poppels and Levy - 2016 - Structure-sensitive Noise Inference Comprehenders.pdf}
}

@article{post.e:1943,
  title = {Formal Reductions of the General Combinatorial Decision Problem},
  author = {Post, Emil L.},
  year = {1943},
  journal = {American Journal of Mathematics},
  volume = {65},
  number = {2},
  eprint = {2371809},
  eprinttype = {jstor},
  pages = {197--215},
  publisher = {{Johns Hopkins University Press}},
  issn = {0002-9327},
  doi = {10.2307/2371809},
  url = {https://www.jstor.org/stable/2371809},
  urldate = {2022-07-15},
  file = {/Users/j/Zotero/storage/XIZKXKKT/Post - 1943 - Formal Reductions of the General Combinatorial Dec.pdf}
}

@article{post.m:2013,
  title = {Bayesian Tree Substitution Grammars as a Usage-Based Approach},
  author = {Post, Matt and Gildea, Daniel},
  year = {2013},
  journal = {Language and Speech},
  volume = {56},
  number = {3},
  pages = {291--308},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:27 -0400},
  keywords = {Tree Substitution Grammar}
}

@misc{prasad.g:2019readingMVRR,
  title = {Rapid Syntactic Adaptation in Self-Paced Reading: Detectable, but Requires Many Participants.},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  publisher = {{Center for Open Science}},
  doi = {10.31234/osf.io/9ptg4},
  url = {https://doi.org/10.31234%2Fosf.io%2F9ptg4},
  bdsk-url-2 = {https://doi.org/10.31234/osf.io/9ptg4},
  date-added = {2021-03-18 11:13:22 -0400},
  date-modified = {2021-03-18 17:40:38 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing,reading time,self-paced reading}
}

@misc{prasad.g:2019readingNPS-NPZ,
  title = {How Much Harder Are Hard Garden-Path Sentences than Easy Ones?},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  journal = {CogSci},
  url = {https://osf.io/syh3j/},
  date-added = {2021-03-18 11:20:31 -0400},
  date-modified = {2021-03-18 17:38:35 -0400},
  howpublished = {OSF preprint},
  keywords = {processing,reading time,self-paced reading}
}

@article{preminger.o:2011,
  title = {Asymmetries between Person and Number in Syntax: A Commentary on {{Baker}}'s {{SCOPA}}},
  author = {Preminger, Omer},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {917--937},
  publisher = {{Springer}},
  date-added = {2020-02-25 21:43:01 -0500},
  date-modified = {2020-02-26 09:11:06 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement,phi features}
}

@book{preminger.o:2014,
  title = {Agreement and Its Failures},
  author = {Preminger, Omer},
  year = {2014},
  volume = {68},
  publisher = {{MIT Press}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@article{prim.r:1957,
  title = {Shortest Connection Networks and Some Generalizations},
  author = {Prim, R. C.},
  year = {1957},
  journal = {The Bell System Technical Journal},
  volume = {36},
  number = {6},
  pages = {1389--1401},
  doi = {10.1002/j.1538-7305.1957.tb01515.x},
  url = {https://doi.org/10.1002/j.1538-7305.1957.tb01515.x},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{prince.e:1990,
  title = {Syntax and Discourse: {{A}} Look at Resumptive Pronouns},
  booktitle = {Proceedings of the Sixteenth Annual Meeting of the {{Berkeley}} Linguistics Society},
  author = {Prince, Ellen F},
  year = {1990},
  volume = {16},
  pages = {482--497},
  isbn = {2377-1666},
  file = {/Users/j/Zotero/storage/5QDTZ6SY/Prince (1990) Syntax and discourse A look at resumptive pronoun.pdf}
}

@article{priva.u:2020,
  title = {The Causal Structure of Lenition: A Case for the Causal Precedence of Durational Shortening},
  author = {Priva, Uriel Cohen and Gleason, Emily},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {2},
  pages = {413--448},
  publisher = {{Project Muse}},
  doi = {10.1353/lan.2020.0025},
  url = {https://doi.org/10.1353%2Flan.2020.0025},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0025},
  date-added = {2022-05-10 10:31:58 -0400},
  date-modified = {2022-05-10 10:32:14 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Zotero/storage/YKU8493F/Priva and Gleason - 2020 - The causal structure of lenition A case for the c.pdf}
}

@inproceedings{przepiorkowski.a:2018arguments-adjuncts-ud,
  title = {Arguments and Adjuncts in {{Universal Dependencies}}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {Przepi{\'o}rkowski, Adam and Patejuk, Agnieszka},
  year = {2018},
  pages = {3837--3852},
  publisher = {{Association for Computational Linguistics}},
  address = {{Santa Fe, New Mexico, USA}},
  url = {https://www.aclweb.org/anthology/C18-1324}
}

@inproceedings{qian.p:2022,
  title = {Flexible Generation from Fragmentary Linguistic Input},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Qian, Peng and Levy, Roger},
  year = {2022},
  month = may,
  pages = {8176--8196},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  doi = {10.18653/v1/2022.acl-long.563},
  url = {https://aclanthology.org/2022.acl-long.563},
  urldate = {2023-02-09},
  abstract = {The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models. But does direct specialization capture how humans approach novel language tasks? We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user. To test this hypothesis, we formulate a set of novel fragmentary text completion tasks, and compare the behavior of three direct-specialization models against a new model we introduce, GibbsComplete, which composes two basic computational motifs central to contemporary models: masked and autoregressive word prediction. We conduct three types of evaluation: human judgments of completion quality, satisfaction of syntactic constraints imposed by the input fragment, and similarity to human behavior in the structural statistics of the completions. With no task-specific parameter tuning, GibbsComplete performs comparably to direct-specialization models in the first two evaluations, and outperforms all direct-specialization models in the third evaluation. These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization.},
  file = {/Users/j/Zotero/storage/3XXMXCYH/Qian and Levy (2022) Flexible Generation from Fragmentary Linguistic In.pdf}
}

@manual{r.coreteam:2021,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {{Vienna, Austria}},
  url = {https://www.R-project.org/},
  organization = {{R Foundation for Statistical Computing}}
}

@article{rabagliati.h:2016,
  title = {Learning to Predict or Predicting to Learn?},
  author = {Rabagliati, Hugh and Gambi, Chiara and Pickering, J},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {94--105},
  doi = {10.1080/23273798.2015.1077979},
  url = {https://doi.org/10.1080/23273798.2015.1077979},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@book{rabinovich.m:2012,
  title = {Principles of Brain Dynamics: Global State Interactions},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  month = jul,
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/9108.001.0001},
  url = {https://doi.org/10.7551/mitpress/9108.001.0001},
  urldate = {2022-07-08},
  abstract = {Experimental and theoretical approaches to global brain dynamics that draw on the latest research in the field.The consideration of time or dynamics is fundamental for all aspects of mental activity\textemdash perception, cognition, and emotion\textemdash because the main feature of brain activity is the continuous change of the underlying brain states even in a constant environment. The application of nonlinear dynamics to the study of brain activity began to flourish in the 1990s when combined with empirical observations from modern morphological and physiological observations. This book offers perspectives on brain dynamics that draw on the latest advances in research in the field. It includes contributions from both theoreticians and experimentalists, offering an eclectic treatment of fundamental issues.Topics addressed range from experimental and computational approaches to transient brain dynamics to the free-energy principle as a global brain theory. The book concludes with a short but rigorous guide to modern nonlinear dynamics and their application to neural dynamics.},
  isbn = {978-0-262-30558-7}
}

@misc{radford.a:2018GPT,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {{OpenAI}},
  url = {https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {generative pre-training,GPT,GPT1,transfer learning}
}

@misc{radford.a:2018GPT-post,
  title = {Improving Language Understanding with Unsupervised Learning},
  shorttitle = {{{GPT}}},
  author = {Radford, Alec},
  year = {2018},
  month = jun,
  journal = {OpenAI},
  url = {https://openai.com/research/language-unsupervised},
  urldate = {2023-03-12},
  abstract = {We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas:~transformers~and~unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse~datasets.},
  langid = {american}
}

@misc{radford.a:2019GPT2,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  url = {https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {GPT,GPT2}
}

@misc{radford.a:2019GPT2-post,
  title = {Better Language Models and Their Implications},
  shorttitle = {{{GPT-2}}},
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniella and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  year = {2019},
  month = feb,
  journal = {OpenAI},
  url = {https://openai.com/research/better-language-models},
  urldate = {2023-03-12},
  abstract = {We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization\textemdash all without task-specific~training.},
  langid = {american}
}

@article{ramgoolam.s:2019,
  title = {Permutation Invariant Gaussian Matrix Models},
  author = {Ramgoolam, Sanjaye},
  year = {2019},
  journal = {Nuclear Physics B},
  pages = {114682},
  publisher = {{Elsevier}},
  date-added = {2019-08-06 08:51:05 +0300},
  date-modified = {2019-08-06 08:52:06 +0300},
  project = {syntactic embedding},
  keywords = {gaussian matrix models,physics}
}

@article{rasmussen.n:2018,
  title = {Left-Corner Parsing with Distributed Associative Memory Produces Surprisal and Locality Effects},
  author = {Rasmussen, Nathan E. and Schuler, William},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1009--1042},
  issn = {1551-6709},
  doi = {10.1111/cogs.12511},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.12511},
  urldate = {2022-06-13},
  abstract = {This article describes a left-corner parser implemented within a cognitively and neurologically motivated distributed model of memory. This parser's approach to syntactic ambiguity points toward a tidy account both of surprisal effects and of locality effects, such as the parsing breakdowns caused by center embedding. The model provides an algorithmic-level (Marr, 1982) account of these breakdowns: The structure of the parser's memory and the nature of incremental parsing produce a smooth degradation of processing accuracy for longer center embeddings, and a steeper degradation when they are nested, in line with recall observations by Miller and Isard (1964) and speed-accuracy trade-off observations by McElree et al. (2003). Modeling results show that this effect is distinct from the effects of ambiguity and exceeds the effect of mere sentence length.},
  langid = {english},
  keywords = {Computational modeling,Computer simulation,Language understanding,Linguistics,Locality,Memory,Sentence processing,Surprisal,Syntax},
  file = {/Users/j/Zotero/storage/P2Q26JLY/Rasmussen and Schuler - 2018 - Left-Corner Parsing With Distributed Associative M.pdf}
}

@misc{rasooli.m:2015,
  title = {Yara {{Parser}}: {{A Fast}} and {{Accurate Dependency Parser}}},
  shorttitle = {Yara {{Parser}}},
  author = {Rasooli, Mohammad Sadegh and Tetreault, Joel},
  year = {2015},
  month = mar,
  number = {arXiv:1503.06733},
  eprint = {1503.06733},
  primaryclass = {cs},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1503.06733},
  urldate = {2022-05-17},
  abstract = {Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https://github.com/yahoo/YaraParser.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotero/storage/5F6CI55T/Rasooli and Tetreault - 2015 - Yara Parser A Fast and Accurate Dependency Parser.pdf;/Users/j/Zotero/storage/SP9552SL/1503.html}
}

@article{ratcliff.r:1978,
  title = {A Theory of Memory Retrieval},
  author = {Ratcliff, Roger},
  year = {1978},
  journal = {Psychological Review},
  volume = {85},
  number = {2},
  pages = {59--108},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.85.2.59},
  abstract = {Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe\textendash memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe\textendash memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study\textendash test, and continuous) and to speed\textendash accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Memory,Theories}
}

@phdthesis{ravishankar.m:1996,
  title = {Efficient Algorithms for Speech Recognition},
  author = {Ravishankar, Mosur K},
  year = {1996},
  month = may,
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.72.3560},
  date-added = {2022-03-25 23:17:15 -0400},
  date-modified = {2022-03-25 23:19:35 -0400},
  school = {Carnegie Mellon University, Department of Computer Science}
}

@article{rayner.k:1998,
  title = {Eye Movements in Reading and Information Processing: 20 Years of Research},
  author = {Rayner, Keith},
  year = {1998},
  journal = {Psychological Bulletin},
  volume = {124},
  pages = {372--422},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.124.3.372},
  abstract = {Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Cognitive Processes,*Eye Movements,Reading}
}

@inproceedings{rehurek.r:2010gensim,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r{u}}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  pages = {45--50},
  publisher = {{ELRA}},
  address = {{Valletta, Malta}},
  url = {http://is.muni.cz/publication/884893/en},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@article{reichle.e:2003,
  title = {The {{E-Z Reader}} Model of Eye-Movement Control in Reading: {{Comparisons}} to Other Models},
  author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
  year = {2003},
  journal = {Behavioral and Brain Sciences},
  volume = {26},
  number = {4},
  pages = {445--476},
  publisher = {{Cambridge University Press (CUP)}},
  doi = {10.1017/s0140525x03000104},
  url = {https://doi.org/10.1017%2Fs0140525x03000104},
  bdsk-url-2 = {https://doi.org/10.1017/s0140525x03000104},
  date-added = {2021-05-22 14:59:06 -0400},
  date-modified = {2022-05-06 15:37:48 -0400},
  keywords = {eye-tracking,processing},
  file = {/Users/j/Zotero/storage/9TJ77U5L/Reichle et al. (2003) The E-Z Reader model of eye-movement control in re.pdf}
}

@article{reiss.c:2018,
  title = {Substance Free Phonology},
  author = {Reiss, Charles},
  editor = {S. J. Hannahs, A. R. K. Bosch},
  year = {2017},
  journal = {The Routledge handbook of phonological theory},
  pages = {425--452},
  publisher = {{Routledge New York}},
  date-added = {2019-06-17 08:29:14 -0400},
  date-modified = {2019-06-17 08:38:02 -0400},
  isbn = {9781138025813},
  keywords = {substance free phonology}
}

@inproceedings{renyi.a:1961,
  title = {On Measures of Entropy and Information},
  booktitle = {Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability},
  author = {R{\'e}nyi, Alfr{\'e}d},
  editor = {Neyman, Jerzy},
  year = {1961},
  volume = {1},
  pages = {547--561},
  url = {https://projecteuclid.org/proceedings/berkeley-symposium-on-mathematical-statistics-and-probability/Proceedings-of-the-Fourth-Berkeley-Symposium-on-Mathematical-Statistics-and/Chapter/On-Measures-of-Entropy-and-Information/bsmsp/1200512181},
  date-added = {2021-10-27 09:20:40 -0400},
  date-modified = {2021-10-27 09:27:42 -0400},
  organization = {{University of California Press}}
}

@article{rezac.m:2008,
  title = {Phi-Agree and Theta-Related Case},
  author = {Rezac, Milan},
  year = {2008},
  journal = {Phi theory: Phi-features across interfaces and modules},
  pages = {83--129},
  publisher = {{Oxford University Press Oxford}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:25:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@unpublished{rezac.m:2016,
  title = {The Ways of Referential Deficiency: {{Impersonal}} {{{\emph{on}}}} and Its Kin},
  author = {Rezac, Milan and Jouitteau, M{\'e}lanie},
  year = {2016},
  url = {https://www.iker.cnrs.fr/IMG/pdf/rezac<sub>j</sub>ouitteau.impersonal.pdf},
  date-added = {2021-03-21 13:51:16 -0400},
  date-modified = {2021-03-21 14:01:49 -0400},
  keywords = {breton,impersonals,phi features}
}

@book{riehl.e:2017,
  title = {Category Theory in Context},
  author = {Riehl, Emily},
  year = {2017},
  publisher = {{Courier Dover Publications}},
  date-added = {2019-08-24 09:26:31 -0400},
  date-modified = {2019-08-24 09:26:50 -0400},
  keywords = {category theory}
}

@article{rigby.r:2005GAMLSS,
  title = {Generalized Additive Models for Location, Scale and Shape},
  author = {Rigby, R. A. and Stasinopoulos, D. M.},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {54},
  number = {3},
  pages = {507--554},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2005.00510.x},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1467-9876.2005.00510.x},
  urldate = {2023-03-01},
  abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton\textendash Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
  langid = {english},
  keywords = {Beta\textendash binomial distribution,Box\textendash Cox transformation,Centile estimation,Cubic smoothing splines,GAMLSS,Generalized linear mixed model,LMS method,Negative binomial distribution,Non-normality,Nonparametric models,Overdispersion,Penalized likelihood,Random effects,Skewness and kurtosis},
  file = {/Users/j/Zotero/storage/AXAM22TY/Rigby and Stasinopoulos (2005) Generalized additive models for location, scale an.pdf}
}

@article{ritchie.d:1986,
  title = {Shannon and {{Weaver}}: Unravelling the Paradox of Information},
  author = {Ritchie, David},
  year = {1986},
  month = apr,
  journal = {Communication Research},
  volume = {13},
  number = {2},
  pages = {278--298},
  publisher = {{SAGE Publications}},
  doi = {10.1177/009365086013002007},
  url = {https://doi.org/10.1177%2F009365086013002007},
  abstract = {A case is presented for separating Shannon's (1949) paper on information theory from Weaver's introduction, which is shown to contain distortions, as well as proofs by coincidence and homonym. Shannon's mathematical tools and methods are distinguished from his theory, which consists of 23 theorems setting forth the conditions for maximum efficiency in electromechanical signal transmission. Attempts to apply Shannon's theory to our field are reviewed, along with previous critiques, and it is recommended that future uses of Shannon's theory adopt a more methodologically rigorous approach. In particular, it is argued that Shannon's assumptions must be shown to hold before his theorems can be successfully applied.},
  bdsk-url-2 = {https://doi.org/10.1177/009365086013002007},
  date-added = {2022-04-07 12:59:21 -0400},
  date-modified = {2022-04-07 13:01:58 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{roark.b:2001,
  title = {Probabilistic Top-down Parsing and Language Modeling},
  author = {Roark, Brian},
  year = {2001},
  journal = {Computational Linguistics},
  volume = {27},
  number = {2},
  pages = {249--276},
  doi = {10.1162/089120101750300526},
  url = {https://www.aclweb.org/anthology/J01-2004},
  bdsk-url-2 = {https://doi.org/10.1162/089120101750300526},
  file = {/Users/j/Zotero/storage/WG7I7NQH/Roark - 2001 - Probabilistic top-down parsing and language modeli.pdf}
}

@inproceedings{roark.b:2009,
  title = {Deriving Lexical and Syntactic Expectation-Based Measures for Psycholinguistic Modeling via Incremental Top-down Parsing},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
  year = {2009},
  pages = {324--333},
  publisher = {{Association for Computational Linguistics}},
  address = {{Singapore}},
  url = {https://www.aclweb.org/anthology/D09-1034}
}

@techreport{roark.b:2011techreport,
  type = {Technical Report},
  title = {Expected Surprisal and Entropy},
  author = {Roark, Brian},
  year = {2011},
  number = {CSLU-11-004},
  institution = {{Center for Spoken Language Processing, Oregon Health and Science University}},
  url = {https://lanzaroark.org/docs/techrpt-CSLU-11-004.pdf},
  date-added = {2021-06-16 09:42:11 -0400},
  date-modified = {2021-06-16 09:48:54 -0400},
  file = {/Users/j/Zotero/storage/LEN85CGT/Roark - 2011 - Expected surprisal and entropy.pdf}
}

@misc{robert.c:2010,
  type = {Blog},
  title = {Effective Sample Size},
  author = {Robert, Christian P.},
  year = {2010},
  month = sep,
  journal = {Xi'an's Og},
  url = {https://xianblog.wordpress.com/2010/09/24/effective-sample-size/},
  urldate = {2022-12-10},
  langid = {english},
  keywords = {effective sample size,importance sampling}
}

@article{rogers.j:2003,
  title = {Syntactic Structures as Multi-Dimensional Trees},
  author = {Rogers, James},
  year = {2003},
  journal = {Research on Language and Computation},
  volume = {1},
  number = {3-4},
  pages = {265--305},
  publisher = {{Springer}},
  date-added = {2019-06-15 11:50:06 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {automata,control languages}
}

@incollection{rohatgi.v:2015,
  title = {Some Special Distributions},
  booktitle = {An {{Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Rohatgi, Vijay K. and Saleh, A. K. Md. Ehsanes},
  year = {2015},
  pages = {173--244},
  publisher = {{John Wiley \& Sons, Ltd}},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1002/9781118799635.ch5},
  urldate = {2022-07-20},
  abstract = {This chapter focuses on some commonly occurring probability distributions and investigates their basic properties. The results of this chapter will be of considerable use in theoretical as well as practical applications. The chapter begins with some univariate and multivariate discrete distributions and proceeds with some continuous models. It then deals with bivariate and multivariate normal distributions and subsequently discusses the exponential family of distributions. Finally, the chapter records briefly some of the several other distributions which are related to these special distributions and their important characteristics.},
  chapter = {5},
  isbn = {978-1-118-79963-5},
  langid = {english},
  keywords = {bivariate normal distribution,continuous distributions,discrete distributions,exponential distribution,multivariate normal distribution},
  file = {/Users/j/Zotero/storage/7RDPDKEA/2015 - Some Special Distributions.pdf}
}

@article{ronai.e:2023,
  title = {Memory versus Expectation: Processing Relative Clauses in a Flexible Word Order Language},
  shorttitle = {Memory versus Expectation},
  author = {Ronai, Eszter and Xiang, Ming},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {1},
  pages = {e13227},
  issn = {1551-6709},
  doi = {10.1111/cogs.13227},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/cogs.13227},
  urldate = {2023-03-09},
  abstract = {Memory limitations and probabilistic expectations are two key factors that have been posited to play a role in the incremental processing of natural language. Relative clauses (RCs) have long served as a key proving ground for such theories of language processing. Across three self-paced reading experiments, we test the online comprehension of Hungarian subject- and object-extracted RCs (SRCs and ORCs, respectively). We capitalize on the syntactic properties of Hungarian that allow for a variety of word orders within RCs, which helps us to delineate the processing costs associated with memory demand and violated expectations. Results showed a processing cost at the RC verb for structures that have longer verb-argument distances, despite those structures being more frequent in the corpus. These findings thus support theories that attribute processing difficulty to memory limitations, rather than theories that attribute difficulty to less expected structures.},
  langid = {english},
  keywords = {Language processing,Memory models,object-extracted relative clauses,ORC,Predictive processing,Relative clauses,Syntactic parsing},
  file = {/Users/j/Zotero/storage/DS962R56/Ronai and Xiang (2023) Memory Versus Expectation Processing Relative Cla.pdf}
}

@misc{rosa.r:2019short,
  title = {Inducing Syntactic Trees from {{BERT}} Representations},
  author = {Rosa, Rudolf and Mare{\v c}ek, David},
  year = {2019},
  eprint = {1906.11511},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-16 11:51:58 -0400}
}

@incollection{rosenbloom.p:1987,
  title = {Learning by Chunking: A Production System Model of Practice},
  booktitle = {Production {{System Models}} of {{Learning}} and {{Development}}},
  author = {Rosenbloom, Paul and Newell, Allen},
  editor = {Klahr, David and Langley, Patrick W. and Neches, Robert T.},
  year = {1987},
  month = jan,
  pages = {221--286},
  publisher = {{The MIT Press}},
  doi = {10.7551/mitpress/5605.003.0007},
  url = {https://doi.org/10.7551/mitpress/5605.003.0007},
  urldate = {2022-09-25},
  isbn = {978-0-262-31596-8}
}

@inproceedings{rosenkrantz.d:1970,
  title = {Deterministic Left Corner Parsing},
  booktitle = {11th Annual Symposium on Switching and Automata Theory (Swat 1970)},
  author = {Rosenkrantz, D. J. and Lewis, P. M.},
  year = {1970},
  pages = {139--152},
  doi = {10.1109/SWAT.1970.5},
  url = {https://doi.org/10.1109/SWAT.1970.5},
  date-added = {2022-03-11 22:35:17 -0500},
  date-modified = {2022-03-11 22:35:18 -0500}
}

@book{rosenthal.j:2006,
  title = {A First Look at Rigorous Probability Theory},
  author = {Rosenthal, Jeffrey S},
  year = {2006},
  month = nov,
  edition = {Second},
  publisher = {{World Scientific}},
  doi = {10.1142/6300},
  url = {https://doi.org/10.1142%2F6300},
  bdsk-url-2 = {https://doi.org/10.1142/6300},
  bdsk-url-3 = {http://probability.ca/jeff/grprobbook.html},
  date-added = {2021-10-15 14:24:25 -0400},
  date-modified = {2021-10-15 14:25:40 -0400}
}

@article{rouder.j:2015,
  title = {The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  shorttitle = {The Lognormal Race},
  author = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  year = {2015},
  month = jun,
  journal = {Psychometrika},
  volume = {80},
  number = {2},
  pages = {491--513},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9396-3},
  url = {http://link.springer.com/10.1007/s11336-013-9396-3},
  urldate = {2022-08-13},
  langid = {english}
}

@book{royden.h:2010,
  title = {Real Analysis},
  author = {Royden, H. L. and Fitzpatrick, Patrick},
  year = {2010},
  edition = {4th ed},
  publisher = {{Prentice Hall}},
  address = {{Boston}},
  abstract = {Real Analysis, Fourth Edition, covers the basic material that every reader should know in the classical theory of functions of a real variable, measure and integration theory, and some of the more important and elementary topics in general topology and normed linear space theory. This text assumes a general background in mathematics and familiarity with the fundamental concepts of analysis. Classical theory of functions, including the classical Banach spaces; General topology and the theory of general Banach spaces; Abstract treatment of measure and integration. For all readers interested in real analysis},
  isbn = {978-0-13-143747-0},
  langid = {english},
  annotation = {OCLC: 456836719}
}

@article{ryskin.r:2018,
  title = {Comprehenders Model the Nature of Noise in the Environment},
  author = {Ryskin, Rachel and Futrell, Richard and Kiran, Swathi and Gibson, Edward},
  year = {2018},
  month = dec,
  journal = {Cognition},
  volume = {181},
  pages = {141--150},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.08.018},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027718302245},
  urldate = {2022-11-28},
  abstract = {In everyday communication, speakers make errors and produce language in a noisy environment. Recent work suggests that comprehenders possess cognitive mechanisms for dealing with noise in the linguistic signal: a noisy-channel model. A key parameter of these models is the noise model: the comprehender's implicit model of how noise affects utterances before they are perceived. Here we examine this noise model in detail, asking whether comprehension behavior reflects a noise model that is adapted to context. We asked readers to correct sentences if they noticed errors, and manipulated context by including exposure sentences containing obvious deletions (A bystander was rescued by the fireman in the nick time.), insertions, exchanges, mixed errors, or no errors. On test sentences (The bat swung the player.), participants' corrections differed depending on the exposure condition. The results demonstrate that participants model specific types of errors and make inferences about the intentions of the speaker accordingly.},
  langid = {english},
  keywords = {Adaptation,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {/Users/j/Zotero/storage/AGV76HVH/Ryskin et al. (2018) Comprehenders model the nature of noise in the env.pdf}
}

@article{ryskin.r:2021,
  title = {An {{ERP}} Index of Real-Time Error Correction within a Noisy-Channel Framework of Human Communication},
  author = {Ryskin, Rachel and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Fedorenko, Evelina and Gibson, Edward},
  year = {2021},
  month = jul,
  journal = {Neuropsychologia},
  volume = {158},
  pages = {107855},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107855},
  url = {https://www.sciencedirect.com/science/article/pii/S0028393221001068},
  urldate = {2022-06-24},
  abstract = {Recent evidence suggests that language processing is well-adapted to noise in the input (e.g., spelling or speech errors, misreading or mishearing) and that comprehenders readily correct the input via rational inference over possible intended sentences given probable noise corruptions. In the current study, we probed the processing of noisy linguistic input, asking whether well-studied ERP components may serve as useful indices of this inferential process. In particular, we examined sentences where semantic violations could be attributed to noise\textemdash for example, in ``The storyteller could turn any incident into an amusing antidote'', where the implausible word ``antidote'' is orthographically and phonologically close to the intended ``anecdote''. We found that the processing of such sentences\textemdash where the probability that the message was corrupted by noise exceeds the probability that it was produced intentionally and perceived accurately\textemdash was associated with a reduced (less negative) N400 effect and an increased P600 effect, compared to semantic violations which are unlikely to be attributed to noise (``The storyteller could turn any incident into an amusing hearse''). Further, the magnitudes of these ERP effects were correlated with the probability that the comprehender retrieved a plausible alternative. This work thus adds to the growing body of literature that suggests that many aspects of language processing are optimized for dealing with noise in the input, and opens the door to electrophysiologic investigations of the computations that support the processing of imperfect input.},
  langid = {english},
  keywords = {electroencephalography,event-related potential,N400,noisy-channel,P600},
  file = {/Users/j/Zotero/storage/85E24ABK/Ryskin et al. - 2021 - An ERP index of real-time error correction within .pdf}
}

@book{sag.i:2003,
  title = {Syntactic Theory: {{A}} Formal Introduction},
  author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M.},
  year = {2003},
  edition = {Second},
  publisher = {{CSLI}},
  address = {{Stanford, CA}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{sag.i:2012,
  title = {Sign-Based Construction Grammar: {{An}} Informal Synopsis},
  booktitle = {Sign\textendash{{Based}} Construction Grammar},
  author = {Sag, Ivan A.},
  editor = {{Boas, Hans abd Sag}, Ivan A.},
  year = {2012},
  pages = {101--107},
  publisher = {{CSLI Publications}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:05 -0400}
}

@article{sainburg.t:2019,
  title = {Parallels in the Sequential Organization of Birdsong and Human Speech},
  author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q},
  year = {2019},
  journal = {Nature communications},
  volume = {10},
  publisher = {{Nature Publishing Group}},
  date-added = {2019-10-01 16:50:44 -0400},
  date-modified = {2019-10-01 16:51:36 -0400},
  keywords = {birdsong,mutual information,structure,syntax}
}

@inproceedings{salimans.t:2015MCVI,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: Bridging the Gap},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  editor = {Bach, Francis and Blei, David},
  year = {2015},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {1218--1226},
  publisher = {{PMLR}},
  address = {{Lille, France}},
  url = {https://proceedings.mlr.press/v37/salimans15.html},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  date-added = {2022-05-05 11:01:25 -0400},
  date-modified = {2022-05-05 11:02:29 -0400},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf},
  keywords = {markov chain monte carlo,markov chain variational inference,variational inference}
}

@misc{salle.a:2019,
  title = {Why so down? {{The}} Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics},
  author = {Salle, Alexandre and Villavicencio, Aline},
  year = {2019},
  eprint = {1908.06941},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2020-07-13 08:39:45 -0400},
  date-modified = {2020-07-13 08:40:45 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,pmi}
}

@inproceedings{sanborn.a:2006cogsci,
  title = {A More Rational Model of Categorization},
  booktitle = {Proceedings of the 28th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Sanborn, Adam and Griffiths, Thomas L. and Navarro, Daniel J.},
  year = {2006},
  publisher = {{LAWRENCEE}},
  address = {{Vancouver, British Columbia, Canada}},
  url = {https://hdl.handle.net/2440/35610},
  urldate = {2022-10-20},
  abstract = {Adam N. Sanborn, Thomas L. Griffiths, Daniel J. Navarro},
  langid = {english},
  keywords = {particle filtering},
  file = {/Users/j/Zotero/storage/PP4FUP6J/Sanborn et al. (2006) A more rational model of categorization.pdf}
}

@article{sanford.a:2002,
  title = {Depth of Processing in Language Comprehension: Not Noticing the Evidence},
  shorttitle = {Depth of Processing in Language Comprehension},
  author = {Sanford, Anthony J. and Sturt, Patrick},
  year = {2002},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {9},
  pages = {382--386},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01958-7},
  url = {https://www.sciencedirect.com/science/article/pii/S1364661302019587},
  urldate = {2022-06-14},
  abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
  langid = {english},
  keywords = {Language interpretation,Representation,Semantic anomalies,Text-change-blindness,Underspecification}
}

@misc{sanh.v:2019distilbert,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019},
  eprint = {1910.01108},
  primaryclass = {cs.CL},
  archiveprefix = {arxiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@article{sanz-alonso.d:2018,
  title = {Importance Sampling and Necessary Sample Size: An Information Theory Approach},
  shorttitle = {Importance Sampling and Necessary Sample Size},
  author = {{Sanz-Alonso}, Daniel},
  year = {2018},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {6},
  number = {2},
  pages = {867--879},
  issn = {2166-2525},
  doi = {10.1137/16M1093549},
  url = {https://epubs.siam.org/doi/10.1137/16M1093549},
  urldate = {2022-12-21},
  langid = {english},
  keywords = {importance sampling},
  file = {/Users/j/Zotero/storage/4W544IBJ/Sanz-Alonso (2018) Importance Sampling and Necessary Sample Size An .pdf}
}

@inproceedings{saphra.n:2018,
  title = {Understanding Learning Dynamics of Language Models with {{SVCCA}}},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Saphra, Naomi and Lopez, Adam},
  year = {2019},
  pages = {3257--3267},
  publisher = {{Association for Computational Linguistics}},
  address = {{Minneapolis, Minnesota}},
  doi = {10.18653/v1/N19-1329},
  url = {https://www.aclweb.org/anthology/N19-1329},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1329}
}

@inproceedings{savinov.n:2022,
  title = {Step-Unrolled Denoising Autoencoders for Text Generation},
  booktitle = {International Conference on Learning Representations},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and {van den Oord}, Aaron},
  year = {2022},
  url = {https://openreview.net/forum?id=T0GpzBQ1Fg6},
  date-added = {2022-05-04 10:51:51 -0400},
  date-modified = {2022-05-04 10:52:30 -0400},
  keywords = {autoencoders,denoising,diffusion processes,language modeling,SUNDAE}
}

@incollection{scha.r:1990,
  title = {Taaltheorie En Taaltechnologie; Competence En Performance.},
  booktitle = {Computertoepassingen in de Neerlandistiek},
  author = {Scha, Remko},
  editor = {{de Kort}, R. and Leerdam, G.L.J.},
  year = {1990},
  pages = {7--22},
  publisher = {{Landelijke Vereniging van Neerlandici}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  pass = {1},
  readinglist = {Thesis}
}

@phdthesis{schabes.y:1990phd,
  title = {Mathematical and Computational Aspects of Lexicalized Grammars},
  author = {Schabes, Yves},
  year = {1990},
  url = {https://repository.upenn.edu/dissertations/AAI9101213},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2022-04-26 21:21:04 -0400},
  project = {syntactic embedding},
  school = {University of Pennsylvania}
}

@misc{schick-poland.k:2021,
  title = {A Partial Information Decomposition for Discrete and Continuous Variables},
  author = {{Schick-Poland}, Kyle and Makkeh, Abdullah and Gutknecht, Aaron J. and Wollstadt, Patricia and Sturm, Anja and Wibral, Michael},
  year = {2021},
  eprint = {2106.12393},
  primaryclass = {cs.IT},
  archiveprefix = {arxiv},
  date-added = {2021-09-30 17:30:27 -0400},
  date-modified = {2021-09-30 17:30:29 -0400}
}

@article{schijndel.m:2013,
  title = {A Model of Language Processing as Hierarchic Sequential Prediction},
  author = {{van Schijndel}, Marten and Exley, Andy and Schuler, William},
  year = {2013},
  month = jun,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {522--540},
  publisher = {{Wiley}},
  doi = {10.1111/tops.12034},
  url = {https://doi.org/10.1111%2Ftops.12034},
  bdsk-url-2 = {https://doi.org/10.1111/tops.12034},
  date-added = {2021-09-13 21:29:14 -0400},
  date-modified = {2021-09-13 21:29:17 -0400}
}

@inproceedings{schijndel.m:2013NAACL,
  title = {An Analysis of Frequency- and Memory-Based Processing Costs},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2013},
  month = jun,
  pages = {95--105},
  publisher = {{Association for Computational Linguistics}},
  address = {{Atlanta, Georgia}},
  url = {https://aclanthology.org/N13-1010},
  urldate = {2023-03-01},
  file = {/Users/j/Zotero/storage/FKHRJTAE/van Schijndel and Schuler (2013) An Analysis of Frequency- and Memory-Based Process.pdf}
}

@inproceedings{schijndel.m:2015,
  title = {Hierarchic Syntax Improves Reading Time Prediction},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2015},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/v1/n15-1183},
  url = {https://doi.org/10.3115%2Fv1%2Fn15-1183},
  bdsk-url-2 = {https://doi.org/10.3115/v1/n15-1183},
  date-added = {2021-09-13 21:36:26 -0400},
  date-modified = {2021-09-13 21:36:30 -0400}
}

@inproceedings{schijndel.m:2017,
  title = {Approximations of Predictive Entropy Correlate with Reading Times},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Cognitive Science Society}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  editor = {Gunzelmann, Glenn and Andrew Howes, Thora Tenbrink and Davelaar, Eddy},
  year = {2017},
  pages = {1266--1271},
  address = {{London, United Kingdom}},
  url = {https://cogsci.mindmodeling.org/2017/papers/0242/index.html},
  date-added = {2022-04-21 09:42:08 -0400},
  date-modified = {2022-04-21 09:44:53 -0400},
  organization = {{Cognitive Science Society}},
  keywords = {entropy reduction,predictability,predictive entropy}
}

@inproceedings{schijndel.m:2018,
  title = {Modeling Garden Path Effects without Explicit Hierarchical Syntax.},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  editor = {{Rogers} and {Rau} and {Zhu} and {Kalish}},
  year = {2018},
  address = {{Madison, Wisconsin}},
  url = {https://cogsci.mindmodeling.org/2018/papers/0496/},
  date-added = {2021-03-18 10:48:52 -0400},
  date-modified = {2021-03-18 11:56:18 -0400},
  isbn = {978-0-9911967-8-4}
}

@inproceedings{schijndel.m:2018a,
  title = {A Neural Model of Adaptation in Reading},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2018},
  pages = {4704--4710},
  publisher = {{Association for Computational Linguistics}},
  address = {{Brussels, Belgium}},
  doi = {10.18653/v1/D18-1499},
  url = {https://www.aclweb.org/anthology/D18-1499},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1499}
}

@inproceedings{schijndel.m:2019,
  title = {Can Entropy Explain Successor Surprisal Effects in Reading?},
  booktitle = {Proceedings of the Society for Computation in Linguistics ({{SCiL}}), {{NYC}}, January 3\textendash 6, 2019},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2019},
  volume = {2},
  pages = {1--7},
  publisher = {{University of Massachusetts Amherst}},
  doi = {10.7275/QTBB-9D05},
  url = {https://scholarworks.umass.edu/scil/vol2/iss1/2/},
  bdsk-url-2 = {https://doi.org/10.7275/QTBB-9D05},
  date-added = {2022-04-21 09:21:31 -0400},
  date-modified = {2022-04-21 09:39:00 -0400}
}

@misc{schijndel.m:2020psyarxiv,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2020},
  doi = {10.31234/osf.io/sgbqy},
  url = {https://doi.org/10.31234/osf.io/sgbqy},
  bdsk-url-1 = {psyarxiv.com/sgbqy},
  date-added = {2021-03-10 11:20:44 -0500},
  date-modified = {2021-03-18 17:20:22 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing}
}

@article{schijndel.m:2021,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12988},
  issn = {1551-6709},
  doi = {10.1111/cogs.12988},
  urldate = {2022-10-11},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  keywords = {Garden paths,Information theory,Neural networks,Self-paced reading,Surprisal},
  file = {/Users/j/Zotero/storage/99C587CD/van Schijndel and Linzen (2021) Single-Stage Prediction Models Do Not Explain the .pdf}
}

@article{schooler.l:1997,
  title = {The Role of Process in the Rational Analysis of Memory},
  author = {Schooler, Lael J. and Anderson, John R.},
  year = {1997},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {32},
  number = {3},
  pages = {219--250},
  issn = {00100285},
  doi = {10.1006/cogp.1997.0652},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0010028597906526},
  urldate = {2022-09-27},
  langid = {english},
  file = {/Users/j/Zotero/storage/Q8WPISAE/Schooler and Anderson (1997) The Role of Process in the Rational Analysis of Me.pdf}
}

@article{schrimpf.m:2021,
  title = {The Neural Architecture of Language: {{Integrative}} Modeling Converges on Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {45},
  pages = {e2105646118},
  publisher = {{Proceedings of the National Academy of Sciences}},
  doi = {10.1073/pnas.2105646118},
  url = {https://www.pnas.org/doi/10.1073/pnas.2105646118},
  urldate = {2023-05-02},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  file = {/Users/j/Zotero/storage/2K6HT3PI/Schrimpf et al. (2021) The neural architecture of language Integrative m.pdf}
}

@inproceedings{schuler.w:2008,
  title = {Toward a Psycholinguistically-Motivated Model of Language Processing},
  booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  author = {Schuler, William and AbdelRahman, Samir and Miller, Tim and Schwartz, Lane},
  year = {2008},
  month = aug,
  pages = {785--792},
  publisher = {{Coling 2008 Organizing Committee}},
  address = {{Manchester, UK}},
  url = {https://aclanthology.org/C08-1099},
  date-added = {2022-05-02 11:58:18 -0400},
  date-modified = {2022-05-02 11:58:33 -0400},
  keywords = {incrementality,parsing,parsing algorithm}
}

@article{schutze.c:2003,
  title = {Syncretism and Double Agreement with {{Icelandic}} Nominative Objects},
  author = {Sch{\"u}tze, Carson T},
  year = {2003},
  bdsk-url-1 = {escholarship.org/uc/item/8vn9b04q},
  date-added = {2020-03-05 10:17:01 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {syncretism}
}

@article{schwartz.m:2008,
  title = {The Importance of Stupidity in Scientific Research},
  author = {Schwartz, Martin A.},
  year = {2008},
  month = jun,
  journal = {Journal of Cell Science},
  volume = {121},
  number = {11},
  pages = {1771},
  issn = {0021-9533},
  doi = {10.1242/jcs.033340},
  url = {https://doi.org/10.1242/jcs.033340},
  urldate = {2022-06-16},
  keywords = {scientific method,stupidity},
  file = {/Users/j/Zotero/storage/VUWEZK4C/Schwartz - 2008 - The importance of stupidity in scientific research.pdf}
}

@misc{schwartz.r:2019,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  eprint = {1907.10597},
  primaryclass = {cs.CY},
  archiveprefix = {arxiv},
  date-added = {2019-09-30 11:48:17 -0400},
  date-modified = {2019-09-30 11:50:22 -0400},
  keywords = {energy,environmental impact}
}

@article{sebastiani.p:2000,
  title = {Maximum Entropy Sampling and Optimal Bayesian Experimental Design},
  author = {Sebastiani, Paola and Wynn, Henry P.},
  year = {2000},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {62},
  number = {1},
  eprint = {2680683},
  eprinttype = {jstor},
  pages = {145--157},
  publisher = {{[Royal Statistical Society, Wiley]}},
  issn = {13697412, 14679868},
  url = {http://www.jstor.org/stable/2680683},
  abstract = {When Shannon entropy is used as a criterion in the optimal design of experiments, advantage can be taken of the classical identity representing the joint entropy of parameters and observations as the sum of the marginal entropy of the observations and the preposterior conditional entropy of the parameters. Following previous work in which this idea was used in spatial sampling, the method is applied to standard parameterized Bayesian optimal experimental design. Under suitable conditions, which include non-linear as well as linear regression models, it is shown in a few steps that maximizing the marginal entropy of the sample is equivalent to minimizing the pre-posterior entropy, the usual Bayesian criterion, thus avoiding the use of conditional distributions. It is shown using this marginal formulation that under normality assumptions every standard model which has a two-point prior distribution on the parameters gives an optimal design supported on a single point. Other results include a new asymptotic formula which applies as the error variance is large and bounds on support size.},
  date-added = {2021-09-15 10:25:32 -0400},
  date-modified = {2021-09-15 10:25:34 -0400}
}

@inproceedings{sennrich.r:2016,
  title = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = aug,
  pages = {1715--1725},
  publisher = {{Association for Computational Linguistics}},
  address = {{Berlin, Germany}},
  doi = {10.18653/v1/P16-1162},
  url = {https://aclanthology.org/P16-1162},
  urldate = {2023-03-03},
  keywords = {BPE,byte-pair encoding,machine translation,neural machine translation,NMT,subword units},
  file = {/Users/j/Zotero/storage/KIB2UYJY/Sennrich et al. (2016) Neural Machine Translation of Rare Words with Subw.pdf}
}

@inproceedings{shain.c:2018,
  title = {Deep Syntactic Annotations for Broad-Coverage Psycholinguistic Modeling},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Shain, Cory and {van Schijndel}, Marten},
  editor = {Devereux, Barry and Shutova, Ekaterina and Huang, Chu-Ren},
  year = {2018},
  pages = {33--37},
  publisher = {{European Language Resources Association (ELRA)}},
  address = {{Paris, France}},
  url = {http://lrec-conf.org/workshops/lrec2018/W9/pdf/9_W9.pdf},
  isbn = {979-10-95546-08-5},
  langid = {english}
}

@inproceedings{shain.c:2018CDR,
  title = {Deconvolutional Time Series Regression: {{A}} Technique for Modeling Temporally Diffuse Effects},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Shain, Cory and Schuler, William},
  year = {2018},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/d18-1288},
  url = {https://doi.org/10.18653%2Fv1%2Fd18-1288},
  bdsk-url-2 = {https://doi.org/10.18653/v1/d18-1288},
  date-added = {2021-09-18 22:25:03 -0400},
  date-modified = {2021-09-18 22:25:04 -0400},
  file = {/Users/j/Zotero/storage/BNGIYMQV/Shain and Schuler (2018) Deconvolutional Time Series Regression A Techniqu.pdf}
}

@article{shain.c:2021,
  title = {Continuous-Time Deconvolutional Regression for Psycholinguistic Modeling},
  author = {Shain, Cory and Schuler, William},
  year = {2021},
  month = oct,
  journal = {Cognition},
  volume = {215},
  pages = {104735},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104735},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027721001542},
  urldate = {2022-10-26},
  abstract = {The influence of stimuli in psycholinguistic experiments diffuses across time because the human response to language is not instantaneous. The linear models typically used to analyze psycholinguistic data are unable to account for this phenomenon due to strong temporal independence assumptions, while existing deconvolutional methods for estimating diffuse temporal structure model time discretely and therefore cannot be directly applied to natural language stimuli where events (words) have variable duration. In light of evidence that continuous-time deconvolutional regression (CDR) can address these issues (Shain \& Schuler, 2018), this article motivates the use of CDR for many experimental settings, exposits some of its mathematical properties, and empirically evaluates the influence of various experimental confounds (noise, multicollinearity, and impulse response misspecification), hyperparameter settings, and response types (behavioral and fMRI). Results show that CDR (1) yields highly consistent estimates across a variety of hyperparameter configurations, (2) faithfully recovers the data-generating model on synthetic data, even under adverse training conditions, and (3) outperforms widely-used statistical approaches when applied to naturalistic reading and fMRI data. In addition, procedures for testing scientific hypotheses using CDR are defined and demonstrated, and empirically-motivated best-practices for CDR modeling are proposed. Results support the use of CDR for analyzing psycholinguistic time series, especially in a naturalistic experimental paradigm.},
  langid = {english},
  file = {/Users/j/Zotero/storage/V4BD2Y8U/Shain and Schuler (2021) Continuous-time deconvolutional regression for psy.pdf}
}

@inproceedings{shain.c:2021CDRNN,
  title = {{{CDRNN}}: Discovering Complex Dynamics in Human Language Processing},
  shorttitle = {Cdrnn},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Shain, Cory},
  year = {2021},
  month = aug,
  pages = {3718--3734},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2021.acl-long.288},
  url = {https://aclanthology.org/2021.acl-long.288},
  urldate = {2022-10-26},
  abstract = {The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain \& Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.},
  file = {/Users/j/Zotero/storage/49DJVQCV/Shain (2021) CDRNN Discovering Complex Dynamics in Human Langu.pdf}
}

@misc{shain.c:2022CDRNNdetails,
  title = {A Deep Learning Approach to Analyzing Continuous-Time Systems},
  author = {Shain, Cory and Schuler, William},
  year = {2022},
  month = sep,
  number = {arXiv:2209.12128},
  eprint = {2209.12128},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2209.12128},
  url = {http://arxiv.org/abs/2209.12128},
  urldate = {2022-10-26},
  abstract = {Scientists often use observational time series data to study complex natural processes, from climate change to civil conflict to brain activity. But regression analyses of these data often assume simplistic dynamics. Recent advances in deep learning have yielded startling improvements to the performance of models of complex processes, from speech comprehension to nuclear physics to competitive gaming. But deep learning is generally not used for scientific analysis. Here, we bridge this gap by showing that deep learning can be used, not just to imitate, but to analyze complex processes, providing flexible function approximation while preserving interpretability. Our approach -- the continuous-time deconvolutional regressive neural network (CDRNN) -- relaxes standard simplifying assumptions (e.g., linearity, stationarity, and homoscedasticity) that are implausible for many natural systems and may critically affect the interpretation of data. We evaluate CDRNNs on incremental human language processing, a domain with complex continuous dynamics. We demonstrate dramatic improvements to predictive likelihood in behavioral and neuroimaging data, and we show that CDRNNs enable flexible discovery of novel patterns in exploratory analyses, provide robust control of possible confounds in confirmatory analyses, and open up research questions that are otherwise hard to study using observational data.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/j/Zotero/storage/B4WPM993/Shain and Schuler (2022) A Deep Learning Approach to Analyzing Continuous-T.pdf}
}

@unpublished{shain.c:2022draft,
  type = {Draft},
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability in Reading},
  author = {Shain, Cory and Meister, Clara and Pimental, Tiago and Levy, Roger P. and Cotterell, Ryan},
  year = {2022},
  copyright = {Confidential},
  file = {/Users/j/Zotero/storage/EK8ZEYVN/Shain (2022) Large-scale evidence for logarithmic effects of wo.pdf}
}

@misc{shain.c:2022preprint,
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time},
  author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger Philip},
  year = {2022},
  month = nov,
  publisher = {{PsyArXiv}},
  doi = {10.31234/osf.io/4hyna},
  url = {https://psyarxiv.com/4hyna/},
  urldate = {2023-01-26},
  abstract = {Words which are less predictable in context are harder to process, but theories of language processing diverge in how they explain this fact. Representational theories emphasize the demands of assembling sentence interpretations in memory; these theories predict a linear effect of predictability on processing cost. Inferential theories emphasize the demands of updating a probability distribution over possible sentence interpretations; these theories predict either a logarithmic or a superlogarithmic effect of predictability on processing cost, depending on whether they posit pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here we revisit this question at scale: we analyze six reading datasets, estimate next-word probabilities with diverse language models, and predict reading times with advanced nonlinear regression methods. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.},
  langid = {american},
  keywords = {Cognitive Psychology,Language,Naturalistic,Nonlinear regression,Prediction,Psycholinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {/Users/j/Zotero/storage/29AJS4YU/Shain et al. (2022) Large-Scale Evidence for Logarithmic Effects of Wo.pdf}
}

@article{shannon.c:1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  month = jun,
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  url = {https://doi.org/10.1002%2Fj.1538-7305.1948.tb01338.x},
  bdsk-url-2 = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  date-added = {2022-04-07 13:03:48 -0400},
  date-modified = {2022-05-04 18:57:35 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{shannon.c:1948reprint,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  journal = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Nokia Bell Labs}},
  url = {https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf},
  project = {information-entropy},
  keywords = {asymptotic equipartition property,information theory}
}

@techreport{sheldon.d:2013,
  type = {Unpublished Report},
  title = {Discrete Adaptive Rejection Sampling},
  author = {Sheldon, Daniel R.},
  year = {2013},
  number = {UM-CS-2013-012},
  institution = {{College of Information and Computer Sciences, University of Massachusetts Amherst}},
  url = {https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.685.7109},
  abstract = {Adaptive rejection sampling (ARS) is an algorithm by Gilks and Wild for drawing samples from a continuous log-concave probability distribution with only black-box access to a function that computes the (unnormalized) density function. The ideas extend in a straightforward way to discrete log-concave distributions, but some details of the extension and its implementation can be tricky. This report provides the details of a discrete ARS algorithm. A companion implementation in C, with a MATLAB interface, accompanies the report. 1},
  file = {/Users/j/Zotero/storage/WX4SZ4PI/Sheldon - 2013 - Discrete adaptive rejection sampling.pdf}
}

@inproceedings{shen.t:2020,
  title = {Blank Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  year = {2020},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2020.emnlp-main.420},
  url = {https://doi.org/10.18653%2Fv1%2F2020.emnlp-main.420},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.420},
  date-added = {2022-04-05 19:37:55 -0400},
  date-modified = {2022-04-05 19:38:07 -0400}
}

@inproceedings{shen.y:2017,
  title = {Neural Language Modeling by Jointly Learning Syntax and Lexicon},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron C.},
  year = {2018},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=rkgOLb-0W},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenLHC18.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{shen.y:2018,
  title = {Ordered Neurons: {{Integrating}} Tree Structures into Recurrent Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron C.},
  year = {2019},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=B1l6qiR5F7},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  file = {/Users/j/Zotero/storage/ATDQRUJ9/Shen et al. - 2019 - Ordered neurons Integrating tree structures into .pdf}
}

@article{shepard.r:1987,
  title = {Toward a Universal Law of Generalization for Psychological Science},
  author = {Shepard, Roger N.},
  year = {1987},
  month = sep,
  journal = {Science},
  volume = {237},
  number = {4820},
  pages = {1317--1323},
  publisher = {{American Association for the Advancement of Science}},
  doi = {10.1126/science.3629243},
  url = {http://www.science.org/doi/10.1126/science.3629243},
  urldate = {2022-10-11},
  file = {/Users/j/Zotero/storage/4QYJFWMP/Shepard (1987) Toward a Universal Law of Generalization for Psych.pdf}
}

@book{shields.p:1996,
  title = {The Ergodic Theory of Discrete Sample Paths},
  author = {Shields, Paul C},
  year = {1996},
  volume = {13},
  publisher = {{American Mathematical Soc.}},
  date-added = {2020-08-08 19:47:15 -0400},
  date-modified = {2020-08-08 19:49:28 -0400},
  project = {information-compositionality},
  keywords = {entropy,information theory}
}

@article{shmueli.g:2010,
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  url = {https://projecteuclid.org/journals/statistical-science/volume-25/issue-3/To-Explain-or-to-Predict/10.1214/10-STS330.full},
  urldate = {2022-09-26},
  file = {/Users/j/Zotero/storage/UHNA8XZV/Shmueli (2010) To Explain or to Predict.pdf}
}

@article{sigurdsson.h:1996,
  title = {Icelandic Finite Verb Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {1996},
  journal = {Working papers in Scandinavian syntax},
  volume = {57},
  pages = {1--46},
  publisher = {{Department of Scandinavian Languages}},
  url = {https://lucris.lub.lu.se/ws/files/4562723/8500167.pdf},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,syncretism},
  file = {/Users/j/Zotero/storage/TVMYQEYL/SigurÃ°sson - 1996 - Icelandic finite verb agreement.pdf}
}

@incollection{sigurdsson.h:2000,
  title = {The Locus of Case and Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {2000},
  series = {Working Papers in {{Scandinavian}} Syntax},
  volume = {65},
  publisher = {{Department of Scandinavian Languages, Lund University}},
  url = {https://portal.research.lu.se/portal/en/publications/the-locus-of-case-and-agreement(10f7ec31-7acf-4f87-a921-814c1dc5b140).html},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2021-03-12 11:36:31 -0500},
  howpublished = {Working paper},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@article{sigurdsson.h:2008,
  title = {Icelandic Dative Intervention: {{Person}} and Number Are Separate Probes},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann and Holmberg, Anders},
  year = {2008},
  journal = {Agreement restrictions},
  pages = {251--280},
  publisher = {{Mouton de Gruyter Berlin}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:26:40 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,split probe}
}

@article{simon.h:1955,
  title = {A {{Behavioral Model}} of {{Rational Choice}}},
  author = {Simon, Herbert A.},
  year = {1955},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {69},
  number = {1},
  pages = {99--118},
  issn = {0033-5533},
  doi = {10.2307/1884852},
  url = {https://doi.org/10.2307/1884852},
  urldate = {2022-06-13},
  abstract = {Introduction, 99. \textemdash{} I. Some general features of rational choice, 100.\textemdash{} II. The essential simplifications, 103. \textemdash{} III. Existence and uniqueness of solutions, 111. \textemdash{} IV. Further comments on dynamics, 113. \textemdash{} V. Conclusion, 114. \textemdash{} Appendix, 115.},
  file = {/Users/j/Zotero/storage/HRPK7MYQ/Simon - 1955 - A Behavioral Model of Rational Choice.pdf}
}

@article{simon.h:1956,
  title = {Rational Choice and the Structure of the Environment},
  author = {Simon, H. A.},
  year = {1956},
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {129--138},
  publisher = {{American Psychological Association}},
  address = {{US}},
  issn = {1939-1471},
  doi = {10.1037/h0042769},
  abstract = {"In this paper I have attempted to identify some of the structural characteristics that are typical of the 'psychological' environments of organisms. We have seen that an organism in an environment with these characteristics requires only very simple perceptual and choice mechanisms to satisfy its several needs and to assure a high probability of its survival over extended periods of time. In particular, no 'utility function' needs to be postulated for the organism, nor does it require any elaborate procedure for calculating marginal rates of substitution among different wants." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Choice Behavior,Decision Making},
  file = {/Users/j/Zotero/storage/NXMHT5CF/Simon - 1956 - Rational choice and the structure of the environme.pdf}
}

@misc{smith.d:2023arxiv_monotile,
  title = {An Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = {2023},
  number = {2303.10798 [math.CO]},
  eprint = {2303.10798},
  primaryclass = {math.CO},
  publisher = {{arXiv}},
  url = {https://doi.org/10.48550/arXiv.2303.10798},
  abstract = {A longstanding open problem asks for an aperiodic monotile, also known as an "einstein": a shape that admits tilings of the plane, but never periodic tilings. We answer this problem for topological disk tiles by exhibiting a continuum of combinatorially equivalent aperiodic polygons. We first show that a representative example, the "hat" polykite, can form clusters called "metatiles", for which substitution rules can be defined. Because the metatiles admit tilings of the plane, so too does the hat. We then prove that generic members of our continuum of polygons are aperiodic, through a new kind of geometric incommensurability argument. Separately, we give a combinatorial, computer-assisted proof that the hat must form hierarchical -- and hence aperiodic -- tilings.},
  archiveprefix = {arXiv}
}

@inproceedings{smith.n:2008cogsci,
  title = {Optimal Processing Times in Reading: A Formal Model and Empirical Investigation},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = jul,
  volume = {30},
  pages = {570--576},
  address = {{Washington, DC, USA}},
  url = {https://escholarship.org/uc/item/3mr8m3rf},
  date-added = {2021-05-31 12:59:02 -0400},
  date-modified = {2021-12-14 20:34:04 -0500},
  file = {/Users/j/Zotero/storage/BKKHXAQN/Smith and Levy (2008) Optimal processing times in reading a formal mode.pdf}
}

@inproceedings{smith.n:2008csdl,
  title = {Probabilistic Prediction and the Continuity of Language Comprehension},
  booktitle = {9th Conference on Conceptual Structure, Discourse, and Language ({{CSDL9}})},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = oct,
  url = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=1295346},
  abstract = {It is well known that humans comprehend language in an incremental fashion, and that while doing so they use diverse contextual clues to make predictions about upcoming words (e.g. Tanenhaus et al., 1995). One way to model such predictions quantitatively is to use conditional probability, with P(continuation|context) denoting the fraction of the time that some continuation is expected to occur in a given context (Hale, 2001). For cognitive linguistics, such probabilities have special appeal. Generally, we seek theories that can faithfully describe the messy world of language and thought, and that are precise enough to make sharp, testable predictions; in practice we rarely achieve both goals simultaneously. Conditional probability provides a (partial) framework for representing linguistic knowledge that is subject to precise mathematics, but at the same time - unlike traditional formalisms - is amenable to incremental learning and gradient representations, and generalizes naturally to all levels of linguistic and extra-linguistic structure. Understanding the details of prediction in online language comprehension, therefore, may eventually pay considerable theoretical dividends. To this end, consider one behavioral correlate of predictability: words which are more predictable are processed faster (e.g. Ehrlich and Rayner, 1981). This effect is well known, but there is currently no agreement on why it occurs; and, since previous studies (Rayner and Well, 1996) have used exclusively factorial comparisons, we know the effect's direction but have little insight into its functional form. To address these issues, we first present a novel theory of linguistic processing time that draws inspiration from two sources: the literature on motor control, and on construction grammar (Fillmore, 1988; Kay and Fillmore, 1999). The model consists of an optimal control system (Todorov, 2004) that, rather than controlling muscles, controls the allocation of preparatory resources in the linguistic processing system. Its challenge is to manage the trade-off between conserving resources and processing quickly; for efficiency, it preferentially allocates resources to more probable continuations, which results in a speedup for predictable words. Next, following the principles of construction grammar, we constrain the model by requiring that it have no preferred scale - we assume that processing proceeds continuously and simultaneously at all levels from phoneme to clause, using similar mechanisms. This turns out to make a strong prediction: processing time for an item should be proportional to the logarithm of that item's probability-in-context. Second, we test this prediction by analyzing the relation between probability and fixation time in the Dundee eye-movement corpus (Kennedy et al., 2003), approximating probability using a computational language model. As compared to previous studies, this approach has the advantage of both improved ecological validity and vastly greater statistical power, allowing the examination of curve shape. We find that after controlling for confounds, probability does have a logarithmic effect on standard reading-time measures, and this effect is substantial and systematic over several orders of magnitude. This result invalidates a number of competing theories which predict different curve shapes, confirms the prediction of our model, and thus provides supporting evidence for constructional accounts of language processing.},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2021-03-16 17:32:08 -0400},
  project = {syntactic embedding}
}

@inproceedings{smith.n:2011cloze,
  title = {Cloze but No Cigar: {{The}} Complex Relationship between Cloze, Corpus, and Subjective Probabilities in Language Processing},
  booktitle = {Proceedings of the 33rd Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2011},
  url = {https://escholarship.org/uc/item/69s3541f},
  date-added = {2021-03-16 14:39:39 -0400},
  date-modified = {2022-03-16 10:06:04 -0400}
}

@article{smith.n:2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  url = {https://www.sciencedirect.com/science/article/pii/S0010027713000413},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability \textendash{} even for differences between highly unpredictable words \textendash{} and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2013.02.013},
  date-added = {2021-03-09 22:52:29 -0500},
  date-modified = {2021-11-14 23:57:40 -0500},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Zotero/storage/N767I5L3/Smith and Levy - 2013 - The effect of word predictability on reading time .pdf}
}

@article{smith.p:1969,
  title = {Coding Strategies in Language},
  author = {Smith, Philip Twitchell},
  year = {1969},
  journal = {Information and Control},
  volume = {14},
  number = {1},
  pages = {72--97},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(69)90033-3},
  url = {https://www.sciencedirect.com/science/article/pii/S0019995869900333},
  abstract = {The problem of selecting a code to transmit four messages over the binary symmetric channel is studied in relation to two types of channel noise (``substitution{$\risingdotseq$} error and ``deletion{$\risingdotseq$} error) and to two types of decoding strategy (maximum hit and minimum error). It is shown that mean Hamming distance is a good general guide to coding efficiency, except in the case of a minimum error strategy with a deletion error channel, where coding efficiency is critically dependent on noise level. An experiment in which subjects selected codes in an artificial language suggests that the process of recall from memory is similar to the process of transmitting over a deletion error channel with a minimum error strategy. A similar interpretation can be placed on the analysis of consonant systems in English, French, German and Welsh, where the sets of consonants of a given class in a given environment are considered as codes whose alphabet is the phonological distinctive feature system of Halle (1958)},
  bdsk-url-2 = {https://doi.org/10.1016/S0019-9958(69)90033-3},
  date-added = {2022-04-27 13:10:31 -0400},
  date-modified = {2022-04-27 13:11:36 -0400},
  keywords = {artificial language,hamming distance,noisy channel coding,phonology}
}

@inproceedings{snyder.b:2009,
  title = {Unsupervised Multilingual Grammar Induction},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {{ACL}} and the 4th International Joint Conference on Natural Language Processing of the {{AFNLP}}: {{Volume}} 1 - Volume 1},
  author = {Snyder, Benjamin and Naseem, Tahira and Barzilay, Regina},
  year = {2009},
  series = {{{ACL}} '09},
  pages = {73--81},
  publisher = {{Association for Computational Linguistics}},
  address = {{USA}},
  url = {http://www.aclweb.org/anthology/P09-1009},
  abstract = {We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting. Using this formalism, our model loosely binds parallel trees while allowing language-specific syntactic structure. We perform inference under this model using Markov Chain Monte Carlo and dynamic programming. Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we find substantial performance gains over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure.},
  date-added = {2022-04-04 12:46:23 -0400},
  date-modified = {2022-04-04 12:47:09 -0400},
  isbn = {978-1-932432-45-9}
}

@inproceedings{socolof.m:2022,
  title = {Characterizing Idioms: Conventionality and Contingency},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Socolof, Michaela and Cheung, Jackie and Wagner, Michael and O'Donnell, Timothy},
  year = {2022},
  month = may,
  pages = {4024--4037},
  publisher = {{Association for Computational Linguistics}},
  address = {{Dublin, Ireland}},
  url = {https://aclanthology.org/2022.acl-long.278},
  abstract = {Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that introducing special machinery to handle idioms may not be warranted.},
  date-added = {2022-05-17 08:04:25 -0400},
  date-modified = {2022-05-17 08:05:15 -0400}
}

@inproceedings{socolof.m:2022coling,
  title = {Measuring Morphological Fusion Using Partial Information Decomposition},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Socolof, Michaela and Hoover, Jacob Louis and Futrell, Richard and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  pages = {44--54},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Gyeongju, Republic of Korea}},
  url = {https://aclanthology.org/2022.coling-1.5},
  abstract = {Morphological systems across languages vary when it comes to the relation between form and meaning. In some languages, a single meaning feature corresponds to a single morpheme, whereas in other languages, multiple meaning features are bundled together into one morpheme. The two types of languages have been called agglutinative and fusional, respectively, but this distinction does not capture the graded nature of the phenomenon. We provide a mathematically precise way of characterizing morphological systems using partial information decomposition, a framework for decomposing mutual information into three components: unique, redundant, and synergistic information. We show that highly fusional languages are characterized by high levels of synergy.},
  copyright = {All rights reserved},
  openaccess = {true},
  file = {/Users/j/Zotero/storage/33YCBYBD/Socolof et al. (2022) Measuring Morphological Fusion Using Partial Infor.pdf}
}

@misc{sohl-dickstein.j:2015,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {{arXiv}},
  url = {http://arxiv.org/abs/1503.03585},
  urldate = {2022-05-17},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,diffusion processes,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/4K59SF2J/Sohl-Dickstein et al. - 2015 - Deep Unsupervised Learning using Nonequilibrium Th.pdf;/Users/j/Zotero/storage/ZGS83AZ8/1503.html}
}

@inproceedings{song.y:2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2019},
  volume = {32},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/3001ef257407d5a371a96dcd947c7d93-Abstract.html},
  urldate = {2022-07-07},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  file = {/Users/j/Zotero/storage/3QU2CCTZ/Song and Ermon - 2019 - Generative Modeling by Estimating Gradients of the.pdf}
}

@inproceedings{song.y:2022,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2022},
  month = feb,
  url = {https://openreview.net/forum?id=PxTIG12RRHS},
  urldate = {2022-07-11},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a...},
  langid = {english},
  file = {/Users/j/Zotero/storage/DH424UAB/Song et al. - 2022 - Score-Based Generative Modeling through Stochastic.pdf}
}

@article{soskuthy.m:2021,
  title = {Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis},
  author = {S{\'o}skuthy, M{\'a}rton},
  year = {2021},
  month = jan,
  journal = {Journal of Phonetics},
  volume = {84},
  pages = {101017},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.wocn.2020.101017},
  url = {https://doi.org/10.1016%2Fj.wocn.2020.101017},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2020.101017},
  date-added = {2022-03-04 16:06:14 -0500},
  date-modified = {2022-03-04 16:06:18 -0500}
}

@inproceedings{stabler.e:1997,
  title = {Derivational Minimalism},
  booktitle = {Logical Aspects of Computational Linguistics},
  author = {Stabler, Edward},
  editor = {Retor{\'e}, Christian},
  year = {1997},
  pages = {68--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052152},
  url = {https://doi.org/10.1007/BFb0052152},
  abstract = {A basic idea of the transformational tradition is that constituents move. More recently, there has been a trend towards the view that all features are lexical features. And in recent ``minimalist'' grammars, structure building operations are assumed to be feature driven. A simple grammar formalism with these properties is presented here and briefly explored. Grammars in this formalism can define languages that are not in the ``mildly context sensitive'' class defined by Vijay-Shanker and Weir (1994).},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  isbn = {978-3-540-69631-5},
  project = {syntactic embedding}
}

@incollection{stabler.e:1997a,
  title = {Derivational Minimalism},
  booktitle = {Logical {{Aspects}} of {{Computational Linguistics}}},
  author = {Stabler, Edward},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Retor{\'e}, Christian},
  year = {1997},
  volume = {1328},
  pages = {68--95},
  publisher = {{Springer Berlin Heidelberg}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/BFb0052152},
  url = {http://link.springer.com/10.1007/BFb0052152},
  urldate = {2022-09-30},
  isbn = {978-3-540-63700-4 978-3-540-69631-5}
}

@article{stabler.e:2013,
  title = {Two Models of Minimalist, Incremental Syntactic Analysis},
  author = {Stabler, Edward P.},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {611--633},
  issn = {17568757},
  doi = {10.1111/tops.12031},
  url = {https://onlinelibrary.wiley.com/doi/10.1111/tops.12031},
  urldate = {2022-09-30},
  langid = {english}
}

@article{stanley.d:1999,
  title = {A {{Multiplicative Calculus}}},
  author = {Stanley, Dick},
  year = {1999},
  month = jan,
  journal = {PRIMUS},
  volume = {9},
  number = {4},
  pages = {310--326},
  publisher = {{Taylor \& Francis}},
  issn = {1051-1970},
  doi = {10.1080/10511979908965937},
  url = {https://doi.org/10.1080/10511979908965937},
  urldate = {2023-04-25},
  abstract = {A new type of calculus called ``multiplicative calculus'' is developed and some basic theorems about derivatives, integrals, and infinite products are proved within this calculus. Multiplicative calculus is based on a multiplicative mechanism in the same sense that the usual calculus is based on an additive mechanism. One consequence is that, just as the usual derivative of a linear function is constant, the multiplicative derivative of an exponential function is constant (and just as the usual derivative of a constant function is 0, the multiplicative derivative of a constant function is 1). Similarly, just as two functions that have a constant difference have the same usual derivative, two functions that have a constant ratio have the same multiplicative derivative. Finally, just as many functions have infinite series representations based on the usual derivative, the same functions have infinite product representations based on the multiplicative derivative. These in turn are derived from exponential approximations to functions that are analogous to the linear approximations of the usual calculus. Multiplicative calculus is a useful supplement to the usual calculus in that it is tailored to situations involving exponential functions in the same sense that the usual calculus is tailored to situations involving linear functions.},
  keywords = {additive,Calculus,differential calculus,infinite products,infinite series,integral calculus,multiplicative}
}

@inproceedings{stanojevic.m:2021,
  title = {Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Stanojevi{\'c}, Milo{\v s} and Bhattasali, Shohini and Dunagan, Donald and Campanelli, Luca and Steedman, Mark and Brennan, Jonathan and Hale, John},
  year = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.cmcl-1.3},
  url = {https://doi.org/10.18653%2Fv1%2F2021.cmcl-1.3},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.3},
  date-added = {2022-04-14 13:33:35 -0400},
  date-modified = {2022-04-14 13:33:47 -0400}
}

@article{staub.a:2010,
  title = {Eye Movements and Processing Difficulty in Object Relative Clauses},
  author = {Staub, Adrian},
  year = {2010},
  month = jul,
  journal = {Cognition},
  volume = {116},
  number = {1},
  pages = {71--86},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2010.04.002},
  abstract = {It is well known that sentences containing object-extracted relative clauses (e.g., The reporter that the senator attacked admitted the error) are more difficult to comprehend than sentences containing subject-extracted relative clauses (e.g., The reporter that attacked the senator admitted the error). Two major accounts of this phenomenon make different predictions about where, in the course of incremental processing of an object relative, difficulty should first appear. An account emphasizing memory processes (Gibson, 1998; Grodner \& Gibson, 2005) predicts difficulty at the relative clause verb, while an account emphasizing experience-based expectations (Hale, 2001; Levy, 2008) predicts earlier difficulty, at the relative clause subject. Two eye movement experiments tested these predictions. Regressive saccades were much more likely from the subject noun phrase of an object relative than from the same noun phrase occurring within a subject relative (Experiment 1) or within a verbal complement clause (Experiment 2). This effect was further amplified when the relative pronoun that was omitted. However, reading time was also inflated on the object relative clause verb in both experiments. These results suggest that the violation of expectations and the difficulty of memory retrieval both contribute to the difficulty of object relative clauses, but that these two sources of difficulty have qualitatively distinct behavioral consequences in normal reading.},
  langid = {english},
  pmid = {20427040},
  keywords = {Eye Movements,Female,{Fixation, Ocular},Humans,Male,Psycholinguistics,Psychomotor Performance,Reading,Semantics,Young Adult}
}

@article{staub.a:2011,
  title = {The Effect of Lexical Predictability on Distributions of Eye Fixation Durations},
  author = {Staub, Adrian},
  year = {2011},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {18},
  number = {2},
  pages = {371--376},
  issn = {1531-5320},
  doi = {10.3758/s13423-010-0046-9},
  url = {https://doi.org/10.3758/s13423-010-0046-9},
  urldate = {2022-10-26},
  abstract = {A word's predictability in context has a well-established effect on fixation durations in reading. To investigate how this effect is manifested in distributional terms, an experiment was carried out in which subjects read each of 50 target words twice, once in a high-predictability context and once in a low-predictability context. The ex-Gaussian distribution was fit to each subject's first-fixation durations and single-fixation durations. For both measures, the {$\mu$} parameter increased when a word was unpredictable, while the {$\tau$} parameter was not significantly affected, indicating that a predictability manipulation shifts the distribution of fixation durations but does not affect the degree of skew. Vincentile plots showed that the mean ex-Gaussian parameters described the typical distribution shapes extremely well. These results suggest that the predictability and frequency effects are functionally distinct, since a frequency manipulation has been shown to influence both {$\mu$} and {$\tau$}. The results may also be seen as consistent with the finding from single-word recognition paradigms that semantic priming affects only {$\mu$}.},
  langid = {english},
  keywords = {Distributional analysis,ex-gaussian,Eye movements in reading,Visual word recognition},
  file = {/Users/j/Zotero/storage/N46BRL28/Staub (2011) The effect of lexical predictability on distributi.pdf}
}

@article{staub.a:2015,
  title = {The Effect of Lexical Predictability on Eye Movements in Reading: {{Critical}} Review and Theoretical Interpretation},
  author = {Staub, Adrian},
  year = {2015},
  journal = {Language and Linguistics Compass},
  volume = {9},
  number = {8},
  pages = {311--327},
  publisher = {{Wiley}},
  doi = {10.1111/lnc3.12151},
  url = {https://doi.org/10.1111%2Flnc3.12151},
  bdsk-url-2 = {https://doi.org/10.1111/lnc3.12151},
  date-added = {2021-05-22 15:55:42 -0400},
  date-modified = {2021-05-22 15:55:54 -0400},
  keywords = {predictability,processing,review}
}

@article{staub.a:2015a,
  title = {The Influence of Cloze Probability and Item Constraint on Cloze Task Response Time},
  author = {Staub, Adrian and Grant, Margaret and Astheimer, Lori and Cohen, Andrew},
  year = {2015},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {82},
  pages = {1--17},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.02.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X15000236},
  urldate = {2022-09-07},
  abstract = {In research on the role of lexical predictability in language comprehension, predictability is generally defined as the probability that a word is provided as a sentence continuation in the cloze task (Taylor, 1953), in which subjects are asked to guess the next word of a sentence. The present experiments investigate the process by which subjects generate a cloze response, by measuring the latency to initiate a response in a version of the task in which subjects produce a spoken continuation to a visually presented sentence fragment. Higher probability responses were produced faster than lower probability responses. The latency to produce a response was also influenced by item constraint: A response at a given level of probability was issued faster when the context was more constraining, i.e., a single response was elicited with high probability. We show that these patterns are naturally produced by an activation-based race model in which potential responses independently race towards a response threshold. Implications for the interpretation of cloze probability as a measure of lexical predictability are discussed.},
  langid = {english},
  keywords = {Cloze task,Language processing,Prediction,Response time}
}

@misc{steedman.m:2000,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {{MIT Press}},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{steedman.m:2000a,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {{The MIT press}},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{steinhardt.j:2014,
  title = {Filtering with Abstract Particles},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  author = {Steinhardt, Jacob and Liang, Percy},
  editor = {Xing, Eric P. and Jebara, Tony},
  year = {2014},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {32},
  pages = {727--735},
  publisher = {{PMLR}},
  address = {{Bejing, China}},
  url = {https://proceedings.mlr.press/v32/steinhardt14.html},
  abstract = {Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue by using ``abstract particles'' that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.},
  date-added = {2022-03-25 12:02:29 -0400},
  date-modified = {2022-03-25 12:02:31 -0400},
  pdf = {http://proceedings.mlr.press/v32/steinhardt14.pdf}
}

@book{stevens.e:2020,
  title = {Deep Learning with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  year = {2020},
  publisher = {{Manning Publications Company}},
  url = {https://pytorch.org/assets/deep-learning/Deep-Learning-with-PyTorch.pdf},
  date-added = {2021-08-02 19:51:35 -0400},
  date-modified = {2021-08-02 19:52:08 -0400},
  isbn = {978-1-61729-526-3}
}

@inproceedings{stolcke.a:1994,
  title = {Inducing Probabilistic Grammars by {{Bayesian}} Model Merging},
  booktitle = {Grammatical {{Inference}} and {{Applications}}},
  author = {Stolcke, Andreas and Omohundro, Stephen},
  editor = {Carrasco, Rafael C. and Oncina, Jose},
  year = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {106--118},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/3-540-58473-0_141},
  abstract = {We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.},
  isbn = {978-3-540-48985-6},
  langid = {english},
  keywords = {Bayesian Posterior Probability,Beam Search,Hide Markov Model,Merging Algorithm,Relative Clause},
  file = {/Users/j/Zotero/storage/G9XMV94D/Stolcke and Omohundro - 1994 - Inducing probabilistic grammars by Bayesian model .pdf}
}

@article{stolcke.a:1995,
  title = {An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities},
  author = {Stolcke, Andreas},
  year = {1995},
  journal = {Computational Linguistics},
  volume = {21},
  number = {2},
  pages = {165--201},
  url = {https://www.aclweb.org/anthology/J95-2002},
  file = {/Users/j/Zotero/storage/YTD5BSTD/Stolcke (1995) An efficient probabilistic context-free parsing al.pdf}
}

@article{stone.m:1960,
  title = {Models for Choice-Reaction Time},
  author = {Stone, Mervyn},
  year = {1960},
  month = sep,
  journal = {Psychometrika},
  volume = {25},
  number = {3},
  pages = {251--260},
  issn = {1860-0980},
  doi = {10.1007/BF02289729},
  url = {https://doi.org/10.1007/BF02289729},
  urldate = {2022-07-04},
  abstract = {In the two-choice situation, the Wald sequential probability ratio decision procedure is applied to relate the mean and variance of the decision times, for each alternative separately, to the error rates and the ratio of the frequencies of presentation of the alternatives. For situations involving more than two choices, a fixed sample decision procedure (selection of the alternative with highest likelihood) is examined, and the relation is found between the decision time (or size of sample), the error rate, and the number of alternatives.},
  langid = {english},
  keywords = {Decision Procedure,Error Rate,High Likelihood,Public Policy,Statistical Theory}
}

@inproceedings{strubell.e:2019,
  title = {Energy and Policy Considerations for Deep Learning in {{NLP}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  pages = {3645--3650},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1355},
  url = {https://www.aclweb.org/anthology/P19-1355},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1355}
}

@article{svenonius.p:2002,
  title = {Icelandic Case and the Structure of Events},
  author = {Svenonius, Peter},
  year = {2002},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {5},
  number = {1-3},
  pages = {197--225},
  publisher = {{Springer}},
  url = {https://rdcu.be/b2Dh6},
  date-added = {2020-03-06 14:58:07 -0800},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@article{szewczyk.j:2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  year = {2022},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.jml.2021.104311},
  url = {https://doi.org/10.1016%2Fj.jml.2021.104311},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2021.104311},
  date-added = {2022-01-24 13:49:32 -0500},
  date-modified = {2022-01-24 13:49:35 -0500}
}

@article{tabor.w:2004,
  title = {Evidence for Self-Organized Sentence Processing: {{Digging-in}} Effects.},
  author = {Tabor, Whitney and Hutchins, Sean},
  year = {2004},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {30},
  number = {2},
  pages = {431--450},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/0278-7393.30.2.431},
  url = {https://doi.org/10.1037%2F0278-7393.30.2.431},
  bdsk-url-2 = {https://doi.org/10.1037/0278-7393.30.2.431},
  date-added = {2021-04-11 18:58:19 -0400},
  date-modified = {2021-04-11 18:58:36 -0400},
  keywords = {diggin in effect,reading time}
}

@inproceedings{takabatake.k:2004,
  title = {Information Geometry of {{Gibbs}} Sampler},
  booktitle = {Proc. of {{WSEAS}} Int. {{Conf}}. on Neural Networks and Applications ({{NNA}})},
  author = {Takabatake, Kazuya},
  year = {2004},
  url = {https://staff.aist.go.jp/k.takabatake/takabatake04.pdf},
  date-added = {2021-03-11 16:56:36 -0500},
  date-modified = {2021-03-11 17:08:06 -0500},
  keywords = {information theory,sampling},
  file = {/Users/j/Zotero/storage/I9SGWKEM/Takabatake - 2004 - Information geometry of Gibbs sampler.pdf}
}

@article{tanenhaus.m:1995,
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Tanenhaus, Michael K. and {Spivey-Knowlton}, Michael J. and Eberhard, Kathleen M. and Sedivy, Julie C.},
  year = {1995},
  month = jun,
  journal = {Science (New York, N.Y.)},
  volume = {268},
  number = {5217},
  pages = {1632--1634},
  publisher = {{American Association for the Advancement of Science (AAAS)}},
  doi = {10.1126/science.7777863},
  url = {https://doi.org/10.1126%2Fscience.7777863},
  abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
  bdsk-url-2 = {https://doi.org/10.1126/science.7777863},
  date-added = {2022-04-20 13:14:59 -0400},
  date-modified = {2022-05-02 14:45:52 -0400},
  keywords = {incrementality}
}

@incollection{taraldsen.k:1995,
  title = {On Agreement and Nominative Objects in {{Icelandic}}},
  booktitle = {Studies in Comparative {{Germanic}} Syntax},
  author = {Taraldsen, Knut Tarald},
  year = {1995},
  pages = {307--327},
  publisher = {{Springer}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:17:31 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,split probe}
}

@article{tax.t:2017,
  title = {The Partial Information Decomposition of Generative Neural Network Models},
  author = {Tax, Tycho M.S. and Mediano, Pedro A.M. and Shanahan, Murray},
  year = {2017},
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {19},
  number = {9},
  issn = {1099-4300},
  doi = {10.3390/e19090474},
  url = {https://www.mdpi.com/1099-4300/19/9/474},
  abstract = {In this work we study the distributed representations learnt by generative neural network models. In particular, we investigate the properties of redundant and synergistic information that groups of hidden neurons contain about the target variable. To this end, we use an emerging branch of information theory called partial information decomposition (PID) and track the informational properties of the neurons through training. We find two differentiated phases during the training process: a first short phase in which the neurons learn redundant information about the target, and a second phase in which neurons start specialising and each of them learns unique information about the target. We also find that in smaller networks individual neurons learn more specific information about certain features of the input, suggesting that learning pressure can encourage disentangled representations.},
  article-number = {474},
  bdsk-url-2 = {https://doi.org/10.3390/e19090474},
  date-added = {2022-05-14 10:28:04 -0400},
  date-modified = {2022-05-14 10:28:21 -0400},
  keywords = {neural networks,partial information decomposition}
}

@misc{tay.y:2022,
  title = {Transformer Memory as a Differentiable Search Index},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  year = {2022},
  publisher = {{arXiv}},
  doi = {10.48550/ARXIV.2202.06991},
  url = {https://arxiv.org/abs/2202.06991},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2202.06991},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 11:01:29 -0400},
  date-modified = {2022-03-31 11:02:20 -0400},
  keywords = {transformer},
  file = {/Users/j/Zotero/storage/KXPFC45L/Tay et al. - 2022 - Transformer memory as a differentiable search inde.pdf}
}

@article{taylor.w:1953,
  title = {`{{Cloze}} Procedure': {{A}} New Tool for Measuring Readability},
  author = {Taylor, Wilson L.},
  year = {1953},
  journal = {Journalism Quarterly},
  volume = {30},
  number = {4},
  pages = {415--433},
  publisher = {{SAGE Publications}},
  doi = {10.1177/107769905303000401},
  url = {https://doi.org/10.1177%2F107769905303000401},
  bdsk-url-2 = {https://doi.org/10.1177/107769905303000401},
  date-added = {2021-03-18 10:41:54 -0400},
  date-modified = {2021-03-18 11:25:18 -0400},
  keywords = {cloze,processing}
}

@inproceedings{teh.y:2006,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman-Yor}} Processes},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Computational Linguistics}} and the 44th Annual Meeting of the {{ACL}} - {{ACL}} 06},
  author = {Teh, Yee Whye},
  year = {2006},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.3115/1220175.1220299},
  url = {https://doi.org/10.3115%2F1220175.1220299},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220299},
  date-added = {2022-04-25 21:37:17 -0400},
  date-modified = {2022-04-25 21:38:48 -0400},
  keywords = {bayesian,HMM,Natural language processing,Pitman-Yor processes}
}

@techreport{teh.y:2006techreport,
  type = {Technical Report},
  title = {A {{Bayesian}} Interpretation of Interpolated {{Kneser-Ney}}},
  author = {Teh, Yee Whye},
  year = {2006},
  number = {TRA2/06},
  institution = {{School of Computing, National University of Singapore}},
  url = {https://dl.comp.nus.edu.sg/xmlui/handle/1900.100/1911},
  abstract = {Interpolated Kneser-Ney is one of the best smoothing methods for n-gram language models. Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method. We propose a novel interpretation of interpolated Kneser-Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman-Yor processes. As opposed to past explanations, our interpretation can recover exactly the formulation of interpolated Kneser-Ney, and performs better than interpolated Kneser-Ney when a better inference procedure is used.},
  date-added = {2022-04-26 10:12:38 -0400},
  date-modified = {2022-04-26 10:16:03 -0400},
  keywords = {bayesian,Dirichlet processes,hierarchical clustering,language modeling,Pitman-Yor processes},
  file = {/Users/j/Zotero/storage/I3DNB6B8/Teh - 2006 - A bayesian interpretation of interpolated kneser-n.pdf}
}

@article{tenenbaum.j:2001,
  title = {Generalization, Similarity, and {{Bayesian}} Inference},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2001},
  month = aug,
  journal = {Behavioral and Brain Sciences},
  volume = {24},
  number = {4},
  pages = {629--640},
  publisher = {{Cambridge University Press}},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X01000061},
  url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/abs/generalization-similarity-and-bayesian-inference/595CAA321C9C56270C624057021DE77A},
  urldate = {2022-10-11},
  abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
  langid = {english},
  keywords = {additive clustering,Bayesian inference,categorization,concept learning,contrast model,features,generalization,psychological space,similarity},
  file = {/Users/j/Zotero/storage/2NBULECD/Tenenbaum and Griffiths (2001) Generalization, similarity, and Bayesian inference.pdf}
}

@inproceedings{tenney.i:2019,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  publisher = {{OpenReview.net}},
  url = {https://openreview.net/forum?id=SJzSgnRcKX},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{tenney.t:2019,
  title = {{{BERT}} Rediscovers the Classical {{NLP}} Pipeline},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  pages = {4593--4601},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1452},
  url = {https://www.aclweb.org/anthology/P19-1452},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1452}
}

@inproceedings{terra.e:2003,
  title = {Frequency Estimates for Statistical Word Similarity Measures},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Terra, Egidio L. and Clarke, Charles L. A.},
  year = {2003},
  pages = {244--251},
  url = {https://www.aclweb.org/anthology/N03-1032}
}

@book{tesniere.l:1959,
  title = {\'Elements de Syntaxe Structurale. {{Pr\'ef}}. de Jean Fourquet},
  author = {Tesni{\`e}re, Lucien},
  year = {1959},
  publisher = {{C. Klincksieck}},
  date-added = {2021-07-16 19:40:41 -0400},
  date-modified = {2021-07-16 19:40:43 -0400}
}

@book{tesniere.l:2015,
  title = {Elements of Structural Syntax},
  author = {Tesni{\`e}re, Lucien},
  year = {2015},
  publisher = {{John Benjamins Publishing Company}},
  doi = {10.1075/z.185},
  url = {https://doi.org/10.1075%2Fz.185},
  bdsk-url-2 = {https://doi.org/10.1075/z.185},
  date-added = {2021-06-24 10:12:36 -0400},
  date-modified = {2021-06-24 10:13:38 -0400}
}

@inproceedings{thai.b:2020,
  title = {Fully {{Convolutional ASR}} for {{Less-Resourced Endangered Languages}}},
  booktitle = {Proceedings of the 1st {{Joint Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages ({{SLTU}}) and {{Collaboration}} and {{Computing}} for {{Under-Resourced Languages}} ({{CCURL}})},
  author = {Thai, Bao and Jimerson, Robert and Ptucha, Raymond and Prud'hommeaux, Emily},
  year = {2020},
  month = may,
  pages = {126--130},
  publisher = {{European Language Resources association}},
  address = {{Marseille, France}},
  url = {https://aclanthology.org/2020.sltu-1.17},
  urldate = {2022-06-06},
  abstract = {The application of deep learning to automatic speech recognition (ASR) has yielded dramatic accuracy increases for languages with abundant training data, but languages with limited training resources have yet to see accuracy improvements on this scale. In this paper, we compare a fully convolutional approach for acoustic modelling in ASR with a variety of established acoustic modeling approaches. We evaluate our method on Seneca, a low-resource endangered language spoken in North America. Our method yields word error rates up to 40\% lower than those reported using both standard GMM-HMM approaches and established deep neural methods, with a substantial reduction in training time. These results show particular promise for languages like Seneca that are both endangered and lack extensive documentation.},
  isbn = {979-10-95546-35-1},
  langid = {english},
  keywords = {automatic speech recognition,computational revitalization,iroquoian},
  file = {/Users/j/Zotero/storage/RIFWW3C3/Thai et al. - 2020 - Fully Convolutional ASR for Less-Resourced Endange.pdf}
}

@book{thrun.s:2005book,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2005},
  publisher = {{MIT Press}},
  url = {https://mitpress.ublish.com/book/probabilistic-robotics},
  date-added = {2022-05-05 10:19:08 -0400},
  date-modified = {2022-05-05 10:21:11 -0400},
  isbn = {978-0-262-36380-8}
}

@misc{tishby.n:2000,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {2000},
  eprint = {physics/0004057},
  archiveprefix = {arxiv},
  date-added = {2020-07-21 08:44:18 -0400},
  date-modified = {2020-07-21 08:47:35 -0400},
  project = {syntactic embedding},
  keywords = {information bottleneck,information theory,variational inference}
}

@inproceedings{titov.i:2007,
  title = {A Latent Variable Model for Generative Dependency Parsing},
  booktitle = {Proceedings of the Tenth International Conference on Parsing Technologies},
  author = {Titov, Ivan and Henderson, James},
  year = {2007},
  month = jun,
  pages = {144--155},
  publisher = {{Association for Computational Linguistics}},
  address = {{Prague, Czech Republic}},
  url = {https://aclanthology.org/W07-2218},
  date-added = {2022-04-26 17:35:02 -0400},
  date-modified = {2022-04-26 17:35:31 -0400},
  keywords = {Dependency Grammar,dependency parsing,generative grammar}
}

@incollection{townsend.j:1974,
  title = {Issues and Models Concerning the Processing of a Finite Number of Inputs},
  booktitle = {Human Information Processing},
  author = {Townsend, James T.},
  year = {1974},
  publisher = {{Routledge}},
  abstract = {The broadest and most critical arena of investigation and contention with regard to these two matters has always been consciousness itself. Broadbent unveiled a theoretical structure that allowed collation of a substantial body of experimental literature and that was seminal in its influence on later developments. Hybrid models have been of limited current theoretical interest, probably due in part to the difficulty in testing them experimentally. As remarked earlier, the quite special case of seriality versus parallelism is difficult enough to discriminate experimentally. Among such hybrid models are those that represent processing as being serial part of the time and partially parallel within trials. There are many occasions in perceptual and memorial experiments where the information sufficient to make a correct response is embedded in only part of the total stimulus pattern presented to the subject. A finding of independence of total completion times, for instance, although perhaps more intuitively associated with parallel models, can be predicted by serial models.},
  isbn = {978-1-00-317668-8},
  file = {/Users/j/Zotero/storage/EXDXLUI2/Townsend - 1974 - Issues and Models Concerning the Processing of a F.pdf}
}

@article{townsend.j:1990,
  title = {Serial vs. Parallel Processing: Sometimes They Look like Tweedledum and Tweedledee but They Can (and Should) Be Distinguished},
  shorttitle = {Serial vs. Parallel Processing},
  author = {Townsend, James T.},
  year = {1990},
  month = jan,
  journal = {Psychological science},
  volume = {1},
  number = {1},
  pages = {46--54},
  publisher = {{SAGE Publications Inc}},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1990.tb00067.x},
  url = {https://doi.org/10.1111/j.1467-9280.1990.tb00067.x},
  urldate = {2022-06-24},
  abstract = {A number of important models of information processing depend on whether processing is serial or parallel. However, many of the studies purporting to settle the case use weak experimental paradigms or results to draw conclusions. A brief history of the issue is given along with examples from the literature. Then a number of promising methods are presented from a variety of sources with some discussion of their potential. A brief discussion of the topic with regard to overall issues of model testing and applications concludes the paper.},
  langid = {english},
  file = {/Users/j/Zotero/storage/NUGB823Y/Townsend - 1990 - Serial vs. Parallel Processing Sometimes They Loo.pdf}
}

@article{townsend.j:2004,
  title = {Parallel versus Serial Processing and Individual Differences in High-Speed Search in Human Memory},
  author = {Townsend, James T. and Fifi{\'c}, Mario},
  year = {2004},
  month = aug,
  journal = {Perception \& Psychophysics},
  volume = {66},
  number = {6},
  pages = {953--962},
  issn = {1532-5962},
  doi = {10.3758/BF03194987},
  url = {https://doi.org/10.3758/BF03194987},
  urldate = {2022-06-24},
  abstract = {Many mental tasks that involve operations on a number of items take place within a few hundred milliseconds. In such tasks, whether the items are processed simultaneously (in parallel) or sequentially (serially) has long been of interest to psychologists. Although certain types of parallel and serial models have been ruled out, it has proven extremely difficult to entirely separate reasonable serial and limitedcapacity parallel models on the basis of typical data. Recent advances in theory-driven methodology now permit strong tests of serial versus parallel processing in such tasks, in ways that bypass the capacity issue and that are distribution and parameter free. We employ new methodologies to assess serial versus parallel processing and find strong evidence for pure serial or pure parallel processing, with some striking apparent differences across individuals and interstimulus conditions.},
  langid = {english},
  keywords = {Memory Search,Parallel Model,Parallel Processing,Serial Processing,Visual Search},
  file = {/Users/j/Zotero/storage/DXVKSQ9T/Townsend and FifiÄ‡ - 2004 - Parallel versus serial processing and individual d.pdf}
}

@article{traxler.m:2002,
  title = {Processing Subject and Object Relative Clauses: {{Evidence}} from Eye Movements},
  shorttitle = {Processing Subject and Object Relative Clauses},
  author = {Traxler, Matthew J and Morris, Robin K and Seely, Rachel E},
  year = {2002},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {47},
  number = {1},
  pages = {69--90},
  issn = {0749-596X},
  doi = {10.1006/jmla.2001.2836},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X01928360},
  urldate = {2023-03-09},
  abstract = {Three eye-movement-monitoring experiments investigated processing of sentences containing subject-relative and object-relative clauses. The first experiment showed that sentences containing object-relative clauses were more difficult to process than sentences containing subject-relative clauses during the relative clause and the matrix verb. The second experiment manipulated the plausibility of the sentential subject and the noun within the relative clause as the agent of the action represented by the verb in the relative clause. Readers experienced greater difficulty during processing of sentences containing object-relative clauses than subject-relative clauses. The third experiment manipulated the animacy of the sentential subject and the noun within the relative clause. This experiment demonstrated that the difficulty associated with object-relative clauses was greatly reduced when the sentential subject was inanimate. We interpret the results with respect to theories of syntactic parsing.},
  langid = {english},
  keywords = {parsing,relative clauses,sentence processing,syntax,working memory.},
  file = {/Users/j/Zotero/storage/6JCHIJ6F/Traxler et al. (2002) Processing Subject and Object Relative Clauses Ev.pdf}
}

@inproceedings{trevisan.l:2009,
  title = {Regularity, {{Boosting}}, and {{Efficiently Simulating Every High-Entropy Distribution}}},
  booktitle = {2009 24th {{Annual IEEE Conference}} on {{Computational Complexity}}},
  author = {Trevisan, Luca and Tulsiani, Madhur and Vadhan, Salil},
  year = {2009},
  month = jul,
  pages = {126--136},
  issn = {1093-0159},
  doi = {10.1109/CCC.2009.41},
  abstract = {We show that every bounded function g: 0,1n rarr [0,1] admits an efficiently computable "simulator" function h: 0,1n rarr [0,1] such that every fixed polynomial size circuit has approximately the same correlation with g as with h. If g describes (up to scaling) a high min-entropy distribution D, then h can be used to efficiently sample a distribution D' of the same min-entropy that is indistinguishable from D by circuits of fixed polynomial size. We state and prove our result in a more abstract setting, in which we allow arbitrary finite domains instead of 0,1n, and arbitrary families of distinguishers, instead of fixed polynomial size circuits. Our result implies (a) the weak Szemeredi regularity Lemma of Frieze and Kannan (b) a constructive version of the dense model theorem of Green, Tao and Ziegler with better quantitative parameters (polynomial rather than exponential in the distinguishing probability), and (c) the Impagliazzo hardcore set Lemma. It appears to be the general result underlying the known connections between "regularity" results in graph theory, "decomposition" results in additive combinatorics, and the hardcore Lemma in complexity theory. We present two proofs of our result, one in the spirit of Nisan's proof of the hardcore Lemma via duality of linear programming, and one similar to Impagliazzo's "boosting" proof. A third proof by iterative partitioning, which gives the complexity of the sampler to be exponential in the distinguishing probability, is also implicit in the Green-Tao-Ziegler proofs of the dense model theorem.},
  keywords = {additive combinatorics,average-case complexity,boosting,Boosting,Circuit simulation,Combinatorial mathematics,Complexity theory,Computational complexity,Computational modeling,Computer science,Computer simulation,Graph theory,Polynomials,pseudorandomness},
  file = {/Users/j/Zotero/storage/C7NJX889/Trevisan et al. - 2009 - Regularity, Boosting, and Efficiently Simulating E.pdf}
}

@article{tversky.a:1971,
  title = {Belief in the Law of Small Numbers.},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  publisher = {{American Psychological Association (APA)}},
  doi = {10.1037/h0031322},
  url = {https://doi.org/10.1037%2Fh0031322},
  bdsk-url-2 = {https://doi.org/10.1037/h0031322},
  date-added = {2021-08-08 20:24:01 -0400},
  date-modified = {2021-08-08 20:24:02 -0400}
}

@article{upper.d:1974,
  title = {The {{Unsuccessful Self-Treatment}} of a {{Case}} of ``{{Writer}}'s {{Block}}''1},
  author = {Upper, Dennis},
  year = {1974},
  journal = {Journal of Applied Behavior Analysis},
  volume = {7},
  number = {3},
  pages = {497--497},
  issn = {1938-3703},
  doi = {10.1901/jaba.1974.7-497a},
  url = {http://onlinelibrary.wiley.com/doi/abs/10.1901/jaba.1974.7-497a},
  urldate = {2022-06-16},
  langid = {english},
  keywords = {humor,writer's block},
  file = {/Users/j/Zotero/storage/8AN7ARSP/Upper - 1974 - The Unsuccessful Self-Treatment of a Case of â€œWrit.pdf}
}

@article{ussery.c:2017,
  title = {Dimensions of Variation},
  author = {Ussery, Cherlon},
  year = {2017},
  journal = {Syntactic variation in insular Scandinavian},
  volume = {1},
  pages = {165},
  publisher = {{John Benjamins Publishing Company}},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:38:48 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,syntactic variation}
}

@article{vandemeerendonk.n:2011,
  title = {Monitoring in Language Perception: {{Electrophysiological}} and Hemodynamic Responses to Spelling Violations},
  shorttitle = {Monitoring in Language Perception},
  author = {{van de Meerendonk}, Nan and Indefrey, Peter and Chwilla, Dorothee J. and Kolk, Herman H. J.},
  year = {2011},
  month = feb,
  journal = {NeuroImage},
  volume = {54},
  number = {3},
  pages = {2350--2363},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.022},
  url = {https://www.sciencedirect.com/science/article/pii/S1053811910013145},
  urldate = {2022-06-24},
  abstract = {The monitoring theory of language perception proposes that competing representations that are caused by strong expectancy violations can trigger a conflict which elicits reprocessing of the input to check for possible processing errors. This monitoring process is thought to be reflected by the P600 component in the EEG. The present study further investigated this monitoring process by comparing syntactic and spelling violations in an EEG and an fMRI experiment. To assess the effect of conflict strength, misspellings were embedded in sentences that were weakly or strongly predictive of a critical word. In support of the monitoring theory, syntactic and spelling violations elicited similarly distributed P600 effects. Furthermore, the P600 effect was larger to misspellings in the strongly compared to the weakly predictive sentences. The fMRI results showed that both syntactic and spelling violations increased activation in the left inferior frontal gyrus (lIFG), while only the misspellings activated additional areas. Conflict strength did not affect the hemodynamic response to spelling violations. These results extend the idea that the lIFG is involved in implementing cognitive control in the presence of representational conflicts in general to the processing of errors in language perception.},
  langid = {english},
  keywords = {Cognitive control,Conflict,Left inferior frontal gyrus,P600,Reprocessing}
}

@article{vandermude.a:1978,
  title = {On the Inference of Stochastic Regular Grammars},
  author = {Van Der Mude, Antony and Walker, Adrian},
  year = {1978},
  journal = {Information and Control},
  volume = {38},
  number = {3},
  pages = {310--329},
  publisher = {{Elsevier}},
  doi = {10.1016/S0019-9958(78)90106-7},
  url = {https://doi.org/10.1016/S0019-9958(78)90106-7},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:33:31 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information}
}

@article{vandyke.j:2006,
  title = {Retrieval Interference in Sentence Comprehension},
  author = {Van Dyke, Julie A. and McElree, Brian},
  year = {2006},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {55},
  number = {2},
  pages = {157--166},
  issn = {0749596X},
  doi = {10.1016/j.jml.2006.03.007},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X0600043X},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/H8HJQEKM/Van Dyke and McElree - 2006 - Retrieval interference in sentence comprehension.pdf}
}

@article{vanerven.t:2014,
  title = {R\'enyi Ivergence and {{Kullback-Leibler}} Divergence},
  author = {{van Erven}, Tim and Harremos, Peter},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {60},
  number = {7},
  pages = {3797--3820},
  issn = {1557-9654},
  doi = {10.1109/TIT.2014.2320500},
  abstract = {R\'enyi divergence is related to R\'enyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by R\'enyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the R\'enyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of R\'enyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of \textbackslash (\textbackslash sigma \textbackslash ) -algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results.},
  keywords = {alpha-divergence,Bhattacharyya distance,Convergence,Data processing,Entropy,information divergence,Kullback-Leibler divergence,Markov processes,Pythagorean inequality,Q measurement,R\'enyi divergence},
  file = {/Users/j/Zotero/storage/C56T2S9J/van Erven and Harremos (2014) RÃ©nyi Divergence and Kullback-Leibler Divergence.pdf}
}

@article{vani.p:2021,
  title = {Using the Interpolated Maze Task to Assess Incremental Processing in {{English}} Relative Clauses},
  author = {Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger},
  year = {2021},
  journal = {Proceedings of the 43rd Annual Meeting of the Cognitive Science Society},
  url = {https://escholarship.org/uc/item/3x34x7dz},
  urldate = {2023-03-09},
  abstract = {In English, Subject Relative Clauses are processed more quickly than Object Relative Clauses, but open questions remain about where in the clause slowdown occurs. The surprisal theory of incremental processing, under which processing difficulty corresponds to probabilistic expectations about upcoming material, predicts that slowdown should occur immediately on material that disambiguates the subject from object relative clause. However, evidence from eye tracking and self-paced reading studies suggests that slowdown occurs downstream of RC-disambiguating material, on the relative clause verb. These methods, however, suffer from well-known spillover effects which makes their results difficult to interpret. To address these issues, we introduce and deploy a novel variant of the Maze task for reading times (Forster, Guerrera, \&amp; Elliot, 2009), called the Interpolated Maze in two English web-based experiments. In Experiment 1, we find that the locus of reading-time differences between SRCs and ORCs falls on immediate disambiguating definite determiner. Experiment 2 provides a control, showing that ORCs are read more slowly than lexically-matching, non-anomalous material. These results provide new evidence for the locus of processing difficulty in relative clauses and support the surprisal theory of incremental processing.},
  langid = {english},
  file = {/Users/j/Zotero/storage/3E7482C6/Vani et al. (2021) Using the Interpolated Maze Task to Assess Increme.pdf}
}

@misc{vanos.m:2022amlap,
  type = {Poster},
  title = {Rational Speech Comprehension: Interaction between Predictability, Acoustic Signal, and Noise},
  author = {{van Os}, Marjolein and Kray, Jutta and Demberg, Vera},
  year = {2022},
  month = sep,
  address = {{York, England}},
  url = {https://virtual.oxfordabstracts.com/#/event/3067/submission/72},
  urldate = {2022-09-09},
  annotation = {link-abstract: https://oxford-abstracts.s3.amazonaws.com/2dbcb575-3c17-4635-a1b8-24e1e8b44d3d.pdf link-poster: https://oxford-abstracts.s3.amazonaws.com/0e28630c-2678-413b-bb42-daa834276886.pdf}
}

@article{vasishth.s:2006,
  title = {Argument-Head Distance and Processing Complexity: Explaining Both Locality and Antilocality Effects},
  author = {Vasishth, Shravan and Lewis, Richard L.},
  year = {2006},
  journal = {Language},
  volume = {82},
  number = {4},
  eprint = {4490268},
  eprinttype = {jstor},
  pages = {767--794},
  publisher = {{Linguistic Society of America}},
  issn = {00978507, 15350665},
  url = {http://www.jstor.org/stable/4490268},
  abstract = {Although proximity between arguments and verbs (locality) is a relatively robust determinant of sentence-processing difficulty (Hawkins 1998, 2001, Gibson 2000), increasing argument-verb distance can also facilitate processing (Konieczny 2000). We present two self-paced reading (SPR) experiments involving Hindi that provide further evidence of antilocality, and a third SPR experiment which suggests that similarity-based interference can attenuate this distance-based facilitation. A unified explanation of interference, locality, and antilocality effects is proposed via an independently motivated theory of activation decay and retrieval interference (Anderson et al. 2004).},
  date-added = {2022-03-31 11:51:04 -0400},
  date-modified = {2022-03-31 11:52:05 -0400},
  keywords = {antilocality effects,Dependency locality theory,locality effects,processing,processing complexity,self-paced reading}
}

@inproceedings{vasishth.s:2006icle,
  title = {On the Proper Treatment of Spillover in Real-Time Reading Studies: {{Consequences}} for Psycholinguistic Theories},
  booktitle = {Proceedings of the International Conference on Linguistic Evidence},
  author = {Vasishth, Shravan},
  year = {2006},
  pages = {96--100}
}

@article{vasishth.s:2010,
  title = {Short-Term Forgetting in Sentence Comprehension: Crosslinguistic Evidence from Verb-Final Structures},
  author = {Vasishth, Shravan and Suckow, Katja and Lewis, Richard L. and Kern, Sabine},
  year = {2010},
  month = may,
  journal = {Language and Cognitive Processes},
  volume = {25},
  number = {4},
  pages = {533--567},
  publisher = {{Informa UK Limited}},
  doi = {10.1080/01690960903310587},
  url = {https://doi.org/10.1080%2F01690960903310587},
  bdsk-url-2 = {https://doi.org/10.1080/01690960903310587},
  date-added = {2022-04-19 22:48:38 -0400},
  date-modified = {2022-04-19 22:48:39 -0400}
}

@article{vasishth.s:2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
  year = {2018},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X18300640},
  urldate = {2022-07-01},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  langid = {english},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error},
  file = {/Users/j/Zotero/storage/74GM97LG/Vasishth et al. - 2018 - The statistical significance filter leads to overo.pdf}
}

@article{vasishth.s:2019,
  title = {Computational Models of Retrieval Processes in Sentence Processing},
  author = {Vasishth, Shravan and Nicenboim, Bruno and Engelmann, Felix and Burchert, Frank},
  year = {2019},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {968--982},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.09.003},
  url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319302220},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/KS5VZ6XE/Vasishth et al. - 2019 - Computational Models of Retrieval Processes in Sen.pdf}
}

@book{vasishth.s:2021,
  title = {Sentence Comprehension as a Cognitive Process: A Computational Approach},
  shorttitle = {Sentence Comprehension as a Cognitive Process},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  month = oct,
  edition = {First},
  publisher = {{Cambridge University Press}},
  doi = {10.1017/9781316459560},
  url = {https://www.cambridge.org/core/product/identifier/9781316459560/type/book},
  urldate = {2022-10-12},
  abstract = {Sentence comprehension - the way we process and understand spoken and written language - is a central and important area of research within psycholinguistics. This book explores the contribution of computational linguistics to the field, showing how computational models of sentence processing can help scientists in their investigation of human cognitive processes. It presents the leading computational model of retrieval processes in sentence processing, the Lewis and Vasishth cue-based retrieval mode, and develops a principled methodology for parameter estimation and model comparison/evaluation using benchmark data, to enable researchers to test their own models of retrieval against the present model. It also provides readers with an overview of the last 20 years of research on the topic of retrieval processes in sentence comprehension, along with source code that allows researchers to extend the model and carry out new research. Comprehensive in its scope, this book is essential reading for researchers in cognitive science.},
  isbn = {978-1-316-45956-0}
}

@incollection{vasishth.s:2021ch3,
  title = {The Core {{ACT-R-based}} Model of Retrieval Processes},
  booktitle = {Sentence {{Comprehension}} as a {{Cognitive Process}}: {{A Computational Approach}}},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  pages = {49--70},
  publisher = {{Cambridge University Press}},
  address = {{Cambridge, England}},
  doi = {10.1017/9781316459560.008},
  url = {https://www.cambridge.org/core/books/sentence-comprehension-as-a-cognitive-process/core-actrbased-model-of-retrieval-processes/E932A3F6C61A70ECAC4A4D43B4CC5A60},
  urldate = {2022-10-12},
  abstract = {The core model of sentence processing used in the book is introduced and its empirical coverage relative to the existing reading time data is considered. Here, we also discuss the Approximate Bayesian Computation method for parameter estimation for model evaluation.},
  isbn = {978-1-107-13311-2},
  keywords = {ACT-R,cue-based retrieval,parsing,psycholinguistics,sentence comprehension},
  file = {/Users/j/Zotero/storage/HDWZE8LD/Vasishth and Engelmann (2021) The Core ACT-R-Based Model of Retrieval Processes.pdf}
}

@inproceedings{vaswani.a:2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, {{December}} 4-9, 2017, {{Long Beach}}, {{CA}}, {{USA}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {5998--6008},
  url = {https://proceedings.neurips.cc/paper/2017/hash/3f5ee243547dee91fbd053c1c4a845aa-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{verdu.s:1998,
  title = {Fifty Years of {{Shannon}} Theory},
  author = {Verdu, S.},
  year = {1998},
  journal = {IEEE Transactions on information theory},
  volume = {44},
  number = {6},
  pages = {2057--2078},
  issn = {1557-9654},
  doi = {10.1109/18.720531},
  url = {https://doi.org/10.1109/18.720531},
  abstract = {A brief chronicle is given of the historical development of the central problems in the theory of fundamental limits of data compression and reliable communication.},
  date-added = {2020-08-17 11:37:37 -0400},
  date-modified = {2020-08-17 11:39:25 -0400},
  project = {information-entropy},
  keywords = {channel capacity,data compression,information theory,rate distortion theory,source coding}
}

@misc{vieira.t:2014blog,
  type = {Blog},
  title = {Gumbel-Max Trick and Weighted Reservoir Sampling},
  author = {Vieira, Tim},
  year = {2014},
  month = aug,
  journal = {Graduate Descent},
  url = {https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/},
  urldate = {2022-11-06}
}

@article{vieira.t:2017,
  title = {Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing},
  author = {Vieira, Tim and Eisner, Jason},
  year = {2017},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {263--278},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00060},
  abstract = {Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the lols algorithm. lols training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier\textemdash i.e., parsers which are more accurate for a given runtime.},
  date-added = {2022-04-28 10:14:22 -0400},
  date-modified = {2022-04-28 10:14:42 -0400},
  keywords = {chart parsing,pruning},
  file = {/Users/j/Zotero/storage/6CJL2LF6/Vieira and Eisner - 2017 - Learning to prune Exploring the frontier of fast .pdf}
}

@inproceedings{vijayakumar.a:2018,
  title = {Diverse Beam Search for Improved Description of Complex Scenes},
  booktitle = {Proceedings of the Thirty-Second {{AAAI}} Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth {{AAAI}} Symposium on Educational Advances in Artificial Intelligence},
  author = {Vijayakumar, Ashwin K. and Cogswell, Michael and Selvaraju, Ramprasaath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  year = {2018},
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  publisher = {{AAAI Press}},
  address = {{New Orleans, Louisiana, USA}},
  url = {https://dl.acm.org/doi/abs/10.5555/3504035.3504938},
  abstract = {A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse \textendash{} with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks \textendash{} image captioning and visual question generation \textendash{} particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.},
  articleno = {903},
  date-added = {2022-03-25 22:32:37 -0400},
  date-modified = {2022-03-25 22:34:11 -0400},
  isbn = {978-1-57735-800-8}
}

@misc{vilnis.l:2022,
  title = {Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models},
  shorttitle = {Arithmetic Sampling},
  author = {Vilnis, Luke and Zemlyanskiy, Yury and Murray, Patrick and Passos, Alexandre and Sanghai, Sumit},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15458},
  eprint = {2210.15458},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2210.15458},
  url = {http://arxiv.org/abs/2210.15458},
  urldate = {2022-11-07},
  abstract = {Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, showing substantially reduced variance when estimating expected BLEU score and up to 1 point increased BLEU in oracle experiments.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/BV87TTFC/Vilnis et al. (2022) Arithmetic Sampling Parallel Diverse Decoding for.pdf}
}

@article{vincent.p:2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  month = dec,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  pages = {3371--3408},
  publisher = {{JMLR.org}},
  issn = {1532-4435},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  issue_date = {3/1/2010}
}

@inproceedings{vinyals.o:2014,
  title = {Grammar as a Foreign Language},
  booktitle = {Advances in Neural Information Processing Systems 28: {{Annual}} Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey E.},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  year = {2015},
  pages = {2773--2781},
  url = {https://proceedings.neurips.cc/paper/2015/hash/277281aada22045c03945dcb2ca6f2ec-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VinyalsKKPSH15.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{voita.e:2020mdlprobing,
  title = {Information-Theoretic Probing with Minimum Description Length},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  pages = {183--196},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-main.14},
  url = {https://www.aclweb.org/anthology/2020.emnlp-main.14},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.14}
}

@article{vul.e:2014,
  title = {One and Done? {{Optimal}} Decisions from Very Few Samples},
  author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  year = {2014},
  journal = {Cognitive Science},
  volume = {38},
  number = {4},
  pages = {599--637},
  publisher = {{Wiley}},
  doi = {10.1111/cogs.12101},
  url = {https://doi.org/10.1111/cogs.12101},
  date-added = {2021-03-16 23:51:30 -0400},
  date-modified = {2021-03-16 23:51:30 -0400},
  keywords = {bayesian,bounded rationality,inference algorithms,sampling}
}

@article{vulkan.n:2000,
  title = {An Economist's Perspective on Probability Matching},
  author = {Vulkan, Nir},
  year = {2000},
  journal = {Journal of Economic Surveys},
  volume = {14},
  number = {1},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-6419.00106},
  pages = {101--118},
  doi = {10.1111/1467-6419.00106},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1111/1467-6419.00106},
  abstract = {The experimental phenomenon known as `probability matching' is often offered as evidence in support of adaptive learning models and against the idea that people maximise their expected utility. Recent interest in dynamic-based equilibrium theories means the term re-appears in Economics. However, there seems to be conflicting views on what is actually meant by the term and about the validity of the data. The purpose of this paper is therefore threefold: First, to introduce today's readers to what is meant by probability matching, and in particular to clarify which aspects of this phenomenon challenge the utility-maximisation hypothesis. Second, to familiarise the reader with the different theoretical approaches to behaviour in such circumstances, and to focus on the differences in predictions between these theories in light of recent advances. Third, to provide a comprehensive survey of repeated, binary choice experiments.},
  bdsk-url-2 = {https://doi.org/10.1111/1467-6419.00106},
  date-added = {2021-05-31 13:57:04 -0400},
  date-modified = {2021-05-31 13:57:05 -0400},
  keywords = {Optimisation,Probability matching,Stochastic learning}
}

@article{wagers.m:2009,
  title = {Agreement Attraction in Comprehension: {{Representations}} and Processes},
  shorttitle = {Agreement Attraction in Comprehension},
  author = {Wagers, Matthew W. and Lau, Ellen F. and Phillips, Colin},
  year = {2009},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {61},
  number = {2},
  pages = {206--237},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2009.04.002},
  url = {https://www.sciencedirect.com/science/article/pii/S0749596X09000448},
  urldate = {2023-04-03},
  abstract = {Much work has demonstrated so-called attraction errors in the production of subject\textendash verb agreement (e.g., `The key to the cabinets are on the table', [Bock, J. K., \& Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45\textendash 93]), in which a verb erroneously agrees with an intervening noun. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject\textendash verb agreement violations when these `attractor' nouns intervene. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories. First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., `The drivers who the runner wave to each morning'). Second, we observe a `grammatical asymmetry': attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb.},
  langid = {english},
  keywords = {Agreement,agreement attraction,Comprehension,Prediction,Retrieval,Syntax},
  file = {/Users/j/Zotero/storage/TR4FPYFI/Wagers et al. (2009) Agreement attraction in comprehension Representat.pdf}
}

@article{wald.a:1939,
  title = {Contributions to the {{Theory}} of {{Statistical Estimation}} and {{Testing Hypotheses}}},
  author = {Wald, Abraham},
  year = {1939},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {10},
  number = {4},
  pages = {299--326},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732144},
  url = {https://projecteuclid.org/journals/annals-of-mathematical-statistics/volume-10/issue-4/Contributions-to-the-Theory-of-Statistical-Estimation-and-Testing-Hypotheses/10.1214/aoms/1177732144.full},
  urldate = {2022-05-21},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {decision theory}
}

@article{wald.a:1947,
  title = {Foundations of a General Theory of Sequential Decision Functions},
  author = {Wald, Abraham},
  year = {1947},
  journal = {Econometrica},
  volume = {15},
  number = {4},
  eprint = {1905331},
  eprinttype = {jstor},
  pages = {279--313},
  publisher = {{[Wiley, Econometric Society]}},
  issn = {0012-9682},
  doi = {10.2307/1905331},
  url = {https://www.jstor.org/stable/1905331},
  urldate = {2022-07-04}
}

@inproceedings{wang.a:2019,
  title = {{{SuperGLUE}}: {{A}} Stickier Benchmark for General-Purpose Language Understanding Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3261--3275},
  url = {https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@misc{wang.b:2021GPT-J-6B,
  title = {{{GPT-J-6B}}: {{A}} 6 Billion Parameter Autoregressive Language Model},
  author = {Wang, Ben and Komatsuzaki, Aran},
  year = {2021},
  month = may,
  url = {https://github.com/kingoflolz/mesh-transformer-jax},
  date-added = {2021-11-30 10:11:28 -0500},
  date-modified = {2021-12-13 19:43:33 -0500},
  howpublished = {Software}
}

@article{ward.j:1963,
  title = {Hierarchical Grouping to Optimize an Objective Function},
  author = {Ward, Jr., Joe H.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {{Taylor \& Francis Group}},
  url = {https://amstat.tandfonline.com/doi/abs/10.1080/01621459.1963.10500845},
  bdsk-url-2 = {https://pdfs.semanticscholar.org/0430/b241bdd0b67d37e1143370f8d24fc46d83e9.pdf},
  bdsk-url-3 = {https://doi.org/10.1080/01621459.1963.10500845},
  date-added = {2019-06-15 15:57:19 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {0},
  keywords = {hierarchical clustering}
}

@article{warstadt.a:2019,
  title = {Neural Network Acceptability Judgments},
  author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
  year = {2019},
  month = nov,
  volume = {7},
  pages = {625--641},
  publisher = {{MIT Press - Journals}},
  doi = {10.1162/tacl_a_00290},
  date-added = {2021-10-19 00:10:14 -0400},
  date-modified = {2021-10-19 00:10:15 -0400},
  file = {/Users/j/Zotero/storage/VTQSLFGV/Warstadt et al. - 2019 - Neural network acceptability judgments.pdf}
}

@article{warstadt.a:2020,
  title = {{{BLiMP}}: {{The Benchmark}} of {{Linguistic Minimal Pairs}} for {{English}}},
  shorttitle = {{{BLiMP}}},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  year = {2020},
  month = jul,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {377--392},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00321},
  url = {https://doi.org/10.1162/tacl_a_00321},
  urldate = {2023-04-30},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs\textemdash that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
  file = {/Users/j/Zotero/storage/J7TZY7ZI/Warstadt et al. (2020) BLiMP The Benchmark of Linguistic Minimal Pairs f.pdf}
}

@book{watrous.j:2018,
  title = {The Theory of Quantum Information},
  author = {Watrous, John},
  year = {2018},
  publisher = {{Cambridge University Press}},
  url = {https://cs.uwaterloo.ca/ watrous/TQI/},
  date-added = {2020-02-15 11:52:26 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {information-entropy},
  keywords = {entropy,information theory,quantum information theory}
}

@article{wieling.m:2016,
  title = {Investigating Dialectal Differences Using Articulography},
  author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Br{\"o}ker, Franziska and Thiele, Samuel and Wood, Simon N. and Baayen, R. Harald},
  year = {2016},
  month = nov,
  journal = {Journal of Phonetics},
  volume = {59},
  pages = {122--143},
  publisher = {{Elsevier BV}},
  doi = {10.1016/j.wocn.2016.09.004},
  url = {https://doi.org/10.1016%2Fj.wocn.2016.09.004},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2016.09.004},
  date-added = {2021-12-03 00:24:24 -0500},
  date-modified = {2021-12-03 00:24:40 -0500}
}

@inproceedings{wilcox.e:2020,
  title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Cognitive Science Society}}},
  author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {1707--1713},
  publisher = {{Cognitive Science Society}},
  address = {{Virtual}},
  url = {https://www.cognitivesciencesociety.org/cogsci20/papers/0375/},
  keywords = {sentence processing},
  file = {/Users/j/Zotero/storage/XF2IV8PJ/Wilcox et al. (2020) On the predictive power of neural language models .pdf}
}

@inproceedings{wilcox.e:2021,
  title = {A Targeted Assessment of Incremental Processing in Neural Language Models and Humans},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
  year = {2021},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.acl-long.76},
  url = {https://doi.org/10.18653%2Fv1%2F2021.acl-long.76},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.76},
  date-added = {2022-04-15 16:04:48 -0400},
  date-modified = {2022-04-15 16:04:50 -0400}
}

@misc{williams.p:2010,
  title = {Nonnegative Decomposition of Multivariate Information},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2010},
  eprint = {1004.2515},
  primaryclass = {cs.IT},
  archiveprefix = {arxiv},
  date-added = {2021-09-29 21:31:54 -0400},
  date-modified = {2021-09-29 21:31:56 -0400}
}

@misc{williams.p:2011,
  title = {Generalized Measures of Information Transfer},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2011},
  eprint = {1102.1507},
  primaryclass = {physics.data-an},
  archiveprefix = {arxiv},
  date-added = {2021-09-29 21:29:14 -0400},
  date-modified = {2021-09-29 21:29:15 -0400}
}

@incollection{wilson.k:1954,
  title = {Applications of Entropy Measures to Problems of Sequential Structure},
  booktitle = {Psycholinguistics},
  author = {Wilson, Kellogg and Carroll, John B},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {103--110},
  publisher = {{Indiana University Press}},
  doi = {10.1037/h0063655},
  url = {https://doi.org/10.1037/h0063655},
  chapter = {5.3},
  date-added = {2021-05-20 11:54:33 -0400},
  date-modified = {2022-04-14 23:51:44 -0400},
  keywords = {entropy reduction}
}

@inproceedings{wolf.t:2020transformers,
  title = {Transformers: {{State-of-the-art}} Natural Language Processing},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: {{System}} Demonstrations},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year = {2020},
  pages = {38--45},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-demos.6}
}

@article{wong.c:1980,
  title = {An Efficient Method for Weighted Sampling without Replacement},
  author = {Wong, C. K. and Easton, M. C.},
  year = {1980},
  month = feb,
  journal = {SIAM Journal on Computing},
  volume = {9},
  number = {1},
  pages = {111--113},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0209009},
  url = {http://epubs.siam.org/doi/10.1137/0209009},
  urldate = {2022-07-10},
  langid = {english},
  file = {/Users/j/Zotero/storage/E5M3QYF9/Wong and Easton - 1980 - An Efficient Method for Weighted Sampling without .pdf}
}

@article{wood.s:2003tprs,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  year = {2003},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  publisher = {{Wiley}},
  doi = {10.1111/1467-9868.00374},
  url = {https://doi.org/10.1111%2F1467-9868.00374},
  bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00374},
  date-added = {2021-12-02 20:55:01 -0500},
  date-modified = {2021-12-02 21:03:21 -0500}
}

@article{wood.s:2004GAMjustGCV,
  title = {Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models},
  author = {Wood, Simon N},
  year = {2004},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673--686},
  publisher = {{Informa UK Limited}},
  doi = {10.1198/016214504000000980},
  url = {https://doi.org/10.1198%2F016214504000000980},
  bdsk-url-2 = {https://doi.org/10.1198/016214504000000980},
  date-added = {2021-12-02 20:52:03 -0500},
  date-modified = {2021-12-02 20:52:18 -0500}
}

@article{wood.s:2011GAMmethod,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N.},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {3--36},
  publisher = {{Wiley}},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  url = {https://doi.org/10.1111%2Fj.1467-9868.2010.00749.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2010.00749.x},
  date-added = {2021-12-02 20:46:39 -0500},
  date-modified = {2021-12-02 20:49:53 -0500}
}

@article{wood.s:2014bam,
  title = {Generalized Additive Models for Large Data Sets},
  author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
  year = {2014},
  month = may,
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {64},
  number = {1},
  pages = {139--155},
  publisher = {{Wiley}},
  doi = {10.1111/rssc.12068},
  url = {https://doi.org/10.1111%2Frssc.12068},
  bdsk-url-2 = {https://doi.org/10.1111/rssc.12068},
  date-added = {2021-12-14 20:13:41 -0500},
  date-modified = {2021-12-14 20:21:11 -0500},
  file = {/Users/j/Zotero/storage/BFM8L3R3/Wood et al. - 2014 - Generalized additive models for large data sets.pdf}
}

@article{wood.s:2016GAMbeyondEF,
  title = {Smoothing Parameter and Model Selection for General Smooth Models},
  author = {Wood, Simon N. and Pya, Natalya and S{\"a}fken, Benjamin},
  year = {2016},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1548--1563},
  publisher = {{Taylor \& Francis}},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1180986},
  url = {https://doi.org/10.1080/01621459.2016.1180986},
  urldate = {2022-09-27},
  abstract = {This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online.},
  keywords = {Additive model,AIC,Distributional regression,GAM,gaulss,location scale additive models,Location scale and shape model,Ordered categorical regression,Penalized regression spline,REML,Smooth Cox model,Smoothing parameter uncertainty,Statistical algorithm,Tweedie distribution.}
}

@book{wood.s:2017GAMoverview,
  title = {Generalized Additive Models},
  author = {Wood, Simon N.},
  year = {2017},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315370279},
  url = {https://doi.org/10.1201%2F9781315370279}
}

@article{wu.b:1999,
  title = {Approximation and Exact Algorithms for Constructing Minimum Ultrametric Trees from Distance Matrices},
  author = {Wu, Bang Ye and Chao, Kun-Mao and Tang, Chuan Yi},
  year = {1999},
  journal = {Journal of Combinatorial Optimization},
  volume = {3},
  number = {2},
  pages = {199--211},
  issn = {1573-2886},
  url = {https://doi.org/10.1023/A:1009885610075},
  abstract = {An edge-weighted tree is called ultrametric if the distances from the root to all the leaves in the tree are equal. For an n by n distance matrix M, the minimum ultrametric tree for M is an ultrametric tree T = (V, E, w) with leaf set \{1,..., n\} such that dT(i, j) {$\geq$} M[i, j] for all i, j and \$\$\textbackslash sum \{\_\{e \textbackslash in E\} w(e)\}\$\$is minimum, where dT(i, j) is the distance between i and j on T. Constructing minimum ultrametric trees from distance matrices is an important problem in computational biology. In this paper, we examine its computational complexity and approximability. When the distances satisfy the triangle inequality, we show that the minimum ultrametric tree problem can be approximated in polynomial time with error ratio 1.5(1 + {$\lceil$}log n{$\rceil$}), where n is the number of species. We also develop an efficient branch-and-bound algorithm for constructing the minimum ultrametric tree for both metric and non-metric inputs. The experimental results show that it can find an optimal solution for 25 species within reasonable time, while, to the best of our knowledge, there is no report of algorithms solving the problem even for 12 species.},
  date-added = {2019-07-17 13:39:35 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@misc{wu.m:2022,
  title = {Foundation Posteriors for Approximate Probabilistic Inference},
  author = {Wu, Mike and Goodman, Noah},
  year = {2022},
  month = may,
  number = {arXiv:2205.09735},
  eprint = {2205.09735},
  primaryclass = {cs, stat},
  publisher = {{arXiv}},
  doi = {10.48550/arXiv.2205.09735},
  url = {http://arxiv.org/abs/2205.09735},
  urldate = {2022-08-18},
  abstract = {Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a ``foundation'' posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.},
  archiveprefix = {arxiv},
  keywords = {Computer Science - Machine Learning,inference,probabilistic programming,Statistics - Machine Learning},
  file = {/Users/j/Zotero/storage/HC3WUQXV/Wu and Goodman - 2022 - Foundation Posteriors for Approximate Probabilisti.pdf}
}

@inproceedings{wu.s:2010,
  title = {Complexity {{Metrics}} in an {{Incremental Right-Corner Parser}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Stephen and Bachrach, Asaf and Cardenas, Carlos and Schuler, William},
  year = {2010},
  month = jul,
  pages = {1189--1198},
  publisher = {{Association for Computational Linguistics}},
  address = {{Uppsala, Sweden}},
  url = {https://aclanthology.org/P10-1121},
  urldate = {2022-05-31},
  file = {/Users/j/Zotero/storage/EMUTBXBZ/Wu et al. - 2010 - Complexity Metrics in an Incremental Right-Corner .pdf}
}

@inproceedings{wu.z:2021,
  title = {Perturbed Masking: {{Parameter-free}} Probing for Analyzing and Interpreting {{BERT}}},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  year = {2020},
  pages = {4166--4176},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online}},
  doi = {10.18653/v1/2020.acl-main.383},
  url = {https://www.aclweb.org/anthology/2020.acl-main.383},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.383},
  file = {/Users/j/Zotero/storage/Q6BTLBFI/Wu et al. - 2020 - Perturbed masking Parameter-free probing for anal.pdf}
}

@inproceedings{yang.k:2020,
  title = {Strongly Incremental Constituency Parsing with Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Kaiyu and Deng, Jia},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {21687--21698},
  publisher = {{Curran Associates, Inc.}},
  url = {https://proceedings.neurips.cc/paper/2020/file/f7177163c833dff4b38fc8d2872f1ec6-Paper.pdf},
  file = {/Users/j/Zotero/storage/EUFIEWAW/Yang and Deng - 2020 - Strongly incremental constituency parsing with gra.pdf}
}

@inproceedings{yang.s:2020,
  title = {Second-Order Unsupervised Neural Dependency Parsing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  author = {Yang, Songlin and Jiang, Yong and Han, Wenjuan and Tu, Kewei},
  year = {2020},
  pages = {3911--3924},
  publisher = {{International Committee on Computational Linguistics}},
  address = {{Barcelona, Spain (Online)}},
  doi = {10.18653/v1/2020.coling-main.347},
  url = {https://www.aclweb.org/anthology/2020.coling-main.347},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.347},
  file = {/Users/j/Zotero/storage/SS59BRKI/Yang et al. - 2020 - Second-order unsupervised neural dependency parsin.pdf}
}

@inproceedings{yang.z:2018,
  title = {Breaking the Softmax Bottleneck: {{A}} High-Rank {{RNN}} Language Model},
  booktitle = {International Conference on Learning Representations},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
  year = {2018},
  url = {https://openreview.net/forum?id=HkwZSG-CZ}
}

@inproceedings{yang.z:2019,
  title = {{{XLNet}}: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  month = dec,
  volume = {32},
  pages = {5754--5764},
  publisher = {{Curran Associates, Inc.}},
  address = {{Vancouver, British Columbia, Canada}},
  url = {https://proceedings.neurips.cc/paper/2019/hash/dc6a7e655d7e5840e66733e9ee67cc69-Abstract.html},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
  date-modified = {2021-09-09 23:04:25 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {/Users/j/Zotero/storage/GMLGB59B/Yang et al. - 2019 - XLNet Generalized autoregressive pretraining for .pdf}
}

@article{ye.l:2020,
  title = {Monte {{Carlo}} Co-Ordinate Ascent Variational Inference},
  author = {Ye, Lifeng and Beskos, Alexandros and De Iorio, Maria and Hao, Jie},
  year = {2020},
  month = jul,
  journal = {Statistics and Computing},
  volume = {30},
  number = {4},
  pages = {887--905},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09924-y},
  url = {https://doi.org/10.1007/s11222-020-09924-y},
  urldate = {2022-06-27},
  abstract = {In variational inference (VI), coordinate-ascent and gradient-based approaches are two major types of algorithms for approximating difficult-to-compute probability densities. In real-world implementations of complex models, Monte Carlo methods are widely used to estimate expectations in coordinate-ascent approaches and gradients in derivative-driven ones. We discuss a Monte Carlo co-ordinate ascent VI (MC-CAVI) algorithm that makes use of Markov chain Monte Carlo (MCMC) methods in the calculation of expectations required within co-ordinate ascent VI (CAVI). We show that, under regularity conditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of the evidence lower bound with any given high probability. In numerical examples, the performance of MC-CAVI algorithm is compared with that of MCMC and\textemdash as a representative of derivative-based VI methods\textemdash of Black Box VI (BBVI). We discuss and demonstrate MC-CAVI's suitability for models with hard constraints in simulated and real examples. We compare MC-CAVI's performance with that of MCMC in an important complex model used in nuclear magnetic resonance spectroscopy data analysis\textemdash BBVI is nearly impossible to be employed in this setting due to the hard constraints involved in the model.},
  langid = {english},
  keywords = {Bayesian inference,Coordinate-ascent,Gradient-based optimisation,Markov chain Monte Carlo,Nuclear magnetic resonance,Variational inference},
  file = {/Users/j/Zotero/storage/5Q3QJVBC/Ye et al. - 2020 - Monte Carlo co-ordinate ascent variational inferen.pdf}
}

@article{yeung.r:1991,
  title = {A New Outlook on {{Shannon}}'s Information Measures},
  author = {Yeung, Raymond W.},
  year = {1991},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {3},
  pages = {466--474},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.79902},
  url = {https://doi.org/10.1109%2F18.79902},
  bdsk-url-2 = {https://doi.org/10.1109/18.79902},
  date-added = {2021-09-20 18:00:57 -0400},
  date-modified = {2021-09-21 18:18:41 -0400}
}

@book{yeung.r:2008,
  title = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  year = {2008},
  series = {Information Technology Transmission Processing and Storage},
  publisher = {{Springer US}},
  doi = {10.1007/978-0-387-79234-7},
  url = {https://doi.org/10.1007%2F978-0-387-79234-7},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7},
  date-added = {2021-09-21 18:16:02 -0400},
  date-modified = {2021-09-21 18:18:28 -0400}
}

@incollection{yeung.r:2008ch3,
  title = {The {{I-Measure}}},
  booktitle = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  series = {Information Technology Transmission Processing and Storage},
  pages = {51--80},
  publisher = {{Springer US}},
  doi = {10.1007/978-0-387-79234-7_3},
  url = {https://doi.org/10.1007%2F978-0-387-79234-7â‚ƒ},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7{$_{3}$}},
  chapter = {3},
  date-added = {2021-09-21 18:19:43 -0400},
  date-modified = {2021-09-21 18:22:32 -0400}
}

@article{yngve.v:1960,
  title = {A Model and an Hypothesis for Language Structure},
  author = {Yngve, Victor H.},
  year = {1960},
  journal = {Proceedings of the American Philosophical Society},
  volume = {104},
  number = {5},
  eprint = {985230},
  eprinttype = {jstor},
  pages = {444--466},
  publisher = {{American Philosophical Society}},
  issn = {0003-049X},
  url = {https://www.jstor.org/stable/985230},
  urldate = {2023-03-10},
  file = {/Users/j/Zotero/storage/P3PVV38C/Yngve (1960) A Model and an Hypothesis for Language Structure.pdf}
}

@inproceedings{yoshida.r:2021,
  title = {Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yoshida, Ryo and Noji, Hiroshi and Oseki, Yohei},
  year = {2021},
  pages = {2964--2973},
  publisher = {{Association for Computational Linguistics}},
  address = {{Online and Punta Cana, Dominican Republic}},
  doi = {10.18653/v1/2021.emnlp-main.235},
  url = {https://aclanthology.org/2021.emnlp-main.235},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Zotero/storage/P7YV7FZM/Yoshida et al. - 2021 - Modeling Human Sentence Processing with Left-Corne.pdf}
}

@inproceedings{yu.l:2022,
  title = {The Neural Noisy Channel},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yu, Lei and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Kocisky, Tomas},
  year = {2022},
  month = jul,
  url = {https://openreview.net/forum?id=SJ25-B5eg},
  urldate = {2022-10-25},
  abstract = {We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.},
  langid = {english},
  file = {/Users/j/Zotero/storage/KJR2SJZI/Yu et al. (2022) The Neural Noisy Channel.pdf}
}

@article{yun.j:2014,
  title = {Uncertainty in Processing Relative Clauses across {{East Asian}} Languages},
  author = {Yun, Jiwon and Chen, Zhong and Hunter, Tim and Whitman, John and Hale, John},
  year = {2014},
  journal = {Journal of East Asian Linguistics},
  volume = {24},
  number = {2},
  pages = {113--148},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10831-014-9126-6},
  url = {https://doi.org/10.1007%2Fs10831-014-9126-6},
  bdsk-url-2 = {https://doi.org/10.1007/s10831-014-9126-6},
  date-added = {2021-03-18 10:38:55 -0400},
  date-modified = {2021-03-18 10:39:18 -0400},
  keywords = {entropy reduction,processing},
  file = {/Users/j/Zotero/storage/I5T464WJ/Yun et al. - 2014 - Uncertainty in processing relative clauses across .pdf}
}

@phdthesis{yuret.d:1998,
  title = {Discovery of Linguistic Relations Using Lexical Attraction},
  author = {Yuret, Deniz},
  year = {1998},
  eprint = {cmp-lg/9805009},
  archiveprefix = {arxiv},
  date-added = {2020-04-23 12:44:41 -0400},
  date-modified = {2020-04-24 12:35:24 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science},
  keywords = {dependency parsing,dependency structures,mutual information}
}

@article{yuret.d:2006,
  title = {Lexical Attraction Models of Language},
  author = {Yuret, Deniz},
  year = {2006},
  journal = {Ms., Ko\c{c} University, Istanbul, Turkey,},
  url = {http://www2.denizyuret.com/pub/lex-attr/},
  date-added = {2019-09-12 19:59:44 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lexical attraction,mutual information}
}

@incollection{zaenen.a:1985a,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1985},
  pages = {93--136},
  publisher = {{Brill}},
  date-added = {2020-02-15 18:32:26 -0500},
  date-modified = {2020-02-15 18:38:25 -0500},
  project = {Icelandic gluttony},
  keywords = {dative subjecthood}
}

@incollection{zaenen.a:1990,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1990},
  pages = {93--136},
  publisher = {{Brill}},
  date-added = {2020-02-03 16:19:23 -0500},
  date-modified = {2020-02-03 16:19:39 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{zaslavsky.n:2018,
  title = {Efficient Compression in Color Naming and Its Evolution},
  author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {31},
  eprint = {https://www.pnas.org/content/115/31/7937.full.pdf},
  pages = {7937--7942},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424},
  url = {https://www.pnas.org/content/115/31/7937},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1800521115},
  date-added = {2019-05-15 00:03:28 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory}
}

@inproceedings{zeman.d:2020,
  title = {Universal {{Dependencies}}},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Tutorial}} Abstracts},
  author = {Nivre, Joakim and Zeman, Daniel and Ginter, Filip and Tyers, Francis},
  year = {2017},
  publisher = {{Association for Computational Linguistics}},
  address = {{Valencia, Spain}},
  url = {https://www.aclweb.org/anthology/E17-5001},
  file = {/Users/j/Zotero/storage/UGAFWW2M/Nivre et al. - 2017 - Universal Dependencies.pdf}
}

@inproceedings{zhan.m:2018,
  title = {Comparing Theories of Speaker Choice Using a Model of Classifier Production in {{Mandarin Chinese}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Zhan, Meilin and Levy, Roger},
  year = {2018},
  month = jun,
  pages = {1997--2005},
  publisher = {{Association for Computational Linguistics}},
  address = {{New Orleans, Louisiana}},
  doi = {10.18653/v1/N18-1181},
  url = {https://aclanthology.org/N18-1181},
  urldate = {2022-10-29},
  abstract = {Speakers often have more than one way to express the same meaning. What general principles govern speaker choice in the face of optionality when near semantically invariant alternation exists? Studies have shown that optional reduction in language is sensitive to contextual predictability, such that more predictable a linguistic unit is, the more likely it is to get reduced. Yet it is unclear whether these cases of speaker choice are driven by audience design versus toward facilitating production. Here we argue that for a different optionality phenomenon, namely classifier choice in Mandarin Chinese, Uniform Information Density and at least one plausible variant of availability-based production make opposite predictions regarding the relationship between the predictability of the upcoming material and speaker choices. In a corpus analysis of Mandarin Chinese, we show that the distribution of speaker choices supports the availability-based production account and not the Uniform Information Density.},
  keywords = {uniform information density}
}

@article{zhang.k:2018,
  title = {Language Modeling Teaches You More Syntax than Translation Does: {{Lessons}} Learned through Auxiliary Task Analysis},
  author = {Zhang, Kelly W. and Bowman, Samuel R.},
  year = {2018},
  journal = {CoRR},
  volume = {abs/1809.10040},
  eprint = {1809.10040},
  url = {http://arxiv.org/abs/1809.10040},
  archiveprefix = {arxiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/abs-1809-10040},
  date-added = {2019-06-16 11:01:31 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {CoVe,ELMo,LSTM,syntactic information},
  timestamp = {Fri, 05 Oct 2018 11:34:52 +0200},
  file = {/Users/j/Zotero/storage/AQID6Y65/Zhang and Bowman - 2018 - Language modeling teaches you more syntax than tra.pdf}
}

@article{zhang.t:2021,
  title = {On the Inductive Bias of Masked Language Modeling: {{From}} Statistical to Syntactic Dependencies},
  author = {Zhang, Tianyi and Hashimoto, Tatsunori},
  year = {2021},
  journal = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher = {{Association for Computational Linguistics}},
  doi = {10.18653/v1/2021.naacl-main.404},
  url = {http://dx.doi.org/10.18653/v1/2021.naacl-main.404},
  date-added = {2021-09-08 11:13:42 -0400},
  date-modified = {2021-09-08 11:13:47 -0400},
  file = {/Users/j/Zotero/storage/4ATKKNTC/Zhang and Hashimoto - 2021 - On the inductive bias of masked language modeling.pdf}
}

@inproceedings{zhang.y:2008,
  title = {A Tale of Two Parsers: Investigating and Combining Graph-Based and Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Yue and Clark, Stephen},
  year = {2008},
  month = oct,
  pages = {562--571},
  publisher = {{Association for Computational Linguistics}},
  address = {{Honolulu, Hawaii}},
  url = {https://aclanthology.org/D08-1059},
  date-added = {2022-03-25 22:14:22 -0400},
  date-modified = {2022-03-25 22:14:24 -0400},
  file = {/Users/j/Zotero/storage/QPGVRH7R/Zhang and Clark - 2008 - A tale of two parsers Investigating and combining.pdf}
}

@inproceedings{zhou.j:2019,
  title = {Head-{{Driven Phrase Structure Grammar}} Parsing on {{Penn Treebank}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Zhou, Junru and Zhao, Hai},
  year = {2019},
  pages = {2396--2408},
  publisher = {{Association for Computational Linguistics}},
  address = {{Florence, Italy}},
  doi = {10.18653/v1/P19-1230},
  url = {https://www.aclweb.org/anthology/P19-1230},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1230},
  file = {/Users/j/Zotero/storage/PATQJVNK/Zhou and Zhao - 2019 - Head-Driven Phrase Structure Grammar parsing on Pe.pdf}
}

@book{zipf.g:1935,
  title = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {2013},
  month = nov,
  edition = {Republished},
  publisher = {{Routledge}},
  address = {{London, United Kingdom}},
  doi = {10.4324/9781315009421},
  url = {https://www.taylorfrancis.com/books/9781136310461},
  urldate = {2022-09-28},
  isbn = {978-1-136-31046-1},
  langid = {english}
}

@book{zipf.g:1949,
  title = {Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology},
  shorttitle = {Human Behavior and the Principle of Least Effort},
  author = {Zipf, George Kingsley},
  year = {2012},
  publisher = {{Martino Publishing}},
  address = {{Mansfield Centre, CT}},
  isbn = {978-1-61427-312-7},
  langid = {english}
}

@inproceedings{zwarts.s:2011,
  title = {The Impact of Language Models and Loss Functions on Repair Disfluency Detection},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zwarts, Simon and Johnson, Mark},
  year = {2011},
  month = jun,
  pages = {703--711},
  publisher = {{Association for Computational Linguistics}},
  address = {{Portland, Oregon, USA}},
  url = {https://aclanthology.org/P11-1071},
  urldate = {2022-10-24},
  keywords = {noisy channel}
}
