@article{aaronson.s:2014,
  title = {The Equivalence of Sampling and Searching},
  author = {Aaronson, Scott},
  year = {2014},
  month = aug,
  journal = {Theory of Computing Systems},
  volume = {55},
  number = {2},
  pages = {281--298},
  issn = {1433-0490},
  doi = {10.1007/s00224-013-9527-3},
  urldate = {2022-10-27},
  abstract = {In a sampling problem, we are given an input x{$\in$}\{0,1\}n, and asked to sample approximately from a probability distribution \${\textbackslash}mathcal\{D\}\_\{x\}\$over \${\textbackslash}operatorname\{poly\} ( n ) \$-bit strings. In a search problem, we are given an input x{$\in$}\{0,1\}n, and asked to find a member of a nonempty set Axwith high probability. (An example is finding a Nash equilibrium.) In this paper, we use tools from Kolmogorov complexity to show that sampling and search problems are ``essentially equivalent.'' More precisely, for any sampling problem S, there exists a search problem RSsuch that, if \${\textbackslash}mathcal\{C\}\$is any ``reasonable'' complexity class, then RSis in the search version of \${\textbackslash}mathcal\{C\}\$if and only if S is in the sampling version. What makes this nontrivial is that the same RSworks for every~\${\textbackslash}mathcal\{C\}\$.},
  langid = {english},
  keywords = {Algorithmic information theory,Extended Church-Turing Thesis,FBQP,Function problems,Kolmogorov complexity,Quantum computing,Relational problems,Sampling problems,Search problems},
  file = {~/Zotfiles/aaronson.s2014 The equivalence of sampling and searchin.pdf}
}

@article{abney.s:1991,
  title = {Memory Requirements and Local Ambiguities of Parsing Strategies},
  author = {Abney, Steven P. and Johnson, Mark},
  year = {1991},
  month = may,
  journal = {Journal of Psycholinguistic Research},
  volume = {20},
  number = {3},
  pages = {233--250},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf01067217},
  abstract = {We present a method for calculating lower bounds on the space required and local ambiguities entailed by parsing strategies. A fast, compact natural language parser must implement a strategy with low space requirements and few local ambiguities. It is also widely assumed in the psycholinguistics literature that extremely limited short-term space is available to the human parser, and that sentences containing center-embedded constructions are incomprehensible because processing them requires more space than is available. However, we show that the parsing strategies most psycholinguists assume require less space for processing center-embedded constructions than for processing other perfectly comprehensible constructions. We present alternative strategies for which center-embedded constructions do require more space than other constructions.},
  bdsk-url-2 = {https://doi.org/10.1007/bf01067217},
  date-added = {2022-03-31 09:33:23 -0400},
  date-modified = {2022-03-31 09:38:15 -0400},
  keywords = {memory,parsing,space-complexity}
}

@incollection{abney.s:1991chunks,
  title = {Parsing by Chunks},
  booktitle = {Studies in Linguistics and Philosophy},
  author = {Abney, Steven P.},
  year = {1991},
  pages = {257--278},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-011-3474-3_10},
  bdsk-url-2 = {https://doi.org/10.1007/978-94-011-3474-3{$_1$}0},
  date-added = {2022-03-31 09:42:19 -0400},
  date-modified = {2022-03-31 09:45:05 -0400},
  keywords = {context free grammar,parsing}
}

@inproceedings{abney.s:1999,
  title = {Relating Probabilistic Grammars and Automata},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}} -},
  author = {Abney, Steven and McAllester, David and Pereira, Fernando},
  year = {1999},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1034678.1034759},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034759},
  date-added = {2022-03-31 09:45:57 -0400},
  date-modified = {2022-03-31 09:47:10 -0400},
  keywords = {automata,context free grammar,parsing,probabilistic context free grammar,push-down automata}
}

@article{adamek.j:2004,
  title = {Abstract and Concrete Categories. {{The}} Joy of Cats},
  author = {Ad{\'a}mek, Ji{\v r}{\'i} and Herrlich, Horst and Strecker, George E},
  year = {2004},
  publisher = {Citeseer},
  date-added = {2019-08-24 09:17:33 -0400},
  date-modified = {2019-08-24 09:18:04 -0400},
  keywords = {category theory}
}

@misc{adams.r:2013blog,
  type = {Blog},
  title = {The {{Gumbel-max}} Trick for Discrete Distributions},
  author = {Adams, Ryan},
  year = {2013},
  month = apr,
  journal = {Laboratory for Intelligent Probabilistic Systems Blog},
  urldate = {2022-11-06},
  howpublished = {https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/},
  keywords = {gumbel-max trick}
}

@article{adani.f:2010,
  title = {Grammatical Feature Dissimilarities Make Relative Clauses Easier: {{A}} Comprehension Study with {{Italian}} Children},
  shorttitle = {Grammatical Feature Dissimilarities Make Relative Clauses Easier},
  author = {Adani, Flavia and Van Der Lely, Heather K.J. and Forgiarini, Matteo and Guasti, Maria Teresa},
  year = {2010},
  month = sep,
  journal = {Lingua},
  volume = {120},
  number = {9},
  pages = {2148--2166},
  issn = {00243841},
  doi = {10.1016/j.lingua.2010.03.018},
  urldate = {2023-10-29},
  langid = {english},
  keywords = {agreement attraction,italian}
}

@article{adelman.j:2008,
  title = {Modeling Lexical Decision: {{The}} Form of Frequency and Diversity Effects},
  shorttitle = {Modeling Lexical Decision},
  author = {Adelman, James S. and Brown, Gordon D. A.},
  year = {2008},
  journal = {Psychological Review},
  volume = {115},
  number = {1},
  pages = {214--227},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.115.1.214},
  abstract = {What is the root cause of word frequency effects on lexical decision times? W. S. Murray and K. I. Forster (2004) argued that such effects are linear in rank frequency, consistent with a serial search model of lexical access. In this article, the authors (a) describe a method of testing models of such effects that takes into account the possibility of parametric overfitting; (b) illustrate the effect of corpus choice on estimates of rank frequency; (c) give derivations of nine functional forms as predictions of models of lexical decision; (d) detail the assessment of these models and the rank model against existing data regarding the functional form of frequency effects; and (e) report further assessments using contextual diversity, a factor confounded with word frequency. The relationship between the occurrence distribution of words and lexical decision latencies to those words does not appear compatible with the rank hypothesis, undermining the case for serial search models of lexical access. Three transformations of contextual diversity based on extensions of instance models do, however, remain as plausible explanations of the effect. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {drift diffusion models,Lexical Access,Lexical Decision,Mathematical Modeling,random walk,Word Frequency},
  file = {~/Zotfiles/adelman.j2008 Modeling lexical decision The form of f 2.pdf;~/Zotfiles/adelman.j2008 Modeling lexical decision The form of f.pdf}
}

@article{adger.d:2009,
  title = {Features in Minimalist Syntax},
  author = {Adger, David and Svenonius, Peter},
  year = {2009},
  journal = {The Oxford Handbook of Minimalist Syntax},
  date-added = {2020-02-20 12:37:20 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,minimalist syntax,phi features}
}

@article{agapiou.s:2017,
  title = {Importance Sampling: Intrinsic Dimension and Computational Cost},
  shorttitle = {Importance Sampling},
  author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
  year = {2017},
  month = aug,
  journal = {Statistical Science},
  volume = {32},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/17-STS611},
  urldate = {2022-12-21},
  keywords = {importance sampling},
  file = {~/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension 2.pdf;~/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension 3.pdf;~/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension.pdf}
}

@techreport{ai@meta.:2024,
  title = {Llama 3 Model Card},
  author = {{AI @ Meta}},
  year = {2024},
  institution = {Meta}
}

@inproceedings{ait-mokhtar.s:1997,
  title = {Incremental Finite-State Parsing},
  booktitle = {Fifth Conference on Applied Natural Language Processing},
  author = {{Ait-Mokhtar}, Salah and Chanod, Jean-Pierre},
  year = {1997},
  pages = {72--79},
  publisher = {Association for Computational Linguistics},
  address = {Washington, DC, USA},
  doi = {10.3115/974557.974569},
  bdsk-url-2 = {https://doi.org/10.3115/974557.974569}
}

@article{aitchison.l:2017,
  title = {With or without You: Predictive Coding and {{Bayesian}} Inference in the Brain},
  shorttitle = {With or without You},
  author = {Aitchison, Laurence and Lengyel, M{\'a}t{\'e}},
  year = {2017},
  month = oct,
  journal = {Current Opinion in Neurobiology},
  series = {Computational {{Neuroscience}}},
  volume = {46},
  pages = {219--227},
  issn = {0959-4388},
  doi = {10.1016/j.conb.2017.08.010},
  urldate = {2025-04-17},
  abstract = {Two theoretical ideas have emerged recently with the ambition to provide a unifying functional explanation of neural population coding and dynamics: predictive coding and Bayesian inference. Here, we describe the two theories and their combination into a single framework: Bayesian predictive coding. We clarify how the two theories can be distinguished, despite sharing core computational concepts and addressing an overlapping set of empirical phenomena. We argue that predictive coding is an algorithmic/representational motif that can serve several different computational goals of which Bayesian inference is but one. Conversely, while Bayesian inference can utilize predictive coding, it can also be realized by a variety of other representations. We critically evaluate the experimental evidence supporting Bayesian predictive coding and discuss how to test it more directly.},
  file = {~/Zotfiles/aitchison.l2017 With or without you predictive coding a.pdf}
}

@misc{alain.g:2015arxiv,
  title = {{{GSNs}} : Generative Stochastic Networks},
  shorttitle = {{{GSNs}}},
  author = {Alain, Guillaume and Bengio, Yoshua and Yao, Li and Yosinski, Jason and {Thibodeau-Laufer}, Eric and Zhang, Saizheng and Vincent, Pascal},
  year = {2015},
  month = mar,
  number = {arXiv:1503.05571},
  eprint = {1503.05571},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.05571},
  urldate = {2023-12-27},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by back-propagation. The theorems provided here generalize recent work on the probabilistic interpretation of denoising auto-encoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). We study how GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {~/Zotfiles/alain.g2015arxiv GSNs  generative stochastic networks.pdf}
}

@inproceedings{alemi.a:2017,
  title = {Deep Variational Information Bottleneck},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/AlemiFD017.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@article{alexiadou.a:2014,
  title = {Opaque and Transparent Datives, and How They Behave in Passives},
  author = {Alexiadou, Artemis and Anagnostopoulou, Elena and Sevdali, Christina},
  year = {2014},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {17},
  number = {1},
  pages = {1--34},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{allen.c:2019,
  title = {Analogies Explained: {{Towards}} Understanding Word Embeddings},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Allen, Carl and Hospedales, Timothy M.},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {223--231},
  publisher = {PMLR},
  address = {Long Beach, California, USA}
}

@article{allopenna.p:1998,
  title = {Tracking the Time Course of Spoken Word Recognition Using Eye Movements: Evidence for Continuous Mapping Models},
  shorttitle = {Tracking the Time Course of Spoken Word Recognition Using Eye Movements},
  author = {Allopenna, Paul D. and Magnuson, James S. and Tanenhaus, Michael K.},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {419--439},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2558},
  urldate = {2023-09-27},
  abstract = {Eye movements to pictures of four objects on a screen were monitored as participants followed a spoken instruction to move one of the objects, e.g., ``Pick up the beaker; now put it below the diamond'' (Experiment 1) or heard progressively larger gates and tried to identify the referent (Experiment 2). The distractor objects included a cohort competitor with a name that began with the same onset and vowel as the name of the target object (e.g.,beetle), a rhyme competitor (e.g.speaker), and an unrelated competitor (e.g.,carriage). In Experiment 1, there was clear evidence for both cohort and rhyme activation as predicted by continuous mapping models such as TRACE (McClelland and Elman, 1986) and Shortlist (Norris, 1994). Additionally, the time course and probabilities of eye movements closely corresponded to response probabilities derived from TRACE simulations using the Luce choice rule (Luce, 1959). In the gating task, which emphasizes word-initial information, there was clear evidence for multiple activation of cohort members, as measured by judgments and eye movements, but no suggestion of rhyme effects. Given that the same sets of pictures were present during the gating task as in Experiment 1, we conclude that the rhyme effects in Experiment 1 were not an artifact of using a small set of visible alternatives.},
  file = {~/Zotfiles/allopenna.p1998 Tracking the Time Course of Spoken Word.pdf}
}

@article{altmann.g:1988,
  title = {Interaction with Context during Human Sentence Processing},
  author = {Altmann, Gerry and Steedman, Mark},
  year = {1988},
  month = dec,
  journal = {Cognition},
  volume = {30},
  number = {3},
  pages = {191--238},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0277(88)90020-0},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0277(88)90020-0},
  date-added = {2022-04-14 13:35:33 -0400},
  date-modified = {2022-04-14 13:35:37 -0400}
}

@article{altmann.g:1999,
  title = {Incremental Interpretation at Verbs: Restricting the Domain of Subsequent Reference},
  shorttitle = {Incremental Interpretation at Verbs},
  author = {Altmann, Gerry T. M and Kamide, Yuki},
  year = {1999},
  month = dec,
  journal = {Cognition},
  volume = {73},
  number = {3},
  pages = {247--264},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(99)00059-1},
  urldate = {2023-10-25},
  abstract = {Participants' eye movements were recorded as they inspected a semi-realistic visual scene showing a boy, a cake, and various distractor objects. Whilst viewing this scene, they heard sentences such as `the boy will move the cake' or `the boy will eat the cake'. The cake was the only edible object portrayed in the scene. In each of two experiments, the onset of saccadic eye movements to the target object (the cake) was significantly later in the move condition than in the eat condition; saccades to the target were launched after the onset of the spoken word cake in the move condition, but before its onset in the eat condition. The results suggest that information at the verb can be used to restrict the domain within the context to which subsequent reference will be made by the (as yet unencountered) post-verbal grammatical object. The data support a hypothesis in which sentence processing is driven by the predictive relationships between verbs, their syntactic arguments, and the real-world contexts in which they occur.},
  keywords = {Eye movements,incremental processing,Parsing,Thematic roles},
  file = {~/Zotfiles/altmann.g1999 Incremental interpretation at verbs res.pdf}
}

@article{amari.s:1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  keywords = {Computer architecture,information geometry,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes},
  file = {~/Zotfiles/amari.s1992 Information geometry of Boltzmann machin.pdf}
}

@book{anagnostopoulou.e:2003book,
  title = {The Syntax of Ditransitives: {{Evidence}} from Clitics},
  author = {Anagnostopoulou, Elena},
  year = {2003},
  volume = {54},
  publisher = {Walter de Gruyter},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:07 -0400},
  project = {Icelandic gluttony},
  keywords = {clitics,hierarchy effects}
}

@article{anagnostopoulou.e:2017,
  title = {The {{Person Case Constraint}}},
  author = {Anagnostopoulou, Elena},
  year = {2017},
  journal = {The Wiley Blackwell Companion to Syntax, Second Edition},
  pages = {1--47},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@misc{anderbois.s:2023,
  title = {The Unspoken Language of Crosswords},
  author = {AnderBois, Scott and Mahowald, Kyle and Tomlin, Nicholas},
  year = {2023},
  month = aug,
  journal = {The Atlantic},
  urldate = {2025-01-15},
  abstract = {Solvers must develop strong intuitions about what entries are possible and how they can be clued.},
  chapter = {Science},
  langid = {english}
}

@book{anderson.j:1990book,
  title = {The Adaptive Character of Thought},
  author = {Anderson, John R.},
  year = {1990},
  month = jan,
  publisher = {Psychology Press},
  bdsk-url-2 = {https://doi.org/10.4324/9780203771730},
  date-added = {2022-04-04 11:54:23 -0400},
  date-modified = {2022-04-04 12:18:17 -0400},
  file = {~/Zotfiles/anderson.j1990 The adaptive character of thought.pdf}
}

@article{anderson.j:1991,
  title = {Reflections of the Environment in Memory},
  author = {Anderson, John R. and Schooler, Lael J.},
  year = {1991},
  month = nov,
  journal = {Psychological Science},
  volume = {2},
  number = {6},
  pages = {396--408},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1991.tb00174.x},
  urldate = {2022-09-08},
  abstract = {Availability of human memories for specific items shows reliable relationships to frequency, recency, and pattern of prior exposures to the item. These relationships have defied a systematic theoretical treatment. A number of environmental sources (New York Times, parental speech, electronic mail) are examined to show that the probability that a memory will be needed also shows reliable relationships to frequency, recency, and pattern of prior exposures. Moreover, the environmental relationships are the same as the memory relationships. It is argued that human memory has the form it does because it is adapted to these environmental relationships. Models for both the environment and human memory are described. Among the memory phenomena addressed are the practice function, the retention function, the effect of spacing of practice, and the relationship between degree of practice and retention.},
  langid = {english},
  file = {~/Zotfiles/anderson.j1991reflections Reflections of the environment in memory.pdf}
}

@incollection{anderson.j:1991architectures,
  title = {The Place of Cognitive Architectures in a Rational Analysis},
  booktitle = {Architectures for Intelligence},
  author = {Anderson, John R.},
  year = {1991},
  pages = {1--24},
  publisher = {Psychology Press},
  abstract = {The basic goal of a theorist in specifying a cognitive architecture is to specify the mind's principles of operation and organization much like you would specify those of a computer. Any cognitive phenomena should be derivative from these principles. As this conference gives witness, there are many cognitive architectures. This chapter will try to make some claims about the role of architectures generally in psychological theory, but it will do this by taking as examples three of the architectures which figure prominently at Carnegie Mellon University. There is the Soar architecture of Laird, Newell, and Rosenbloom (1987) my own ACT* architecture (Anderson, 1983), and the PDP architecture of McClelland and Rumelhart (McClelland \& Rumelhart, 1986, Rumelhart \& McClelland, 1986).},
  isbn = {978-1-315-80784-3},
  file = {~/Zotfiles/anderson.j1991architectures The place of cognitive architectures in.pdf}
}

@article{anderson.j:1991BBS,
  title = {Is Human Cognition Adaptive?},
  author = {Anderson, John R.},
  year = {1991},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {14},
  number = {3},
  pages = {471--485},
  publisher = {Cambridge University Press},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00070801},
  urldate = {2022-06-12},
  abstract = {Can the output of human cognition be predicted from the assumption that it is an optimal response to the information-processing demands of the environment? A methodology called rational analysis is described for deriving predictions about cognitive phenomena using optimization assumptions. The predictions flow from the statistical structure of the environment and not the assumed structure of the mind. Bayesian inference is used, assuming that people start with a weak prior model of the world which they integrate with experience to develop stronger models of specific aspects of the world. Cognitive performance maximizes the difference between the expected gain and cost of mental effort. (1) Memory performance can be predicted on the assumption that retrieval seeks a maximal trade-off between the probability of finding the relevant memories and the effort required to do so; in (2) categorization performance there is a similar trade-off between accuracy in predicting object features and the cost of hypothesis formation; in (3) casual inference the trade-off is between accuracy in predicting future events and the cost of hypothesis formation; and in (4) problem solving it is between the probability of achieving goals and the cost of both external and mental problem-solving search. The implemention of these rational prescriptions in neurally plausible architecture is also discussed.},
  langid = {english},
  keywords = {Bayes,categorization,causal inference,computation,memory,optimality,problem solving,rational analysis,rationality},
  file = {~/Zotfiles/anderson.j1991 Is human cognition adaptive.pdf}
}

@article{anderson.j:1998,
  title = {An Integrated Theory of List Memory},
  author = {Anderson, John R. and Bothell, Dan and Lebiere, Christian and Matessa, Michael},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {341--380},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2553},
  urldate = {2022-09-08},
  abstract = {The ACT-R theory (Anderson, 1993; Anderson \& Lebiere, 1998) is applied to the list memory paradigms of serial recall, recognition memory, free recall, and implicit memory. List memory performance in ACT-R is determined by the level of activation of declarative chunks which encode that items occur in the list. This level of activation is in turn determined by amount of rehearsal, delay, and associative fan from a list node. This theory accounts for accuracy and latency profiles in backward and forward serial recall, set size effects in the Sternberg paradigm, length--strength effects in recognition memory, the Tulving--Wiseman function, serial position, length and practice effects in free recall, and lexical priming in implicit memory paradigms. This wide variety of effects is predicted with minimal parameter variation. It is argued that the strength of the ACT-R theory is that it offers a completely specified processing architecture that serves to integrate many existing models in the literature.},
  langid = {english}
}

@book{anderson.j:1998book,
  title = {The Atomic Components of Thought},
  author = {Anderson, John R. and Lebiere, Christian},
  year = {1998},
  publisher = {Psychology Press},
  address = {New York},
  doi = {10.4324/9781315805696},
  isbn = {978-1-315-80569-6},
  langid = {english},
  file = {~/Zotfiles/anderson.j1998atomic The atomic components of thought.pdf}
}

@article{anderson.j:2004,
  title = {An Integrated Theory of the Mind},
  author = {Anderson, John R. and Bothell, Daniel and Byrne, Michael D. and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {4},
  pages = {1036--1060},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.111.4.1036},
  urldate = {2022-09-17},
  langid = {english},
  file = {~/Zotfiles/anderson.j2004 An integrated theory of the mind.pdf}
}

@misc{andral.c:2022arxiv,
  title = {An Attempt to Trace the Birth of Importance Sampling},
  author = {Andral, Charly},
  year = {2022},
  month = jun,
  number = {arXiv:2206.12286},
  eprint = {2206.12286},
  primaryclass = {math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2206.12286},
  urldate = {2025-02-17},
  abstract = {In this note, we try to trace the birth of importance sampling (IS) back to 1949. We found the classical formulation of IS in a paper from Kahn in June 1949. As for the appearance of the expression importance sampling itself, it may have appeared a few months later, maybe in 1949 during conferences, but there is no published article with the name before 1950.},
  archiveprefix = {arXiv},
  keywords = {Mathematics - History and Overview,Physics - History and Philosophy of Physics,Statistics - Computation},
  file = {~/Zotfiles/andral.c2022arxiv An attempt to trace the birth of importa.pdf}
}

@inproceedings{andreas.j:2022,
  title = {Language {{Models}} as {{Agent Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Andreas, Jacob},
  year = {2022},
  month = dec,
  pages = {5769--5779},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-07-29},
  abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them---a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that---even in today's non-robust and error-prone models---LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
  file = {~/Zotfiles/andreas.j2022 Language Models as Agent Models.pdf}
}

@article{andrews.s:1996,
  title = {Lexical {{Retrieval}} and {{Selection Processes}}: {{Effects}} of {{Transposed-Letter Confusability}}},
  shorttitle = {Lexical {{Retrieval}} and {{Selection Processes}}},
  author = {Andrews, Sally},
  year = {1996},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {35},
  number = {6},
  pages = {775--800},
  issn = {0749-596X},
  doi = {10.1006/jmla.1996.0040},
  urldate = {2023-10-29},
  abstract = {Three experiments investigated performance for words which differ from another word only by the transposition of two letters (e.g.,salt, slat). In Experiment 1, high frequency words from transposed-letter (TL) confusable pairs were responded to more slowly than carefully matched control words in both the lexical decision and word naming task. Low frequency TL words were responded to less accurately than control words in the naming but not the lexical decision task. Experiment 2 replicated the naming data of Experiment 1 and also revealed that naming accuracy for TL word targets was reduced when they were preceded by a brief masked presentation of their confusable mate. Experiment 3 provided a third replication of the impaired naming performance for TL target words and demonstrated that the effect was insensitive to concurrent dual task demands. These TL confusability effects provide strong constraints that can contribute to evaluation and specification of current models of visual word recognition.}
}

@article{andrieu.c:2010PMCMC,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  abstract = {Summary. Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L{\'e}vy-driven stochastic volatility model.},
  keywords = {bayesian inference,conditional sequential monte carlo,markov chain monte Carlo,sequential monte carlo,state space models},
  file = {~/Zotfiles/andrieu.c2010PMCMC Particle Markov chain Monte Carlo method.pdf}
}

@article{angele.b:2015,
  title = {Do Successor Effects in Reading Reflect Lexical Parafoveal Processing? {{Evidence}} from Corpus-Based and Experimental Eye Movement Data},
  author = {Angele, Bernhard and Schotter, Elizabeth R. and Slattery, Timothy J. and Tenenbaum, Tara L. and Bicknell, Klinton and Rayner, Keith},
  year = {2015},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {79--80},
  pages = {76--96},
  publisher = {Elsevier BV},
  doi = {10.1016/j.jml.2014.11.003},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2014.11.003},
  date-added = {2022-04-21 09:33:17 -0400},
  date-modified = {2022-04-21 09:33:18 -0400}
}

@inproceedings{arehalli.s:2022,
  title = {Syntactic Surprisal from Neural Models Predicts, but Underestimates, Human Processing Difficulty from Syntactic Ambiguities},
  booktitle = {Proceedings of the 26th Conference on Computational Natural Language Learning ({{CoNLL}})},
  author = {Arehalli, Suhas and Dillon, Brian and Linzen, Tal},
  year = {2022},
  month = dec,
  pages = {301--313},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates (Hybrid)},
  abstract = {Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory (Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.}
}

@article{arehalli.s:2024,
  title = {Neural Networks as Cognitive Models of the Processing of Syntactic Constraints},
  author = {Arehalli, Suhas and Linzen, Tal},
  year = {2024},
  month = may,
  journal = {Open Mind},
  volume = {8},
  pages = {558--614},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00137},
  urldate = {2025-09-12},
  abstract = {Languages are governed by syntactic constraints---structural rules that determine which sentences are grammatical in the language. In English, one such constraint is subject-verb agreement, which dictates that the number of a verb must match the number of its corresponding subject: ``the dogs run'', but ``the dog runs''. While this constraint appears to be simple, in practice speakers make agreement errors, particularly when a noun phrase near the verb differs in number from the subject (for example, a speaker might produce the ungrammatical sentence ``the key to the cabinets are rusty''). This phenomenon, referred to as agreement attraction, is sensitive to a wide range of properties of the sentence; no single existing model is able to generate predictions for the wide variety of materials studied in the human experimental literature. We explore the viability of neural network language models---broad-coverage systems trained to predict the next word in a corpus---as a framework for addressing this limitation. We analyze the agreement errors made by Long Short-Term Memory (LSTM) networks and compare them to those of humans. The models successfully simulate certain results, such as the so-called number asymmetry and the difference between attraction strength in grammatical and ungrammatical sentences, but failed to simulate others, such as the effect of syntactic distance or notional (conceptual) number. We further evaluate networks trained with explicit syntactic supervision, and find that this form of supervision does not always lead to more human-like syntactic behavior. Finally, we show that the corpus used to train a network significantly affects the pattern of agreement errors produced by the network, and discuss the strengths and limitations of neural networks as a tool for understanding human syntactic processing.},
  file = {/Users/v/Zotfiles/arehalli.s2024 Neural Networks as Cognitive Models of t.pdf;/Users/v/Zotero/storage/F3BCHKL6/opmi_a_00137.html}
}

@misc{arel-bundock.v:2024,
  title = {{{{\textbf{marginaleffects}}}}: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests},
  author = {{Arel-Bundock}, Vincent},
  year = {2024}
}

@misc{armengol-estape.j:2021,
  title = {On the Multilingual Capabilities of Very Large-Scale {{English}} Language Models},
  author = {{Armengol-Estap{\'e}}, Jordi and {de Gibert Bonet}, Ona and Melero, Maite},
  year = {2021},
  eprint = {2108.13349},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-12-13 19:48:21 -0500},
  date-modified = {2021-12-13 19:48:22 -0500}
}

@misc{arora.k:2023arxiv,
  title = {The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis and Algorithm for Robust Natural Language Generation},
  shorttitle = {The Stable Entropy Hypothesis and Entropy-Aware Decoding},
  author = {Arora, Kushal and O'Donnell, Timothy J. and Precup, Doina and Weston, Jason and Cheung, Jackie C. K.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06784},
  eprint = {2302.06784},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-05},
  abstract = {State-of-the-art language generation models can degenerate when applied to open-ended generation problems such as text completion, story generation, or dialog modeling. This degeneration usually shows up in the form of incoherence, lack of vocabulary diversity, and self-repetition or copying from the context. In this paper, we postulate that ``human-like'' generations usually lie in a narrow and nearly flat entropy band, and violation of these entropy bounds correlates with degenerate behavior. Our experiments show that this stable narrow entropy zone exists across models, tasks, and domains and confirm the hypothesis that violations of this zone correlate with degeneration. We then use this insight to propose an entropy-aware decoding algorithm that respects these entropy bounds resulting in less degenerate, more contextual, and "human-like" language generation in open-ended text generation settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/arora.k2023 The stable entropy hypothesis and entrop.pdf}
}

@misc{arroyo-fernandez.i:2019,
  title = {On the Possibility of Rewarding Structure Learning Agents: {{Mutual}} Information on Linguistic Random Sets},
  author = {{Arroyo-Fern{\'a}ndez}, Ignacio and {Carrasco-Ru{\'i}z}, Mauricio and {Arias-Aguilar}, J. Anibal},
  year = {2019},
  eprint = {1910.04023},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2020-01-27 11:44:31 -0500},
  date-modified = {2020-01-27 11:47:03 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised grammar induction}
}

@article{atlamaz.u:2018,
  title = {On Partial Agreement and Oblique Case},
  author = {Atlamaz, {\"U}mit and Baker, Mark},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {3},
  pages = {195--237},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{attias.h:1999,
  title = {A Variational Baysian Framework for Graphical Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Attias, Hagai},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {1999},
  volume = {12},
  publisher = {MIT Press},
  urldate = {2022-06-27},
  abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
  file = {~/Zotfiles/attias.h1999 A variational baysian framework for grap.pdf}
}

@book{attneave.f:1959book,
  title = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  shorttitle = {Applications of Information Theory to Psychology},
  author = {Attneave, Fred},
  year = {1959},
  series = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  publisher = {Henry Holt},
  address = {Oxford, England},
  abstract = {Summarizes existing informational methods used in psychological research, and illustrates the methods of calculating some of the measures. Chapter 1 develops quantitative expressions of uncertainty and redundancy from qualitative examples. Chapter 2 describes informational methods for analyzing sequences of events. Chapter 3 gives methods of describing rates of transmission of information and reviews pertinent research. Chapter 4 concerns possible applications of information measures, particularly to the study of perceptual problems of patterning. Appendices illustrate the calculation of information measures from variance statistics and provide convenient tables and a nomograph used in calculating information measures. 87 refs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {information theory,surprisal}
}

@article{aurnhammer.c:2019,
  title = {Evaluating Information-Theoretic Measures of Word Prediction in Naturalistic Sentence Reading},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  month = nov,
  journal = {Neuropsychologia},
  volume = {134},
  number = {107198},
  publisher = {Elsevier BV},
  doi = {10.1016/j.neuropsychologia.2019.107198},
  abstract = {We review information-theoretic measures of cognitive load during sentence processing that have been used to quantify word prediction effort. Two such measures, surprisal and next-word entropy, suffer from shortcomings when employed for a predictive processing view. We propose a novel metric, lookahead information gain, that can overcome these short-comings. We estimate the different measures using probabilistic language models. Subsequently, we put them to the test by analysing how well the estimated measures predict human processing effort in three data sets of naturalistic sentence reading. Our results replicate the well known effect of surprisal on word reading effort, but do not indicate a role of next-word entropy or lookahead information gain. Our computational results suggest that, in a predictive processing system, the costs of predicting may outweigh the gains. This idea poses a potential limit to the value of a predictive mechanism for the processing of language. The result illustrates the unresolved problem of finding estimations of word-by-word prediction that, first, are truly independent of perceptual processing of the to-be-predicted words, second, are statistically reliable predictors of experimental data, and third, can be derived from more general assumptions about the cognitive processes involved.},
  bdsk-url-2 = {https://doi.org/10.1016/j.neuropsychologia.2019.107198},
  date-added = {2021-11-29 11:28:26 -0500},
  date-modified = {2022-04-21 09:12:00 -0400}
}

@inproceedings{aurnhammer.c:2019cogsci,
  title = {Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing},
  booktitle = {Proceedings of the 41st {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  eprint = {2066/213724},
  eprinttype = {hdl},
  pages = {112--118},
  date-added = {2021-11-29 11:40:22 -0500},
  date-modified = {2021-11-29 12:44:15 -0500},
  file = {~/Zotfiles/aurnhammer.c2019cogsci Comparing gated and simple recurrent neu.pdf}
}

@book{awodey.s:2010book,
  title = {Category Theory},
  author = {Awodey, Steve},
  year = {2010},
  publisher = {Oxford University Press},
  date-added = {2019-08-24 09:18:40 -0400},
  date-modified = {2019-08-24 09:19:06 -0400},
  keywords = {category theory}
}

@book{axler.s:2020book,
  title = {Measure, {{Integration}} \& {{Real Analysis}}},
  author = {Axler, Sheldon},
  year = {2020},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {282},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-33143-6},
  urldate = {2022-06-23},
  isbn = {978-3-030-33142-9},
  langid = {english}
}

@article{aylett.m:2004,
  title = {The Smooth Signal Redundancy Hypothesis: A Functional Explanation for Relationships between Redundancy, Prosodic Prominence, and Duration in Spontaneous Speech},
  author = {Aylett, Matthew and Turk, Alice},
  year = {2004},
  journal = {Language and Speech},
  volume = {47},
  number = {1},
  eprint = {https://doi.org/10.1177/00238309040470010201},
  pages = {31--56},
  doi = {10.1177/00238309040470010201},
  abstract = {This paper explores two related factors which influence variation in duration, prosodic structure and redundancy in spontaneous speech. We argue that the constraint of producing robust communication while efficiently expending articulatory effort leads to an inverse relationship between language redundancy and duration. The inverse relationship improves communication robustness by spreading information more evenly across the speech signal, yielding a smoother signal redundancy profile.We argue that prosodic prominence is a linguistic means of achieving smooth signal redundancy. Prosodic prominence increases syllable duration and coincides to a large extent with unpredictable sections of speech, and thus leads to a smoother signal redundancy.The results of linear regressions carried out between measures of redundancy, syllable duration and prosodic structure in a large corpus of spontaneous speech confirm: (1) an inverse relationship between language redundancy and duration, and (2) a strong relationship between prosodic prominence and duration.The fact that a large proportion of the variance predicted by language redundancy and prosodic prominence is nonunique suggests that, in English, prosodic prominence structure is the means with which constraints caused by a robust signal requirement are expressed in spontaneous speech.},
  date-added = {2022-04-27 12:19:58 -0400},
  date-modified = {2022-04-27 12:20:19 -0400},
  pmid = {15298329},
  keywords = {noisy channel coding}
}

@incollection{baayen.r:2001,
  title = {Word Frequencies},
  booktitle = {Word {{Frequency Distributions}}},
  author = {Baayen, R. Harald},
  editor = {Baayen, R. Harald},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  pages = {1--38},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-0844-0_1},
  urldate = {2022-10-02},
  abstract = {This chapter introduces two fundamental issues in lexical statistics. The first issue concerns the role of the sample size, the number of words in a text or corpus. The sample size crucially determines a great many measures that have been proposed as characteristic text constants. However, the values of these measures change systematically as a function of the sample size. Similarly, the parameters of many models for word frequency distribution are highly dependent on the sample size. This property sets lexical statistics apart from most other areas in statistics, where an increase in the sample size leads to enhanced accuracy and not to systematic changes in basic measures and parameters.},
  isbn = {978-94-010-0844-0},
  langid = {english},
  file = {~/Zotfiles/baayen.r2001bookch1 Word frequencies.pdf}
}

@book{baayen.r:2001book,
  title = {Word Frequency Distributions},
  author = {Baayen, R. Harald},
  editor = {Ide, Nancy and V{\'e}ronis, Jean},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {18},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-0844-0},
  urldate = {2022-10-02},
  isbn = {978-1-4020-0927-3 978-94-010-0844-0},
  keywords = {best fit,corpus,Estimator},
  file = {~/Zotfiles/baayen.r2001book Word frequency distributions.pdf}
}

@techreport{babai.l:1979,
  type = {Technical Report},
  title = {Monte-{{Carlo}} Algorithms in Graph Isomorphism Testing},
  author = {Babai, L{\'a}szl{\'o}},
  year = {1979},
  number = {79-10},
  pages = {42},
  institution = {D{\'e}p. Math. et Stat., Universit{\'e} de Montr{\'e}al},
  keywords = {Las Vegas algorithms}
}

@phdthesis{bachrach.a:2008,
  title = {Imaging Neural Correlates of Syntactic Complexity in a Naturalistic Context},
  author = {Bachrach, Asaf},
  year = {2008},
  eprint = {1721.1/45900},
  eprinttype = {hdl},
  date-added = {2021-06-09 09:00:47 -0400},
  date-modified = {2022-04-20 10:20:04 -0400},
  school = {Massachusetts Institute of Technology}
}

@unpublished{bachrach.a:2009ms,
  type = {Unpublished Manuscript},
  title = {Incremental Prediction in Naturalistic Language Processing: {{An fMRI}} Study},
  author = {Bachrach, Asaf and Roark, Brian and Marantz, Alex and {Whitfield-Gabrieli}, Susan and Cardenas, Carlos and Gabrieli, John},
  year = {2009}
}

@book{bacon.a:2018book,
  title = {Vagueness and {{Thought}}},
  author = {Bacon, Andrew},
  year = {2018},
  series = {Oxford {{Philosophical Monographs}}},
  publisher = {Oxford University Press USA - OSO},
  address = {Oxford},
  isbn = {978-0-19-871206-0 978-0-19-102003-2},
  langid = {english}
}

@incollection{bader.m:1994,
  title = {German Verb-Final Clauses and Sentence Processing: {{Evidence}} for Immediate Attachment},
  shorttitle = {German Verb-Final Clauses and Sentence Processing},
  booktitle = {Perspectives on Sentence Processing},
  author = {Bader, Markus and Lasser, Ingeborg},
  editor = {Clifton, Jr., Charles and Frazier, Lyn and Rayner, Keith},
  year = {1994},
  pages = {225--242},
  publisher = {Lawrence Erlbaum Associates, Inc},
  address = {Hillsdale, NJ, US},
  abstract = {one central question in sentence processing concerns the relationship between knowledge of language and the way this knowledge is put to use / this relationship . . . has received a great deal of attention in the discussion of models of the human parsing mechanism [responsible for computing syntactic structures] / despite this attention, the issue of how linguistic knowledge is used during sentence comprehension is far from settled / goal [is] to narrow down the number of possible parsing models by introducing some on-line data from German  discuss a particular class of parsers that assumes both principles and parameters theory and a transparent grammar--parser relationship / call these parsers head-driven licensing parsers / on the basis of experimental evidence from German verb-final structures, we reject the particular interpretation of grammar-parser transparency / introduce certain properties of current syntactic theory and then show how these properties have found their way into head-driven licensing parsers / report an experiment that aims to test the prediction of head-driven licensing parsers for verb-final clauses; namely, that, in these clauses, all attachments have to be delayed until the end of the clause / [Ss were 24 native German speaking university students] (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-1581-8 978-0-8058-1582-5},
  keywords = {Grammar,Psycholinguistics,Sentence Comprehension,Sentence Structure,Syntax,Verbs}
}

@misc{bahdanau.d:2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2022-05-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {~/Zotfiles/bahdanau.d2016 Neural Machine Translation by Jointly Le.pdf}
}

@inproceedings{bailly.r:2020,
  title = {Emergence of Syntax Needs Minimal Supervision},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Bailly, Rapha{\"e}l and G{\'a}bor, Kata},
  year = {2020},
  pages = {477--487},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.46},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.46}
}

@article{baker.j:1979,
  title = {Trainable Grammars for Speech Recognition},
  author = {Baker, J. K.},
  year = {1979},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {65},
  number = {S1},
  pages = {S132-S132},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.2017061},
  urldate = {2022-07-04}
}

@book{baldi.p:2001book,
  title = {Bioinformatics, Second Edition: {{The Machine Learning Approach}}},
  shorttitle = {Bioinformatics, Second Edition},
  author = {Baldi, Pierre and Brunak, S{\o}ren},
  year = {2001},
  month = jul,
  series = {Adaptive {{Computation}} and {{Machine Learning}} Series},
  publisher = {MIT Press},
  abstract = {A guide to machine learning approaches and their application to the analysis of biological data.An unprecedented wealth of data is being generated by genome sequencing projects and other experimental efforts to determine the structure and function of biological molecules. The demands and opportunities for interpreting these data are expanding rapidly. Bioinformatics is the development and application of computer methods for management, analysis, interpretation, and prediction, as well as for the design of experiments. Machine learning approaches (e.g., neural networks, hidden Markov models, and belief networks) are ideally suited for areas where there is a lot of data but little theory, which is the situation in molecular biology. The goal in machine learning is to extract useful information from a body of data by building good probabilistic models---and to automate the process as much as possible.In this book Pierre Baldi and S{\o}ren Brunak present the key machine learning approaches and apply them to the computational problems encountered in the analysis of biological data. The book is aimed both at biologists and biochemists who need to understand new data-driven algorithms and at those with a primary background in physics, mathematics, statistics, or computer science who need to know more about applications in molecular biology.This new second edition contains expanded coverage of probabilistic graphical models and of the applications of neural networks, as well as a new chapter on microarrays and gene expression. The entire text has been extensively revised.},
  isbn = {978-0-262-02506-5},
  langid = {english},
  keywords = {Computers / Computer Science}
}

@incollection{baldi.p:2002,
  title = {A Computational Theory of Surprise},
  booktitle = {Information, {{Coding}} and {{Mathematics}}: {{Proceedings}} of {{Workshop}} Honoring {{Prof}}. {{Bob McEliece}} on His 60th Birthday},
  author = {Baldi, Pierre},
  editor = {Blaum, Mario and Farrell, Patrick G. and {van Tilborg}, Henk C. A.},
  year = {2002},
  series = {The {{Springer International Series}} in {{Engineering}} and {{Computer Science}}},
  pages = {1--25},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4757-3585-7_1},
  urldate = {2024-02-11},
  abstract = {While eminently successful for the transmission of data, Shannon's theory of information does not address semantic and subjective dimensions of data, such as relevance and surprise. We propose an observer-dependent computational theory of surprise where surprise is defined by the relative entropy between the prior and the posterior distributions of an observer. Surprise requires integration over the space of models in contrast with Shannon's entropy, which requires integration over the space of data. We show how surprise can be computed exactly in a number of discrete and continuous cases using distributions from the exponential family with conjugate priors. We show that during sequential Bayesian learning, surprise decreases like 1/N and study how surprise differs and complements Shannon's definition of information.},
  isbn = {978-1-4757-3585-7},
  langid = {english},
  keywords = {Bayesian Probabilities,Entropy,Information,kl theory,Relative Entropy.,Relevance,Surprise},
  file = {~/Zotfiles/baldi.p2002 A computational theory of surprise.pdf}
}

@article{baldi.p:2010,
  title = {Of Bits and Wows: {{A Bayesian}} Theory of Surprise with Applications to Attention},
  shorttitle = {Of Bits and Wows},
  author = {Baldi, Pierre and Itti, Laurent},
  year = {2010},
  month = jun,
  journal = {Neural Networks},
  volume = {23},
  number = {5},
  pages = {649--666},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.12.007},
  urldate = {2024-02-11},
  abstract = {The amount of information contained in a piece of data can be measured by the effect this data has on its observer. Fundamentally, this effect is to transform the observer's prior beliefs into posterior beliefs, according to Bayes theorem. Thus the amount of information can be measured in a natural way by the distance (relative entropy) between the prior and posterior distributions of the observer over the available space of hypotheses. This facet of information, termed ``surprise'', is important in dynamic situations where beliefs change, in particular during learning and adaptation. Surprise can often be computed analytically, for instance in the case of distributions from the exponential family, or it can be numerically approximated. During sequential Bayesian learning, surprise decreases as the inverse of the number of training examples. Theoretical properties of surprise are discussed, in particular how it differs and complements Shannon's definition of information. A computer vision neural network architecture is then presented capable of computing surprise over images and video stimuli. Hypothesizing that surprising data ought to attract natural or artificial attention systems, the output of this architecture is used in a psychophysical experiment to analyze human eye movements in the presence of natural video stimuli. Surprise is found to yield robust performance at predicting human gaze (ROC-like ordinal dominance score {$\sim$}0.7 compared to {$\sim$}0.8 for human inter-observer repeatability, {$\sim$}0.6 for simpler intensity contrast-based predictor, and 0.5 for chance). The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  keywords = {Attention,Eye movements,Information,Relative entropy,Surprise},
  file = {~/Zotfiles/baldi.p2010 Of bits and wows A Bayesian theory of s.pdf}
}

@inproceedings{balog.m:2017,
  title = {Lost Relatives of the {{Gumbel}} Trick},
  booktitle = {Proceedings of the 34th {{International Conference}} on {{Machine Learning}}},
  author = {Balog, Matej and Tripuraneni, Nilesh and Ghahramani, Zoubin and Weller, Adrian},
  year = {2017},
  month = jul,
  pages = {371--379},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-02-19},
  abstract = {The Gumbel trick is a method to sample from a discrete probability distribution, or to estimate its normalizing partition function. The method relies on repeatedly applying a random perturbation to the distribution in a particular way, each time solving for the most likely configuration. We derive an entire family of related methods, of which the Gumbel trick is one member, and show that the new methods have superior properties in several settings with minimal additional computational cost. In particular, for the Gumbel trick to yield computational benefits for discrete graphical models, Gumbel perturbations on all configurations are typically replaced with so-called low-rank perturbations. We show how a subfamily of our new methods adapts to this setting, proving new upper and lower bounds on the log partition function and deriving a family of sequential samplers for the Gibbs distribution. Finally, we balance the discussion by showing how the simpler analytical form of the Gumbel trick enables additional theoretical results.},
  langid = {english},
  file = {~/Zotfiles/balog.m2017 Lost Relatives of the Gumbel Trick_1.pdf;~/Zotfiles/balog.m2017 Lost Relatives of the Gumbel Trick.pdf}
}

@article{balota.d:1985,
  title = {The Interaction of Contextual Constraints and Parafoveal Visual Information in Reading},
  author = {Balota, David A and Pollatsek, Alexander and Rayner, Keith},
  year = {1985},
  journal = {Cognitive Psychology},
  volume = {17},
  number = {3},
  pages = {364--390},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0285(85)90013-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(85)90013-1},
  date-added = {2021-05-22 15:35:25 -0400},
  date-modified = {2021-05-22 15:35:39 -0400},
  keywords = {predictability,processing}
}

@misc{bao.h:2020arxiv,
  title = {{{UniLMv2}}: {{Pseudo-masked}} Language Models for Unified Language Model Pre-Training},
  shorttitle = {Unilmv2},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2020},
  month = feb,
  number = {arXiv:2002.12804},
  eprint = {2002.12804},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM}
}

@article{bar-hillel.y:1953,
  title = {A Quasi-Arithmetical Notation for Syntactic Description},
  author = {{Bar-Hillel}, Yehoshua},
  year = {1953},
  journal = {Language},
  volume = {29},
  number = {1},
  pages = {47},
  publisher = {JSTOR},
  doi = {10.2307/410452},
  bdsk-url-2 = {https://doi.org/10.2307/410452},
  date-added = {2021-06-25 00:50:06 -0400},
  date-modified = {2021-06-25 00:50:07 -0400}
}

@incollection{barber.d:2003,
  title = {The {{IM}} Algorithm: A Variational Approach to Information Maximization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Barber, David and Agakov, Felix},
  year = {2003},
  pages = {201--208},
  bdsk-url-2 = {https://papers.nips.cc/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},
  date-added = {2019-10-08 23:33:35 -0400},
  date-modified = {2021-03-07 16:09:06 -0500},
  project = {syntactic embedding},
  keywords = {mutual information,variational inference}
}

@article{barnard.g:1946,
  title = {Sequential Tests in Industrial Statistics},
  author = {Barnard, G. A.},
  year = {1946},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {8},
  number = {1},
  pages = {1--21},
  issn = {2517-617X},
  doi = {10.2307/2983610},
  urldate = {2022-07-04},
  abstract = {After an introductory and an historical note, an elementary problem of simple qualitative inspection of a box of components is treated by using a ``lattice diagram representation.'' This leads to the consideration of sequential tests for such cases. Procedures for determining ``Target-Handicap'' forms of inspection, and their operating and sample size properties are given. This leads to a consideration of general linear sequential tests, which are those test procedures which can be formulated in terms of a ``score.'' Such procedures are shown to be similar to classical games of chance, and to physical diffusion processes. The diffusion analogy leads to a differential equation which gives the approximate characteristics of any such linear test. In many cases, Wald's ``Probability Ratio Sequential Test'' takes the form of a linear test. The conditions for this are determined. The P.R.S. test is seen to be ``best possible linear test,'' in the sense of minimizing average sample size. The effects of deviations from normality, and general distributions are considered. Reference is made to Wald's work on tests which involve parameters other than those being estimated, and then consideration is restricted to tests for the mean of normal populations where the variance is unknown. Methods of reducing such tests to simple binomial tests are indicated. A number of procedures for use with 2 {\texttimes} 2 comparative trials, and double dichotomies, are given, and their properties discussed. Returning to general inspection problems, the paper indicates that these are not always to be identified with problems involving merely tests of statistical hypotheses. The notions of Consumer's Lot, Producer's Batch, the Lot Quality Curve, the Process Curve, are explained, and their importance indicated. A distinction is made between Acceptance Inspection schemes and Rectifying Inspection schemes, and the notions of Operating Characteristic Curve, Operating Characteristic Matrix, and the Sample Size distribution function are explained. The lattice diagram is used to bring out relationships between notions involved in general inspection, and some other uses are also indicated. Finally, some reflections on the relevance of the matters discussed to matters of current debate among statisticians are given.},
  langid = {english}
}

@inproceedings{barrett.m:2015,
  title = {The {{Dundee}} Treebank},
  booktitle = {The 14th International Workshop on Treebanks and Linguistic Theories ({{TLT}} 14)},
  author = {Barrett, Maria and Agic, {\v Z}eljko and S{\o}gaard, Anders},
  year = {2015},
  pages = {242--248},
  date-added = {2021-09-16 13:19:25 -0400},
  date-modified = {2021-09-16 13:20:18 -0400}
}

@article{barton.s:1993,
  title = {A Case Study of Anomaly Detection: {{Shallow}} Semantic Processing and Cohesion Establishment},
  shorttitle = {A Case Study of Anomaly Detection},
  author = {Barton, Stephen B. and Sanford, Anthony J.},
  year = {1993},
  month = jul,
  journal = {Memory \& Cognition},
  volume = {21},
  number = {4},
  pages = {477--487},
  issn = {1532-5946},
  doi = {10.3758/BF03197179},
  urldate = {2024-05-28},
  abstract = {Although the establishment of a coherent mental representation depends on semantic analysis, such analysis is not necessarily complete. This is illustrated by failures to notice the anomaly in questions such as, ``When an airplane crashes, where should the survivors be buried?'' Four experiments were carried out to extend knowledge of what determines the incidental detection of the critical item. Detection is a function of the goodness of global fit of the item (Experiments 1 and 2) and the extent to which the scenario predicts the item (Experiment 3). Global good fit appears to result in shallow processing of details. In Experiment 4, it is shown that if satisfactory coherence can be established without detailed semantic analysis, through the recruitment of suitable information from a sentence, then processing is indeed shallow. The studies also show that a text is not understood by first producing a local semantic representation and then incorporating this into a global model, and that semantic processing is not strictly incremental.},
  langid = {english},
  keywords = {Anomaly Detection,Bicycle Accident,Critical Item,Detection Rate,Noun Phrase},
  file = {~/Zotfiles/barton.s1993 A case study of anomaly detection Shall.pdf}
}

@article{bashirov.a:2008,
  title = {Multiplicative Calculus and Its Applications},
  author = {Bashirov, Agamirza E. and Kurp{\i}nar, Emine M{\i}s{\i}rl{\i} and {\"O}zyap{\i}c{\i}, Ali},
  year = {2008},
  month = jan,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {337},
  number = {1},
  pages = {36--48},
  issn = {0022-247X},
  doi = {10.1016/j.jmaa.2007.03.081},
  urldate = {2023-04-25},
  abstract = {Two operations, differentiation and integration, are basic in calculus and analysis. In fact, they are the infinitesimal versions of the subtraction and addition operations on numbers, respectively. In the period from 1967 till 1970 Michael Grossman and Robert Katz gave definitions of a new kind of derivative and integral, moving the roles of subtraction and addition to division and multiplication, and thus established a new calculus, called multiplicative calculus. In the present paper our aim is to bring up this calculus to the attention of researchers and demonstrate its usefulness.},
  langid = {english},
  keywords = {Calculus,Calculus of variations,Derivative,Differential equation,haar measure,Integral,Limit,multiplicative integral,product integral,Semigroup}
}

@article{bates.c:2020,
  title = {Efficient Data Compression in Perception and Perceptual Memory.},
  author = {Bates, Christopher J. and Jacobs, Robert A.},
  year = {2020},
  month = oct,
  journal = {Psychological Review},
  volume = {127},
  number = {5},
  pages = {891--917},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000197},
  urldate = {2022-11-28},
  abstract = {Efficient data compression is essential for capacity-limited systems, such as biological perception and perceptual memory. We hypothesize that the need for efficient compression shapes biological systems in many of the same ways that it shapes engineered systems. If true, then the tools that engineers use to analyze and design systems, namely rate-distortion theory (RDT), can profitably be used to understand human perception and memory. The first portion of this article discusses how three general principles for efficient data compression provide accounts for many important behavioral phenomena and experimental results. We also discuss how these principles are embodied in RDT. The second portion notes that exact RDT methods are computationally feasible only in low-dimensional stimulus spaces. To date, researchers have used deep neural networks to approximately implement RDT in high-dimensional spaces, but these implementations have been limited to tasks in which the sole goal is compression with respect to reconstruction error. Here, we introduce a new deep neural network architecture that approximately implements RDT. An important property of our architecture is that it is trained ``end-to-end,'' operating on raw perceptual input (e.g., pixel values) rather than intermediate levels of abstraction, as is the case with most psychological models. The article's final portion conjectures on how efficient compression can occur in memory over time, thereby providing motivations for multiple memory systems operating at different time scales, and on how efficient compression may explain some attentional phenomena such as RTs in visual search.},
  langid = {english},
  file = {~/Zotfiles/bates.c2020 Efficient data compression in perception.pdf}
}

@article{bates.d:2015,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {{{\textbf{lme4}}}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2024-02-25},
  langid = {english}
}

@article{baum.c:1994,
  title = {A Sequential Procedure for Multihypothesis Testing},
  author = {Baum, C.W. and Veeravalli, V.V.},
  year = {1994},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {40},
  number = {6},
  pages = {1994--2007},
  issn = {1557-9654},
  doi = {10.1109/18.340472},
  abstract = {The sequential testing of more than two hypotheses has important applications in direct-sequence spread spectrum signal acquisition, multiple-resolution-element radar, and other areas. A useful sequential test which we term the MSPRT is studied in this paper. The test is shown to be a generalization of the sequential probability ratio test. Under Bayesian assumptions, it is argued that the MSPRT approximates the much more complicated optimal test when error probabilities are small and expected stopping times are large. Bounds on error probabilities are derived, and asymptotic expressions for the stopping time and error probabilities are given. A design procedure is presented for determining the parameters of the MSPRT. Two examples involving Gaussian densities are included, and comparisons are made between simulation results and asymptotic expressions. Comparisons with Bayesian fixed sample size tests are also made, and it is found that the MSPRT requires two to three times fewer samples on average.{$<>$}},
  keywords = {Bayesian methods,Clinical trials,Error probability,Fault detection,Medical tests,Radar applications,Sequential analysis,Spread spectrum radar,Testing},
  file = {~/Zotfiles/baum.c1994 A sequential procedure for multihypothes.pdf}
}

@article{baumann.s:2018,
  title = {What Makes a Word Prominent? {{Predicting}} Untrained {{German}} Listeners' Perceptual Judgments},
  author = {Baumann, Stefan and Winter, Bodo},
  year = {2018},
  journal = {Journal of Phonetics},
  volume = {70},
  pages = {20--38},
  publisher = {Elsevier},
  bdsk-url-2 = {https://www.sciencedirect.com/science/article/pii/S0095447017301298},
  date-added = {2020-02-27 22:24:39 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {intonation,phonetics,prominence,prosody,random forests}
}

@article{bayes.t:1763,
  title = {{An essay towards solving a problem in the doctrine of chances}},
  author = {Bayes, Thomas and Price, Richard},
  year = {1763},
  month = dec,
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {53},
  pages = {370--418},
  issn = {0261-0523, 2053-9223},
  doi = {10.1098/rstl.1763.0053},
  urldate = {2024-05-14},
  abstract = {Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
  copyright = {https://royalsociety.org/journals/ethics-policies/data-sharing-mining/},
  langid = {latin},
  annotation = {note: By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S.},
  file = {~/Zotfiles/bayes.t1763 An essay towards solving a problem in th.pdf}
}

@misc{beauchene.c:2022,
  title = {Dynamic Cognitive States Predict Individual Variability in Behavior and Modulate with {{EEG}} Functional Connectivity during Working Memory},
  author = {Beauchene, Christine and Hinault, Thomas and Sarma, Sridevi V. and Courtney, Susan},
  year = {2022},
  month = jan,
  pages = {2021.08.02.454757},
  institution = {bioRxiv},
  doi = {10.1101/2021.08.02.454757},
  urldate = {2022-06-13},
  abstract = {Fluctuations in strategy, attention, or motivation can cause large variability in performance across task trials. Typically, this variability is treated as noise, and assumed to cancel out, leaving supposedly stable relationships among behavior, neural activity, and experimental task conditions. Those relationships, however, could change with a participant's internal cognitive states, and variability in performance may carry important information regarding those states, which cannot be directly measured. Therefore, we used a mathematical, state-space modeling framework to estimate internal states from measured behavioral data, quantifying each participant's sensitivity to factors such as past errors or distractions, to predict their reaction time fluctuations. We show how modeling these states greatly improves trial-by-trial prediction of behavior. Further, we identify EEG functional connectivity features that modulate with each state. These results illustrate the potential of this approach and how it could enable quantification of intra- and inter-individual differences and provide insight into their neural bases. Statement of Relevance Cognitive behavioral performance and its neural bases vary both across individuals and within individuals over time. Understanding this variability may be key to the success of clinical or educational interventions. Internal cognitive states reflecting differences in strategy, attention, and motivation may drive much of these inter- and intra-individual differences, but often cannot be reliably controlled or measured in cognitive neuroscience research. The mathematical modeling framework developed here uses measured data to estimate a participant's dynamic, internal cognitive states, with each state derived from specific factors hypothesized to affect attention, motivation or strategy. The results highlight potential sources of behavioral variability and reveal EEG features that modulate with each state. Our method quantifies and characterizes individual behavioral differences and highlights their underlying neural mechanisms, which could be used for future targeted training or neuromodulation therapies to improve cognitive performance.},
  chapter = {New Results},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {~/Zotfiles/beauchene.c2022 Dynamic cognitive states predict individ.pdf}
}

@phdthesis{behrenfeldt.j:2009,
  title = {A Linguist's Survey of Pumping Lemmata},
  author = {Behrenfeldt, Johan},
  year = {2009},
  date-added = {2020-02-12 12:04:22 -0500},
  date-modified = {2021-03-12 11:46:23 -0500},
  school = {University of Gothenburg},
  keywords = {formal languages,pumping lemmata}
}

@incollection{bejar.s:2003,
  title = {Person Licensing and the Derivation of {{PCC}} Effects},
  booktitle = {Romance Linguistics: {{Theory}} and Acquisition},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  editor = {{Perez-Leroux}, Ana Teresa and Roberg, Yves},
  year = {2003},
  pages = {49--62},
  publisher = {J. Benjamins},
  address = {Amsterdam},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:56 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{bejar.s:2009,
  title = {Cyclic Agree},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  year = {2009},
  journal = {Linguistic Inquiry},
  volume = {40},
  number = {1},
  pages = {35--73},
  publisher = {MIT Press},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:27:46 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects}
}

@inproceedings{belinkov.y:2017,
  title = {What Do Neural Machine Translation Models Learn about Morphology?},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  year = {2017},
  pages = {861--872},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1080},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1080}
}

@misc{bell-souder.d:2021HSP,
  title = {Language Modeling Using a Neural Network Shows Effects on {{N400}} beyond Just Surprisal},
  author = {{Bell-Souder}, Don and McKnight, Shannon and Zhdanov, Vladimir and Mullen, Sean and Miyake, Akira and Gilley, Phillip and Kim, Albert},
  year = {2021},
  month = mar,
  address = {University of Pennsylvania, Philadelphia, PA},
  urldate = {2023-05-23},
  langid = {american},
  file = {~/Zotfiles/bell-souder.d2021cuny Language modeling using a neural network.pdf}
}

@inproceedings{bell.a:2003,
  title = {The Co-Information Lattice},
  booktitle = {Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation: {{ICA}}},
  author = {Bell, Anthony J},
  year = {2003},
  volume = {2003},
  date-added = {2019-05-15 00:08:25 -0400},
  date-modified = {2021-07-19 22:04:55 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,synergy}
}

@article{belletti.a:2012,
  title = {Does Gender Make a Difference? {{Comparing}} the Effect of Gender on Children's Comprehension of Relative Clauses in {{Hebrew}} and {{Italian}}},
  shorttitle = {Does Gender Make a Difference?},
  author = {Belletti, Adriana and Friedmann, Naama and Brunato, Dominique and Rizzi, Luigi},
  year = {2012},
  month = aug,
  journal = {Lingua},
  volume = {122},
  number = {10},
  pages = {1053--1069},
  issn = {00243841},
  doi = {10.1016/j.lingua.2012.02.007},
  urldate = {2023-10-29},
  langid = {english},
  keywords = {agreement attraction,hebrew}
}

@misc{belrose.n:2023arxiv,
  title = {Eliciting Latent Predictions from Transformers with the Tuned Lens},
  author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08112},
  eprint = {2303.08112},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08112},
  urldate = {2023-03-16},
  abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{bender.e:2021,
  title = {On the Dangers of Stochastic Parrots: {{Can}} Language Models Be Too Big? },
  shorttitle = {On the Dangers of Stochastic Parrots},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  urldate = {2022-11-16},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {~/Zotfiles/bender.e2021 On the dangers of stochastic parrots Ca.pdf}
}

@inproceedings{bengio.y:2000,
  title = {A Neural Probabilistic Language Model},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  year = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2024-05-15},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  file = {~/Zotfiles/bengio.y2000 A neural probabilistic language model.pdf}
}

@article{bengio.y:2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Feb},
  pages = {1137--1155},
  issn = {ISSN 1533-7928},
  urldate = {2024-05-15},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {~/Zotfiles/bengio.y2003 A neural probabilistic language model.pdf}
}

@inproceedings{bengio.y:2014,
  title = {Deep Generative Stochastic Networks Trainable by Backprop},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Laufer, Eric and Alain, Guillaume and Yosinski, Jason},
  editor = {Xing, Eric P. and Jebara, Tony},
  year = {2014},
  month = jun,
  volume = {32},
  pages = {226--234},
  publisher = {PMLR},
  address = {Beijing, China},
  issn = {1938-7228},
  urldate = {2023-12-27},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.},
  langid = {english},
  file = {~/Zotfiles/bengio.y2014 Deep generative stochastic networks trai 2.pdf;~/Zotfiles/bengio.y2014 Deep generative stochastic networks trai.pdf}
}

@article{bennett.r:2009,
  title = {English Resumptive Pronouns and the Highest-Subject Restriction: {{A}} Corpus Study},
  author = {Bennett, Ryan},
  year = {2009},
  journal = {Trilateral (TREND) Linguistics Weekend, UC Santa Cruz},
  file = {~/Zotfiles/bennett.r2009 English resumptive pronouns and the high 2.pdf;~/Zotfiles/bennett.r2009 English resumptive pronouns and the high.pdf}
}

@techreport{bennett.v:2018,
  title = {Wasatch {{Solar Project}} Final Report},
  author = {Bennett, Vicki and Bowman, Kate and Wright, Sarah},
  year = {2018},
  number = {DOE-SLC-6903-1},
  address = {Salt Lake City, UT},
  institution = {Salt Lake City Corporation},
  doi = {10.2172/1474305},
  date-added = {2021-03-12 11:39:14 -0500},
  date-modified = {2021-03-12 11:43:47 -0500}
}

@book{bernardo.j:1994book,
  title = {Bayesian Theory},
  author = {Bernardo, Jos{\'e} M. and Smith, Adrian F. M.},
  year = {1994},
  month = may,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9780470316870},
  urldate = {2024-05-21},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-49464-5 978-0-470-31687-0},
  langid = {english},
  file = {~/Zotfiles/bernardo.j1994bt1 Bayesian theory.pdf}
}

@article{berwick.r:1982,
  title = {Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories},
  author = {Berwick, Robert C. and Weinberg, Amy S.},
  year = {1982},
  journal = {Linguistic Inquiry},
  volume = {13},
  number = {2},
  eprint = {4178272},
  eprinttype = {jstor},
  pages = {165--191},
  publisher = {The MIT Press},
  issn = {00243892, 15309150},
  date-added = {2022-03-29 20:33:12 -0400},
  date-modified = {2022-03-29 20:33:17 -0400}
}

@inproceedings{bicknell.k:2009,
  title = {A Model of Local Coherence Effects in Human Sentence Processing as Consequences of Updates from Bottom-up Prior to Posterior Beliefs},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the {{North American}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2009},
  month = jun,
  pages = {665--673},
  publisher = {Association for Computational Linguistics},
  address = {Boulder, Colorado},
  date-added = {2022-04-21 10:50:28 -0400},
  date-modified = {2022-04-21 10:50:29 -0400},
  file = {~/Zotfiles/bicknell.k2009 A model of local coherence effects in hu.pdf}
}

@inproceedings{bicknell.k:2010,
  title = {A Rational Model of Eye Movement Control in Reading},
  booktitle = {Proceedings of the 48th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2010},
  pages = {1168--1178},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  file = {~/Zotfiles/bicknell.k2010 A rational model of eye movement control.pdf}
}

@phdthesis{bicknell.k:2011,
  title = {Eye Movements in Reading as Rational Behavior},
  author = {Bicknell, Klinton},
  year = {2011},
  school = {University of California, San Diego},
  file = {~/Zotfiles/bicknell.k2011 Eye movements in reading as rational beh.pdf}
}

@inproceedings{bicknell.k:2012cogsci,
  title = {Word Predictability and Frequency Effects in a Rational Model of Reading},
  booktitle = {Proceedings of the 34th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2012},
  volume = {34},
  pages = {126--131},
  publisher = {Cognitive Science Society},
  address = {Sapporo, Japan},
  abstract = {This paper presents results from the first rational model of eye movement control in reading to make predictions for the full range of the eye movement record. The model identifies the text through Bayesian inference and makes eye movement de-cisions to maximize the efficiency of text identification, go-ing beyond leading approaches which select model parame-ters to maximize the fit to human data. Two simulations with the model demonstrate that it can produce effects of word pre-dictability and frequency on eye movements in reading similar to those produced by humans, providing evidence that many properties of human reading behavior may be understood as following from the nature of efficient text identification.},
  keywords = {surprisal theory},
  file = {~/Zotfiles/bicknell.k2012cogsci Word predictability and frequency effect.pdf}
}

@book{billingsley.p:1995book3,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  year = {1995},
  edition = {3},
  publisher = {Wiley},
  bdsk-url-2 = {https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf},
  date-added = {2021-03-28 11:47:36 -0400},
  date-modified = {2021-08-21 16:16:00 -0400},
  keywords = {measure theory,probability theory}
}

@book{bishop.c:2006book,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  edition = {1},
  publisher = {Springer},
  address = {New York, USA},
  urldate = {2022-06-10},
  isbn = {978-0-387-31073-2},
  langid = {english},
  file = {~/Zotfiles/bishop.c2006 Pattern Recognition and Machine Learning.pdf}
}

@article{bitzer.s:2014,
  title = {Perceptual Decision Making: Drift-Diffusion Model Is Equivalent to a {{Bayesian}} Model},
  shorttitle = {Perceptual Decision Making},
  author = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel, Stefan},
  year = {2014},
  journal = {Frontiers in Human Neuroscience},
  volume = {8},
  issn = {1662-5161},
  urldate = {2022-07-04},
  abstract = {Behavioral data obtained with perceptual decision making experiments are typically analyzed with the drift-diffusion model. This parsimonious model accumulates noisy pieces of evidence toward a decision bound to explain the accuracy and reaction times of subjects. Recently, Bayesian models have been proposed to explain how the brain extracts information from noisy input as typically presented in perceptual decision making tasks. It has long been known that the drift-diffusion model is tightly linked with such functional Bayesian models but the precise relationship of the two mechanisms was never made explicit. Using a Bayesian model, we derived the equations which relate parameter values between these models. In practice we show that this equivalence is useful when fitting multi-subject data. We further show that the Bayesian model suggests different decision variables which all predict equal responses and discuss how these may be discriminated based on neural correlates of accumulated evidence. In addition, we discuss extensions to the Bayesian model which would be difficult to derive for the drift-diffusion model. We suggest that these and other extensions may be highly useful for deriving new experiments which test novel hypotheses.},
  file = {~/Zotfiles/bitzer.s2014 Perceptual decision making drift-diffus.pdf}
}

@article{blachman.n:1968,
  title = {The Amount of Information That $y$ Gives about $X$},
  author = {Blachman, N.},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {1},
  pages = {27--31},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tit.1968.1054094},
  keywords = {entropy,entropy reduction,surprisal},
  file = {~/Zotfiles/blachman.n1968 The amount of information that y gives a.pdf}
}

@misc{black.s:2021,
  title = {{{GPT-Neo}}: {{Large}} Scale Autoregressive Language Modeling with Meshtensorflow},
  shorttitle = {{{GPT-Neo}}},
  author = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  year = {2021},
  month = oct,
  doi = {10.5281/ZENODO.5551208},
  urldate = {2024-02-01},
  abstract = {An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@inproceedings{black.s:2022,
  title = {{{GPT-NeoX-20B}}: {{An}} Open-Source Autoregressive Language Model},
  shorttitle = {{{GPT-NeoX-20B}}},
  booktitle = {Proceedings of {{BigScience Episode}} \#5 -- {{Workshop}} on {{Challenges}} \& {{Perspectives}} in {{Creating Large Language Models}}},
  author = {Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, Usvsn Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  editor = {Fan, Angela and Ilic, Suzana and Wolf, Thomas and Gall{\'e}, Matthias},
  year = {2022},
  month = may,
  pages = {95--136},
  publisher = {Association for Computational Linguistics},
  address = {virtual+Dublin},
  doi = {10.18653/v1/2022.bigscience-1.9},
  urldate = {2024-02-01},
  abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B's architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.}
}

@article{blanco-elorrieta.e:2021,
  title = {Adaptation to Mis-Pronounced Speech: Evidence for a Prefrontal-Cortex Repair Mechanism},
  shorttitle = {Adaptation to Mis-Pronounced Speech},
  author = {{Blanco-Elorrieta}, Esti and Gwilliams, Laura and Marantz, Alec and Pylkk{\"a}nen, Liina},
  year = {2021},
  month = jan,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {97},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-020-79640-0},
  urldate = {2024-12-04},
  abstract = {Speech is a complex and ambiguous acoustic signal that varies significantly within and across speakers. Despite the processing challenge that such variability poses, humans adapt to systematic variations in pronunciation rapidly. The goal of this study is to uncover the neurobiological bases of the attunement process that enables such fluent comprehension. Twenty-four native English participants listened to words spoken by a ``canonical'' American speaker and two non-canonical speakers, and performed a word-picture matching task, while magnetoencephalography was recorded. Non-canonical speech was created by including systematic phonological substitutions within the word (e.g. [s]\,{$\rightarrow$}\,[sh]). Activity in the auditory cortex (superior temporal gyrus) was greater in response to substituted phonemes, and, critically, this was not attenuated by exposure. By contrast, prefrontal regions showed an interaction between the presence of a substitution and the amount of exposure: activity decreased for canonical speech over time, whereas responses to non-canonical speech remained consistently elevated. Grainger causality analyses further revealed that prefrontal responses serve to modulate activity in auditory regions, suggesting the recruitment of top-down processing to decode non-canonical pronunciations. In sum, our results suggest that the behavioural deficit in processing mispronounced phonemes may be due to a disruption to the typical exchange of information between the prefrontal and auditory cortices as observed for canonical speech.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Language,Perception},
  file = {~/Zotfiles/blanco-elorrieta.e2021 Adaptation to mis-pronounced speech evi.pdf}
}

@article{blei.d:2017,
  title = {Variational Inference: A Review for Statisticians},
  shorttitle = {Variational Inference},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  urldate = {2022-06-29},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {~/Zotfiles/blei.d2017 Variational inference a review for stat.pdf}
}

@inproceedings{blevins.t:2018,
  title = {Deep {{RNNs}} Encode Soft Hierarchical Syntax},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  year = {2018},
  pages = {14--19},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-2003},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2003}
}

@article{bloch.i:1996,
  title = {Information Combination Operators for Data Fusion: A Comparative Review with Classification},
  shorttitle = {Information Combination Operators for Data Fusion},
  author = {Bloch, I.},
  year = {1996},
  month = jan,
  journal = {IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans},
  volume = {26},
  number = {1},
  pages = {52--67},
  issn = {1558-2426},
  doi = {10.1109/3468.477860},
  urldate = {2025-07-15},
  abstract = {In most data fusion systems, the information extracted from each sensor (either numerical or symbolic) is represented as a degree of belief in an event with real values, taking in this way into account the imprecise, uncertain, and incomplete nature of the information. The combination of such degrees of belief is performed through numerical fusion operators. A very large variety of such operators has been proposed in the literature. We propose in this paper a classification of these operators issued from the different data fusion theories with respect to their behavior. Three classes are thus defined. This classification provides a guide for choosing an operator in a given problem. This choice can then be refined from the desired properties of the operators, from their decisiveness, and by examining how they deal with conflictive situations.},
  keywords = {Data mining,ensembles,Fuzzy set theory,Image processing,Image sensors,Reconstruction algorithms,Roads,Sensor fusion,Sensor phenomena and characterization,Sensor systems,Uncertainty},
  file = {~/Zotfiles/bloch.i1996 Information combination operators for da.pdf}
}

@misc{bncconsortium:2007,
  type = {Corpus},
  title = {British {{National Corpus}}, {{XML}} Edition},
  author = {{BNC Consortium}},
  year = {2007},
  month = mar,
  publisher = {Oxford Text Archive},
  address = {ota:2554},
  urldate = {2023-12-18},
  abstract = {British National Corpus is a snapshot of British English in the early 1990s. The British National Corpus is (i) a sample corpus: composed of text samples generally no longer than 45,000 words; (ii) a synchronic corpus: the corpus includes imaginative texts from 1960-1990, informative texts from 1975-1990; (iii) a general corpus: not specifically restricted to any particular subject field, register or genre; (iv) a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English; (v) a mixed corpus: it contains examples of both spoken and written language. The corpus is described in full in the Users Reference Guide at http://www.natcorp.ox.ac.uk/docs/URG/. Some XSL files are available for reformatting the XML texts in various ways, also from the BNC web site.},
  copyright = {Distributed by the University of Oxford under the BNC User Licence. Clicking to download implies acceptance of the licence conditions.},
  langid = {english},
  file = {/Users/j/DATA/BNC_XML/download/welcome.htm}
}

@book{boas.r:1997book4,
  title = {A Primer of Real Functions},
  author = {Boas, Ralph P. and Boas, Harold P.},
  year = {1997},
  month = jan,
  series = {The {{Carus}} Mathematical Monographs},
  edition = {4},
  number = {no. 13},
  publisher = {Mathematical Association of America},
  address = {Washington, D.C.},
  isbn = {978-0-88385-029-9},
  lccn = {QA331.5 .B57 1996},
  keywords = {Functions of real variables}
}

@article{bobaljik.j:1996,
  title = {Subject Positions and the Roles of {{TP}}},
  author = {Bobaljik, Jonathan David and Jonas, Dianne},
  year = {1996},
  journal = {Linguistic Inquiry},
  volume = {27},
  number = {2},
  eprint = {4178934},
  eprinttype = {jstor},
  pages = {195--236},
  publisher = {The MIT Press},
  issn = {00243892, 15309150},
  abstract = {We propose that the specifier of a VP-external functional projection-Tense Phrase-may host subject NPs under certain conditions. We present empirical evidence that nonspecific subject NPs that have elsewhere been analyzed as remaining VP-internal occupy this position. We also offer theoretical arguments that transitive subjects may never remain internal to the VP at S-Structure in languages for which the Extended Projection Principle holds. Extending work by Bures (1992, 1993), we argue further that [Spec, TP] is implicated as a subject position in NP object shift constructions. Parametric availability of this one position accounts for a cluster of properties within the Germanic languages.},
  date-added = {2019-06-14 09:34:00 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {expletives,subject positions}
}

@article{bobaljik.j:2006,
  title = {Where's Phi},
  author = {Bobaljik, Jonathan David},
  year = {2006},
  journal = {Agreement as a postsyntactic operation. Ms. University of Connecticut},
  publisher = {Citeseer},
  date-added = {2020-02-02 22:23:49 -0500},
  date-modified = {2020-02-02 22:24:22 -0500},
  project = {Icelandic gluttony},
  keywords = {dative intervention,phi features}
}

@article{bochud.t:2007,
  title = {Optimal Approximations of Power Laws with Exponentials: Application to Volatility Models with Long Memory},
  shorttitle = {Optimal Approximations of Power Laws with Exponentials},
  author = {Bochud, Thierry and {and Challet}, Damien},
  year = {2007},
  month = dec,
  journal = {Quantitative Finance},
  volume = {7},
  number = {6},
  pages = {585--589},
  publisher = {Routledge},
  issn = {1469-7688},
  doi = {10.1080/14697680701278291},
  urldate = {2025-04-12},
  file = {~/Zotfiles/bochud.t2007 Optimal approximations of power laws wit.pdf}
}

@article{bock.k:1991,
  title = {Broken Agreement},
  author = {Bock, Kathryn and Miller, Carol A},
  year = {1991},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {23},
  number = {1},
  pages = {45--93},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(91)90003-7},
  urldate = {2023-04-03},
  abstract = {The subjects and verbs of English sentences agree in number. This superficially simple syntactic operation is regularly implemented by speakers, but occasionally derails in sentences such as The cost of the improvements have not yet been estimated. We examined whether the incidence of such errors was related to the presence of subject-like semantic features in the immediate preverbal nouns, in light of current questions about the semantic versus syntactic nature of sentence subjects and the interactivity of language processing. In three experiments, speakers completed sentence fragments designed to elicit erroneous agreement. We varied the number and animacy of the head noun and the immediate preverbal (local) noun, as well as the amount of material separating the head noun from the verb. The plurality of the local noun phrase had a large and reliable effect on the incidence of agreement errors, but neither its animacy nor its length affected their occurrence. The latter findings suggest, respectively, that the semantic features of sentence subjects are of minimal relevance to the syntactic and morphological processes that implement agreement, and that agreement features are specified at a point in processing where the eventual length of sentential constituents has little effect on syntactic planning. Both results follow naturally from explanations of language production that emphasize the segregation of sentence formulation processes into relatively autonomous components.},
  langid = {english},
  keywords = {agreement attraction}
}

@incollection{bod.r:2003,
  title = {Data-Oriented Parsing},
  booktitle = {Data-Oriented Parsing},
  editor = {Bod, Rens and Scha, Remko and Sima'an, Khalil},
  year = {2003},
  publisher = {CSLI},
  address = {Stanford, CA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{bod.r:2006,
  title = {An All-Subtrees Approach to Unsupervised Parsing},
  booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  author = {Bod, Rens},
  year = {2006},
  pages = {865--872},
  publisher = {Association for Computational Linguistics},
  address = {Sydney, Australia},
  doi = {10.3115/1220175.1220284},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220284}
}

@article{boeckx.c:2000,
  title = {Quirky Agreement},
  author = {Boeckx, Cedric},
  year = {2000},
  journal = {Studia linguistica},
  volume = {54},
  number = {3},
  pages = {354--380},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{bogacz.r:2017,
  title = {A Tutorial on the Free-Energy Framework for Modelling Perception and Learning},
  author = {Bogacz, Rafal},
  year = {2017},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  series = {Model-Based {{Cognitive Neuroscience}}},
  volume = {76},
  pages = {198--211},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2015.11.003},
  urldate = {2022-08-07},
  abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
  langid = {english},
  file = {~/Zotfiles/bogacz.r2017 A tutorial on the free-energy framework.pdf}
}

@article{bolliger.l:2025,
  title = {{{EMTeC}}: {{A}} Corpus of Eye Movements on Machine-Generated Texts},
  shorttitle = {{{EMTeC}}},
  author = {Bolliger, Lena S. and Haller, Patrick and Cretton, Isabelle C. R. and Reich, David R. and Kew, Tannon and J{\"a}ger, Lena A.},
  year = {2025},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {57},
  number = {7},
  pages = {189},
  issn = {1554-3528},
  doi = {10.3758/s13428-025-02677-4},
  urldate = {2025-06-11},
  abstract = {The Eye movements on Machine-generated Texts Corpus (EMTeC) is a naturalistic eye-movements-while-reading corpus of 107 native English speakers reading machine-generated texts. The texts are generated by three large language models using five different decoding strategies, and they fall into six different text-type categories. EMTeC entails the eye movement data at all stages of pre-processing, i.e., the raw coordinate data sampled at 2000~Hz, the fixation sequences, and the reading measures. It further provides both the original and a corrected version of the fixation sequences, accounting for vertical calibration drift. Moreover, the corpus includes the language models' internals that underlie the generation of the stimulus texts: the transition scores, the attention scores, and the hidden states. The stimuli are annotated for a range of linguistic features both at text and at word level. We anticipate EMTeC to be utilized for a variety of use cases such as, but not restricted to, the investigation of reading behavior on machine-generated text and the impact of different decoding strategies; reading behavior on different text types; the development of new pre-processing, data filtering, and drift correction algorithms; the cognitive interpretability and enhancement of language models; and the assessment of the predictive power of surprisal and entropy for human reading times. The data at all stages of pre-processing, the model internals, and the code to reproduce the stimulus generation, data pre-processing, and analyses can be accessed via https://github.com/DiLi-Lab/EMTeC/.},
  langid = {english},
  keywords = {Computational Linguistics,Corpus Linguistics,Decoding,ESCRT,Eye-tracking,Language Processing,Machine Translation,Machine-generated,Natural Language Processing (NLP),Reading},
  file = {~/Zotfiles/bolliger.l2025 EMTeC A corpus of eye movements on mach.pdf}
}

@misc{bommasani.r:2022arxiv,
  title = {On the Opportunities and Risks of Foundation Models},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Daniel and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2024-05-23},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {Note: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI)},
  file = {~/Zotfiles/bommasani.r2022arxiv On the opportunities and risks of founda.pdf}
}

@article{bonawitz.e:2014,
  title = {Probabilistic Models, Learning Algorithms, and Response Variability: Sampling in Cognitive Development},
  shorttitle = {Probabilistic Models, Learning Algorithms, and Response Variability},
  author = {Bonawitz, Elizabeth and Denison, Stephanie and Griffiths, Thomas L. and Gopnik, Alison},
  year = {2014},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {18},
  number = {10},
  pages = {497--500},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2014.06.006},
  urldate = {2025-03-06},
  langid = {english},
  pmid = {25001609},
  keywords = {approximate Bayesian inference,causal learning,cognitive development,sampling hypothesis,the sampling hypothesis},
  file = {~/Zotfiles/bonawitz.e2014 Probabilistic models, learning algorithm.pdf}
}

@phdthesis{bonet.m:1991,
  title = {Morphology after Syntax: {{Pronominal}} Clitics in {{Romance}}},
  author = {Bonet, M. Eul{\'a}lia},
  year = {1991},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:14:38 -0400},
  project = {Icelandic gluttony},
  school = {MIT},
  keywords = {clitics,hierarchy effects}
}

@misc{bornschein.j:2015arxiv,
  title = {Reweighted {{Wake-Sleep}}},
  author = {Bornschein, J{\"o}rg and Bengio, Yoshua},
  year = {2015},
  month = apr,
  number = {arXiv:1406.2751},
  eprint = {1406.2751},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1406.2751},
  urldate = {2025-07-31},
  abstract = {Training deep directed graphical models with many hidden variables and performing inference remains a major challenge. Helmholtz machines and deep belief networks are such models, and the wake-sleep algorithm has been proposed to train them. The wake-sleep algorithm relies on training not just the directed generative model but also a conditional generative model (the inference network) that runs backward from visible to latent, estimating the posterior distribution of latent given visible. We propose a novel interpretation of the wake-sleep algorithm which suggests that better estimators of the gradient can be obtained by sampling latent variables multiple times from the inference network. This view is based on importance sampling as an estimator of the likelihood, with the approximate inference network as a proposal distribution. This interpretation is confirmed experimentally, showing that better likelihood can be achieved with this reweighted wake-sleep procedure. Based on this interpretation, we propose that a sigmoidal belief network is not sufficiently powerful for the layers of the inference network in order to recover a good estimator of the posterior distribution of latent variables. Our experiments show that using a more powerful layer model, such as NADE, yields substantially better generative models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {~/Zotfiles/bornschein.j2015arxiv Reweighted Wake-Sleep.pdf}
}

@article{boston.m:2008,
  title = {Parsing Costs as Predictors of Reading Difficulty: {{An}} Evaluation Using the {{Potsdam Sentence Corpus}}},
  shorttitle = {Parsing Costs as Predictors of Reading Difficulty},
  author = {Boston, Marisa Ferrara and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
  year = {2008},
  month = sep,
  journal = {Journal of Eye Movement Research},
  volume = {2},
  number = {1},
  issn = {1995-8692},
  doi = {10.16910/jemr.2.1.1},
  urldate = {2022-10-13},
  abstract = {The surprisal of a word on a probabilistic grammar constitutes a promising complexity metric for human sentence comprehension difficulty. Using two different grammar types, surprisal is shown to have an effect on fixation durations and regression probabilities in a sample of German readers' eye movements, the Potsdam Sentence Corpus. A linear mixed-effects model was used to quantify the effect of surprisal while taking into account unigram frequency and bigram frequency (transitional probability), word length, and empirically-derived word predictability; the socalled ``early'' and ``late'' measures of processing difficulty both showed an effect of surprisal. Surprisal is also shown to have a small but statistically non-significant effect on empirically-derived predictability itself. This work thus demonstrates the importance of including parsing costs as a predictor of comprehension difficulty in models of reading, and suggests that a simple identification of syntactic parsing costs with early measures and late measures with durations of post-syntactic events may be difficult to uphold.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {parsing costs,parsing difficulty,potsdam sentence corpus,surprisal},
  file = {~/Zotfiles/boston.m2008 Parsing costs as predictors of reading d.pdf}
}

@incollection{boston.m:2009,
  title = {Dependency Structures Derived from Minimalist Grammars},
  booktitle = {The Mathematics of Language},
  author = {Boston, Marisa Ferrara and Hale, John T. and Kuhlmann, Marco},
  year = {2009},
  pages = {1--12},
  publisher = {Springer},
  date-added = {2019-06-15 11:31:22 -0400},
  date-modified = {2022-04-20 13:49:51 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,minimalist grammars}
}

@inproceedings{bouchard-cote.a:2009,
  title = {Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Bouchard-C{\^o}t{\'e}}, Alexandre and Petrov, Slav and Klein, Dan},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  date-added = {2022-03-31 10:05:14 -0400},
  date-modified = {2022-03-31 22:30:35 -0400},
  keywords = {markov chain monte carlo,pruning},
  file = {~/Zotfiles/bouchard-cote.a2009 Randomized pruning efficiently calculat.pdf}
}

@inproceedings{bouma.g:2009,
  title = {Normalized (Pointwise) Mutual Information in Collocation Extraction},
  booktitle = {Von Der {{Form}} Zur {{Bedeutung}}: {{Texte}} Automatisch Verarbeiten / {{From Form}} to {{Meaning}}: {{Processing Texts Automatically}}},
  author = {Bouma, Gerlof},
  editor = {Chiarcos, Christian and {de Castilho}, Richard Eckart and Stede, Manfred},
  year = {2009},
  month = sep,
  pages = {43--53},
  publisher = {Narr Francke Attempto Verlag GmbH + Co. KG},
  address = {T{\"u}bingen},
  abstract = {In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.},
  langid = {english},
  file = {~/Zotfiles/bouma.g2009npmi Normalized (pointwise) mutual informatio.pdf}
}

@inproceedings{bousquet.o:2021,
  title = {A Theory of Universal Learning},
  booktitle = {Proceedings of the 53rd {{Annual ACM SIGACT Symposium}} on {{Theory}} of {{Computing}}},
  author = {Bousquet, Olivier and Hanneke, Steve and Moran, Shay and Van Handel, Ramon and Yehudayoff, Amir},
  year = {2021},
  month = jun,
  pages = {532--541},
  publisher = {ACM},
  address = {Virtual Italy},
  doi = {10.1145/3406325.3451087},
  urldate = {2025-04-12},
  isbn = {978-1-4503-8053-9},
  langid = {english},
  file = {~/Zotfiles/bousquet.o2021 A theory of universal learning.pdf}
}

@misc{boyce.v:2018HSP,
  type = {Poster},
  title = {Implicit Gender Biases in the Production and Comprehension of Pronominal References},
  author = {Boyce, Veronica and {von der Malsburg}, Titus and Poppels, Till and Levy, Roger},
  year = {2018},
  urldate = {2024-12-21}
}

@misc{boyce.v:2019poster,
  type = {Poster},
  title = {Female Gender Is Consistently Under-Expressed in Pronoun Production and under-Inferred in Comprehension},
  author = {Boyce, Veronica and {von der Malsburg}, Titus and Poppels, Till and Levy, Roger and Pancheva, R. and Iskarous, K.},
  year = {2019},
  urldate = {2024-12-21},
  file = {~/Zotfiles/boyce.v2019LSA Female gender is consistently under-expr.pdf}
}

@misc{boyce.v:2020,
  type = {Talk},
  title = {A-Maze of {{Natural Stories}}: Texts Are Comprehensible Using the {{Maze}} Task},
  author = {Boyce, Veronica and Levy, Roger},
  year = {2020},
  month = sep,
  address = {Potsdam, Germany},
  collaborator = {{von der Malsburg}, Titus and Vasishth, Shravan and Wartenburger, Isabell}
}

@article{boyce.v:2020a,
  title = {Maze Made Easy: Better and Easier Measurement of Incremental Processing Difficulty},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger},
  year = {2020},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {111},
  pages = {104082},
  publisher = {Elsevier BV},
  doi = {10.1016/j.jml.2019.104082},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2019.104082},
  date-added = {2021-09-30 07:52:16 -0400},
  date-modified = {2022-04-20 13:48:24 -0400},
  file = {~/Zotfiles/boyce.v2020jml Maze made easy better and easier measur.pdf}
}

@misc{boyce.v:2022,
  title = {Amaze-Natural-Stories},
  author = {Boyce, Veronica},
  year = {2022},
  month = mar,
  urldate = {2022-09-24},
  abstract = {Materials, data, code for A-maze of Natural Stories talk},
  keywords = {amlap}
}

@misc{boyce.v:2022psyarxiv,
  title = {A-Maze of {{Natural Stories}}: Comprehension and Surprisal in the Maze Task},
  shorttitle = {A-Maze of Natural Stories},
  author = {Boyce, Veronica and Levy, Roger Philip},
  year = {2022},
  month = aug,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/8xkrf},
  urldate = {2023-03-01},
  abstract = {Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task. Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, with little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task.},
  langid = {american},
  keywords = {A-Maze,incremental processing,Linguistics,naturalistic text,Psycholinguistics and Neurolinguistics,self-paced reading,Social and Behavioral Sciences,surprisal}
}

@article{boyce.v:2023,
  title = {A-Maze of {{Natural Stories}}: Comprehension and Surprisal in the {{Maze}} Task},
  shorttitle = {A-Maze of Natural Stories},
  author = {Boyce, Veronica and Levy, Roger},
  year = {2023},
  month = apr,
  journal = {Glossa Psycholinguistics},
  volume = {2},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G6011190},
  urldate = {2023-08-02},
  abstract = {Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task. Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, with little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task.}
}

@book{brasoveanu.a:2020book,
  title = {Computational Cognitive Modeling and Linguistic Theory},
  author = {Brasoveanu, Adrian and Dotla{\v c}il, Jakub},
  year = {2020},
  publisher = {Springer Nature},
  doi = {10.1007/978-3-030-31846-8},
  urldate = {2022-09-08},
  abstract = {This open access book introduces a general framework that allows natural language researchers to enhance existing competence theories with fully specified performance and processing components. Gradually developing increasingly complex and cognitively realistic competence-performance models, it provides running code for these models and shows how to fit them to real-time experimental data. This computational cognitive modeling approach opens up exciting new directions for research in formal semantics, and linguistics more generally, and offers new ways of (re)connecting semantics and the broader field of cognitive science. The approach of this book is novel in more ways than one. Assuming the mental architecture and procedural modalities of Anderson's ACT-R framework, it presents fine-grained computational models of human language processing tasks which make detailed quantitative predictions that can be checked against the results of self-paced reading and other psycho-linguistic experiments. All models are presented as computer programs that readers can run on their own computer and on inputs of their choice, thereby learning to design, program and run their own models. But even for readers who won't do all that, the book will show how such detailed, quantitatively predicting modeling of linguistic processes is possible. A methodological breakthrough and a must for anyone concerned about the future of linguistics! (Hans Kamp) This book constitutes a major step forward in linguistics and psycholinguistics. It constitutes a unique synthesis of several different research traditions: computational models of psycholinguistic processes, and formal models of semantics and discourse processing. The work also introduces a sophisticated python-based software environment for modeling linguistic processes. This book has the potential to revolutionize not only formal models of linguistics, but also models of language processing more generally. (Shravan Vasishth)},
  isbn = {978-3-030-31846-8},
  langid = {english},
  keywords = {ACT-R Based Left-corner Parser,bic Book Industry Communication::C Language::CF linguistics::CFA Philosophy of language,bic Book Industry Communication::C Language::CF linguistics::CFD Psycholinguistics,bic Book Industry Communication::C Language::CF linguistics::CFG Semantics,Cataphoric Presupposition Resolution,Cognitive Aspects of Processing Semantic Representations,discourse analysis,Enriched Semantics,etc,Incremental Dynamic Predicate Logic,Language Interpretation Processes,Linguistics,Meaning Representations in Formal Semantics,Natural Language Processing,Open Access,Philosophy of language,Philosophy of Language,Processing Enriched Logical Forms,Processing of Lexical Semantic and Syntactic Representations,Psycholinguistics,Psycholinguistics and Cognitive Lingusitics,Psycholinguistics on Incremental Interpretation,Real-time Construction of Syntactic Representations,Real-time Semantic Interpretation,Semantics,Semantics and Processing,stylistics},
  file = {~/Zotfiles/brasoveanu.a2020 Computational cognitive modeling and lin.pdf}
}

@article{braun.d:2014,
  title = {Information-Theoretic Bounded Rationality and {$\varepsilon$}-Optimality},
  author = {Braun, Daniel A. and Ortega, Pedro A.},
  year = {2014},
  month = aug,
  journal = {Entropy},
  volume = {16},
  number = {8},
  pages = {4662--4676},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e16084662},
  urldate = {2022-06-08},
  abstract = {Bounded rationality concerns the study of decision makers with limited information processing resources. Previously, the free energy difference functional has been suggested to model bounded rational decision making, as it provides a natural trade-off between an energy or utility function that is to be optimized and information processing costs that are measured by entropic search costs. The main question of this article is how the information-theoretic free energy model relates to simple {$\varepsilon$}-optimality models of bounded rational decision making, where the decision maker is satisfied with any action in an {$\varepsilon$}-neighborhood of the optimal utility. We find that the stochastic policies that optimize the free energy trade-off comply with the notion of {$\varepsilon$}-optimality. Moreover, this optimality criterion even holds when the environment is adversarial. We conclude that the study of bounded rationality based on {$\varepsilon$}-optimality criteria that abstract away from the particulars of the information processing constraints is compatible with the information-theoretic free energy model of bounded rationality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {-optimality,ambiguity,bounded rationality,decision theory,information theory,probabilistic choice},
  file = {~/Zotfiles/braun.d2014 Information-theoretic bounded rationalit.pdf}
}

@inproceedings{brekelmans.r:2022,
  title = {Improving Mutual Information Estimation with Annealed and Energy-Based Bounds},
  booktitle = {International Conference on Learning Representations},
  author = {Brekelmans, Rob and Huang, Sicong and Ghassemi, Marzyeh and Steeg, Greg Ver and Grosse, Roger Baker and Makhzani, Alireza},
  year = {2022},
  file = {~/Zotfiles/brekelmans.r2022 Improving mutual information estimation.pdf}
}

@book{bresnan.j:2001book,
  title = {Lexical-Functional Syntax},
  author = {Bresnan, Joan},
  year = {2001},
  publisher = {Wiley-Blackwell},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{brodbeck.c:2022,
  title = {Parallel Processing in Speech Perception with Local and Global Representations of Linguistic Context},
  author = {Brodbeck, Christian and Bhattasali, Shohini and Cruz Heredia, Aura AL and Resnik, Philip and Simon, Jonathan Z and Lau, Ellen},
  editor = {{van Wassenhove}, Virginie and {Shinn-Cunningham}, Barbara G and Brennan, Jonathan},
  year = {2022},
  month = jan,
  journal = {eLife},
  volume = {11},
  pages = {e72056},
  publisher = {eLife Sciences Publications, Ltd},
  issn = {2050-084X},
  doi = {10.7554/eLife.72056},
  urldate = {2025-04-01},
  abstract = {Speech processing is highly incremental. It is widely accepted that human listeners continuously use the linguistic context to anticipate upcoming concepts, words, and phonemes. However, previous evidence supports two seemingly contradictory models of how a predictive context is integrated with the bottom-up sensory input: Classic psycholinguistic paradigms suggest a two-stage process, in which acoustic input initially leads to local, context-independent representations, which are then quickly integrated with contextual constraints. This contrasts with the view that the brain constructs a single coherent, unified interpretation of the input, which fully integrates available information across representational hierarchies, and thus uses contextual constraints to modulate even the earliest sensory representations. To distinguish these hypotheses, we tested magnetoencephalography responses to continuous narrative speech for signatures of local and unified predictive models. Results provide evidence that listeners employ both types of models in parallel. Two local context models uniquely predict some part of early neural responses, one based on sublexical phoneme sequences, and one based on the phonemes in the current word alone; at the same time, even early responses to phonemes also reflect a unified model that incorporates sentence-level constraints to predict upcoming phonemes. Neural source localization places the anatomical origins of the different predictive models in nonidentical parts of the superior temporal lobes bilaterally, with the right hemisphere showing a relative preference for more local models. These results suggest that speech processing recruits both local and unified predictive models in parallel, reconciling previous disparate findings. Parallel models might make the perceptual system more robust, facilitate processing of unexpected inputs, and serve a function in language acquisition.},
  keywords = {entropy,MEG,predictive coding,surprisal,surprisal theory,temporal response functions},
  file = {~/Zotfiles/brodbeck.c2022 Parallel processing in speech perception.pdf}
}

@inproceedings{brody.s:2010,
  title = {It Depends on the Translation: {{Unsupervised}} Dependency Parsing via Word Alignment},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  author = {Brody, Samuel},
  year = {2010},
  pages = {1214--1222},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, MA}
}

@article{brothers.t:2021,
  title = {Word Predictability Effects Are Linear, Not Logarithmic: Implications for Probabilistic Models of Sentence Comprehension},
  author = {Brothers, Trevor and Kuperberg, Gina R.},
  year = {2021},
  journal = {Journal of Memory and Language},
  volume = {116},
  pages = {104174},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104174},
  abstract = {During language comprehension, we routinely use information from the prior context to help identify the meaning of individual words. While measures of online processing difficulty, such as reading times, are strongly influenced by contextual predictability, there is disagreement about the mechanisms underlying this lexical predictability effect, with different models predicting different linking functions -- linear (Reichle, Rayner, \& Pollatsek, 2003) or logarithmic (Levy, 2008). To help resolve this debate, we conducted two highly-powered experiments (self-paced reading, N = 216; cross-modal picture naming, N = 36), and a meta-analysis of prior eye-tracking while reading studies (total N = 218). We observed a robust linear relationship between lexical predictability and word processing times across all three studies. Beyond their methodological implications, these findings also place important constraints on predictive processing models of language comprehension. In particular, these results directly contradict the empirical predictions of surprisal theory, while supporting a proportional pre-activation account of lexical prediction effects in comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2020.104174},
  date-added = {2021-03-09 22:52:14 -0500},
  date-modified = {2021-11-14 23:57:23 -0500},
  keywords = {Information theory,Language comprehension,Prediction,Psycholinguistics,Reading,surprisal},
  file = {~/Zotfiles/brothers.t2021 Word predictability effects are linear,.pdf}
}

@article{brouwer.h:2021,
  title = {Neurobehavioral {{Correlates}} of {{Surprisal}} in {{Language Comprehension}}: {{A Neurocomputational Model}}},
  shorttitle = {Neurobehavioral {{Correlates}} of {{Surprisal}} in {{Language Comprehension}}},
  author = {Brouwer, Harm and Delogu, Francesca and Venhuizen, Noortje J. and Crocker, Matthew W.},
  year = {2021},
  month = feb,
  journal = {Frontiers in Psychology},
  volume = {12},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2021.615538},
  urldate = {2025-04-03},
  abstract = {{$<$}p{$>$}Expectation-based theories of language comprehension, in particular Surprisal Theory, go a long way in accounting for the behavioral correlates of word-by-word processing difficulty, such as reading times. An open question, however, is in which component(s) of the Event-Related brain Potential (ERP) signal Surprisal is reflected, and how these electrophysiological correlates relate to behavioral processing indices. Here, we address this question by instantiating an explicit neurocomputational model of incremental, word-by-word language comprehension that produces estimates of the N400 and the P600---the two most salient ERP components for language processing---as well as estimates of ``comprehension-centric'' Surprisal for each word in a sentence. We derive model predictions for a recent experimental design that directly investigates ``world-knowledge''-induced Surprisal. By relating these predictions to both empirical electrophysiological and behavioral results, we establish a close link between Surprisal, as indexed by reading times, and the P600 component of the ERP signal. The resultant model thus offers an integrated neurobehavioral account of processing difficulty in language comprehension.{$<$}/p{$>$}},
  langid = {english},
  keywords = {event-related potentials (ERPs),Language comprehension,N400,P600,Surprisal theory},
  file = {~/Zotfiles/brouwer.h2021 Neurobehavioral Correlates of Surprisal.pdf}
}

@article{brown.p:1993,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
  year = {1993},
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {263--311}
}

@inproceedings{brown.t:2020GPT3,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  date-modified = {2022-04-30 13:50:00 -0400},
  keywords = {GPT,GPT3},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100}
}

@article{bruening.b:2012,
  title = {{\emph{By}} Phrases in Passives and Nominals},
  author = {Bruening, Benjamin},
  year = {2012},
  journal = {Syntax (Oxford, England)},
  volume = {16},
  number = {1},
  pages = {1--41},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9612.2012.00171.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2012.00171.x},
  date-added = {2021-03-22 13:11:47 -0400},
  date-modified = {2021-03-24 11:10:53 -0400}
}

@unpublished{bruening.b:2015ms,
  title = {Idioms: {{Movement}} and Non-Movement Dependencies},
  author = {Bruening, Benjamin},
  year = {2015},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {Idioms}
}

@misc{bubeck.s:2023arxiv,
  title = {Sparks of Artificial General Intelligence: Early Experiments with {{GPT-4}}},
  shorttitle = {Sparks of Artificial General Intelligence},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2024-05-23},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/bubeck.s2023arxiv Sparks of artificial general intelligenc.pdf}
}

@article{buckley.c:2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  shorttitle = {The Free Energy Principle for Action and Perception},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  year = {2017},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {55--79},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.004},
  urldate = {2022-07-27},
  abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  langid = {english},
  keywords = {Action,Agent-based model,Bayesian brain,free energy principle,Free energy principle,Inference,Perception},
  file = {~/Zotfiles/buckley.c2017 The free energy principle for action and.pdf}
}

@inproceedings{bunescu.r:2022,
  title = {Distribution-{{Based Measures}} of {{Surprise}} for {{Creative Language}}: {{Experiments}} with {{Humor}} and {{Metaphor}}},
  shorttitle = {Distribution-{{Based Measures}} of {{Surprise}} for {{Creative Language}}},
  booktitle = {Proceedings of the 3rd {{Workshop}} on {{Figurative Language Processing}} ({{FLP}})},
  author = {Bunescu, Razvan C. and Uduehi, Oseremen O.},
  editor = {Ghosh, Debanjan and Beigman Klebanov, Beata and Muresan, Smaranda and Feldman, Anna and Poria, Soujanya and Chakrabarty, Tuhin},
  year = {2022},
  month = dec,
  pages = {68--78},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates (Hybrid)},
  doi = {10.18653/v1/2022.flp-1.10},
  urldate = {2025-06-11},
  abstract = {Novelty or surprise is a fundamental attribute of creative output. As such, we postulate that a writer's creative use of language leads to word choices and, more importantly, corresponding semantic structures that are unexpected for the reader. In this paper we investigate measures of surprise that rely solely on word distributions computed by language models and show empirically that creative language such as humor and metaphor is strongly correlated with surprise. Surprisingly at first, information content is observed to be at least as good a predictor of creative language as any of the surprise measures investigated. However, the best prediction performance is obtained when information and surprise measures are combined, showing that surprise measures capture an aspect of creative language that goes beyond information content.},
  file = {~/Zotfiles/bunescu.r2022 Distribution-Based Measures of Surprise.pdf}
}

@article{burchill.z:2024,
  title = {How Reliable Are Standard Reading Time Analyses? {{Hierarchical}} Bootstrap Reveals Substantial Power over-Optimism and Scale-Dependent {{Type I}} Error Inflation},
  shorttitle = {How Reliable Are Standard Reading Time Analyses?},
  author = {Burchill, Zachary J. and Jaeger, T. Florian},
  year = {2024},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {136},
  pages = {104494},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2023.104494},
  urldate = {2024-02-04},
  abstract = {We investigate the statistical power and Type I error rate of the two most common approaches to reading time (RT) analyses: assuming normality of residuals and homogeneity of variance in raw or log-transformed RTs. We first show that the assumptions of such analyses---such as t-tests, ANOVAs, and linear mixed-effects models---are neither consistently met by raw RTs, nor by log-transformed RTs (or any other common power transforms, incl. inverse-transformed RTs). Only a non-power transform (log-shift) provides a decent fit for all data sets and data preparation steps we consider. We then compare the statistical power and Type I error rate for linear mixed-effects models over raw or log-transformed RTs. Previous studies on this matter relied on parametrically generated data. We show why this is problematic, and introduce as an alternative a hierarchical bootstrap approach over naturally distributed reading times. This approach yields substantially different---and arguably more informative---results than the parametric simulation approaches we compare it to. Our results suggests that it is time to heed the advice others have provided for reading research: for any but the simplest designs, we find both the rate of spurious significances and the rate of undetected true effects can strongly depend on the scale (e.g., raw or log-RTs) in which effects are assumed to be linear. Researchers should thus clearly motivate the choice of analysis based on theoretical grounds, assess the robustness of findings under different analysis approaches, and discuss potential mismatches between analyses. The R scripts and libraries shared in the accompanying OSF repo allow researchers to assess the reliability of their analyses via hierarchical bootstrap over their own data.},
  keywords = {Data analysis,Hierarchical bootstrap,Power,Reading times,Type I error},
  file = {~/Zotfiles/burchill.z2024 How reliable are standard reading time a.pdf}
}

@misc{burda.y:2016arxiv,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2016},
  month = nov,
  number = {arXiv:1509.00519},
  eprint = {1509.00519},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1509.00519},
  urldate = {2025-07-31},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {~/Zotfiles/burda.y2016arxiv Importance Weighted Autoencoders.pdf}
}

@article{burkner.p:2017,
  title = {{{{\textbf{brms}}}}: An {{R}} Package for {{Bayesian}} Multilevel Models Using {{Stan}}},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8}
}

@book{burnham.k:2004book,
  title = {Model {{Selection}} and {{Multimodel Inference}}},
  editor = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/b97636},
  urldate = {2024-03-14},
  isbn = {978-0-387-95364-9},
  langid = {english},
  keywords = {data analysis,Estimator,Inference,information theory,Likelihood,Model Selection},
  file = {~/Zotfiles/burnham.k2004book Model Selection and Multimodel Inference.pdf}
}

@inproceedings{buys.j:2015,
  title = {A {{Bayesian}} Model for Generative Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015)},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = aug,
  pages = {58--67},
  publisher = {Uppsala University, Uppsala, Sweden},
  address = {Uppsala, Sweden},
  date-added = {2022-04-25 20:09:07 -0400},
  date-modified = {2022-04-25 20:09:41 -0400},
  keywords = {bayesian,dependency parsing},
  file = {~/Zotfiles/buys.j2015 A Bayesian model for generative transiti.pdf}
}

@inproceedings{buys.j:2015short,
  title = {Generative {{Incremental Dependency Parsing}} with {{Neural Networks}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = jul,
  pages = {863--869},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-2142},
  urldate = {2022-06-14},
  file = {~/Zotfiles/buys.j2015short Generative Incremental Dependency Parsin.pdf}
}

@phdthesis{buys.j:2018phd,
  title = {Incremental Generative Models for Syntactic and Semantic Natural Language Processing},
  author = {Buys, Jan},
  year = {2018},
  urldate = {2022-06-14},
  abstract = {{$<$}p{$>$}This thesis investigates the role of linguistically-motivated generative models of syntax and semantic structure in natural language processing (NLP). Syntactic well-formedness is crucial in language generation, but most statistical models do not account for the hierarchical structure of sentences. Many applications exhibiting natural language understanding rely on structured semantic representations to enable querying, inference and reasoning. Yet most semantic parsers produce domain-specific or inadequately expressive representations.{$<$}/p{$>$} {$<$}p{$>$}We propose a series of generative transition-based models for dependency syntax which can be applied as both parsers and language models while being amenable to supervised or unsupervised learning. Two models are based on Markov assumptions commonly made in NLP: The first is a Bayesian model with hierarchical smoothing, the second is parameterised by feed-forward neural networks. The Bayesian model enables careful analysis of the structure of the conditioning contexts required for generative parsers, but the neural network is more accurate. As a language model the syntactic neural model outperforms both the Bayesian model and n-gram neural networks, pointing to the complementary nature of distributed and structured representations for syntactic prediction. We propose approximate inference methods based on particle filtering. The third model is parameterised by recurrent neural networks (RNNs), dropping the Markov assumptions. Exact inference with dynamic programming is made tractable here by simplifying the structure of the conditioning contexts.{$<$}/p{$>$} {$<$}p{$>$}We then shift the focus to semantics and propose models for parsing sentences to labelled semantic graphs. We introduce a transition-based parser which incrementally predicts graph nodes (predicates) and edges (arguments). This approach is contrasted against predicting top-down graph traversals. RNNs and pointer networks are key components in approaching graph parsing as an incremental prediction problem. The RNN architecture is augmented to condition the model explicitly on the transition system configuration. We develop a robust parser for Minimal Recursion Semantics, a linguistically-expressive framework for compositional semantics which has previously been parsed only with grammar-based approaches. Our parser is much faster than the grammar-based model, while the same approach improves the accuracy of neural Abstract Meaning Representation parsing.{$<$}/p{$>$}},
  langid = {english},
  school = {University of Oxford},
  file = {~/Zotfiles/buys.j2018phd Incremental generative models for syntac.pdf}
}

@inproceedings{cai.w:2014,
  title = {Making Comparisons Fair: How {{LS-means}} Unify the Analysis of Linear Models},
  booktitle = {Proceedings of the {{SAS Global Forum}}},
  author = {Cai, Weijie},
  year = {2014},
  volume = {Paper SAS060-2014},
  pages = {1--22},
  publisher = {SAS Institute Inc.},
  address = {Washington, D.C.}
}

@article{cai.z:2022,
  title = {How Do People Interpret Implausible Sentences?},
  author = {Cai, Zhenguang G. and Zhao, Nan and Pickering, Martin J.},
  year = {2022},
  month = aug,
  journal = {Cognition},
  volume = {225},
  pages = {105101},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105101},
  urldate = {2025-02-03},
  abstract = {People sometimes interpret implausible sentences nonliterally, for example treating The mother gave the candle the daughter as meaning the daughter receiving the candle. But how do they do so? We contrasted a nonliteral syntactic analysis account, according to which people compute a syntactic analysis appropriate for this nonliteral meaning, with a nonliteral semantic interpretation account, according to which they arrive at this meaning via purely semantic processing. The former but not the latter account postulates that people consider not only a literal-but-implausible double-object (DO) analysis in comprehending The mother gave the candle the daughter, but also a nonliteral-but-plausible prepositional-object (PO) analysis (i.e., including to before the daughter). In three structural priming experiments, participants heard a plausible or implausible DO or PO prime sentence. They then answered a comprehension question first or described a picture of a dative event first. In accord with the nonliteral syntactic analysis account, priming was reduced following implausible sentences than following plausible sentences and following nonliterally interpreted implausible sentences than literally interpreted implausible sentences. The results suggest that comprehenders constructed a nonliteral syntactic analysis, which we argue was predicted early in the sentence.},
  keywords = {Implausible sentences,Semantic interpretation,Structural priming,Syntactic analysis,Syntactic prediction},
  file = {~/Zotfiles/cai.z2022 How do people interpret implausible sent.pdf}
}

@inproceedings{cao.k:2021,
  title = {You Should Evaluate Your Language Model on Marginal Likelihood over Tokenisations},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Cao, Kris and Rimell, Laura},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {2104--2114},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.161},
  urldate = {2024-02-25},
  abstract = {Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.},
  file = {~/Zotfiles/cao.k2021 You should evaluate your language model.pdf}
}

@misc{cao.q:2023arxiv,
  title = {Unnatural {{Error Correction}}: {{GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text}}},
  shorttitle = {Unnatural {{Error Correction}}},
  author = {Cao, Qi and Kojima, Takeshi and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2023},
  month = nov,
  number = {arXiv:2311.18805},
  eprint = {2311.18805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-07},
  abstract = {While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95\%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/cao.q2023arxiv Unnatural Error Correction GPT-4 Can Al.pdf}
}

@article{cappe.o:2007,
  title = {An Overview of Existing Methods and Recent Advances in Sequential {{Monte Carlo}}},
  author = {Cappe, Olivier and Godsill, Simon J. and Moulines, Eric},
  year = {2007},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/jproc.2007.893250},
  bdsk-url-2 = {https://doi.org/10.1109/jproc.2007.893250},
  date-added = {2022-03-25 11:26:52 -0400},
  date-modified = {2022-03-25 11:26:52 -0400}
}

@article{carpenter.b:2017,
  title = {{\textbf{Stan}}: A Probabilistic Programming Language},
  shorttitle = {{\emph{Stan}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  urldate = {2024-05-21},
  langid = {english},
  file = {~/Zotfiles/carpenter.b2017Stan Stan a probabilistic programming.pdf}
}

@article{carpenter.r:1995,
  title = {Neural Computation of Log Likelihood in Control of Saccadic Eye Movements},
  author = {Carpenter, R. H. S. and Williams, M. L. L.},
  year = {1995},
  month = sep,
  journal = {Nature},
  volume = {377},
  number = {6544},
  pages = {59--62},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/377059a0},
  urldate = {2022-06-24},
  abstract = {THE latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals1. In one such model2, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s0 until it reaches a fixed threshold 0, when a saccade is initiated. One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present. Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if s0 simply reflects the prior log likelihood of the stimulus.},
  copyright = {1995 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {~/Zotfiles/carpenter.r1995 Neural computation of log likelihood in.pdf}
}

@article{carreiras.m:1997,
  title = {Effects of the Orthographic Neighborhood in Visual Word Recognition: {{Cross-task}} Comparisons},
  shorttitle = {Effects of the Orthographic Neighborhood in Visual Word Recognition},
  author = {Carreiras, Manuel and Perea, Manuel and Grainger, Jonathan},
  year = {1997},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {23},
  number = {4},
  pages = {857--871},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285},
  doi = {10.1037/0278-7393.23.4.857},
  abstract = {Effects of orthographic neighborhood in visual word recognition in Spanish were examined in 5 paradigms: progressive demasking, standard lexical decision, lexical decision with blocking of neighborhood density, naming, and semantic categorization. The results showed inhibitory effects of neighborhood frequency in the progressive-demasking task, in both lexical-decision tasks, as well as for low-density words in the naming task, and for high-density words in the semantic-categorization task. Higher levels of neighborhood density produced an inhibitory trend in the progressive-demasking task, facilitation in lexical decision (significant only when neighborhood density was blocked), and a robust facilitation effect in naming (only for words with higher frequency neighbors). A global analysis across tasks and one simulation study helped outline some of the underlying task-specific and task-independent mechanisms. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Classification (Cognitive Process),Lexical Decision,Naming,Orthography,Semantics,Visual Masking,Word Recognition},
  file = {~/Zotfiles/carreiras.m1997 Effects of the orthographic neighborhood.pdf}
}

@techreport{carroll.g:1992,
  type = {{{AAAI}} Technical Report},
  title = {Two Experiments on Learning Probabilistic Dependency Grammars from Corpora},
  author = {Carroll, Glenn and Charniak, Eugene},
  year = {1992},
  journal = {Working notes of the AAAI workshop statistically-based NLP techniques},
  number = {WS-92-01},
  pages = {1--13},
  institution = {AAAI},
  abstract = {We present a scheme for learning prohabilistic dependency grammars from positive training examples plus constraints on rules. In particular we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.},
  date-added = {2021-04-18 10:28:37 -0400},
  date-modified = {2021-04-18 10:33:11 -0400},
  project = {syntactic embedding},
  keywords = {Dependency Grammar,dependency parsing,dependency structures,mutual information,pmi,unsupervised parsing}
}

@article{casadio.c:2002,
  title = {A Tale of Four Grammars},
  author = {Casadio, Claudia and Lambek, Joachim},
  year = {2002},
  journal = {Studia Logica. An International Journal for Symbolic Logic},
  volume = {71},
  number = {3},
  pages = {315--329},
  publisher = {Springer},
  date-added = {2019-10-25 23:52:18 -0400},
  date-modified = {2019-10-25 23:54:15 -0400},
  keywords = {bilinear logic,category theory,pregroup grammar}
}

@article{chalmers.d:1994,
  title = {On Implementing a Computation},
  author = {Chalmers, David J.},
  year = {1994},
  month = nov,
  journal = {Minds and Machines},
  volume = {4},
  number = {4},
  pages = {391--402},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/BF00974166},
  urldate = {2023-05-26},
  langid = {english}
}

@article{chaloner.k:1995,
  title = {Bayesian Experimental Design: A Review},
  author = {Chaloner, Kathryn and Verdinelli, Isabella},
  year = {1995},
  journal = {Statistical Science},
  volume = {10},
  number = {3},
  eprint = {2246015},
  eprinttype = {jstor},
  pages = {273--304},
  publisher = {Institute of Mathematical Statistics},
  issn = {08834237},
  abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision-theoretic approach. This framework casts criteria from the Bayesian literature of design as part of a single coherent approach. The decision-theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility function are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion.},
  date-added = {2021-09-15 10:23:51 -0400},
  date-modified = {2021-09-15 10:23:53 -0400}
}

@misc{chandra.k:2025psyarxiv,
  title = {A {{Domain-Specific Probabilistic Programming Language}} for {{Reasoning About Reasoning}} (or: A Memo on Memo)},
  shorttitle = {A {{Domain-Specific Probabilistic Programming Language}} for {{Reasoning About Reasoning}}},
  author = {Chandra, Kartik and Chen, Tony and Tenenbaum, Joshua and {Ragan-Kelley}, Jonathan},
  year = {2025},
  month = jan,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/pt863},
  urldate = {2025-04-04},
  abstract = {The human ability to think about thinking ("theory of mind") is a fundamental object of study in many disciplines. In recent decades, researchers across these disciplines have converged on a rich computational paradigm for modeling theory of mind, grounded in recursive probabilistic reasoning. However, practitioners often find programming in this paradigm extremely challenging: first, because thinking-about-thinking is confusing for programmers, and second, because models are extremely slow to run. This paper presents memo, a new domain-specific probabilistic programming language that overcomes these challenges: first, by providing specialized syntax and semantics for theory of mind, and second, by taking a unique approach to inference that scales well on modern hardware via array programming. memo enables practitioners to write dramatically faster models with much less code, and has already been adopted by several research groups.},
  langid = {american},
  keywords = {bayesian modeling,inverse planning,probabilistic programming,rational speech act,rational speech acts,RSA,theory of mind},
  file = {~/Zotfiles/chandra.k2025psyarxiv A Domain-Specific Probabilistic Programm.pdf}
}

@inproceedings{chang.h:2022,
  title = {Softmax {{Bottleneck Makes Language Models Unable}} to {{Represent Multi-mode Word Distributions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chang, Haw-Shiuan and McCallum, Andrew},
  year = {2022},
  month = may,
  pages = {8048--8073},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.554},
  urldate = {2022-06-23},
  abstract = {Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.},
  file = {~/Zotfiles/chang.h2022 Softmax Bottleneck Makes Language Models.pdf}
}

@article{chang.j:1997,
  title = {Conditioning as Disintegration},
  author = {Chang, Joseph T and Pollard, David},
  year = {1997},
  journal = {Statistica Neerlandica},
  volume = {51},
  number = {3},
  pages = {287--317},
  publisher = {Wiley Online Library},
  doi = {10.1111/1467-9574.00056},
  date-added = {2021-02-05 12:20:15 -0500},
  date-modified = {2021-02-05 12:22:28 -0500},
  keywords = {probability theory}
}

@article{chang.y:2024,
  title = {A Survey on Evaluation of Large Language Models},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  year = {2024},
  month = jun,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {15},
  number = {3},
  pages = {1--45},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3641289},
  urldate = {2024-05-23},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions:               what to evaluate               ,               where to evaluate               , and               how to evaluate               . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:               https://github.com/MLGroupJLU/LLM-eval-survey},
  langid = {english},
  file = {~/Zotfiles/chang.y2024 A survey on evaluation of large language.pdf}
}

@incollection{chater.n:1998,
  title = {The Rational Analysis of Inquiry: The Case of Parsing},
  shorttitle = {The Rational Analysis of Inquiry},
  booktitle = {Rational Models of Cognition},
  author = {Chater, Nick and Crocker, Matthew J. and Pickering, Martin J.},
  editor = {Oaksford, Mike and Chater, Nick},
  year = {1998},
  month = nov,
  pages = {441--468},
  publisher = {Oxford University Press},
  address = {Oxford, England},
  doi = {10.1093/oso/9780198524151.003.0020},
  urldate = {2024-05-14},
  abstract = {The cognitive system is not merely a passive receiver of information. It has some measure of control of what information it receives; and how that information is processed. Control over the information received may be exercised in a wide variety of ways, from adjustments to the sense organs (e.g. by moving the eyes), to decisions concerning which newspaper to read. Control over how information is processed is equally ubiquitous, ranging from attentional mechanisms (presuming that such mechanisms at least to some degree bias the resources applied to processing different aspects of the sensory input) to how much effort to spend thinking about a possible move in a chess game, or on a decision in everyday life.},
  isbn = {978-0-19-852415-1},
  langid = {english},
  file = {~/Zotfiles/chater.n1998 The rational analysis of inquiry the ca.pdf}
}

@article{chater.n:1999,
  title = {Ten Years of the Rational Analysis of Cognition},
  author = {Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, Nick and Oaksford, Mike},
  year = {1999},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {2},
  pages = {57--65},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(98)01273-X},
  urldate = {2022-07-07},
  langid = {english},
  pmid = {10234228},
  keywords = {Memory,Neuroscience,Rational analysis,Rationality,Reasoning},
  file = {~/Zotfiles/chater.n1999 Ten years of the rational analysis of co.pdf}
}

@article{chater.n:2006,
  title = {Probabilistic Models of Cognition: {{Conceptual}} Foundations},
  shorttitle = {Probabilistic Models of Cognition},
  author = {Chater, Nick and Tenenbaum, Joshua B. and Yuille, Alan},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {7},
  pages = {287--291},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2006.05.007},
  urldate = {2024-05-26},
  langid = {english},
  pmid = {16807064},
  file = {~/Zotfiles/chater.n2006trends Probabilistic models of cognition Conce.pdf}
}

@article{chater.n:2006a,
  title = {Probabilistic Models of Language Processing and Acquisition},
  author = {Chater, Nick and Manning, Christopher D.},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  series = {Special Issue: {{Probabilistic}} Models of Cognition},
  volume = {10},
  number = {7},
  pages = {335--344},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.05.006},
  urldate = {2022-10-22},
  abstract = {Probabilistic methods are providing new explanatory approaches to fundamental cognitive science questions of how humans structure, process and acquire language. This review examines probabilistic models defined over traditional symbolic structures. Language comprehension and production involve probabilistic inference in such models; and acquisition involves choosing the best model, given innate constraints and linguistic and other input. Probabilistic models can account for the learning and processing of language, while maintaining the sophistication of symbolic models. A recent burgeoning of theoretical developments and online corpus creation has enabled large models to be tested, revealing probabilistic constraints in processing, undermining acquisition arguments based on a perceived poverty of the stimulus, and suggesting fruitful links with probabilistic theories of categorization and ambiguity resolution in perception.},
  langid = {english},
  file = {~/Zotfiles/chater.n2006 Probabilistic models of language process.pdf}
}

@book{chater.n:2008book,
  title = {The Probabilistic Mind: Prospects for {{Bayesian}} Cognitive Science},
  shorttitle = {The Probabilistic Mind},
  editor = {Chater, Nick and Oaksford, Mike},
  year = {2008},
  month = mar,
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199216093.001.0001},
  urldate = {2024-05-26},
  abstract = {Abstract. The rational analysis method, first proposed by John R. Anderson, has been enormously influential in helping us understand high-level cognitive p},
  isbn = {978-0-19-169597-1},
  langid = {english}
}

@article{chater.n:2020,
  title = {Probabilistic {{Biases Meet}} the {{Bayesian Brain}}},
  author = {Chater, Nick and Zhu, Jian-Qiao and Spicer, Jake and Sundh, Joakim and {Le{\'o}n-Villagr{\'a}}, Pablo and Sanborn, Adam},
  year = {2020},
  month = oct,
  journal = {Current Directions in Psychological Science},
  volume = {29},
  number = {5},
  pages = {506--512},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/0963721420954801},
  urldate = {2025-02-18},
  abstract = {In Bayesian cognitive science, the mind is seen as a spectacular probabilistic-inference machine. But judgment and decision-making (JDM) researchers have spent half a century uncovering how dramatically and systematically people depart from rational norms. In this article, we outline recent research that opens up the possibility of an unexpected reconciliation. The key hypothesis is that the brain neither represents nor calculates with probabilities but approximates probabilistic calculations by drawing samples from memory or mental simulation. Sampling models diverge from perfect probabilistic calculations in ways that capture many classic JDM findings, which offers the hope of an integrated explanation of classic heuristics and biases, including availability, representativeness, and anchoring and adjustment.},
  langid = {english},
  keywords = {bayesian cognitive science},
  file = {~/Zotfiles/chater.n2020 Probabilistic Biases Meet the Bayesian B.pdf}
}

@article{chatterjee.s:2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  year = {2018},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {28},
  number = {2},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/17-aap1326},
  file = {~/Zotfiles/chatterjee.s2018 The sample size required in importance s.pdf}
}

@misc{chen.c:2023arxiv,
  title = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01318},
  eprint = {2302.01318},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-15},
  abstract = {We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,language model decoding,rejection sampling,speculative sampling},
  file = {~/Zotfiles/chen.c2023 Accelerating large language model decodi.pdf}
}

@inproceedings{chen.d:2014,
  title = {A Fast and Accurate Dependency Parser Using Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Chen, Danqi and Manning, Christopher},
  year = {2014},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/v1/d14-1082},
  bdsk-url-2 = {https://doi.org/10.3115/v1/d14-1082},
  date-added = {2022-05-06 15:52:04 -0400},
  date-modified = {2022-05-06 15:53:01 -0400},
  keywords = {dependency parsing,dependency structures,parsing,stanford dependencies}
}

@inproceedings{chen.j:2024,
  title = {Language Model Based Unsupervised Dependency Parsing with Conditional Mutual Information and Grammatical Constraints},
  booktitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Junjie and He, Xiangheng and Miyao, Yusuke},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  year = {2024},
  month = jun,
  pages = {6355--6366},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  urldate = {2024-06-24},
  abstract = {Previous methods based on Large Language Models (LLM) perform unsupervised dependency parsing by maximizing bi-lexical dependence scores. However, these previous methods adopt dependence scores that are difficult to interpret. These methods cannot incorporate grammatical constraints that previous grammar-based parsing research has shown beneficial to improving parsing performance. In this work, we apply Conditional Mutual Information (CMI), an interpretable metric, to measure the bi-lexical dependence and incorporate grammatical constraints into LLM-based unsupervised parsing. We incorporate Part-Of-Speech information as a grammatical constraint at the CMI estimation stage and integrate two additional grammatical constraints at the subsequent tree decoding stage. We find that the CMI score positively correlates with syntactic dependencies and has a stronger correlation with the syntactic dependencies than baseline scores. Our experiment confirms the benefits and applicability of the proposed grammatical constraints across five languages and eight datasets. The CMI parsing model outperforms state-of-the-art LLM-based models and similarly constrained grammar-based models. Our analysis reveals that the CMI model is strong in retrieving dependency relations with rich lexical interactions but is weak in retrieving relations with sparse lexical interactions, indicating a potential limitation in CMI-based unsupervised parsing methods.},
  keywords = {dependency parsing,dependency structures,pmi},
  file = {~/Zotfiles/chen.j2024 Language Model Based Unsupervised Depend.pdf}
}

@inproceedings{chen.r:2021cogsci,
  title = {On Factors Influencing Typing Time: Insights from a Viral Online Typing Game},
  shorttitle = {On Factors Influencing Typing Time},
  booktitle = {Proceedings of the 43rd {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Chen, Robert and Levy, Roger and Eisape, Tiwalayo},
  year = {2021},
  urldate = {2023-09-08},
  abstract = {Context effects in human spoken language are well-documented and play a central role in the theory of language production. However, the role of context in written language production is far less well understood, even though a considerable proportion of the language produced by many people today is written. Here we analyze the factors predictive of English language typing times in a large, naturalistic corpus from the popular TypeRacer.com website. We find broad consistency with the major documented effects of linguistic context on spoken language production, suggesting potential modality-independence in the cognitive mechanisms underlying language production and/or similar optimization pressures on the production systems in both modalities.},
  langid = {english},
  file = {~/Zotfiles/chen.r2021cogsci On factors influencing typing time insi.pdf}
}

@inproceedings{chen.s:1995,
  title = {Bayesian Grammar Induction for Language Modeling},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Stanley F.},
  year = {1995},
  month = jun,
  series = {{{ACL}} '95},
  pages = {228--235},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/981658.981689},
  urldate = {2022-07-04},
  abstract = {We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.},
  file = {~/Zotfiles/chen.s1995 Bayesian grammar induction for language.pdf}
}

@article{chen.s:2023,
  title = {The Effect of Context on Noisy-Channel Sentence Comprehension},
  author = {Chen, Sihan and Nathaniel, Sarah and Ryskin, Rachel and Gibson, Edward},
  year = {2023},
  month = sep,
  journal = {Cognition},
  volume = {238},
  pages = {105503},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2023.105503},
  urldate = {2023-07-28},
  abstract = {The process of sentence comprehension must allow for the possibility of noise in the input, e.g., from speaker error, listener mishearing, or environmental noise. Consequently, semantically implausible sentences such as The girl tossed the apple the boy are often interpreted as a semantically plausible alternative (e.g., The girl tossed the apple to the boy). Previous investigations of noisy-channel comprehension have relied exclusively on paradigms with isolated sentences. Because supportive contexts alter the expectations of possible interpretations, the noisy channel framework predicts that context should encourage more inference in interpreting implausible sentences, relative to null contexts (i.e. a lack of context) or unsupportive contexts. In the present work, we tested this prediction in four types of sentence constructions: two where inference is relatively frequent (double object - prepositional object), and two where inference is rare (active-passive). We found evidence that in the two sentence types that commonly elicit inference, supportive contexts encourage noisy-channel inferences about the intended meaning of implausible sentences more than non-supportive contexts or null contexts. These results suggest that noisy-channel inference may be more pervasive in everyday language processing than previously assumed based on work with isolated sentences.},
  langid = {english},
  keywords = {Context,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {~/Zotfiles/chen.s2023 The effect of context on noisy-channel s.pdf}
}

@inproceedings{chen.x:2016,
  title = {{{InfoGAN}}: {{Interpretable}} Representation Learning by Information Maximizing Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 29: {{Annual}} Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and {von Luxburg}, Ulrike and Guyon, Isabelle and Garnett, Roman},
  year = {2016},
  pages = {2172--2180},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChenCDHSSA16.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{chen.y:2005,
  title = {Another Look at Rejection Sampling through Importance Sampling},
  author = {Chen, Yuguo},
  year = {2005},
  month = may,
  journal = {Statistics \& Probability Letters},
  volume = {72},
  number = {4},
  pages = {277--283},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2005.01.002},
  urldate = {2023-01-01},
  abstract = {We show that rejection sampling is inferior to the importance sampling algorithm in terms of the {$\chi$}2 distance between the proposal distribution and the target distribution. Similar conclusions are drawn for comparing rejection control with importance sampling.},
  langid = {english},
  keywords = {distance,Effective sample size,importance sampling,Importance sampling,Rejection control,rejection sampling,Rejection sampling},
  file = {~/Zotfiles/chen.y2005 Another look at rejection sampling throu.pdf}
}

@article{cheyette.s:2020,
  title = {A Unified Account of Numerosity Perception},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  year = {2020},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {12},
  pages = {1265--1272},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00946-0},
  urldate = {2022-05-19},
  abstract = {People can identify the number of objects in sets of four or fewer items with near-perfect accuracy but exhibit linearly increasing error for larger sets. Some researchers have taken this discontinuity as evidence of two distinct representational systems. Here, we present a mathematical derivation showing that this behaviour is an optimal representation of cardinalities under a limited informational capacity, indicating that this behaviour can emerge from a single system. Our derivation predicts how the amount of information accessible to viewers should influence the perception of quantity for both large and small sets. In a series of four preregistered experiments (N\,=\,100 each), we varied the amount of information accessible to participants in number estimation. We find tight alignment between the model and human performance for both small and large quantities, implicating efficient representation as the common origin behind key phenomena of human and animal numerical cognition.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational models,Human behaviour,Object vision},
  file = {~/Zotfiles/cheyette.s2020 A unified account of numerosity percepti.pdf}
}

@book{chierchia.g:2013book,
  title = {Logic in Grammar: {{Polarity}}, Free Choice, and Intervention},
  author = {Chierchia, Gennaro},
  year = {2013},
  publisher = {Oxford},
  date-added = {2019-05-19 22:01:50 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {logic,mathematical linguistics,semantics}
}

@inproceedings{chierchia.g:2019,
  title = {Ultrametric Fitting by Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Chierchia, Giovanni and Perret, Benjamin},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3175--3186},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChierchiaP19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@unpublished{chomsky.n:1955ms,
  type = {Unpublished Manuscript},
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {[1975] 1955},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  keywords = {information theory,syntactic information,syntax}
}

@article{chomsky.n:1957,
  title = {Review of {{A Manual}} of {{Phonology}}},
  author = {Chomsky, Noam},
  year = {1957},
  journal = {International Journal of American Linguistics},
  volume = {23},
  number = {3},
  eprint = {1263617},
  eprinttype = {jstor},
  pages = {223--234},
  publisher = {University of Chicago Press},
  issn = {0020-7071},
  urldate = {2024-02-25},
  collaborator = {Hockett, Charles F.},
  file = {~/Zotfiles/chomsky.n1957a Review of A Manual of Phonology.pdf}
}

@book{chomsky.n:1957book,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {1957},
  publisher = {{Mouton and Co.}},
  address = {The Hague},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{chomsky.n:1965book,
  title = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  year = {1965},
  publisher = {MIT Press},
  date-added = {2022-04-14 12:41:20 -0400},
  date-modified = {2022-04-14 12:42:21 -0400},
  keywords = {generative grammar,syntax},
  file = {~/Zotfiles/chomsky.n1965aspects Aspects of the theory of syntax 2.pdf;~/Zotfiles/chomsky.n1965aspects Aspects of the theory of syntax.pdf}
}

@incollection{chomsky.n:1967,
  title = {Recent {{Contributions}} to the {{Theory}} of {{Innate Ideas}}},
  booktitle = {Proceedings of the {{Boston Colloquium}} for the {{Philosophy}} of {{Science}} 1964/1966: {{In Memory}} of {{Norwood Russell Hanson}}},
  author = {Chomsky, Noam},
  editor = {Cohen, Robert S. and Wartofsky, Marx W.},
  year = {1967},
  pages = {81--90},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-3508-8_4},
  urldate = {2024-04-12},
  abstract = {I think that it will be useful to separate two issues in the discussion of our present topic --- one is the issue of historical interpretation, namely, what in fact was the content of the classical doctrine of innate ideas, let us say, in Descartes and Leibniz; the second is the substantive issue, namely, in the light of the information presently available, what can we say about the prerequisites for the acquisition of knowledge --- what can we postulate regarding the psychologically a priori principles that determine the character of learning and the nature of what is acquired.},
  isbn = {978-94-010-3508-8},
  langid = {english}
}

@book{chomsky.n:1975book,
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {1975},
  publisher = {Springer},
  date-added = {2020-06-05 14:43:06 -0400},
  date-modified = {2020-06-05 14:44:48 -0400},
  project = {syntactic embedding},
  keywords = {syntax}
}

@incollection{chomsky.n:1980,
  title = {A Review of {{B}}. {{F}}. {{Skinner}}'s Verbal Behavior},
  booktitle = {Readings in Philosophy of Psychology},
  author = {Chomsky, Noam},
  editor = {Block, Ned},
  year = {1980},
  month = jan,
  series = {The Language and Thought Series},
  volume = {1},
  pages = {48--66},
  publisher = {Harvard University Press},
  address = {Cambridge, MA and London, England},
  doi = {10.4159/harvard.9780674594623.c6},
  urldate = {2024-05-26},
  isbn = {978-0-674-59462-3}
}

@incollection{chomsky.n:1995,
  title = {Bare Phrase Structure},
  booktitle = {Government and Binding Theory and the Minimalist Program},
  author = {Chomsky, Noam},
  editor = {Webelhuth, Gerth},
  year = {1995},
  pages = {383--349},
  publisher = {Blackwell},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {X-bar}
}

@book{chomsky.n:1995book,
  title = {The Minimalist Program},
  author = {Chomsky, Noam},
  year = {1995},
  publisher = {MIT Press},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{chomsky.n:2002book2,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {2002},
  month = nov,
  edition = {2},
  publisher = {Mouton de Gruyter},
  doi = {10.1515/9783110218329},
  bdsk-url-2 = {https://doi.org/10.1515/9783110218329},
  date-added = {2021-09-24 08:27:37 -0400},
  date-modified = {2021-09-24 08:30:27 -0400},
  file = {~/Zotfiles/chomsky.n2002ss2e Syntactic structures.pdf}
}

@book{chopin.n:2020book,
  title = {An Introduction to Sequential {{Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  month = oct,
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {1},
  publisher = {Springer},
  address = {Cham, Switzerland},
  doi = {10.1007/978-3-030-47845-2},
  abstract = {Offers a general and gentle introduction to all aspects of particle filtering: the algorithms, their uses in different areas, their computer implementation in Python and the supporting theory Covers both the basics and more advanced, cutting-edge developments, such as PMCMC (particle Markov chain Monte Carlo) and SQMC (Sequential quasi-Monte Carlo) Comes with a freely available Python library (particles), which implements all the algorithms discussed in the book. Each chapter ends with a ``Python corner'' that discusses how the methods covered can be implemented in Python},
  isbn = {978-3-030-47844-5},
  langid = {english},
  keywords = {monte carlo,sampling,statistics},
  file = {~/Zotfiles/chopin.n2020 An introduction to sequential Monte Carl.pdf}
}

@article{chow.c:1968,
  title = {Approximating Discrete Probability Distributions with Dependence Trees},
  author = {Chow, C and Liu, Cong},
  year = {1968},
  journal = {IEEE transactions on Information Theory},
  volume = {14},
  number = {3},
  pages = {462--467},
  publisher = {IEEE},
  date-added = {2019-10-09 20:57:36 -0400},
  date-modified = {2019-10-09 20:58:15 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,mutual information}
}

@misc{chowdhery.a:2022arxiv,
  title = {{{PaLM}}: Scaling Language Modeling with Pathways},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2204.02311},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.02311},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-04-19 13:50:14 -0400},
  date-modified = {2022-04-28 12:21:29 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@incollection{christensen.k:2016,
  title = {The Dead Ends of Language: The (Mis)Interpretation of a Grammatical Illusion},
  shorttitle = {The Dead Ends of Language},
  booktitle = {Let Us Have Articles Betwixt Us: {{Papers}} in {{Historical}} and {{Comparative Linguistics}} in {{Honour}} of {{Johanna L}}. {{Wood}}},
  author = {Christensen, Ken Ramsh{\o}j},
  editor = {Vikner, Sten and J{\o}rgensen, Henrik and Gelderen, Elly Van},
  year = {2016},
  pages = {129--159},
  publisher = {Department of English, University of Aarhus},
  doi = {10.7146/aul.119.107},
  urldate = {2023-08-01},
  isbn = {978-87-7507-359-7}
}

@article{christianson.k:2010,
  title = {Effects of Plausibility on Structural Priming},
  author = {Christianson, Kiel and Luke, Steven G. and Ferreira, Fernanda},
  year = {2010},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {36},
  number = {2},
  pages = {538--544},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285},
  doi = {10.1037/a0018027},
  abstract = {We report a replication and extension of Ferreira (2003), in which it was observed that native adult English speakers misinterpret passive sentences that relate implausible but not impossible semantic relationships (e.g., The angler was caught by the fish) significantly more often than they do plausible passives or plausible or implausible active sentences. In the experiment reported here, participants listened to the same plausible and implausible passive and active sentences as in Ferreira (2003), answered comprehension questions, and then orally described line drawings of simple transitive actions. The descriptions were analyzed as a measure of structural priming (Bock, 1986). Question accuracy data replicated Ferreira (2003). Production data yielded an interaction: Passive descriptions were produced more often after plausible passives and implausible actives. We interpret these results as indicative of a language processor that proceeds along differentiated morphosyntactic and semantic routes. The processor may end up adjudicating between conflicting outputs from these routes by settling on a ``good enough'' representation that is not completely faithful to the input. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Comprehension,good enough processing,Language,plausibility,Priming,Semantics},
  file = {~/Zotfiles/christianson.k2010 Effects of plausibility on structural pr.pdf}
}

@article{christianson.k:2016,
  title = {When Language Comprehension Goes Wrong for the Right Reasons: {{Good-enough}}, Underspecified, or Shallow Language Processing},
  shorttitle = {When Language Comprehension Goes Wrong for the Right Reasons},
  author = {Christianson, Kiel},
  year = {2016},
  month = may,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {5},
  pages = {817--828},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1080/17470218.2015.1134603},
  urldate = {2023-08-01},
  abstract = {This paper contains an overview of language processing that can be described as ``good enough'', ``underspecified'', or ``shallow''. The central idea is that a nontrivial proportion of misunderstanding, misinterpretation, and miscommunication can be attributed not to random error, but instead to processing preferences of the human language processing system. In other words, the very architecture of the language processor favours certain types of processing errors because in a majority of instances, this ``fast and frugal'', less effortful processing is good enough to support communication. By way of historical background, connections are made between this relatively recent facet of psycholinguistic study, other recent language processing models, and related concepts in other areas of cognitive science. Finally, the nine papers included in this special issue are introduced as representative of novel explorations of good-enough, or underspecified, language processing.},
  langid = {english}
}

@article{chung.f:1984,
  title = {On Optimal Linear Arrangements of Trees},
  author = {Chung, F.R.K.},
  year = {1984},
  journal = {Computers and Mathematics with Applications},
  volume = {10},
  number = {1},
  pages = {43--60},
  issn = {0898-1221},
  bdsk-url-2 = {https://doi.org/10.1016/0898-1221(84)90085-3},
  date-added = {2019-05-14 23:56:39 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,linearization}
}

@inproceedings{church.k:1989,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  year = {1989},
  pages = {76--83},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, British Columbia, Canada},
  doi = {10.3115/981623.981633},
  bdsk-url-2 = {https://doi.org/10.3115/981623.981633}
}

@article{clark.a:2013,
  title = {Whatever next? {{Predictive}} Brains, Situated Agents, and the Future of Cognitive Science},
  shorttitle = {Whatever Next?},
  author = {Clark, Andy},
  year = {2013},
  month = jun,
  journal = {Behavioral and Brain Sciences},
  volume = {36},
  number = {3},
  pages = {181--204},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X12000477},
  urldate = {2025-02-12},
  abstract = {Brains, it has recently been argued, are essentially prediction machines. They are bundles of cells that support perception and action by constantly attempting to match incoming sensory inputs with top-down expectations or predictions. This is achieved using a hierarchical generative model that aims to minimize prediction error within a bidirectional cascade of cortical processing. Such accounts offer a unifying model of perception and action, illuminate the functional role of attention, and may neatly capture the special contribution of cortical processing to adaptive success. This target article critically examines this ``hierarchical prediction machine'' approach, concluding that it offers the best clue yet to the shape of a unified science of mind and action. Sections 1 and 2 lay out the key elements and implications of the approach. Section 3 explores a variety of pitfalls and challenges, spanning the evidential, the methodological, and the more properly conceptual. The paper ends (sections 4 and 5) by asking how such approaches might impact our more general vision of mind, experience, and agency.},
  langid = {english},
  keywords = {action,attention,Bayesian brain,expectation,generative model,hierarchy,perception,precision,prediction,prediction error,predictive coding,top-down processing},
  file = {~/Zotfiles/clark.a2013 Whatever next Predictive brains, situat.pdf}
}

@inproceedings{clark.c:2019,
  title = {Don't Take the Easy Way out: {{Ensemble}} Based Methods for Avoiding Known Dataset Biases},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  year = {2019},
  pages = {4069--4082},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1418},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1418}
}

@book{clark.h:1996book,
  title = {Using {{Language}}},
  author = {Clark, Herbert H.},
  year = {1996},
  series = {'{{Using}}' {{Linguistic Books}}},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511620539},
  urldate = {2025-03-03},
  abstract = {This book, first published in 1996, argues that language use is more than the sum of a speaker speaking and a listener listening. It is the joint action that emerges when speakers and listeners - writers and readers - perform their individual actions in coordination, as ensembles. The author argues strongly that language use embodies both individual and social processes.},
  isbn = {978-0-521-56158-7},
  keywords = {language as action}
}

@inproceedings{clark.k:2019,
  title = {What Does {{BERT}} Look at? {{An}} Analysis of {{BERT}}'s Attention},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  pages = {276--286},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4828},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4828}
}

@article{clark.t:2022cogsci,
  title = {Evidence for Availability Effects on Speaker Choice in the {{Russian}} Comparative Alternation},
  author = {Clark, Thomas and Wilcox, Ethan Gotlieb and Gibson, Edward and Levy, Roger},
  year = {2022},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {44},
  urldate = {2022-10-29},
  abstract = {When a language offers multiple options for expressing the same meaning, what principles govern a speaker's choice? Two well-known principles proposed for explaining wide-ranging speaker preference are Uniform Information Density and Availability-Based Production. Here we test the predictions of these theories in a previously uninvestigated case of speaker choice. Russian has two ways of expressing the comparative: an {\textbackslash}textsc\{explicit\} option ({\textbackslash}textit\{Ona bystree chem ja\}/She fast\{{\textbackslash}sc-comp\} than me\{{\textbackslash}sc-nom\}) and a {\textbackslash}textsc\{genitive\} option ({\textbackslash}textit\{Ona bystree menya/She fast\{{\textbackslash}sc-comp\} me\{{\textbackslash}sc-gen\}\}). We lay out several potential predictions of each theory for speaker choice in the Russian comparative construction, including effects of post-comparative word predictability, phrase length, syntactic complexity, and semantic association between the comparative adjective and subsequent noun. In a corpus study, we find that the explicit construction is used preferentially when the post-comparative noun phrase is longer, has a relative clause, and is less semantically associated with the comparative adjective. A follow-up production experiment using visual scene stimuli to elicit comparative sentences replicates the corpus finding that Russian native speakers prefer the explicit form when post-comparative phrases are longer. These findings offer no clear support for the predictions of Uniform Information Density, but are broadly supportive of Availability-Based Production, with the explicit option serving as an unreduced form that eases speakers' planning of complex or low-availability utterances. Code for this study is available at https://github.mit.edu/thclark/russian\_uid},
  langid = {english},
  keywords = {uniform information density}
}

@article{clark.t:2023,
  title = {A Cross-Linguistic Pressure for Uniform Information Density in Word Order},
  author = {Clark, Thomas Hikaru and Meister, Clara and Pimentel, Tiago and Hahn, Michael and Cotterell, Ryan and Futrell, Richard and Levy, Roger},
  year = {2023},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1048--1065},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00589},
  urldate = {2024-05-08},
  abstract = {While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1},
  file = {~/Zotfiles/clark.t2023 A cross-linguistic pressure for uniform.pdf}
}

@inproceedings{clark.t:2025cogsci,
  title = {A Model of Approximate and Incremental Noisy-Channel Language Processing},
  booktitle = {Proceedings of the 47th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Clark, Thomas Hikaru and Vigly, Jacob Hoover and Gibson, Edward and Levy, Roger},
  year = {2025},
  month = jul,
  address = {San Francisco, CA},
  langid = {english},
  file = {~/Zotfiles/clark.t2025cogsci A model of approximate and incremental n.pdf}
}

@misc{clark.t:2025HSP,
  type = {Poster},
  title = {Modeling Human Inferences and Reading Behavior with an Incremental, Resource-Rational Model of Noisy-Channel Language Processing},
  author = {Clark, Thomas Hikaru and Vigly, Jacob Hoover and Gibson, Edward and Levy, Roger},
  year = {2025},
  month = mar,
  address = {University of Maryland, College Park}
}

@unpublished{clark.t:2025ms,
  type = {{{ACL ARR}} 2025 {{May Submission}}},
  title = {Resource-Rational Noisy-Channel Language Processing: Testing the Effect of Algorithmic Constraints on Inferences},
  author = {Clark, Thomas Hikaru and Vigly, Jacob Hoover and Gibson, Edward and Levy, Roger P.},
  year = {2025}
}

@article{clayards.m:2008,
  title = {Perception of Speech Reflects Optimal Use of Probabilistic Speech Cues},
  author = {Clayards, Meghan and Tanenhaus, Michael K. and Aslin, Richard N. and Jacobs, Robert A.},
  year = {2008},
  month = sep,
  journal = {Cognition},
  volume = {108},
  number = {3},
  pages = {804--809},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.04.004},
  urldate = {2024-05-26},
  abstract = {Listeners are exquisitely sensitive to fine-grained acoustic detail within phonetic categories for sounds and words. Here we show that this sensitivity is optimal given the probabilistic nature of speech cues. We manipulated the probability distribution of one probabilistic cue, voice onset time (VOT), which differentiates word initial labial stops in English (e.g., ``beach'' and ``peach''). Participants categorized words from distributions of VOT with wide or narrow variances. Uncertainty about word identity was measured by four-alternative forced-choice judgments and by the probability of looks to pictures. Both measures closely reflected the posterior probability of the word given the likelihood distributions of VOT, suggesting that listeners are sensitive to these distributions.},
  keywords = {Categorization,Ideal observer model,Speech perception,Word recognition},
  file = {~/Zotfiles/clayards.m2008 Perception of speech reflects optimal us.pdf}
}

@misc{coecke.b:2010,
  title = {Mathematical Foundations for a Compositional Distributional Model of Meaning},
  author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
  year = {2010},
  eprint = {1003.4394},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2019-08-06 08:49:05 +0300},
  date-modified = {2019-08-06 08:50:43 +0300},
  project = {syntactic embedding},
  keywords = {compositionality,distributional models}
}

@article{cohen.j:1994,
  title = {The Earth Is Round ({{p$<$.05}}).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0003-066x.49.12.997},
  bdsk-url-2 = {https://doi.org/10.1037/0003-066x.49.12.997},
  date-added = {2021-08-08 11:16:17 -0400},
  date-modified = {2021-08-08 20:26:36 -0400}
}

@article{cohen.l:1981,
  title = {Can Human Irrationality Be Experimentally Demonstrated?},
  author = {Cohen, L. Jonathan},
  year = {1981},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {4},
  number = {3},
  pages = {317--331},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00009092},
  urldate = {2024-05-13},
  abstract = {The object of this paper is to show why recent research in the psychology of deductive and probabilistic reasoning does not have "bleak implications for human rationality," as has sometimes been supposed. The presence of fallacies in reasoning is evaluated by referring to normative criteria which ultimately derive their own credentials from a systematisation of the intuitions that agree with them. These normative criteria cannot be taken, as some have suggested, to constitute a part of natural science, nor can they be established by metamathematical proof. Since a theory of competence has to predict the very same intuitions, it must ascribe rationality to ordinary people.Accordingly, psychological research on this topic falls into four categories. In the first, experimenters investigate conditions under which their subjects suffer from genuine cognitive illusions. The search for explanations of such performance errors may then generate hypotheses about the ways in which the relevant information-processing mechanisms operate. In the second category, experimenters investigate circumstances in which their subjects exhibit mathematical or scientific ignorance: these are tests of the subjects' intelligence or education. In the third and fourth categories, experimenters impute a fallacy where none exists, either because they are applying the relevant normative criteria in an inappropriate way or because the normative criteria being applied are not the appropriate ones.},
  langid = {english},
  keywords = {competence,deduction,fallacy,intuition,probability judgments,rationality,reasoning}
}

@book{cohen.p:2014book2,
  title = {Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences},
  author = {Cohen, Patricia and Cohen, Patricia and West, Stephen G. and Aiken, Leona S.},
  year = {2014},
  month = apr,
  edition = {2},
  publisher = {Psychology Press},
  address = {New York},
  doi = {10.4324/9781410606266},
  abstract = {This classic text on multiple regression is noted for its nonmathematical, applied, and data-analytic approach. Readers profit from its verbal-conceptual},
  isbn = {978-1-4106-0626-6}
}

@article{collins.k:2024,
  title = {Building Machines That Learn and Think with People},
  author = {Collins, Katherine M. and Sucholutsky, Ilia and Bhatt, Umang and Chandra, Kartik and Wong, Lionel and Lee, Mina and Zhang, Cedegao E. and {Zhi-Xuan}, Tan and Ho, Mark and Mansinghka, Vikash and Weller, Adrian and Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2024},
  month = oct,
  journal = {Nature Human Behaviour},
  volume = {8},
  number = {10},
  pages = {1851--1863},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01991-9},
  urldate = {2024-10-24},
  abstract = {What do we want from machine intelligence? We envision machines that are not just tools for thought but partners in thought: reasonable, insightful, knowledgeable, reliable and trustworthy systems that think with us. Current artificial intelligence systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called `thought partners', systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and artificial intelligence thought partners can engage, and we propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {~/Zotfiles/collins.k2024 Building machines that learn and think w.pdf}
}

@phdthesis{collins.m:1999phd,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael John},
  year = {1999},
  address = {Philadelphia, PA, USA},
  urldate = {2022-10-16},
  abstract = {Statistical models for parsing natural language have recently shown considerable success in broad-coverage domains. Ambiguity often leads to an input sentence having many possible parse trees; statistical approaches assign a probability to each tree, thereby ranking competing trees in order of plausibility. The probability for each candidate tree is calculated as a product of terms, each term corresponding to some sub-structure within the tree. The choice of parameterization is the choice of how to break down the tree. There are two critical questions regarding the parameterization of the problem: (1) What linguistic objects (e.g., context-free rules, parse moves) should the model's parameters be associated with? I.e., How should trees be decomposed into smaller fragments? (2) How can this choice be instantiated in a sound probabilistic model? This thesis argues that the locality of a lexical head's influence in a tree should motivate modeling choices in the parsing problem. In the final parsing models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The goals of the work are two-fold. First, we aim to advance the state of the art. We report tests on Wall Street Journal text showing that the models give improved accuracy over other methods in the literature. The models recover richer representations than previous approaches, adding the complement/adjunct distinction and information regarding wh-movement. Second, we aim to increase understanding of statistical parsing models. Each parameter type is motivated through tree examples where it provides discriminative information. An empirical study of prepositional phrase attachment ambiguity is used to investigate the effectiveness of dependency parameters for ambiguity resolution. A number of parsing models are tested, and we give a breakdown of their performance on different types of construction. Finally, we give a detailed comparison of the models to others in the literature.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780599258747},
  langid = {english},
  school = {University of Pennsylvania},
  keywords = {Applied sciences,Head-driven,Natural language,Parameterization,Parsing,Statistical models},
  file = {~/Zotfiles/collins.m1999phd Head-driven statistical models for natur.pdf}
}

@article{collins.m:2003,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael},
  year = {2003},
  month = dec,
  journal = {Computational Linguistics},
  volume = {29},
  number = {4},
  pages = {589--637},
  issn = {0891-2017},
  doi = {10.1162/089120103322753356},
  urldate = {2022-10-16},
  abstract = {This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.},
  file = {~/Zotfiles/collins.m2003 Head-driven statistical models for natur.pdf}
}

@inproceedings{collins.m:2004,
  title = {Incremental Parsing with the Perceptron Algorithm},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Collins, Michael and Roark, Brian},
  year = {2004},
  pages = {111--118},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1218970},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218970}
}

@article{collins.m:2013,
  title = {Information Density and Dependency Length as Complementary Cognitive Models},
  author = {Collins, Michael Xavier},
  year = {2013},
  month = sep,
  volume = {43},
  number = {5},
  pages = {651--681},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10936-013-9273-3},
  bdsk-url-2 = {https://doi.org/10.1007/s10936-013-9273-3},
  date-added = {2021-10-18 22:13:42 -0400},
  date-modified = {2021-10-18 22:13:43 -0400}
}

@incollection{coltheart.m:1977,
  title = {Access to the {{Internal Lexicon}}},
  booktitle = {Attention and Performance {{VI}}: {{Proceedings}} of the Sixth International Symposium on Attention and Performance, {{Stockholm}}, {{Sweden}}, {{July}} 28--{{August}} 1, 1975},
  author = {Coltheart, Max and Davelaar, Eileen and Jonasson, Jon Torfi and Besner, Derek},
  year = {1977},
  pages = {535--555},
  publisher = {Routledge},
  abstract = {In order to understand how a word is read for meaning, we need to know how a reader proceeds from the printed representation of a word to the word's entry in the reader's internal lexicon, where the word's meaning is stored. This raises two principal questions: what is the code in which the word is represented when this process, lexical access, is being carried out, and what is the procedure by which this representation is used to find the word's entry in the lexicon. The lexical-decision task is a suitable one for the investigation of these questions. Two experiments using this task are reported. In one, it was found that a letter string's similarity to English words influenced the ``no'' response latency, but not the ``yes'' response latency, and it is argued that this result favors the view that lexical access is ``direct,'' rather than requiring search. The other experiment showed that a nonword's phonological properties influenced the time taken to say ``no'' to it. Thus phonological encoding is occurring in these experiments. It remains to be shown, however, that this is any more than an epiphenomenon; neither these findings, nor those of previous investigators, compel us to abandon the view that skilled reading of single words proceeds solely by making use of visual representations of printed words.},
  isbn = {978-1-003-30973-4},
  keywords = {Coltheart's N},
  file = {~/Zotfiles/coltheart.m1977 Access to the Internal Lexicon.pdf}
}

@inproceedings{conneau.a:2018,
  title = {What You Can Cram into a Single \$\&!\#* Vector: {{Probing}} Sentence Embeddings for Linguistic Properties},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"i}c and Baroni, Marco},
  year = {2018},
  pages = {2126--2136},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1198},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1198}
}

@inproceedings{conneau.a:2019,
  title = {Cross-Lingual Language Model Pretraining},
  booktitle = {Advances in Neural Information Processing Systems 32 ({{NeurIPS}} 2019)},
  author = {Conneau, Alexis and Lample, Guillaume},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  month = dec,
  pages = {7057--7067},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ConneauL19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@misc{coon.j:2019lingbuzz,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2019},
  publisher = {LingBuzz},
  archiveprefix = {LingBuzz},
  date-added = {2020-06-16 10:17:01 -0400},
  date-modified = {2020-06-16 10:17:01 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@article{coon.j:2021,
  title = {Feature {{Gluttony}}},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2021},
  month = oct,
  journal = {Linguistic Inquiry},
  volume = {52},
  number = {4},
  pages = {655--710},
  issn = {0024-3892},
  doi = {10.1162/ling_a_00386},
  urldate = {2025-09-11},
  abstract = {This article develops a new approach to a family of hierarchy-effect-inducing configurations, with a focus on Person Case Constraint effects, dative-nominative configurations, and copula constructions. The main line of approach in the recent literature is to attribute these effects to failures of {$\phi$}-Agree or, more specifically, failures of nominal licensing or case checking. We propose that the problem in these configurations is unrelated to nominal licensing, but is instead the result of a probe participating in more than one Agree dependency, a configuration we refer to as feature gluttony. Feature gluttony does not in and of itself lead to ungrammaticality; rather, it can create irresolvably conflicting requirements for subsequent operations. We argue that in the case of clitic configurations, a probe that agrees with more than one DP creates an intervention problem for clitic doubling. In violations involving morphological agreement, gluttony in features may result in a configuration with no available morphological output.},
  file = {/Users/v/Zotfiles/coon.j2021 Feature Gluttony.pdf;/Users/v/Zotero/storage/N48EQCL4/ling_a_00386.html}
}

@article{costa.f:2003,
  title = {Towards Incremental Parsing of Natural Language Using Recursive Neural Networks},
  author = {Costa, F.},
  year = {2003},
  journal = {Applied Intelligence},
  volume = {19},
  number = {1/2},
  pages = {9--25},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1023860521975},
  bdsk-url-2 = {https://doi.org/10.1023/a:1023860521975},
  date-added = {2021-06-22 15:15:28 -0400},
  date-modified = {2021-06-22 15:16:11 -0400},
  file = {~/Zotfiles/costa.f2003 Towards incremental parsing of natural l.pdf}
}

@book{cover.t:2006book2,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  edition = {2},
  publisher = {Wiley},
  doi = {10.1002/047174882x},
  bdsk-url-2 = {https://doi.org/10.1002/047174882x},
  date-added = {2022-04-28 11:28:46 -0400},
  date-modified = {2022-04-28 11:29:11 -0400},
  file = {~/Zotfiles/cover.t2006 Elements of information theory.pdf}
}

@inproceedings{cramer.b:2007,
  title = {Limitations of Current Grammar Induction Algorithms},
  booktitle = {Proceedings of the {{ACL}} 2007 Student Research Workshop},
  author = {Cramer, Bart},
  year = {2007},
  pages = {43--48},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@article{crameri.f:2020,
  title = {The Misuse of Colour in Science Communication},
  author = {Crameri, Fabio and Shephard, Grace E. and Heron, Philip J.},
  year = {2020},
  month = oct,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5444},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19160-7},
  urldate = {2022-10-12},
  abstract = {The accurate representation of data is essential in science communication. However, colour maps that visually distort data through uneven colour gradients or are unreadable to those with colour-vision deficiency remain prevalent in science. These include, but are not limited to, rainbow-like and red--green colour maps. Here, we present a simple guide for the scientific use of colour. We show how scientifically derived colour maps report true data variations, reduce complexity, and are accessible for people with colour-vision deficiencies. We highlight ways for the scientific community to identify and prevent the misuse of colour in science, and call for a proactive step away from colour misuse among the community, publishers, and the press.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Scientific community,Software}
}

@article{crocker.m:2000,
  title = {Wide-Coverage Probabilistic Sentence Processing},
  author = {Crocker, Matthew W. and Brants, Thorsten},
  year = {2000},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {29},
  number = {6},
  pages = {647--669},
  issn = {1573-6555},
  doi = {10.1023/A:1026560822390},
  urldate = {2022-10-13},
  abstract = {This paper describes a fully implemented, broad-coverage model of human syntactic processing. The model uses probabilistic parsing techniques, which combine phrase structure, lexical category, and limited subcategory probabilities with an incremental, left-to-right ``pruning'' mechanism based on cascaded Markov models. The parameters of the system are established through a uniform training algorithm, which determines maximum-likelihood estimates from a parsed corpus. The probabilistic parsing mechanism enables the system to achieve good accuracy on typical, ``garden-variety'' language (i.e., when tested on corpora). Furthermore, the incremental probabilistic ranking of the preferred analyses during parsing also naturally explains observed human behavior for a range of garden-path structures. We do not make strong psychological claims about the specific probabilistic mechanism discussed here, which is limited by a number of practical considerations. Rather, we argue incremental probabilistic parsing models are, in general, extremely well suited to explaining this dual nature---generally good and occasionally pathological---of human linguistic performance.},
  langid = {english},
  keywords = {frequency,Markov models,probabilistic parsing},
  file = {~/Zotfiles/crocker.m2000 Wide-coverage probabilistic sentence pro.pdf}
}

@techreport{cronbach.l:1953,
  type = {Technical Report},
  title = {A Consideration of Information Theory and Utility Theory as Tools for Psychometric Problems},
  author = {Cronbach, Lee J.},
  year = {1953},
  month = nov,
  number = {1, contract N60ri-07146, Office of Naval Research},
  institution = {University of Illinois},
  urldate = {2024-05-14}
}

@book{crooks.g:2019book,
  title = {Field {{Guide}} to {{Continuous Probability Distributions}}},
  author = {Crooks, Gavin E.},
  year = {2019},
  edition = {1},
  publisher = {Berkeley Institute for Theoretical Science},
  urldate = {2022-06-22},
  abstract = {Over 170 continuous univariate probability distributions (and at least as many synonyms) organized into 20 families.},
  isbn = {978-1-7339381-0-5},
  langid = {english}
}

@incollection{culicover.p:1999,
  title = {Syntactic Nuts: {{Hard}} Cases in Syntax},
  booktitle = {Syntactic Nuts: {{Hard}} Cases in Syntax.},
  author = {Culicover, Peter},
  year = {1999},
  series = {Foundations of Syntax},
  publisher = {Oxford University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{culicover.p:2005,
  title = {Simpler Syntax},
  booktitle = {Simpler Syntax},
  author = {Culicover, Peter and Jackendoff, Ray},
  year = {2005},
  publisher = {Oxford University Press},
  address = {Oxford},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:43 -0400}
}

@article{cuskley.c:2024,
  title = {The Limitations of Large Language Models for Understanding Human Language and Cognition},
  author = {Cuskley, Christine and Woods, Rebecca and Flaherty, Molly},
  year = {2024},
  month = aug,
  journal = {Open Mind},
  volume = {8},
  pages = {1058--1083},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00160},
  urldate = {2024-08-31},
  abstract = {Researchers have recently argued that the capabilities of Large Language Models (LLMs) can provide new insights into longstanding debates about the role of learning and/or innateness in the development and evolution of human language. Here, we argue on two grounds that LLMs alone tell us very little about human language and cognition in terms of acquisition and evolution. First, any similarities between human language and the output of LLMs are purely functional. Borrowing the ``four questions'' framework from ethology, we argue that what LLMs do is superficially similar, but how they do it is not. In contrast to the rich multimodal data humans leverage in interactive language learning, LLMs rely on immersive exposure to vastly greater quantities of unimodal text data, with recent multimodal efforts built upon mappings between images and text. Second, turning to functional similarities between human language and LLM output, we show that human linguistic behavior is much broader. LLMs were designed to imitate the very specific behavior of human writing; while they do this impressively, the underlying mechanisms of these models limit their capacities for meaning and naturalistic interaction, and their potential for dealing with the diversity in human language. We conclude by emphasising that LLMs are not theories of language, but tools that may be used to study language, and that can only be effectively applied with specific hypotheses to motivate research.},
  file = {~/Zotfiles/cuskley.c2024 The Limitations of Large Language Models.pdf}
}

@inproceedings{cusumano-towner.m:2018,
  title = {Incremental Inference for Probabilistic Programs},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {{Cusumano-Towner}, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
  year = {2018},
  month = jun,
  series = {{{PLDI}} 2018},
  pages = {571--585},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3192366.3192399},
  urldate = {2025-08-08},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
  isbn = {978-1-4503-5698-5},
  file = {~/Zotfiles/cusumano-towner.m2018a Incremental inference for probabilistic.pdf}
}

@inproceedings{cusumano-towner.m:2018a,
  title = {Incremental Inference for Probabilistic Programs},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {{Cusumano-Towner}, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
  year = {2018},
  series = {{{PLDI}} 2018},
  pages = {571--585},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3192366.3192399},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
  date-added = {2022-05-03 21:06:22 -0400},
  date-modified = {2022-05-03 21:07:50 -0400},
  isbn = {978-1-4503-5698-5},
  keywords = {incremental computation,probabilistic programming,sequential Monte Carlo},
  file = {~/Zotfiles/cusumano-towner.m2018a Incremental inference for probabilistic_1.pdf}
}

@article{cutter.m:2022,
  title = {No Evidence of Word-Level Uncertainty in Younger and Older Adults in Self-Paced Reading},
  author = {Cutter, Michael G and Paterson, Kevin B and Filik, Ruth},
  year = {2022},
  month = jun,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {75},
  number = {6},
  pages = {1085--1093},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1177/17470218211045987},
  urldate = {2024-03-06},
  abstract = {In a self-paced reading study, we investigated whether older adults maintain a greater level of uncertainty about the identity of words in a sentence than younger adults, potentially due to deficits in visuo-perceptual processing of high-spatial frequencies associated with normal aging. In the experiment, 60 older adults and 60 younger adults read sentences in which an early preposition was either perceptually confusable with another word (at; confusable with as) or not (toward), and in which the reading of a subsequent ambiguous verb (e.g., tossed) should be affected by the confusability of the preposition, while the reading of an unambiguous verb (e.g., thrown) should not be. This design replicated that of an earlier study conducted by Levy et al. (2009) that found evidence in favour of participants maintaining uncertainty about the confusable preposition in go-past times during natural reading. However, in our study, there was no evidence that either younger or older adults maintained uncertainty about the identity of the perceptually confusable preposition, such that there was no interaction between the preposition's form and subsequent verb ambiguity in self-paced reading times, although we did observe a main effect of verb ambiguity. This represents a failure to replicate the effect observed by Levy et al. when using a different experimental paradigm, and we consider potential causes of our findings at both a methodological and theoretical level.},
  langid = {english},
  file = {~/Zotfiles/cutter.m2022replicationSPRT No evidence of word-level uncertainty in 2.pdf;~/Zotfiles/cutter.m2022replicationSPRT No evidence of word-level uncertainty in.pdf}
}

@article{cutter.m:2022replication,
  title = {Do Readers Maintain Word-Level Uncertainty during Reading? {{A}} Pre-Registered Replication Study},
  shorttitle = {Do Readers Maintain Word-Level Uncertainty during Reading?},
  author = {Cutter, Michael G. and Filik, Ruth and Paterson, Kevin B.},
  year = {2022},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {125},
  pages = {104336},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2022.104336},
  urldate = {2024-03-06},
  abstract = {We present a replication of Levy, Bicknell, Slattery, and Rayner (2009). In this prior study participants read sentences in which a perceptually confusable preposition (at; confusable with as) or non-confusable preposition (toward) was followed by a verb more likely to appear in the syntactic structure formed by replacing at with as (e.g. tossed) or a verb that was not more likely to appear in this structure (e.g. thrown). Readers experienced processing difficulty upon fixating verbs like tossed following at, but not toward. Levy et al. argued that this suggests readers maintained uncertainty about previously fixated words' identities. We argue that this finding has wide-ranging implications for language processing theories, and that a replication is required. On the basis of a Bayes Factor Design Analysis we conducted a replication study with 56 items and 72 participants in order to determine whether Levy et al.'s effects are replicable. Using Bayesian statistical techniques we show that in our dataset there is evidence against the existence of the interaction Levy et al. found, and thus conclude that this study is non-replicable.},
  keywords = {Eye-movements,Noisy-channel processing,Reading,Sentence processing,Word-level uncertainty},
  file = {~/Zotfiles/cutter.m2022replication Do readers maintain word-level uncertain.pdf}
}

@article{cutter.m:2024,
  title = {Eye-Movements during Reading and Noisy-Channel Inference Making},
  author = {Cutter, Michael G. and Paterson, Kevin B. and Filik, Ruth},
  year = {2024},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {137},
  pages = {104513},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104513},
  urldate = {2024-03-05},
  abstract = {This novel experiment investigates the relationship between readers' eye movements and their use of ``noisy channel'' inferences when reading implausible sentences, and how this might be affected by cognitive aging. Young (18--26~years) and older (65--87~years) adult participants read sentences which were either plausible or implausible. Crucially, readers could assign a plausible interpretation to the implausible sentences by inferring that a preposition (i.e., to) had been unintentionally omitted or included. Our results reveal that readers' fixation locations within such sentences are associated with the likelihood of them inferring the presence or absence of this critical preposition to reach a plausible interpretation. Moreover, our older adults were more likely to make these noisy-channel inferences than the younger adults, potentially because their poorer visual processing and greater linguistic experience promote such inference-making. We propose that the present findings provide novel experimental evidence for a perceptual contribution to noisy-channel inference-making during reading.},
  keywords = {Eye-movements during reading,Noisy-channel language processing,Oculomotor control,Sentence processing},
  file = {~/Zotfiles/cutter.m2024 Eye-movements during reading and noisy-c.pdf}
}

@article{dai.c:2022,
  title = {An {{Invitation}} to {{Sequential Monte Carlo Samplers}}},
  author = {Dai, Chenguang and {Heng, Jeremy} and {Jacob, Pierre E.} and Whiteley, Nick},
  year = {2022},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {117},
  number = {539},
  pages = {1587--1600},
  publisher = {ASA Website},
  issn = {0162-1459},
  doi = {10.1080/01621459.2022.2087659},
  urldate = {2025-06-29},
  abstract = {Statisticians often use Monte Carlo methods to approximate probability distributions, primarily with Markov chain Monte Carlo and importance sampling. Sequential Monte Carlo samplers are a class of algorithms that combine both techniques to approximate distributions of interest and their normalizing constants. These samplers originate from particle filtering for state space models and have become general and scalable sampling techniques. This article describes sequential Monte Carlo samplers and their possible implementations, arguing that they remain under-used in statistics, despite their ability to perform sequential inference and to leverage parallel processing resources among other potential benefits. Supplementary materials for this article are available online.},
  keywords = {sampling,sequential Monte Carlo},
  file = {/Users/j/MIT Dropbox/Jacob Vigly/Zotfiles/dai.c2022 Supplement.pdf;~/Zotfiles/dai.c2022 An Invitation to Sequential Monte Carlo.pdf}
}

@inproceedings{dai.z:2019,
  title = {Transformer-{{XL}}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
  year = {2019},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/p19-1285},
  bdsk-url-2 = {https://doi.org/10.18653/v1/p19-1285},
  date-added = {2021-11-30 13:59:20 -0500},
  date-modified = {2021-11-30 13:59:33 -0500}
}

@inproceedings{daliri.m:2024,
  title = {Simple Analysis of Priority Sampling},
  booktitle = {2024 {{Symposium}} on {{Simplicity}} in {{Algorithms}} ({{SOSA}})},
  author = {Daliri, Majid and Freire, Juliana and Musco, Christopher and Santos, A{\'e}cio and Zhang, Haoxiang},
  year = {2024},
  month = jan,
  series = {Proceedings},
  eprint = {https://epubs.siam.org/doi/pdf/10.1137/1.9781611977936.21},
  pages = {224--229},
  publisher = {{Society for Industrial and Applied Mathematics}},
  address = {Alexandria, VA, USA},
  doi = {10.1137/1.9781611977936.21},
  abstract = {Abstract We prove a tight upper bound on the variance of the priority sampling method (aka sequential Poisson sampling). Our proof is significantly shorter and simpler than the original proof given by Mario Szegedy at STOC 2006, which resolved a conjecture by Duffield, Lund, and Thorup.},
  isbn = {978-1-61197-793-6},
  langid = {english},
  file = {~/Zotfiles/daliri.m2024 Simple analysis of priority sampling.pdf}
}

@misc{damani.m:2024arxiv,
  title = {Learning How Hard to Think: Input-Adaptive Allocation of {{LM}} Computation},
  shorttitle = {Learning How Hard to Think},
  author = {Damani, Mehul and Shenfeld, Idan and Peng, Andi and Bobu, Andreea and Andreas, Jacob},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04707},
  eprint = {2410.04707},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04707},
  urldate = {2024-10-17},
  abstract = {Computationally intensive decoding procedures--including search, reranking, and self-critique--can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50\% at no cost to response quality, or improve quality by up to 10\% at a fixed computational budget.},
  archiveprefix = {arXiv},
  keywords = {adaptive complexity,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/damani.m2024arxiv Learning How Hard to Think Input-Adapti.pdf}
}

@article{danon.g:2006,
  title = {Caseless Nominals and the Projection of {{DP}}},
  author = {Danon, Gabi},
  year = {2006},
  journal = {Natural Language \& Linguistic Theory},
  volume = {24},
  number = {4},
  pages = {977},
  issn = {1573-0859},
  doi = {10.1007/s11049-006-9005-6},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:43:43 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case,subject positions}
}

@article{danon.g:2011,
  title = {Agreement and {{DP-Internal}} Feature Distribution},
  author = {Danon, Gabi},
  year = {2011},
  journal = {Syntax (Oxford, England)},
  volume = {14},
  number = {4},
  pages = {297--317},
  doi = {10.1111/j.1467-9612.2011.00154.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2011.00154.x},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:45:11 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,subject positions}
}

@incollection{darwiche.a:2009,
  title = {Approximate {{Inference}} by {{Stochastic Sampling}}},
  booktitle = {Modeling and {{Reasoning}} with {{Bayesian Networks}}},
  editor = {Darwiche, Adnan},
  year = {2009},
  pages = {378--416},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511811357.016},
  urldate = {2025-02-19},
  abstract = {We discuss in this chapter a class of approximate inference algorithms based on stochastic sampling: a process by which we repeatedly simulate situations according to their probability and then estimate the probabilities of events based on the frequency of their occurrence in the simulated situations.IntroductionConsider the Bayesian network in Figure 15.1 and suppose that our goal is to estimate the probability of some event, say, wet grass. Stochastic sampling is a method for estimating such probabilities that works by measuring the frequency at which events materialize in a sequence of situations simulated according to their probability of occurrence. For example, if we simulate 100 situations and find out that the grass is wet in 30 of them, we estimate the probability of wet grass to be 3/10. As we see later, we can efficiently simulate situations according to their probability of occurrence by operating on the corresponding Bayesian network, a process that provides the basis for many of the sampling algorithms we consider in this chapter.The statements of sampling algorithms are remarkably simple compared to the methods for exact inference discussed in previous chapters, and their accuracy can be made arbitrarily high by increasing the number of sampled situations. However, the design of appropriate sampling methods may not be trivial as we may need to focus the sampling process on a set of situations that are of particular interest.},
  keywords = {importance sampling,likelihood weighting},
  file = {~/Zotfiles/darwiche.a2009ch15 Approximate Inference by Stochastic Samp.pdf}
}

@inproceedings{darwin.o:2022,
  title = {On the {{Sequential Probability Ratio Test}} in {{Hidden Markov Models}}},
  booktitle = {33rd {{International Conference}} on {{Concurrency Theory}} ({{CONCUR}} 2022)},
  author = {Darwin, Oscar and Kiefer, Stefan},
  editor = {Klin, Bartek and Lasota, S{\l}awomir and Muscholl, Anca},
  year = {2022},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {243},
  pages = {9:1--9:16},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address = {Dagstuhl, Germany},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.CONCUR.2022.9},
  urldate = {2023-11-07},
  isbn = {978-3-95977-246-4},
  keywords = {hidden Markov models,Markov chains,probabilistic systems,verification},
  file = {~/Zotfiles/darwin.o2022 On the Sequential Probability Ratio Test.pdf}
}

@inproceedings{dasgupta.s:2016,
  title = {A Cost Function for Similarity-Based Hierarchical Clustering},
  booktitle = {Proceedings of the 48th Annual {{ACM SIGACT}} Symposium on Theory of Computing, {{STOC}} 2016, Cambridge, {{MA}}, {{USA}}, June 18-21, 2016},
  author = {Dasgupta, Sanjoy},
  editor = {Wichs, Daniel and Mansour, Yishay},
  year = {2016},
  pages = {118--127},
  publisher = {ACM},
  doi = {10.1145/2897518.2897527},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/stoc/Dasgupta16.bib},
  timestamp = {Tue, 06 Nov 2018 00:00:00 +0100}
}

@inproceedings{dathathri.s:2020,
  title = {Plug and Play Language Models: {{A}} Simple Approach to Controlled Text Generation},
  shorttitle = {Plug and Play Language Models},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  year = {2020},
  month = apr,
  urldate = {2022-07-11},
  abstract = {We control the topic and sentiment of text generation (almost) without any training.},
  langid = {english},
  file = {~/Zotfiles/dathathri.s2020 Plug and play language models A simple.pdf}
}

@misc{davies.m:2008COCA,
  title = {The {{Corpus}} of {{Contemporary American English}} ({{COCA}})},
  author = {Davies, Mark},
  year = {2008}
}

@misc{davies.m:2008COCAfreq,
  title = {Word Frequency Data from the {{Corpus}} of {{Contemporary American English}} ({{COCA}})},
  author = {Davies, Mark},
  year = {2008},
  urldate = {2023-11-22}
}

@article{dawid.a:2004,
  title = {Probability, Causality and the Empirical World: {{A Bayes}}--de {{Finetti}}--{{Popper}}--{{Borel}} Synthesis},
  author = {Dawid, A. P.},
  year = {2004},
  month = feb,
  journal = {Statistical Science},
  volume = {19},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/088342304000000125},
  bdsk-url-2 = {https://doi.org/10.1214/088342304000000125},
  date-added = {2022-04-13 19:51:33 -0400},
  date-modified = {2022-04-13 19:51:34 -0400},
  file = {~/Zotfiles/dawid.a2004 Probability, causality and the empirical.pdf}
}

@article{dayan.p:1995,
  title = {The {{Helmholtz}} Machine},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  journal = {Neural Computation},
  volume = {7},
  number = {5},
  pages = {889--904},
  issn = {0899-7667},
  doi = {10.1162/neco.1995.7.5.889},
  urldate = {2022-11-30},
  abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
  file = {~/Zotfiles/dayan.p1995 The Helmholtz machine.pdf}
}

@inproceedings{deal.a:2015,
  title = {Interaction and Satisfaction in {{$\varphi$}}-Agreement},
  booktitle = {Proceedings of {{NELS}}},
  author = {Deal, Amy Rose},
  year = {2015},
  volume = {45},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:25 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@article{decaire.r:2017,
  title = {On Optionality in {{Mohawk}} Noun Incorporation},
  author = {DeCaire, Ryan and Johns, Alana and Ku{\v c}erov{\'a}, Ivona},
  year = {2017},
  month = dec,
  journal = {Toronto Working Papers in Linguistics},
  volume = {39},
  issn = {1718-3510},
  urldate = {2022-05-30},
  abstract = {Noun incorporation is a phenomenon much discussed within Iroquoian language literature. In this paper, we consider noun incorporation in Mohawk, a language within the Iroquoian language family, and argue that what has often been considered to be optional noun incorporation is in fact primarily determined by the information structure of the clause. We show that with the exception of lexically-determined verbs that always or never incorporate, every verb may or may not incorporate its nominal object. We analyse the incorporated version as the default structure. The non-incorporated counterpart is licensed only under particular information-structure properties. We provide evidence that noun excorporation arises whenever the verb or the object noun is focused, and in turn moves to the left periphery.},
  copyright = {Copyright (c) 2017 Ryan DeCaire, Alana Johns, Ivona Ku{\v c}erov{\'a}},
  langid = {english},
  keywords = {excorporation,iroquoian,Mohawk,noun incorporation},
  file = {~/Zotfiles/decaire.r2017 On optionality in Mohawk noun incorporat.pdf}
}

@misc{deepseek-ai.:2025arxiv,
  title = {{{DeepSeek-R1}}: Incentivizing Reasoning Capability in {{LLMs}} via Reinforcement Learning},
  shorttitle = {{{DeepSeek-R1}}},
  author = {{DeepSeek-AI} and Guo, Daya and Yang, Dejian and Zhang, Haowei and Song, Junxiao and Zhang, Ruoyu and Xu, Runxin and Zhu, Qihao and Ma, Shirong and Wang, Peiyi and Bi, Xiao and Zhang, Xiaokang and Yu, Xingkai and Wu, Yu and Wu, Z. F. and Gou, Zhibin and Shao, Zhihong and Li, Zhuoshu and Gao, Ziyi and Liu, Aixin and Xue, Bing and Wang, Bingxuan and Wu, Bochao and Feng, Bei and Lu, Chengda and Zhao, Chenggang and Deng, Chengqi and Zhang, Chenyu and Ruan, Chong and Dai, Damai and Chen, Deli and Ji, Dongjie and Li, Erhang and Lin, Fangyun and Dai, Fucong and Luo, Fuli and Hao, Guangbo and Chen, Guanting and Li, Guowei and Zhang, H. and Bao, Han and Xu, Hanwei and Wang, Haocheng and Ding, Honghui and Xin, Huajian and Gao, Huazuo and Qu, Hui and Li, Hui and Guo, Jianzhong and Li, Jiashi and Wang, Jiawei and Chen, Jingchang and Yuan, Jingyang and Qiu, Junjie and Li, Junlong and Cai, J. L. and Ni, Jiaqi and Liang, Jian and Chen, Jin and Dong, Kai and Hu, Kai and Gao, Kaige and Guan, Kang and Huang, Kexin and Yu, Kuai and Wang, Lean and Zhang, Lecong and Zhao, Liang and Wang, Litong and Zhang, Liyue and Xu, Lei and Xia, Leyi and Zhang, Mingchuan and Zhang, Minghua and Tang, Minghui and Li, Meng and Wang, Miaojun and Li, Mingming and Tian, Ning and Huang, Panpan and Zhang, Peng and Wang, Qiancheng and Chen, Qinyu and Du, Qiushi and Ge, Ruiqi and Zhang, Ruisong and Pan, Ruizhe and Wang, Runji and Chen, R. J. and Jin, R. L. and Chen, Ruyi and Lu, Shanghao and Zhou, Shangyan and Chen, Shanhuang and Ye, Shengfeng and Wang, Shiyu and Yu, Shuiping and Zhou, Shunfeng and Pan, Shuting and Li, S. S. and Zhou, Shuang and Wu, Shaoqing and Ye, Shengfeng and Yun, Tao and Pei, Tian and Sun, Tianyu and Wang, T. and Zeng, Wangding and Zhao, Wanjia and Liu, Wen and Liang, Wenfeng and Gao, Wenjun and Yu, Wenqin and Zhang, Wentao and Xiao, W. L. and An, Wei and Liu, Xiaodong and Wang, Xiaohan and Chen, Xiaokang and Nie, Xiaotao and Cheng, Xin and Liu, Xin and Xie, Xin and Liu, Xingchao and Yang, Xinyu and Li, Xinyuan and Su, Xuecheng and Lin, Xuheng and Li, X. Q. and Jin, Xiangyue and Shen, Xiaojin and Chen, Xiaosha and Sun, Xiaowen and Wang, Xiaoxiang and Song, Xinnan and Zhou, Xinyi and Wang, Xianzu and Shan, Xinxia and Li, Y. K. and Wang, Y. Q. and Wei, Y. X. and Zhang, Yang and Xu, Yanhong and Li, Yao and Zhao, Yao and Sun, Yaofeng and Wang, Yaohui and Yu, Yi and Zhang, Yichao and Shi, Yifan and Xiong, Yiliang and He, Ying and Piao, Yishi and Wang, Yisong and Tan, Yixuan and Ma, Yiyang and Liu, Yiyuan and Guo, Yongqiang and Ou, Yuan and Wang, Yuduan and Gong, Yue and Zou, Yuheng and He, Yujia and Xiong, Yunfan and Luo, Yuxiang and You, Yuxiang and Liu, Yuxuan and Zhou, Yuyang and Zhu, Y. X. and Xu, Yanhong and Huang, Yanping and Li, Yaohui and Zheng, Yi and Zhu, Yuchen and Ma, Yunxian and Tang, Ying and Zha, Yukun and Yan, Yuting and Ren, Z. Z. and Ren, Zehui and Sha, Zhangli and Fu, Zhe and Xu, Zhean and Xie, Zhenda and Zhang, Zhengyan and Hao, Zhewen and Ma, Zhicheng and Yan, Zhigang and Wu, Zhiyu and Gu, Zihui and Zhu, Zijia and Liu, Zijun and Li, Zilin and Xie, Ziwei and Song, Ziyang and Pan, Zizheng and Huang, Zhen and Xu, Zhipeng and Zhang, Zhongyu and Zhang, Zhen},
  year = {2025},
  month = jan,
  number = {arXiv:2501.12948},
  eprint = {2501.12948},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2501.12948},
  urldate = {2025-01-30},
  abstract = {We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@book{definetti.b:1972book,
  title = {Probability, Induction and Statistics: The Art of Guessing},
  shorttitle = {Probability, Induction and Statistics},
  author = {{de Finetti}, Bruno},
  year = {1972},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  address = {London New York},
  isbn = {978-0-471-20140-3},
  langid = {english},
  file = {~/Zotfiles/definetti.b1972 Probability, induction and statistics t.djvu}
}

@book{definetti.b:2017book,
  title = {Theory of Probability: A Critical Introductory Treatment},
  shorttitle = {Theory of Probability},
  author = {{de Finetti}, Bruno},
  editor = {Mach{\'i}, Antonio and Smith, Adrian},
  year = {2017},
  month = feb,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781119286387},
  urldate = {2024-05-13},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-1-119-28637-0 978-1-119-28638-7},
  langid = {english}
}

@article{degen.j:2023,
  title = {The Rational Speech Act Framework},
  author = {Degen, Judith},
  year = {2023},
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {9},
  number = {Volume 9, 2023},
  pages = {519--540},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-031220-010811},
  urldate = {2024-05-15},
  abstract = {The past decade has seen the rapid development of a new approach to pragmatics that attempts to integrate insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science in the study of meaning: probabilistic pragmatics. The most influential probabilistic approach to pragmatics is the Rational Speech Act (RSA) framework. In this review, I demonstrate the basic mechanics and commitments of RSA as well as some of its standard extensions, highlighting the key features that have led to its success in accounting for a wide variety of pragmatic phenomena. Fundamentally, it treats language as probabilistic, informativeness as gradient, alternatives as context-dependent, and subjective prior beliefs (world knowledge) as a crucial facet of interpretation. It also provides an integrated account of the link between production and interpretation. I highlight key challenges for RSA, which include scalability, the treatment of the boundedness of cognition, and the incremental and compositional nature of language.},
  langid = {english},
  file = {~/Zotfiles/degen.j2023 The rational speech act framework.pdf}
}

@inproceedings{dehaene.d:2019,
  title = {Iterative Energy-Based Projection on a Normal Data Manifold for Anomaly Localization},
  booktitle = {International {{Conference}} on {{Learning Representations}}, 2020},
  author = {Dehaene, David and Frigo, Oriel and Combrexelle, S{\'e}bastien and Eline, Pierre},
  year = {2019},
  month = sep,
  urldate = {2023-08-09},
  abstract = {Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder's loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-defined optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset.},
  langid = {english},
  file = {~/Zotfiles/dehaene.d2019 Iterative energy-based projection on a n.pdf}
}

@inproceedings{dehghani.m:2019,
  title = {Universal Transformers},
  booktitle = {7th {{International Conference}} on {{Learning Representations}} ({{ICLR}} 2019)},
  author = {Dehghani, Mostafa and Gouws, Stephan and Vinyals, Oriol and Uszkoreit, Jakob and Kaiser, Lukasz},
  year = {2019},
  month = may,
  publisher = {OpenReview.net},
  address = {New Orleans, Louisiana, USA},
  file = {~/Zotfiles/dehghani.m2019 Universal transformers.pdf}
}

@misc{deletang.g:2022arxiv,
  title = {Neural Networks and the {{Chomsky}} Hierarchy},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and {Grau-Moya}, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  year = {2022},
  month = oct,
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-11},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (10250 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {~/Zotfiles/deletang.g2022 Neural networks and the Chomsky hierarch.pdf}
}

@book{delmoral.p:2004book,
  title = {Feynman-{{Kac}} Formulae: Genealogical and Interacting Particle Systems with Applications},
  author = {Del Moral, Pierre},
  editor = {Gani, J. and Heyde, C. C. and Kurtz, T. G.},
  year = {2004},
  month = mar,
  series = {Probability and Its {{Applications}}},
  edition = {1},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4684-9393-1},
  urldate = {2023-04-22},
  isbn = {978-1-4684-9393-1},
  keywords = {algorithms,Feynman-Kac formula,filtering problem,genetic algorithms,interacting particle system,Markov chain,Markov kernel,Markov process,Monte Carlo method,Statistical Physics},
  file = {~/Zotfiles/delmoral.p2004 Feynman-Kac formulae genealogical and i.pdf}
}

@article{delmoral.p:2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
  date-added = {2022-04-30 17:47:24 -0400},
  date-modified = {2022-04-30 22:51:54 -0400},
  keywords = {particle filtering,sampling,sequential monte carlo},
  file = {~/Zotfiles/del-moral.p2006 Sequential Monte Carlo samplers 2.pdf;~/Zotfiles/del-moral.p2006 Sequential Monte Carlo samplers.pdf}
}

@phdthesis{demarcken.c:1996,
  title = {Unsupervised Language Acquisition},
  author = {{de Marcken}, Carl},
  year = {1996},
  eprint = {1721.1/10640},
  eprinttype = {hdl},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/phd/ndltd/Marcken96},
  date-added = {2020-01-27 11:53:05 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Cambridge, MA, USA},
  keywords = {information theory,unsupervised grammar induction},
  timestamp = {Mon, 08 May 2017 16:29:45 +0200}
}

@incollection{demarcken.c:1999,
  title = {On the Unsupervised Induction of Phrase-Structure Grammars},
  booktitle = {Natural Language Processing Using Very Large Corpora},
  author = {{de Marcken}, C.},
  editor = {Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  year = {1999},
  pages = {191--208},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-2390-9_12},
  abstract = {Researchers investigating the acquisition of phrase-structure grammars from raw text have had only mixed success. In particular, unsupervised learning techniques, such as the inside-outside algorithm (Baker, 1979) for estimating the parameters of stochastic context-free grammars (SCFGs), tend to produce grammars that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to constrain the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint, and currently the best performing parsers are trained on treebanks (Black et al., 1992; Magerman, 1995).},
  date-added = {2020-01-27 11:50:06 -0500},
  date-modified = {2021-07-16 11:27:47 -0400},
  isbn = {978-94-017-2390-9},
  project = {syntactic embedding},
  keywords = {information theory,unsupervised grammar induction}
}

@inproceedings{demarneffe.m:2006,
  title = {Generating Typed Dependency Parses from Phrase Structure Parses},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({{LREC}}'06)},
  author = {{de Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
  year = {2006},
  publisher = {European Language Resources Association (ELRA)},
  address = {Genoa, Italy}
}

@inproceedings{demarneffe.m:2008,
  title = {The {{Stanford}} Typed Dependencies Representation},
  booktitle = {Coling 2008: {{Proceedings}} of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher D.},
  year = {2008},
  pages = {1--8},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK}
}

@manual{demarneffe.m:2008a,
  type = {Manual},
  title = {Stanford Typed Dependencies Manual},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher},
  year = {2008},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-03-12 10:47:01 -0500},
  organization = {Stanford NLP},
  project = {syntactic embedding},
  version = {Stanford Parser v.3.7.0},
  keywords = {dependency structures,stanford dependencies}
}

@article{demarneffe.m:2019,
  title = {Dependency Grammar},
  author = {{de Marneffe}, Marie-Catherine and Nivre, Joakim},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {197--218},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011718-011842},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011842},
  date-added = {2021-07-16 19:34:18 -0400},
  date-modified = {2021-07-16 19:34:19 -0400}
}

@article{demberg.v:2008,
  title = {Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity},
  author = {Demberg, Vera and Keller, Frank},
  year = {2008},
  journal = {Cognition},
  volume = {109},
  number = {2},
  pages = {193--210},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.07.008},
  abstract = {We evaluate the predictions of two theories of syntactic processing complexity, dependency locality theory (DLT) and surprisal, against the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text. Our results show that DLT integration cost is not a significant predictor of reading times for arbitrary words in the corpus. However, DLT successfully predicts reading times for nouns. We also find evidence for integration cost effects at auxiliaries, not predicted by DLT. For surprisal, we demonstrate that an unlexicalized formulation of surprisal can predict reading times for arbitrary words in the corpus. Comparing DLT integration cost and surprisal, we find that the two measures are uncorrelated, which suggests that a complete theory will need to incorporate both aspects of processing complexity. We conclude that eye-tracking corpora, which provide reading time data for naturally occurring, contextualized sentences, can complement experimental evidence as a basis for theories of processing complexity.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2008.07.008},
  keywords = {Corpus data,Dependency locality theory,Eye-tracking,Processing complexity,Surprisal},
  file = {~/Zotfiles/demberg.v2008 Data from eye-tracking corpora as eviden.pdf}
}

@incollection{demberg.v:2019,
  title = {Cognitive Models of Syntax and Sentence Processing},
  booktitle = {Human {{Language}}: {{From Genes}} and {{Brains}} to {{Behavior}}},
  author = {Demberg, Vera and Keller, Frank},
  editor = {Hagoort, Peter},
  year = {2019},
  month = oct,
  pages = {293--312},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/10841.003.0027},
  urldate = {2022-10-22},
  isbn = {978-0-262-35386-1},
  file = {~/Zotfiles/demberg.v2019 Cognitive models of syntax and sentence.pdf}
}

@article{denison.s:2013,
  title = {Rational Variability in Children's Causal Inferences: {{The Sampling Hypothesis}}},
  shorttitle = {Rational Variability in Children's Causal Inferences},
  author = {Denison, Stephanie and Bonawitz, Elizabeth and Gopnik, Alison and Griffiths, Thomas L.},
  year = {2013},
  month = feb,
  journal = {Cognition},
  volume = {126},
  number = {2},
  pages = {285--300},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2012.10.010},
  urldate = {2025-03-06},
  abstract = {We present a proposal---``The Sampling Hypothesis''---suggesting that the variability in young children's responses may be part of a rational strategy for inductive inference. In particular, we argue that young learners may be randomly sampling from the set of possible hypotheses that explain the observed data, producing different hypotheses with frequencies that reflect their subjective probability. We test the Sampling Hypothesis with four experiments on 4- and 5-year-olds. In these experiments, children saw a distribution of colored blocks and an event involving one of these blocks. In the first experiment, one block fell randomly and invisibly into a machine, and children made multiple guesses about the color of the block, either immediately or after a 1-week delay. The distribution of guesses was consistent with the distribution of block colors, and the dependence between guesses decreased as a function of the time between guesses. In Experiments 2 and 3 the probability of different colors was systematically varied by condition. Preschoolers' guesses tracked the probabilities of the colors, as should be the case if they are sampling from the set of possible explanatory hypotheses. Experiment 4 used a more complicated two-step process to randomly select a block and found that the distribution of children's guesses matched the probabilities resulting from this process rather than the overall frequency of different colors. This suggests that the children's probability matching reflects sophisticated probabilistic inferences and is not merely the result of a na{\"i}ve tabulation of frequencies. Taken together the four experiments provide support for the Sampling Hypothesis, and the idea that there may be a rational explanation for the variability of children's responses in domains like causal inference.},
  keywords = {Approximate Bayesian inference,Causal learning,Cognitive development,Probability matching,Sampling Hypotheses,the sampling hypothesis},
  file = {~/Zotfiles/denison.s2013 Rational variability in childrens causa.pdf}
}

@book{dennett.d:2002book2,
  title = {Content and Consciousness},
  author = {Dennett, Daniel C.},
  year = {2002},
  month = jan,
  edition = {2},
  publisher = {Routledge},
  doi = {10.4324/9780203005729},
  urldate = {2024-05-13},
  isbn = {978-1-134-84830-0},
  langid = {english}
}

@inproceedings{depaivaalves.e:1996,
  title = {The Selection of the Most Probable Dependency Structure in {{Japanese}} Using Mutual Information},
  booktitle = {34th Annual Meeting of the Association for Computational Linguistics},
  author = {{de Paiva Alves}, Eduardo},
  year = {1996},
  pages = {372--374},
  publisher = {Association for Computational Linguistics},
  address = {Santa Cruz, California, USA},
  doi = {10.3115/981863.981919},
  bdsk-url-2 = {https://doi.org/10.3115/981863.981919}
}

@inproceedings{devlin.j:2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423},
  file = {~/Zotfiles/devlin.j2019 BERT pre-training of deep bidirectional.pdf}
}

@article{diaconis.p:2008,
  title = {The {{Markov}} Chain {{Monte Carlo}} Revolution},
  author = {Diaconis, Persi},
  year = {2008},
  month = nov,
  journal = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {179--205},
  publisher = {American Mathematical Society (AMS)},
  doi = {10.1090/s0273-0979-08-01238-x},
  bdsk-url-2 = {https://doi.org/10.1090/s0273-0979-08-01238-x},
  date-added = {2022-03-17 13:44:06 -0400},
  date-modified = {2022-03-17 13:44:07 -0400},
  file = {~/Zotfiles/diaconis.p2008 The Markov chain Monte Carlo revolution.pdf}
}

@inproceedings{dieng.a:2017,
  title = {Variational Inference via {$\chi$} Upper Bound Minimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-01-01},
  keywords = {chi-squared divergence,CHIVI,variational inference},
  file = {~/Zotfiles/dieng.a2017 Variational inference via  upper bound.pdf}
}

@misc{domke.j:2025arxiv,
  title = {Large {{Language Bayes}}},
  author = {Domke, Justin},
  year = {2025},
  month = apr,
  number = {arXiv:2504.14025},
  eprint = {2504.14025},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.14025},
  urldate = {2025-05-07},
  abstract = {Many domain experts do not have the time or training to write formal Bayesian models. This paper takes an informal problem description as input, and combines a large language model and a probabilistic programming language to create a joint distribution over formal models, latent variables, and data. A posterior over latent variables follows by conditioning on observed data and integrating over formal models. This presents a challenging inference problem. We suggest an inference recipe that amounts to generating many formal models from the large language model, performing approximate inference on each, and then doing a weighted average. This is justified an analyzed as a combination of self-normalized importance sampling, MCMC, and variational inference. We show that this produces sensible predictions without the need to specify a formal model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {~/Zotfiles/domke.j2025arxiv Large Language Bayes.pdf}
}

@misc{dong.l:2019arxiv,
  title = {Unified Language Model Pre-Training for Natural Language Understanding and Generation},
  author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2019},
  month = oct,
  number = {arXiv:1905.03197},
  eprint = {1905.03197},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM},
  file = {~/Zotfiles/dong.l2019UniLM Unified language model pre-training for.pdf}
}

@article{dotlacil.j:2021,
  title = {Parsing as a Cue-Based Retrieval Model},
  author = {Dotla{\v c}il, Jakub},
  year = {2021},
  journal = {Cognitive science},
  volume = {45},
  number = {8},
  pages = {e13020},
  issn = {1551-6709},
  doi = {10.1111/cogs.13020},
  urldate = {2022-07-01},
  abstract = {This paper develops a novel psycholinguistic parser and tests it against experimental and corpus reading data. The parser builds on the recent research into memory structures, which argues that memory retrieval is content-addressable and cue-based. It is shown that the theory of cue-based memory systems can be combined with transition-based parsing to produce a parser that, when combined with the cognitive architecture ACT-R, can model reading and predict online behavioral measures (reading times and regressions). The parser's modeling capacities are tested against self-paced reading experimental data (Grodner \& Gibson, 2005), eye-tracking experimental data (Staub, 2011), and a self-paced reading corpus (Futrell et al., 2018).},
  langid = {english},
  keywords = {ACT-R,Computational psycholinguistics,Cue-based retrieval,Memory retrieval,Modeling reading data,Processing},
  file = {~/Zotfiles/dotlacil.j2021 Parsing as a cue-based retrieval model.pdf}
}

@article{douc.r:2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  author = {Douc, Randal and Capp{\'e}, Olivier and Moulines, Eric},
  year = {2005},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.CS/0507025},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.CS/0507025},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  date-added = {2022-03-29 20:05:42 -0400},
  date-modified = {2022-03-29 20:05:43 -0400},
  keywords = {and Science (cs.CE),Computational Engineering,Finance,FOS: Computer and information sciences},
  file = {~/Zotfiles/douc.r2005 Comparison of resampling schemes for par.pdf}
}

@incollection{doucet.a:2001,
  title = {An Introduction to Sequential {{Monte Carlo}} Methods},
  booktitle = {Sequential Monte Carlo Methods in Practice},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  pages = {3--14},
  publisher = {Springer New York},
  doi = {10.1007/978-1-4757-3437-9_1},
  date-added = {2021-03-22 19:46:53 -0400},
  date-modified = {2021-03-22 19:46:54 -0400},
  file = {~/Zotfiles/doucet.a2001 An introduction to sequential Monte Carl.pdf}
}

@book{doucet.a:2001book,
  title = {Sequential {{Monte Carlo}} Methods in Practice},
  editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  series = {Statistics for Engineering and Information Science},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-1-4757-3437-9},
  bdsk-url-2 = {https://doi.org/10.1007/978-1-4757-3437-9},
  date-added = {2022-03-25 22:05:58 -0400},
  date-modified = {2022-03-25 22:10:04 -0400}
}

@incollection{doucet.a:2008,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {The {{Oxford Handbook}} of {{Nonlinear Filtering}}},
  author = {Doucet, Arnaud and Johansen, Adam M.},
  editor = {Crisan, Dan and Rozovski{\u \i}, Boris},
  year = {2008},
  month = dec,
  series = {Oxford {{Handbooks}}},
  pages = {656--704},
  publisher = {Oxford University Press},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision,econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
  annotation = {Note: Version 1.1 -- December 2008 with typographical corrections March 2012},
  file = {~/Zotfiles/doucet.a2008 A tutorial on particle filtering and smo 2.pdf;~/Zotfiles/doucet.a2008 A tutorial on particle filtering and smo.pdf}
}

@inproceedings{dozat.t:2017,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Dozat, Timothy and Manning, Christopher D.},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@incollection{drieghe.d:2011,
  title = {Parafoveal-on-Foveal Effects on Eye Movements during Reading},
  booktitle = {The Oxford Handbook of Eye Movements},
  author = {Drieghe, Denis},
  editor = {Liversedge, Simon P. and Gilchrist, Iain and Everling, Stefan},
  year = {2011},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780199539789.013.0046},
  bdsk-url-2 = {https://doi.org/10.1093/oxfordhb/9780199539789.013.0046},
  date-added = {2022-04-21 11:02:00 -0400},
  date-modified = {2022-04-21 11:03:00 -0400}
}

@inproceedings{du.l:2023,
  title = {A {{Measure-Theoretic Characterization}} of {{Tight Language Models}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Li and Torroba Hennigen, Lucas and Pimentel, Tiago and Meister, Clara and Eisner, Jason and Cotterell, Ryan},
  year = {2023},
  month = jul,
  pages = {9744--9770},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-24},
  abstract = {Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can ``leak'' onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.},
  file = {~/Zotfiles/du.l2023 A Measure-Theoretic Characterization of.pdf}
}

@inproceedings{du.w:2020,
  title = {Exploiting Syntactic Structure for Better Language Modeling: {{A}} Syntactic Distance Approach},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Du, Wenyu and Lin, Zhouhan and Shen, Yikang and O'Donnell, Timothy J. and Bengio, Yoshua and Zhang, Yue},
  year = {2020},
  pages = {6611--6628},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.591},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.591},
  date-modified = {2022-05-17 08:09:30 -0400}
}

@inproceedings{du.z:2022,
  title = {{{GLM}}: {{General}} Language Model Pretraining with Autoregressive Blank Infilling},
  shorttitle = {Glm},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  year = {2022},
  month = may,
  pages = {320--335},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.26},
  urldate = {2023-05-31},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\textbackslash}mbox{\textbackslash}times parameters of BERT Large , demonstrating its generalizability to different downstream tasks.},
  file = {~/Zotfiles/du.z2022GLM GLM General language model pretraining.pdf}
}

@article{duffield.n:2005,
  title = {Learn {{More}}, {{Sample Less}}: {{Control}} of {{Volume}} and {{Variance}} in {{Network Measurement}}},
  shorttitle = {Learn {{More}}, {{Sample Less}}},
  author = {Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
  year = {2005},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {51},
  number = {5},
  pages = {1756--1775},
  issn = {0018-9448},
  doi = {10.1109/TIT.2005.846400},
  urldate = {2025-02-24},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  keywords = {sampling,threshold sampling}
}

@article{duffield.n:2007,
  title = {Priority Sampling for Estimation of Arbitrary Subset Sums},
  author = {Duffield, Nick and Lund, Carsten and Thorup, Mikkel},
  year = {2007},
  month = dec,
  journal = {Journal of the Association for Computing Machinery (ACM)},
  volume = {54},
  number = {6},
  pages = {32-es},
  issn = {0004-5411},
  doi = {10.1145/1314690.1314696},
  urldate = {2025-02-24},
  abstract = {From a high-volume stream of weighted items, we want to create a generic sample of a certain limited size that we can later use to estimate the total weight of arbitrary subsets. Applied to Internet traffic analysis, the items could be records summarizing the flows of packets streaming by a router. Subsets could be flow records from different time intervals of a worm attack whose signature is later determined. The samples taken in the past thus allow us to trace the history of the attack even though the worm was unknown at the time of sampling.Estimation from the samples must be accurate even with heavy-tailed distributions where most of the weight is concentrated on a few heavy items. We want the sample to be weight sensitive, giving priority to heavy items. At the same time, we want sampling without replacement in order to avoid selecting heavy items multiple times. To fulfill these requirements we introduce priority sampling, which is the first weight-sensitive sampling scheme without replacement that works in a streaming context and is suitable for estimating subset sums. Testing priority sampling on Internet traffic analysis, we found it to perform an order of magnitude better than previous schemes.Priority sampling is simple to define and implement: we consider a steam of items i = 0,{\dots},n - 1 with weights wi. For each item i, we generate a random number {$\alpha$}i {$\in$} (0,1] and create a priority qi = wi/{$\alpha$}i. The sample S consists of the k highest priority items. Let {$\tau$} be the (k + 1)th highest priority. Each sampled item i in S gets a weight estimate {\^w}i = max\{wi, {$\tau$}\}, while nonsampled items get weight estimate {\^w}i = 0.Magically, it turns out that the weight estimates are unbiased, that is, E[{\^w}i] = wi, and by linearity of expectation, we get unbiased estimators over any subset sum simply by adding the sampled weight estimates from the subset. Also, we can estimate the variance of the estimates, and find, surprisingly, that the covariance between estimates {\^w}i and {\^w}j of different weights is zero.Finally, we conjecture an extremely strong near-optimality; namely that for any weight sequence, there exists no specialized scheme for sampling k items with unbiased weight estimators that gets smaller variance sum than priority sampling with k + 1 items. Szegedy settled this conjecture at STOC'06.},
  keywords = {categorical sampling,rejection sampling},
  file = {~/Zotfiles/duffield.n2007 Priority sampling for estimation of arbi.pdf}
}

@article{dupoux.e:2018,
  title = {Cognitive Science in the Era of Artificial Intelligence: A Roadmap for Reverse-Engineering the Infant Language-Learner},
  author = {Dupoux, Emmanuel},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {43--59},
  publisher = {Elsevier BV},
  doi = {10.1016/j.cognition.2017.11.008},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2017.11.008},
  date-added = {2021-10-04 22:09:38 -0400},
  date-modified = {2021-10-04 22:09:56 -0400}
}

@inproceedings{dyer.c:2016,
  title = {Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
  year = {2016},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/n16-1024},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n16-1024},
  date-added = {2021-09-13 21:32:13 -0400},
  date-modified = {2021-09-13 21:32:16 -0400}
}

@article{earley.j:1970,
  title = {An Efficient Context-Free Parsing Algorithm},
  author = {Earley, Jay},
  year = {1970},
  month = feb,
  journal = {Communications of the ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  urldate = {2022-06-13},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  file = {~/Zotfiles/earley.j1970 An efficient context-free parsing algori.pdf}
}

@book{earman.j:1992book,
  title = {Bayes or Bust? A Critical Examination of {{Bayesian}} Confirmation Theory},
  shorttitle = {Bayes or Bust?},
  author = {Earman, John},
  year = {1992},
  series = {A {{Bradford}} Book},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-51900-7},
  langid = {english}
}

@article{ebbinghaus.h:1885,
  title = {Memory: {{A Contribution}} to {{Experimental Psychology}}},
  shorttitle = {Memory},
  author = {Ebbinghaus, Hermann},
  translator = {Ruger, Henry A. and Bussenius, Clara E.},
  year = {2013},
  journal = {Annals of Neurosciences},
  series = {Annals {{Classics}}},
  volume = {20},
  number = {4},
  pages = {155--156},
  issn = {09727531, 09763260},
  doi = {10.5214/ans.0972.7531.200408},
  urldate = {2022-06-13},
  abstract = {The language of life as well as of science in attributing a memory to the mind attempts to point out the facts and their interpretation somewhat as follows: Mental states of every kind, -- sensations, feelings, ideas, -- which were at one time present in consciousness and then have disappeared from it, have not with their disappearance absolutely ceased to exist. Although the inwardly-turned look may no longer be able to find them, nevertheless they have not been utterly destroyed and annulled, but in a certain manner they continue to exist, stored up, so to speak, in the memory. We cannot, of course, directly observe their present existence, but it is revealed by the effects which come to our knowledge with a certainty like that with which we infer the existence of the stars below the horizon. These effects are of different kinds.},
  langid = {english},
  file = {~/Zotfiles/ebbinghaus.h1885 Memory A Contribution to Experimental P.pdf}
}

@article{eberhard.k:1995,
  title = {Eye Movements as a Window into Real-Time Spoken Language Comprehension in Natural Contexts},
  author = {Eberhard, Kathleen M. and {Spivey-Knowlton}, Michael J. and Sedivy, Julie C. and Tanenhaus, Michael K.},
  year = {1995},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {24},
  number = {6},
  pages = {409--436},
  issn = {1573-6555},
  doi = {10.1007/BF02143160},
  urldate = {2022-10-13},
  abstract = {When listeners follow spoken instructions to manipulate real objects, their eye movements to the objects are closely time locked to the referring words. We review five experiments showing that this time-locked characteristic of eye movements provides a detailed profile of the processes that underlie real-time spoken language comprehension. Together, the first four experiments showed that listerners immediately integrated lexical, sublexical, and prosodic information in the spoken input with information from the visual context to reduce the set of referents to the intended one. The fifth experiment demonstrated that a visual referential context affected the initial structuring of the linguistic input, eliminating even strong syntactic preferences that result in clear garden paths when the referential context is introduced linguistically. We argue that context affected the earliest moments of language processing because it was highly accessible and relevant to the behavioral goals of the listener.},
  langid = {english},
  keywords = {Cognitive Psychology,Initial Structure,Language Comprehension,Language Processing,Real Object},
  file = {~/Zotfiles/eberhard.k1995 Eye movements as a window into real-time.pdf}
}

@article{edwards.w:1961,
  title = {Behavioral {{Decision Theory}}},
  author = {Edwards, Ward},
  year = {1961},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {12},
  number = {Volume 12, 1961},
  pages = {473--498},
  publisher = {Annual Reviews},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.ps.12.020161.002353},
  urldate = {2025-03-05},
  langid = {english},
  keywords = {bayesian inference,statistical decision theory}
}

@article{efraimidis.p:2006,
  title = {Weighted Random Sampling with a Reservoir},
  author = {Efraimidis, Pavlos S. and Spirakis, Paul G.},
  year = {2006},
  month = mar,
  journal = {Information Processing Letters},
  volume = {97},
  number = {5},
  pages = {181--185},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2005.11.003},
  urldate = {2022-11-06},
  abstract = {In this work, a new algorithm for drawing a weighted random sample of size m from a population of n weighted items, where m{$\leq$}n, is presented. The algorithm can generate a weighted random sample in one-pass over unknown populations.},
  langid = {english},
  keywords = {Data streams,Parallel algorithms,Randomized algorithms,Reservoir sampling,Weighted random sampling}
}

@article{ehrlich.s:1981,
  title = {Contextual Effects on Word Perception and Eye Movements during Reading},
  author = {Ehrlich, Susan F. and Rayner, Keith},
  year = {1981},
  journal = {Journal of Memory and Language},
  volume = {20},
  number = {6},
  pages = {641},
  publisher = {Academic Press},
  doi = {10.1016/S0022-5371(81)90220-6},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  file = {~/Zotfiles/ehrlich.s1981 Contextual effects on word perception an.pdf}
}

@inproceedings{eisape.t:2020,
  title = {Cloze Distillation: Improving Neural Language Models with Human next-Word Prediction},
  shorttitle = {Cloze Distillation},
  booktitle = {Proceedings of the 24th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Eisape, Tiwalayo and Zaslavsky, Noga and Levy, Roger},
  editor = {Fern{\'a}ndez, Raquel and Linzen, Tal},
  year = {2020},
  month = nov,
  pages = {609--619},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.conll-1.49},
  urldate = {2024-08-01},
  abstract = {Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.},
  file = {~/Zotfiles/eisape.t2020 Cloze Distillation Improving Neural Lan.pdf}
}

@inproceedings{eisner.j:1996,
  title = {Three New Probabilistic Models for Dependency Parsing: An Exploration},
  booktitle = {{{COLING}} 1996 Volume 1: {{The}} 16th International Conference on Computational Linguistics},
  author = {Eisner, Jason M.},
  year = {1996}
}

@techreport{eisner.j:1997,
  type = {Technical Report},
  title = {An Empirical Comparison of Probability Models for Dependency Grammar},
  author = {Eisner, Jason M.},
  year = {1997},
  number = {IRCS-96-11},
  institution = {Institute for Research in Cognitive Science, University of Pennsylvania},
  keywords = {dependency parsing,parsing algorithm,projective dependencies,technical report},
  file = {~/Zotfiles/eisner.j1997 An empirical comparison of probability m.pdf}
}

@inproceedings{eisner.j:2016,
  title = {Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)},
  booktitle = {Proceedings of the {{Workshop}} on {{Structured Prediction}} for {{NLP}}},
  author = {Eisner, Jason},
  year = {2016},
  month = nov,
  pages = {1--17},
  publisher = {Association for Computational Linguistics},
  address = {Austin, TX},
  doi = {10.18653/v1/W16-5901},
  urldate = {2022-07-05},
  file = {~/Zotfiles/eisner.j2016 Inside-outside and forward-backward algo.pdf}
}

@inproceedings{ellis.k:2021,
  title = {{{DreamCoder}}: Bootstrapping Inductive Program Synthesis with Wake-Sleep Library Learning},
  shorttitle = {{{DreamCoder}}},
  booktitle = {Proceedings of the 42nd {{ACM SIGPLAN International Conference}} on {{Programming Language Design}} and {{Implementation}}},
  author = {Ellis, Kevin and Wong, Catherine and Nye, Maxwell and {Sabl{\'e}-Meyer}, Mathias and Morales, Lucas and Hewitt, Luke and Cary, Luc and {Solar-Lezama}, Armando and Tenenbaum, Joshua B.},
  year = {2021},
  month = jun,
  series = {{{PLDI}} 2021},
  pages = {835--850},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3453483.3454080},
  urldate = {2025-08-07},
  abstract = {We present a system for inductive program synthesis called DreamCoder, which inputs a corpus of synthesis problems each specified by one or a few examples, and automatically derives a library of program components and a neural search policy that can be used to efficiently solve other similar synthesis problems. The library and search policy bootstrap each other iteratively through a variant of "wake-sleep" approximate Bayesian learning. A new refactoring algorithm based on E-graph matching identifies common sub-components across synthesized programs, building a progressively deepening library of abstractions capturing the structure of the input domain. We evaluate on eight domains including classic program synthesis areas and AI tasks such as planning, inverse graphics, and equation discovery. We show that jointly learning the library and neural search policy leads to solving more problems, and solving them more quickly.},
  isbn = {978-1-4503-8391-2},
  file = {/Users/j/MIT Dropbox/Jacob Vigly/Zotfiles/ellis.k2021 DreamCoder with supplement.pdf;~/Zotfiles/ellis.k2021 DreamCoder bootstrapping inductive prog.pdf}
}

@article{ellis.k:2022,
  title = {Synthesizing Theories of Human Language with {{Bayesian}} Program Induction},
  author = {Ellis, Kevin and Albright, Adam and {Solar-Lezama}, Armando and Tenenbaum, Joshua B. and O'Donnell, Timothy J.},
  year = {2022},
  month = aug,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {5024},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-32012-w},
  urldate = {2024-04-13},
  abstract = {Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. We present a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds. We integrate Bayesian inference with program synthesis and representations inspired by linguistic theory and cognitive models of learning and discovery. Across 70 datasets from 58 diverse languages, our system synthesizes human-interpretable models for core aspects of each language's morpho-phonology, sometimes approaching models posited by human linguists. Joint inference across all 70 data sets automatically synthesizes a meta-model encoding interpretable cross-language typological tendencies. Finally, the same algorithm captures few-shot learning dynamics, acquiring new morphophonological rules from just one or a few examples. These results suggest routes to more powerful machine-enabled discovery of interpretable models in linguistics and other scientific domains.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {~/Zotfiles/ellis.k2022 Synthesizing theories of human language.pdf}
}

@article{elvira.v:2017,
  title = {Adapting the Number of Particles in Sequential {{Monte Carlo}} Methods through an Online Scheme for Convergence Assessment},
  author = {Elvira, V{\'i}ctor and M{\'i}guez, Joaqu{\'i}n and Djuri{\'c}, Petar M.},
  year = {2017},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {7},
  pages = {1781--1794},
  issn = {1941-0476},
  doi = {10.1109/TSP.2016.2637324},
  abstract = {Particle filters are broadly used to approximate posterior distributions of hidden states in state-space models by means of sets of weighted particles. While the convergence of the filter is guaranteed when the number of particles tends to infinity, the quality of the approximation is usually unknown but strongly dependent on the number of particles. In this paper, we propose a novel method for assessing the convergence of particle filters in an online manner, as well as a simple scheme for the online adaptation of the number of particles based on the convergence assessment. The method is based on a sequential comparison between the actual observations and their predictive probability distributions approximated by the filter. We provide a rigorous theoretical analysis of the proposed methodology and, as an example of its practical use, we present simulations of a simple algorithm for the dynamic and online adaptation of the number of particles during the operation of a particle filter on a stochastic version of the Lorenz 63 system.},
  keywords = {Adaptation models,adaptive complexity,computational complexity,Computational modeling,Convergence,convergence analysis,convergence assessment,Heuristic algorithms,Monte Carlo methods,Particle filtering,predictive distribution,Probability density function,sequential Monte Carlo,Signal processing algorithms},
  file = {~/Zotfiles/elvira.v2017 Adapting the number of particles in sequ.pdf}
}

@incollection{elvira.v:2021,
  title = {Advances in {{Importance Sampling}}},
  booktitle = {Wiley {{StatsRef}}: {{Statistics Reference Online}}},
  author = {Elvira, V{\'i}ctor and Martino, Luca},
  year = {2021},
  pages = {1--14},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118445112.stat08284},
  urldate = {2025-02-20},
  abstract = {Importance sampling (IS) is a Monte Carlo technique for the approximation of intractable distributions and integrals with respect to them. The origin of IS dates from the early 1950s. In the past decades, the rise of the Bayesian paradigm and the increase of the available computational resources have propelled the interest in this theoretically sound methodology. In this article, we first describe the basic IS algorithm and then revisit the recent advances in this methodology. We pay particular attention to two sophisticated lines. First, we focus on multiple IS (MIS), the case where more than one proposal is available. Second, we describe adaptive IS (AIS), the generic methodology for adapting one or more proposals.},
  copyright = {Copyright {\copyright} 2021 John Wiley \& Sons, Ltd. All rights reserved.},
  isbn = {978-1-118-44511-2},
  langid = {english},
  keywords = {computational statistics,importance sampling,Monte Carlo methods},
  file = {~/Zotfiles/elvira.v2021 Advances in Importance Sampling.pdf}
}

@article{elvira.v:2022,
  title = {Rethinking the Effective Sample Size},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2022},
  journal = {International Statistical Review},
  volume = {90},
  number = {3},
  pages = {525--550},
  issn = {1751-5823},
  doi = {10.1111/insr.12500},
  urldate = {2022-12-08},
  abstract = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit the approximation of the ESS in the specific context of importance sampling. The derivation of this approximation, that we will denote as ESS{\textasciicircum}, is partially available in a 1992 foundational technical report of Augustine Kong. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of ESS{\textasciicircum} make it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS{\textasciicircum} in the multiple importance sampling setting, we display numerical examples and we discuss several avenues for developing alternative metrics. This paper does not cover the use of ESS for Markov chain Monte Carlo algorithms.},
  langid = {english},
  keywords = {Bayesian inference,effective sample size,importance sampling,Monte Carlo methods},
  file = {~/Zotfiles/elvira.v2022 Rethinking the effective sample size.pdf}
}

@article{engbert.r:2005,
  title = {{{SWIFT}}: {{A}} Dynamical Model of Saccade Generation during Reading.},
  author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
  year = {2005},
  journal = {Psychological Review},
  volume = {112},
  number = {4},
  pages = {777--813},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0033-295x.112.4.777},
  bdsk-url-2 = {https://doi.org/10.1037/0033-295x.112.4.777},
  date-added = {2021-06-02 14:39:17 -0400},
  date-modified = {2021-06-02 14:39:49 -0400},
  keywords = {eye-tracking,frequency effects,predictability}
}

@article{engelmann.f:2013,
  title = {A Framework for Modeling the Interaction of Syntactic Processing and Eye Movement Control},
  author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {452--474},
  issn = {17568757},
  doi = {10.1111/tops.12026},
  urldate = {2022-08-28},
  langid = {english},
  file = {~/Zotfiles/engelmann.f2013 A framework for modeling the interaction.pdf}
}

@phdthesis{engelmann.f:2016phd,
  title = {Toward an Integrated Model of Sentence Processing in Reading},
  author = {Engelmann, Felix},
  year = {2016},
  address = {Potsdam, Germany},
  urldate = {2022-10-12},
  abstract = {In experiments investigating sentence processing, eye movement measures such as fixation durations and regression proportions while reading are commonly used to draw conclusions about processing difficulties. However, these measures are the result of an interaction of multiple cognitive levels and processing strategies and thus are only indirect indicators of processing difficulty. In order to properly interpret an eye movement response, one has to understand the underlying principles of adaptive processing such as trade-off mechanisms between reading speed and depth of comprehension that interact with task demands and individual differences. Therefore, it is necessary to establish explicit models of the respective mechanisms as well as their causal relationship with observable behavior. There are models of lexical processing and eye movement control on the one side and models on sentence parsing and memory processes on the other. However, no model so far combines both sides with explicitly defined linking assumptions. In this thesis, a model is developed that integrates oculomotor control with a parsing mechanism and a theory of cue-based memory retrieval. On the basis of previous empirical findings and independently motivated principles, adaptive, resource-preserving mechanisms of underspecification are proposed both on the level of memory access and on the level of syntactic parsing. The thesis first investigates the model of cue-based retrieval in sentence comprehension of Lewis \& Vasishth (2005) with a comprehensive literature review and computational modeling of retrieval interference in dependency processing. The results reveal a great variability in the data that is not explained by the theory. Therefore, two principles, 'distractor prominence' and 'cue confusion', are proposed as an extension to the theory, thus providing a more adequate description of systematic variance in empirical results as a consequence of experimental design, linguistic environment, and individual differences. In the remainder of the thesis, four interfaces between parsing and eye movement control are defined: Time Out, Reanalysis, Underspecification, and Subvocalization. By comparing computationally derived predictions with experimental results from the literature, it is investigated to what extent these four interfaces constitute an appropriate elementary set of assumptions for explaining specific eye movement patterns during sentence processing. Through simulations, it is shown how this system of in itself simple assumptions results in predictions of complex, adaptive behavior.  In conclusion, it is argued that, on all levels, the sentence comprehension mechanism seeks a balance between necessary processing effort and reading speed on the basis of experience, task demands, and resource limitations. Theories of linguistic processing therefore need to be explicitly defined and implemented, in particular with respect to linking assumptions between observable behavior and underlying cognitive processes. The comprehensive model developed here integrates multiple levels of sentence processing that hitherto have only been studied in isolation. The model is made publicly available as an expandable framework for future studies of the interactions between parsing, memory access, and eye movement control.},
  copyright = {https://rightsstatements.org/vocab/InC/1.0/},
  langid = {english},
  school = {Universit{\"a}t Potsdam},
  keywords = {ACT-R},
  file = {~/Zotfiles/engelmann.f2016PhD Toward an integrated model of sentence p.pdf}
}

@article{engelmann.f:2019,
  title = {The Effect of Prominence and Cue Association on Retrieval Processes: A Computational Account},
  shorttitle = {The Effect of Prominence and Cue Association on Retrieval Processes},
  author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {12},
  pages = {e12800},
  issn = {1551-6709},
  doi = {10.1111/cogs.12800},
  urldate = {2022-10-12},
  abstract = {We present a comprehensive empirical evaluation of the ACT-R--based model of sentence processing developed by Lewis and Vasishth (2005) (LV05). The predictions of the model are compared with the results of a recent meta-analysis of published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject--verb dependencies (J{\"a}ger, Engelmann, \& Vasishth, 2017). The comparison shows that the model has only partial success in explaining the data; and we propose that its prediction space is restricted by oversimplifying assumptions. We then implement a revised model that takes into account differences between individual experimental designs in terms of the prominence of the target and the distractor in memory- and context-dependent cue-feature associations. The predictions of the original and the revised model are quantitatively compared with the results of the meta-analysis. Our simulations show that, compared to the original LV05 model, the revised model accounts for the data better. The results suggest that effects of prominence and variable cue-feature associations need to be considered in the interpretation of existing empirical results and in the design and planning of future experiments. With regard to retrieval interference in sentence processing and to the broader field of psycholinguistic studies, we conclude that well-specified models in tandem with high-powered experiments are needed in order to uncover the underlying cognitive processes.},
  langid = {english},
  keywords = {ACT-R,Computational modeling,Cue-based retrieval,Dependency completion,Retrieval interference,Sentence processing},
  file = {~/Zotfiles/engelmann.f2019 The effect of prominence and cue associa.pdf}
}

@article{ennever.t:2017,
  title = {A Replicable Acoustic Measure of Lenition and the Nature of Variability in {{Gurindji}} Stops},
  author = {Ennever, Thomas and Meakins, Felicity and Round, Erich R.},
  year = {2017},
  month = aug,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  number = {1},
  publisher = {Open Library of the Humanities},
  doi = {10.5334/labphon.18},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.18},
  date-added = {2022-05-10 10:59:44 -0400},
  date-modified = {2022-05-10 10:59:54 -0400},
  keywords = {causality,lenition},
  file = {~/Zotfiles/ennever.t2017 A replicable acoustic measure of lenitio.pdf}
}

@article{erickson.t:1981,
  title = {From Words to Meaning: {{A}} Semantic Illusion},
  shorttitle = {From Words to Meaning},
  author = {Erickson, Thomas D. and Mattson, Mark E.},
  year = {1981},
  month = oct,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {20},
  number = {5},
  pages = {540--551},
  issn = {0022-5371},
  doi = {10.1016/S0022-5371(81)90165-1},
  urldate = {2024-05-28},
  abstract = {How are the meanings of individual words combined to form a more global description of meaning? This paper describes a phenomenon which sheds some light on one aspects of this process. Consider the following question: How many animals of each kind did Moses take on the Ark? Most people answer ``two'' even though they know quite well that it was Noah and not Moses who sailed the Ark. This illusion occurs even when time pressure is eliminated and subjects are told that questions may be ``wrong'' and given an example of a question with an inconsistent name in it. Two explanations of this illusion---that people are skipping over the name or that the focus of the question is leading them astry---are eliminated. Results indicate that it is important that the inconsistent name share semantic featuers with the correct name. An explanation of the illusion is developed.},
  file = {~/Zotfiles/erickson.t1981 From words to meaning A semantic illusi.pdf}
}

@misc{erion.g:2019,
  title = {Learning Explainable Models Using Attribution Priors},
  author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  year = {2019},
  eprint = {1906.10670},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:04:57 -0400},
  date-modified = {2019-07-05 11:06:04 -0400},
  project = {syntactic embedding},
  keywords = {gradient attribution priors}
}

@article{estes.w:1959,
  title = {Foundations of Linear Models},
  author = {Estes, William K and Suppes, Patrick},
  year = {1959},
  journal = {Studies in mathematical learning theory},
  pages = {137--179},
  publisher = {Stanford University Press Stanford},
  date-added = {2021-05-31 13:41:13 -0400},
  date-modified = {2021-05-31 13:41:26 -0400},
  keywords = {probability matching}
}

@inproceedings{ettinger.a:2016cogsci,
  title = {Modeling {{N400}} Amplitude Using Vector Space Models of Word Representation},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Ettinger, Allyson and Feldman, Naomi and Resnik, Philip and Phillips, Colin},
  year = {2016},
  volume = {38},
  urldate = {2025-04-03},
  abstract = {We use a vector space model (VSM) to simulate semantic relat-edness effects in sentence processing, and use this connectionto predict N400 amplitude in an ERP study by Federmeierand Kutas (1999). We find that the VSM-based model is ableto capture key elements of the authors' manipulations and re-sults, accounting for aspects of the results that are unexplainedby cloze probability. This demonstration provides a proof ofconcept for use of VSMs in modeling the particular contextrepresentations and corresponding facilitation processes thatseem to influence non-cloze-like behavior in the N400.},
  langid = {english},
  file = {~/Zotfiles/ettinger.a2016cogsci Modeling N400 amplitude using vector spa.pdf}
}

@book{fano.r:1961book,
  title = {Transmission of Information: A Statistical Theory of Communications},
  author = {Fano, Robert M},
  year = {1961},
  edition = {1},
  publisher = {MIT Press},
  address = {Cambdridge, Mass},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-07 09:14:32 -0400}
}

@incollection{farmer.t:2012,
  title = {Individual Differences in Sentence Processing},
  booktitle = {The {{Cambridge Handbook}} of {{Psycholinguistics}}},
  author = {Farmer, Thomas A. and Misyak, Jennifer B. and Christiansen, Morten H.},
  editor = {McRae, Ken and Joanisse, Marc and Spivey, Michael},
  year = {2012},
  series = {Cambridge {{Handbooks}} in {{Psychology}}},
  pages = {353--364},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139029377.018},
  urldate = {2022-10-13},
  isbn = {978-0-521-86064-2},
  file = {~/Zotfiles/farmer.t2012 Individual differences in sentence proce.pdf}
}

@inproceedings{faruqui.m:2018,
  title = {{{WikiAtomicEdits}}: {{A Multilingual Corpus}} of {{Wikipedia Edits}} for {{Modeling Language}} and {{Discourse}}},
  shorttitle = {{{WikiAtomicEdits}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Faruqui, Manaal and Pavlick, Ellie and Tenney, Ian and Das, Dipanjan},
  year = {2018},
  month = oct,
  pages = {305--315},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1028},
  urldate = {2023-09-07},
  abstract = {We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.},
  file = {~/Zotfiles/faruqui.m2018 WikiAtomicEdits A Multilingual Corpus o.pdf}
}

@article{fasiolo.m:2020,
  title = {Fast Calibrated Additive Quantile Regression},
  author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Rapha{\"e}l and Goude, Yannig},
  year = {2020},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {535},
  pages = {1402--1412},
  publisher = {Informa UK Limited},
  issn = {1537-274X},
  doi = {10.1080/01621459.2020.1725521},
  date-added = {2022-03-10 22:30:30 -0500},
  date-modified = {2022-03-10 22:30:33 -0500}
}

@article{fearnhead.p:2018,
  title = {Particle Filters and Data Assimilation},
  author = {Fearnhead, Paul and K{\"u}nsch, Hans R.},
  year = {2018},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {5},
  number = {1},
  pages = {421--449},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031017-100232},
  urldate = {2024-09-30},
  abstract = {State-space models can be used to incorporate subject knowledge on the underlying dynamics of a time series by the introduction of a latent Markov state process. A user can specify the dynamics of this process together with how the state relates to partial and noisy observations that have been made. Inference and prediction then involve solving a challenging inverse problem: calculating the conditional distribution of quantities of interest given the observations. This article reviews Monte Carlo algorithms for solving this inverse problem, covering methods based on the particle filter and the ensemble Kalman filter. We discuss the challenges posed by models with high-dimensional states, joint estimation of parameters and the state, and inference for the history of the state process. We also point out some potential new developments that will be important for tackling cutting-edge filtering applications.},
  langid = {english},
  file = {~/Zotfiles/fearnhead.p2018review Particle Filters and Data Assimilation.pdf}
}

@misc{feder.a:2021,
  title = {Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and {Wood-Doughty}, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2109.00725},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2109.00725},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-10 11:31:28 -0400},
  date-modified = {2022-05-10 11:31:44 -0400},
  keywords = {causality,machine learning,natural language processing},
  file = {~/Zotfiles/feder.a2021 Causal inference in natural language pro.pdf}
}

@article{federmeier.k:1999,
  title = {A Rose by Any Other Name: Long-Term Memory Structure and Sentence Processing},
  shorttitle = {A Rose by Any Other Name},
  author = {Federmeier, Kara D. and Kutas, Marta},
  year = {1999},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {41},
  number = {4},
  pages = {469--495},
  issn = {0749-596X},
  doi = {10.1006/jmla.1999.2660},
  urldate = {2023-12-12},
  abstract = {The effects of sentential context and semantic memory structure during on-line sentence processing were examined by recording event-related brain potentials as individuals read pairs of sentences for comprehension. The first sentence established an expectation for a particular exemplar of a semantic category, while the second ended with (1) that expected exemplar, (2) an unexpected exemplar from the same (expected) category, or (3) an unexpected item from a different (unexpected) category. Expected endings elicited a positivity between 250 and 550 ms while all unexpected endings elicited an N400, which was significantly smaller to items from the expected category. This N400 reduction varied with the strength of the contextually induced expectation: unexpected, categorically related endings elicited smaller N400s in more constraining contexts, despite their poorer fit to context (lower plausibility). This pattern of effects is best explained as reflecting the impact of context-independent long-term memory structure on sentence processing. The results thus suggest that physical and functional similarities that hold between objects in the world---i.e., category structure---influence neural organization and, in turn, routine language comprehension processes.},
  keywords = {categorization,event-related potentials,N400,sentence processing},
  file = {~/Zotfiles/federmeier.k1999 A rose by any other name long-term memo.pdf}
}

@article{federmeier.k:2007,
  title = {Thinking Ahead: {{The}} Role and Roots of Prediction in Language Comprehension},
  shorttitle = {Thinking Ahead},
  author = {Federmeier, Kara D.},
  year = {2007},
  journal = {Psychophysiology},
  volume = {44},
  number = {4},
  pages = {491--505},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2007.00531.x},
  urldate = {2023-12-12},
  abstract = {Reviewed are studies using event-related potentials to examine when and how sentence context information is used during language comprehension. Results suggest that, when it can, the brain uses context to predict features of likely upcoming items. However, although prediction seems important for comprehension, it also appears susceptible to age-related deterioration and can be associated with processing costs. The brain may address this trade-off by employing multiple processing strategies, distributed across the two cerebral hemispheres. In particular, left hemisphere language processing seems to be oriented toward prediction and the use of top-down cues, whereas right hemisphere comprehension is more bottom-up, biased toward the veridical maintenance of information. Such asymmetries may arise, in turn, because language comprehension mechanisms are integrated with language production mechanisms only in the left hemisphere (the PARLO framework).},
  langid = {english},
  keywords = {Aging,Event-related potentials,Hemispheric differences,Language,N400,Sentence processing},
  file = {~/Zotfiles/federmeier.k2007 Thinking ahead The role and roots of pr.pdf}
}

@article{fedorenko.e:2004cogsci,
  title = {Verbal Working Memory in Sentence Comprehension},
  author = {Fedorenko, Evelina and Gibson, Edward and Rohde, Douglas},
  year = {2004},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {26},
  number = {26},
  urldate = {2022-10-26},
  abstract = {Author(s): Fedorenko, Evelina; Gibson, Edward; Rohde, Douglas},
  langid = {english},
  keywords = {uniform information density},
  file = {~/Zotfiles/fedorenko.e2004 Verbal working memory in sentence compre.pdf}
}

@article{feldman.n:2009,
  title = {The Influence of Categories on Perception: {{Explaining}} the Perceptual Magnet Effect as Optimal Statistical Inference},
  shorttitle = {The Influence of Categories on Perception},
  author = {Feldman, Naomi H. and Griffiths, Thomas L. and Morgan, James L.},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {4},
  pages = {752--782},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/a0017196},
  abstract = {A variety of studies have demonstrated that organizing stimuli into categories can affect the way the stimuli are perceived. We explore the influence of categories on perception through one such phenomenon, the perceptual magnet effect, in which discriminability between vowels is reduced near prototypical vowel sounds. We present a Bayesian model to explain why this reduced discriminability might occur: It arises as a consequence of optimally solving the statistical problem of perception in noise. In the optimal solution to this problem, listeners' perception is biased toward phonetic category means because they use knowledge of these categories to guide their inferences about speakers' target productions. Simulations show that model predictions closely correspond to previously published human data, and novel experimental results provide evidence for the predicted link between perceptual warping and noise. The model unifies several previous accounts of the perceptual magnet effect and provides a framework for exploring categorical effects in other domains. (PsycINFO Database Record (c) 2019 APA, all rights reserved)},
  keywords = {Auditory Discrimination,Auditory Stimulation,Classification (Cognitive Process),Inference,Phonetics,Speech Perception,Statistical Inference,Statistical Probability},
  file = {~/Zotfiles/feldman.n2009 The influence of categories on perceptio.pdf}
}

@inproceedings{feller.w:1949,
  title = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Proceedings of the [{{First}}] Berkeley Symposium on Mathematical Statistics and Probability},
  author = {Feller, William},
  year = {1949},
  pages = {403--432},
  keywords = {diffusion processes}
}

@article{fenk.a:1980,
  title = {Konstanz Im Kurzzeitged{\"a}chtnis - Konstanz Im Sprachlichen Informationsflu{\ss}?},
  author = {Fenk, August and {Fenk-Oczlon}, Gertraud},
  year = {1980},
  month = jan,
  journal = {Zeitschrift f{\"u}r experimentelle und angewandte Psychologie},
  volume = {27},
  number = {3},
  pages = {400--414},
  date-added = {2021-10-26 14:23:04 -0400},
  date-modified = {2021-10-27 09:20:37 -0400}
}

@inproceedings{fernandezmonsalve.i:2012,
  title = {Lexical Surprisal as a General Predictor of Reading Time},
  booktitle = {Proceedings of the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fernandez Monsalve, Irene and Frank, Stefan L. and Vigliocco, Gabriella},
  year = {2012},
  month = apr,
  pages = {398--408},
  publisher = {Association for Computational Linguistics},
  address = {Avignon, France},
  date-added = {2021-11-29 10:29:43 -0500},
  date-modified = {2021-11-29 10:29:44 -0500}
}

@article{ferreira.f:2001,
  title = {Misinterpretations of {{Garden-Path Sentences}}: {{Implications}} for {{Models}} of {{Sentence Processing}} and {{Reanalysis}}},
  shorttitle = {Misinterpretations of {{Garden-Path Sentences}}},
  author = {Ferreira, Fernanda and Christianson, Kiel and Hollingworth, Andrew},
  year = {2001},
  month = jan,
  journal = {Journal of Psycholinguistic Research},
  volume = {30},
  number = {1},
  pages = {3--20},
  issn = {1573-6555},
  doi = {10.1023/A:1005290706460},
  urldate = {2022-06-11},
  abstract = {Theories of sentence comprehension have addressed both initial parsing processes and mechanisms responsible for reanalysis. Three experiments are summarized that were designed to investigate the reanalysis and interpretation of relatively difficult garden-path sentences (e.g., While Anna dressed the baby spit up on the bed). After reading such sentences, participants correctly believed that the baby spit up on the bed; however, they often confidently, yet incorrectly, believed that Anna dressed the baby. These results demonstrate that garden-path reanalysis is not an all-or-nothing process and that thematic roles initially assigned for the subordinate clause verb are not consistently revised. The implications of the partial reanalysis phenomenon for Fodor and Inoue's (1998) model of reanalysis and sentence processing are discussed. In addition, we discuss the possibility that language processing often creates ``good enough'' structures rather than ideal structures.},
  langid = {english},
  keywords = {parsing,reanalysis,semantics,syntactic ambiguity},
  file = {~/Zotfiles/ferreira.f2001 Misinterpretations of Garden-Path Senten.pdf}
}

@article{ferreira.f:2002,
  title = {Good-Enough Representations in Language Comprehension},
  author = {Ferreira, Fernanda and Bailey, Karl G.D. and Ferraro, Vittoria},
  year = {2002},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {11},
  number = {1},
  pages = {11--15},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1111/1467-8721.00158},
  urldate = {2022-06-11},
  abstract = {People comprehend utterances rapidly and without conscious effort. Traditional theories assume that sentence processing is algorithmic and that meaning is derived compositionally. The language processor is believed to generate representations of the linguistic input that are complete, detailed, and accurate. However, recent findings challenge these assumptions. Investigations of the misinterpretation of both garden-path and passive sentences have yielded support for the idea that the meaning people obtain for a sentence is often not a reflection of its true content. Moreover, incorrect interpretations may persist even after syntactic reanalysis has taken place. Our good-enough approach to language comprehension holds that language processing is sometimes only partial and that semantic representations are often incomplete. Future work will elucidate the conditions under which sentence processing is simply good enough.},
  langid = {english},
  keywords = {language comprehension,linguistic ambiguity,satisficing,syntax},
  file = {~/Zotfiles/ferreira.f2002 Good-enough representations in language.pdf}
}

@article{ferreira.f:2003,
  title = {The Misinterpretation of Noncanonical Sentences},
  author = {Ferreira, Fernanda},
  year = {2003},
  month = sep,
  journal = {Cognitive Psychology},
  volume = {47},
  number = {2},
  pages = {164--203},
  issn = {0010-0285},
  doi = {10.1016/S0010-0285(03)00005-7},
  urldate = {2025-02-03},
  abstract = {Research on language comprehension has focused on the resolution of syntactic ambiguities, and most studies have employed garden-path sentences to determine the system's preferences and to assess its use of nonsyntactic sources information. A topic that has been neglected is how syntactically challenging but essentially unambiguous sentences are processed, including passives and object-clefts---sentences that require thematic roles to be assigned in an atypical order. The three experiments described here tested the idea that sentences are processed both algorithmically and heuristically. Sentences were presented aurally and the participants' task was to identify the thematic roles in the sentence (e.g., Who was the do-er?). The first experiment demonstrates that passives are frequently and systematically misinterpreted, especially when they express implausible ideas. The second shows that the surface frequency of a syntactic form does not determine ease of processing, as active sentences and subject-clefts were comprehended equally easily despite the rareness of the latter type. The third experiment compares the processing of subject- and object-clefts, and the results show that they are similar to actives and passives, respectively, again despite the infrequent occurrence in English of any type of cleft. The results of the three experiments suggest that a comprehensive theory of language comprehension must assume that simple processing heuristics are used during processing in addition to (and perhaps sometimes instead of) syntactic algorithms. Moreover, the experiments support the idea that language processing is often based on shallow processing, yielding a merely ``good enough'' rather than a detailed linguistic representation of an utterance's meaning.}
}

@incollection{ferreira.f:2016,
  title = {Chapter {{Six}} - {{Prediction}}, {{Information Structure}}, and {{Good-Enough Language Processing}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Ferreira, Fernanda and Lowder, Matthew W.},
  editor = {Ross, Brian H.},
  year = {2016},
  month = jan,
  volume = {65},
  pages = {217--247},
  publisher = {Academic Press},
  doi = {10.1016/bs.plm.2016.04.002},
  urldate = {2022-06-14},
  abstract = {The good-enough language processing approach emphasizes people's tendency to generate superficial and even inaccurate interpretations of sentences. At the same time, a number of researchers have argued that prediction plays a key role in comprehension, allowing people to anticipate features of the input and even specific upcoming words based on sentential constraint. In this chapter, we review evidence from our lab supporting both approaches, even though at least superficially these two perspectives seem incompatible. We then argue that what allows us to link good-enough processing and prediction is the concept of information structure, which states that sentences are organized to convey both given or presupposed information, and new or focused information. Our fundamental proposal is that given or presupposed information is processed in a good-enough manner, while new or focused information is the target of the comprehender's prediction efforts. The result is a theory that brings together three different literatures that have been treated almost entirely independently, and which can be evaluated using a combination of behavioral, computational, and neural methods.},
  langid = {english},
  keywords = {Comprehension,Good-enough processing,Information structure,Language processing,Prediction}
}

@article{ferrer-i-cancho.r:2011,
  title = {Information Content versus Word Length in Random Typing},
  author = {{Ferrer-i-Cancho}, Ramon and {Moscoso del Prado Mart{\'i}n}, Ferm{\'i}n},
  year = {2011},
  month = dec,
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  volume = {2011},
  number = {12},
  pages = {L12002},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/2011/12/L12002},
  urldate = {2025-03-06},
  abstract = {Recently, it has been claimed that a linear relationship between a measure of information content and word length is expected from word length optimization and it has been shown that this linearity is supported by a strong correlation between information content and word length in many languages (Piantadosi et al 2011 Proc. Nat. Acad. Sci. 108 3825). Here, we study in detail some connections between this measure and standard information theory. The relationship between the measure and word length is studied for the popular random typing process where a text is constructed by pressing keys at random from a keyboard containing letters and a space behaving as a word delimiter. Although this random process does not optimize word lengths according to information content, it exhibits a linear relationship between information content and word length. The exact slope and intercept are presented for three major variants of the random typing process. A strong correlation between information content and word length can simply arise from the units making a word (e.g., letters) and not necessarily from the interplay between a word and its context as proposed by Piantadosi and co-workers. In itself, the linear relation does not entail the results of any optimization process.},
  langid = {english},
  file = {~/Zotfiles/ferrer-i-cancho.r2011 Information content versus word length i.pdf}
}

@article{ferrer-i-cancho.r:2015,
  title = {The Placement of the Head That Minimizes Online Memory: A Complex Systems Approach},
  author = {{Ferrer-i-Cancho}, Ramon},
  year = {2015},
  journal = {Language Dynamics and Change},
  volume = {5},
  number = {1},
  pages = {114--137},
  publisher = {Brill},
  date-added = {2019-05-15 00:10:35 -0400},
  date-modified = {2019-06-17 21:57:08 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,memory}
}

@inproceedings{finkel.j:2008,
  title = {Efficient, Feature-Based, Conditional Random Field Parsing},
  booktitle = {Proceedings of {{ACL-08}}: {{HLT}}},
  author = {Finkel, Jenny Rose and Kleeman, Alex and Manning, Christopher D.},
  year = {2008},
  month = jun,
  pages = {959--967},
  publisher = {Association for Computational Linguistics},
  address = {Columbus, Ohio},
  keywords = {pruning}
}

@article{fitz.h:2019,
  title = {Language {{ERPs}} Reflect Learning through Prediction Error Propagation},
  author = {Fitz, Hartmut and Chang, Franklin},
  year = {2019},
  month = jun,
  journal = {Cognitive Psychology},
  volume = {111},
  pages = {15--52},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2019.03.002},
  urldate = {2025-04-17},
  abstract = {Event-related potentials (ERPs) provide a window into how the brain is processing language. Here, we propose a theory that argues that ERPs such as the N400 and P600 arise as side effects of an error-based learning mechanism that explains linguistic adaptation and language learning. We instantiated this theory in a connectionist model that can simulate data from three studies on the N400 (amplitude modulation by expectancy, contextual constraint, and sentence position), five studies on the P600 (agreement, tense, word category, subcategorization and garden-path sentences), and a study on the semantic P600 in role reversal anomalies. Since ERPs are learning signals, this account explains adaptation of ERP amplitude to within-experiment frequency manipulations and the way ERP effects are shaped by word predictability in earlier sentences. Moreover, it predicts that ERPs can change over language development. The model provides an account of the sensitivity of ERPs to expectation mismatch, the relative timing of the N400 and P600, the semantic nature of the N400, the syntactic nature of the P600, and the fact that ERPs can change with experience. This approach suggests that comprehension ERPs are related to sentence production and language acquisition mechanisms.},
  keywords = {Comprehension,Connectionist model,Development,Error back-propagation,Event-related potentials,Learning,Linguistic adaptation,N400,P600,predictive coding,Semantic P600},
  file = {~/Zotfiles/fitz.h2019 Language ERPs reflect learning through p.pdf}
}

@article{flake.j:2020,
  title = {Measurement Schmeasurement: Questionable Measurement Practices and How to Avoid Them},
  shorttitle = {Measurement Schmeasurement},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {456--465},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920952393},
  urldate = {2024-09-23},
  abstract = {In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study's inferences, and are necessary for meaningful replication studies.},
  langid = {english},
  file = {~/Zotfiles/flake.j2020 Measurement Schmeasurement Questionable.pdf}
}

@inproceedings{flock.f:2014,
  title = {{{WikiWho}}: Precise and Efficient Attribution of Authorship of Revisioned Content},
  shorttitle = {{{WikiWho}}},
  booktitle = {Proceedings of the 23rd International Conference on {{World}} Wide Web},
  author = {Fl{\"o}ck, Fabian and Acosta, Maribel},
  year = {2014},
  month = apr,
  series = {{{WWW}} '14},
  pages = {843--854},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2566486.2568026},
  urldate = {2023-10-16},
  abstract = {Revisioned text content is present in numerous collaboration platforms on the Web, most notably Wikis. To track authorship of text tokens in such systems has many potential applications; the identification of main authors for licensing reasons or tracing collaborative writing patterns over time, to name some. In this context, two main challenges arise. First, it is critical for such an authorship tracking system to be precise in its attributions, to be reliable for further processing. Second, it has to run efficiently even on very large datasets, such as Wikipedia. As a solution, we propose a graph-based model to represent revisioned content and an algorithm over this model that tackles both issues effectively. We describe the optimal implementation and design choices when tuning it to a Wiki environment. We further present a gold standard of 240 tokens from English Wikipedia articles annotated with their origin. This gold standard was created manually and confirmed by multiple independent users of a crowdsourcing platform. It is the first gold standard of this kind and quality and our solution achieves an average of 95\% precision on this data set. We also perform a first-ever precision evaluation of the state-of-the-art algorithm for the task, exceeding it by over 10\% on average. Our approach outperforms the execution time of the state-of-the-art by one order of magnitude, as we demonstrate on a sample of over 240 English Wikipedia articles. We argue that the increased size of an optional materialization of our results by about 10\% compared to the baseline is a favorable trade-off, given the large advantage in runtime performance.},
  isbn = {978-1-4503-2744-2},
  keywords = {authorship,collaborative writing,community-driven content creation,content modeling,online collaboration,version control,wikipedia},
  file = {~/Zotfiles/flock.f2014 WikiWho precise and efficient attributi.pdf}
}

@misc{flock.f:2017arxiv,
  title = {{{TokTrack}}: {{A Complete Token Provenance}} and {{Change Tracking Dataset}} for the {{English Wikipedia}}},
  shorttitle = {{{TokTrack}}},
  author = {Fl{\"o}ck, Fabian and Erdogan, Kenan and Acosta, Maribel},
  year = {2017},
  month = mar,
  number = {arXiv:1703.08244},
  eprint = {1703.08244},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {We present a dataset that contains every instance of all tokens ({\textasciitilde} words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. We show how this data enables, on token-level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics, in the process gaining several novel insights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/flock.f2017arxiv TokTrack A Complete Token Provenance an.pdf}
}

@misc{flock.f:2017TokTrack,
  title = {{{TokTrack}}: {{A Complete Token Provenance}} and {{Change Tracking Dataset}} for the {{English Wikipedia}}},
  shorttitle = {{{TokTrack}}},
  author = {Fl{\"o}ck, Fabian and Erdogan, Kenan and Acosta, Maribel},
  year = {2017},
  month = jul,
  doi = {10.5281/zenodo.834557},
  urldate = {2023-09-28},
  abstract = {Fixes in version 1.1 (= Zenodo's "version 2") *In 20161101-revisions-part1-12-1728.csv, missing first data line is added. *In Current\_content and Deleted\_content files, some token values ('str' column) which contain regular quotes ('"') are fixed. *In Current\_content and Deleted\_content files, some wrong revision ID values for 'origin\_rev\_id', 'in' and 'out' columns are fixed. ------ This dataset contains every instance of all tokens ({$\approx$} words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. This dataset, its creation process and use cases are described in a dedicated dataset paper of the same name, published at the ICWSM 2017 conference. In this paper, we show how this data enables, on token level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics. Tokenization used: https://gist.github.com/faflo/3f5f30b1224c38b1836d63fa05d1ac94 Toy example for how the token metadata is generated: https://gist.github.com/faflo/8bd212e81e594676f8d002b175b79de8 Be sure to read the ReadMe.txt or - even more detailed - the supporting paper which is referenced under "related identifiers".},
  langid = {english}
}

@article{floridi.l:2020,
  title = {{{GPT-3}}: {{Its}} Nature, Scope, Limits, and Consequences},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  year = {2020},
  journal = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s11023-020-09548-1},
  bdsk-url-2 = {https://doi.org/10.1007/s11023-020-09548-1},
  date-added = {2021-06-05 22:29:51 -0400},
  date-modified = {2021-06-05 22:29:53 -0400}
}

@article{fodor.j:1978,
  title = {Parsing Strategies and Constraints on Transformations},
  author = {Fodor, Janet Dean},
  year = {1978},
  journal = {Linguistic Inquiry},
  volume = {9},
  number = {3},
  eprint = {4178071},
  eprinttype = {jstor},
  pages = {427--473},
  publisher = {The MIT Press},
  issn = {0024-3892},
  urldate = {2023-02-22},
  file = {~/Zotfiles/fodor.j1978 Parsing strategies and constraints on tr.pdf}
}

@book{folland.g:1999book2,
  title = {Real {{Analysis}}: {{Modern Techniques}} and {{Their Applications}}},
  shorttitle = {Real {{Analysis}}},
  author = {Folland, Gerald B.},
  year = {1999},
  month = apr,
  series = {Pure and {{Applied Mathematics}}: {{A Wiley Series}} of {{Texts}}, {{Monographs}} and {{Tracts}}},
  edition = {2},
  urldate = {2022-06-23},
  abstract = {An in-depth look at real analysis and its applications-now expanded and revised. This new edition of the widely used analysis book continues to cover real analysis in greater detail and at a more advanced level than most books on the subject. Encompassing several subjects that underlie much of modern analysis, the book focuses on measure and integration theory, point set topology, and the basics of functional analysis. It illustrates the use of the general theories and introduces readers to other branches of analysis such as Fourier analysis, distribution theory, and probability theory. This edition is bolstered in content as well as in scope-extending its usefulness to students outside of pure analysis as well as those interested in dynamical systems. The numerous exercises, extensive bibliography, and review chapter on sets and metric spaces make Real Analysis: Modern Techniques and Their Applications, Second Edition invaluable for students in graduate-level analysis courses. New features include: * Revised material on the n-dimensional Lebesgue integral. * An improved proof of Tychonoffs theorem. * Expanded material on Fourier analysis. * A newly written chapter devoted to distributions and differential equations. * Updated material on Hausdorff dimension and fractal dimension.},
  isbn = {978-0-471-31716-6},
  langid = {english},
  file = {~/Zotfiles/folland.g1999 Real Analysis Modern Techniques and The.djvu}
}

@article{forster.k:2009,
  title = {The Maze Task: {{Measuring}} Forced Incremental Sentence Processing Time},
  shorttitle = {The Maze Task},
  author = {Forster, Kenneth I. and Guerrera, Christine and Elliot, Lisa},
  year = {2009},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {1},
  pages = {163--171},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.1.163},
  urldate = {2023-10-22},
  abstract = {The maze task is an online measure of sentence processing time that provides an alternative to the standard moving window version of self-paced reading. Rather than each word of the sentence being presented in succession, two words are presented at the same time, and the participant must choose which word is a grammatical continuation of the sentence. This procedure forces the reader into an incremental mode of processing in which each word must be fully integrated with the preceding context before the next word can be considered. Previous research with this technique has not considered whether it is sufficiently sensitive to syntactic complexity effects or to garden path effects. Four experiments are reported demonstrating that reliable differences in processing time for subject relatives and object relatives can be obtained, and that this technique generates garden path effects that correspond closely with the data from eyetracking experiments, but without the spillover effects that are sometimes obtained with eyetracking. It is also shown that the task is sensitive to word frequency effects, producing estimates well in excess of those found with eyetracking.},
  langid = {english},
  keywords = {Ambiguous Word,Lexical Decision Task,Maze Task,Object Relative,Relative Clause}
}

@inproceedings{fossum.v:2012,
  title = {Sequential vs. {{Hierarchical}} Syntactic Models of Human Incremental Sentence Processing},
  booktitle = {Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({{CMCL}} 2012)},
  author = {Fossum, Victoria and Levy, Roger},
  year = {2012},
  month = jun,
  pages = {61--69},
  publisher = {Association for Computational Linguistics},
  address = {Montr{\'e}al, Canada},
  date-added = {2021-11-29 10:02:26 -0500},
  date-modified = {2021-11-29 10:02:27 -0500}
}

@inproceedings{foster.a:2019,
  title = {Variational Bayesian Optimal Experimental Design},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah D.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {14036--14047},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/FosterJBHTRG19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{fox.c:2012,
  title = {A Tutorial on Variational {{Bayesian}} Inference},
  author = {Fox, Charles W. and Roberts, Stephen J.},
  year = {2012},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {38},
  number = {2},
  pages = {85--95},
  issn = {1573-7462},
  doi = {10.1007/s10462-011-9236-8},
  urldate = {2022-06-27},
  abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  langid = {english},
  keywords = {Mean-field,Tutorial,Variational Bayes},
  file = {~/Zotfiles/fox.c2012 A tutorial on variational Bayesian infer.pdf}
}

@article{fox.d:2003,
  title = {Adapting the Sample Size in Particle Filters through {{KLD-Sampling}}},
  author = {Fox, Dieter},
  year = {2003},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {22},
  number = {12},
  pages = {985--1003},
  publisher = {SAGE Publications},
  doi = {10.1177/0278364903022012001},
  bdsk-url-2 = {https://doi.org/10.1177/0278364903022012001},
  date-added = {2022-05-05 09:45:16 -0400},
  date-modified = {2022-05-05 09:46:04 -0400},
  keywords = {bayes filtering,KLD-sampling,particle filtering}
}

@inproceedings{frank.s:2009cogsci,
  title = {Surprisal-Based Comparison between a Symbolic and a Connectionist Model of Sentence Processing},
  booktitle = {Proceedings of the 31st {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Frank, Stefan L.},
  year = {2009},
  urldate = {2022-10-12},
  langid = {english},
  file = {~/Zotfiles/frank.s2009cogsci Surprisal-based comparison between a sym.pdf}
}

@article{frank.s:2011,
  title = {Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author = {Frank, Stefan L. and Bod, Rens},
  year = {2011},
  month = may,
  journal = {Psychological Science},
  volume = {22},
  number = {6},
  pages = {829--834},
  publisher = {SAGE Publications},
  doi = {10.1177/0956797611409589},
  bdsk-url-2 = {https://doi.org/10.1177/0956797611409589},
  date-added = {2021-11-29 10:04:00 -0500},
  date-modified = {2021-11-29 10:04:02 -0500}
}

@inproceedings{frank.s:2013,
  title = {Word Surprisal Predicts {{N400}} Amplitude during Reading},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2013},
  pages = {878--883},
  publisher = {Association for Computational Linguistics},
  address = {Sofia, Bulgaria},
  date-added = {2021-12-15 09:07:40 -0500},
  date-modified = {2021-12-15 09:07:44 -0500}
}

@article{frank.s:2013corpus,
  title = {Reading Time Data for Evaluating Broad-Coverage Models of {{English}} Sentence Processing},
  author = {Frank, Stefan L. and Monsalve, Irene Fernandez and Thompson, Robin L. and Vigliocco, Gabriella},
  year = {2013},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {4},
  pages = {1182--1190},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-012-0313-y},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-012-0313-y},
  date-added = {2021-09-16 13:31:43 -0400},
  date-modified = {2021-12-15 09:07:19 -0500}
}

@article{frank.s:2015,
  title = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2015},
  month = jan,
  journal = {Brain and Language},
  volume = {140},
  pages = {1--11},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2014.10.006},
  urldate = {2024-10-17},
  abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence's hierarchical structure for generating expectations about the upcoming word.},
  keywords = {Entropy,Event-related potentials,Information theory,Reading,Sentence comprehension,Surprisal},
  file = {~/Zotfiles/frank.s2015 The ERP response to the amount of inform.pdf}
}

@article{frank.s:2021,
  title = {Toward Computational Models of Multilingual Sentence Processing},
  author = {Frank, Stefan L.},
  year = {2021},
  journal = {Language Learning},
  volume = {71},
  number = {S1},
  pages = {193--218},
  issn = {1467-9922},
  doi = {10.1111/lang.12406},
  urldate = {2022-10-22},
  abstract = {Although computational models can simulate aspects of human sentence processing, research on this topic has remained almost exclusively limited to the single language case. The current review presents an overview of the state of the art in computational cognitive models of sentence processing, and discusses how recent sentence-processing models can be used to study bi- and multilingualism. Recent results from cognitive modeling and computational linguistics suggest that phenomena specific to bilingualism can emerge from systems that have no dedicated components for handling multiple languages. Hence, accounting for human bi-/multilingualism may not require models that are much more sophisticated than those for the monolingual case.},
  langid = {english},
  keywords = {computational models,multilingualism,neural networks,probabilistic grammars,sentence processing},
  file = {~/Zotfiles/frank.s2021 Toward computational models of multiling.pdf}
}

@article{frazier.l:1978,
  title = {The Sausage Machine: {{A}} New Two-Stage Parsing Model},
  shorttitle = {The Sausage Machine},
  author = {Frazier, Lyn and Fodor, Janet Dean},
  year = {1978},
  month = jan,
  journal = {Cognition},
  volume = {6},
  number = {4},
  pages = {291--325},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(78)90002-1},
  urldate = {2022-06-12},
  abstract = {It is proposed that the human sentence parsing device assigns phrase structure to word strings in two steps. The first stage parser assigns lexical and phrasal nodes to substrings of roughly six words. The second stage parser then adds higher nodes to link these phrasal packages together into a complete phrase marker. This model of the parser is compared with ATN models, and with the two-stage models of Kimball (1973) and Fodor, Bever and Garrett (1974). Our assumption that the units which are shunted from the first stage to the second stage are defined by their length, rather than by their syntactic type, explains the effects of constituent length on perceptual complexity in center embedded sentences and in sentences of the kind that fall under Kimball's principle of Right Association. The particular division of labor between the two parsing units allows us to explain, without appeal to any ad hoc parsing strategies, why the parser makes certain `shortsighted' errors even though, in general, it is able to make intelligent use of all the information that is available to it. R{\'e}sum{\'e} Dans cet article on propose un m{\'e}canisme de segmentation des {\'e}nonc{\'e}s qui assigne en deux {\'e}tapes une structure syntagmatique aux suites de mots. La premi{\`e}re m{\'e}thode de segmentation assigne des noeuds lexicaux et syntagmatiques {\`a} des suites de 6 mots environ. La seconde ajoute des noeuds {\`a} un niveau sup{\'e}rieur pour lier ces blocs syntagmatiques et obtenir ainsi un marqueur syntagmatique complet. Ce mod{\`e}le de segmentation est compar{\'e} d'une part aux mod{\`e}les ATN et d'autre part au mod{\`e}le en deux {\'e}tapes de Kimball (1973) et Fodor, Bever et Garrett (1974). Nous pensons que les unit{\'e}s qui passent du ler au 2{\`e} niveau sont caract{\'e}ris{\'e}es par leur longueur plut{\^o}t que par leur forme syntaxique. Ceci expliquerait les effects de la longueur des constituants sur la complexit{\'e} perceptuelle des phrases enclass{\'e}es et des phrases du type de celles qui tombent sous le principe de l'association {\`a} droite de Kimball. La distinction sp{\'e}cifique du travail entre les deux unit{\'e}s de segmentation permet d'expliquer, sans faire intervenir des strat{\'e}gies ad hoc, certaines erreurs de segmentation m{\^e}me si, en g{\'e}n{\'e}ral, il est possible de faire un usage intelligent de toutes les informations disponibles.},
  langid = {english}
}

@article{frazier.l:1982,
  title = {Making and Correcting Errors during Sentence Comprehension: Eye Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  year = {1982},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {14},
  number = {2},
  pages = {178--210},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0285(82)90008-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(82)90008-1},
  date-added = {2022-05-06 15:36:26 -0400},
  date-modified = {2022-05-06 15:36:46 -0400},
  keywords = {eye-tracking,regressions}
}

@article{frazier.l:1987,
  title = {Syntactic Processing: {{Evidence}} from {{Dutch}}},
  shorttitle = {Syntactic Processing},
  author = {Frazier, Lyn},
  year = {1987},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  volume = {5},
  number = {4},
  pages = {519--559},
  issn = {1573-0859},
  doi = {10.1007/BF00138988},
  urldate = {2022-10-13},
  langid = {english},
  keywords = {Artificial Intelligence,eager processing,Syntactic Processing},
  file = {~/Zotfiles/frazier.l1987 Syntactic processing Evidence from Dutc.pdf}
}

@inproceedings{freer.c:2010,
  title = {When Are Probabilistic Programs Probably Computationally Tractable?},
  booktitle = {{{NIPS}} Workshop on {{Monte Carlo}} Methods for Modern Applications},
  author = {Freer, Cameron and Mansinghka, Vikash K. and Roy, Daniel},
  year = {2010},
  month = dec,
  address = {Whistler, British Columbia, Canada},
  date-added = {2021-03-09 22:52:16 -0500},
  date-modified = {2022-04-14 10:57:16 -0400},
  keywords = {probabilistic programming,rejection sampling,sampling,surprisal},
  file = {~/Zotfiles/freer.f2010 When are probabilistic programs probably.pdf}
}

@article{frinsel.f:2024,
  title = {Capturing Individual Differences in Sentence Processing: {{How}} Reliable Is the Self-Paced Reading Task?},
  shorttitle = {Capturing Individual Differences in Sentence Processing},
  author = {Frinsel, Felicity F. and Christiansen, Morten H.},
  year = {2024},
  journal = {Behavior Research Methods},
  doi = {10.3758/s13428-024-02355-x},
  urldate = {2024-02-21},
  abstract = {Advances in research on language processing have originally come from group-level comparisons, but there is now a growing interest in individual differences. To investigate individual differences, tasks that have shown robust group-level differences are often used with the implicit assumption that they will also be reliable when used as an individual differences measure. Here, we examined whether one of the primary tasks used in psycholinguistic research on language processing, the self-paced reading task, can reliably measure individual differences in relative clause processing. We replicated the well-established effects of relative clauses at the group level, with object relative clauses being more difficult to process than subject relative clauses. However, when using difference scores, the reliability of the size of the relative clause effect was close to zero because the self-paced reading times for the different relative clause types were highly correlated within individuals. Nonetheless, we found that the self-paced reading task can be used to reliably capture individual differences in overall reading speed as well as key sentence regions when the two types of relative clause sentences are considered separately. Our results indicate that both the reliability and validity of different sentence regions need to be assessed to determine whether and when self-paced reading can be used to examine individual differences in language processing.},
  langid = {english}
}

@article{friston.k:2005,
  title = {A Theory of Cortical Responses},
  author = {Friston, Karl},
  year = {2005},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {360},
  number = {1456},
  issn = {0962-8436},
  doi = {10.1098/rstb.2005.1622},
  urldate = {2023-08-18},
  abstract = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain's free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brains attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective. In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
  langid = {english},
  keywords = {empirical Bayes,predictive coding},
  file = {~/Zotfiles/friston.k2005 A theory of cortical responses.pdf}
}

@article{friston.k:2006,
  title = {A Free Energy Principle for the Brain},
  author = {Friston, Karl and Kilner, James and Harrison, Lee},
  year = {2006},
  month = jul,
  journal = {Journal of Physiology-Paris},
  series = {Theoretical and {{Computational Neuroscience}}: {{Understanding Brain Functions}}},
  volume = {100},
  number = {1},
  pages = {70--87},
  issn = {0928-4257},
  doi = {10.1016/j.jphysparis.2006.10.001},
  urldate = {2025-02-21},
  abstract = {By formulating Helmholtz's ideas about perception, in terms of modern-day theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts: using constructs from statistical physics, the problems of inferring the causes of sensory input and learning the causal structure of their generation can be resolved using exactly the same principles. Furthermore, inference and learning can proceed in a biologically plausible fashion. The ensuing scheme rests on Empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organisation and responses. In this paper, we show these perceptual processes are just one aspect of emergent behaviours of systems that conform to a free energy principle. The free energy considered here measures the difference between the probability distribution of environmental quantities that act on the system and an arbitrary distribution encoded by its configuration. The system can minimise free energy by changing its configuration to affect the way it samples the environment or change the distribution it encodes. These changes correspond to action and perception respectively and lead to an adaptive exchange with the environment that is characteristic of biological systems. This treatment assumes that the system's state and structure encode an implicit and probabilistic model of the environment. We will look at the models entailed by the brain and how minimisation of its free energy can explain its dynamics and structure.},
  keywords = {Action,Attention,Free energy,Hierarchical,Inference,Learning,Perception,Selection,Variational Bayes},
  file = {~/Zotfiles/friston.k2006 A free energy principle for the brain.pdf}
}

@article{friston.k:2009opinion,
  title = {The Free-Energy Principle: A Rough Guide to the Brain?},
  shorttitle = {The Free-Energy Principle},
  author = {Friston, Karl},
  year = {2009},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {13},
  number = {7},
  pages = {293--301},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2009.04.005},
  urldate = {2025-02-21},
  langid = {english},
  pmid = {19559644},
  file = {~/Zotfiles/friston.k2009opinion The free-energy principle a rough guide.pdf}
}

@article{friston.k:2010,
  title = {The Free-Energy Principle: A Unified Brain Theory?},
  shorttitle = {The Free-Energy Principle},
  author = {Friston, Karl},
  year = {2010},
  month = feb,
  journal = {Nature Reviews Neuroscience},
  volume = {11},
  number = {2},
  pages = {127--138},
  publisher = {Nature Publishing Group},
  issn = {1471-0048},
  doi = {10.1038/nrn2787},
  urldate = {2025-02-21},
  abstract = {Adaptive agents must occupy a limited repertoire of states and therefore minimize the long-term average of surprise associated with sensory exchanges with the world. Minimizing surprise enables them to resist a natural tendency to disorder.Surprise rests on predictions about sensations, which depend on an internal generative model of the world. Although surprise cannot be measured directly, a free-energy bound on surprise can be, suggesting that agents minimize free energy by changing their predictions (perception) or by changing the predicted sensory inputs (action).Perception optimizes predictions by minimizing free energy with respect to synaptic activity (perceptual inference), efficacy (learning and memory) and gain (attention and salience). This furnishes Bayes-optimal (probabilistic) representations of what caused sensations (providing a link to the Bayesian brain hypothesis).Bayes-optimal perception is mathematically equivalent to predictive coding and maximizing the mutual information between sensations and the representations of their causes. This is a probabilistic generalization of the principle of efficient coding (the infomax principle) or the minimum-redundancy principle.Learning under the free-energy principle can be formulated in terms of optimizing the connection strengths in hierarchical models of the sensorium. This rests on associative plasticity to encode causal regularities and appeals to the same synaptic mechanisms as those underlying cell assembly formation.Action under the free-energy principle reduces to suppressing sensory prediction errors that depend on predicted (expected or desired) movement trajectories. This provides a simple account of motor control, in which action is enslaved by perceptual (proprioceptive) predictions.Perceptual predictions rest on prior expectations about the trajectory or movement through the agent's state space. These priors can be acquired (as empirical priors during hierarchical inference) or they can be innate (epigenetic) and therefore subject to selective pressure.Predicted motion or state transitions realized by action correspond to policies in optimal control theory and reinforcement learning. In this context, value is inversely proportional to surprise (and implicitly free energy), and rewards correspond to innate priors that constrain policies.},
  copyright = {2010 Springer Nature Limited},
  langid = {english},
  keywords = {Control theory,Neural encoding},
  file = {~/Zotfiles/friston.k2010 The free-energy principle a unified bra.pdf}
}

@article{friston.k:2010response,
  title = {Some Free-Energy Puzzles Resolved: Response to {{Thornton}}},
  shorttitle = {Some Free-Energy Puzzles Resolved},
  author = {Friston, Karl},
  year = {2010},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {14},
  number = {2},
  pages = {54--55},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2009.11.008},
  urldate = {2025-02-21},
  pmcid = {PMC2825601},
  pmid = {null},
  keywords = {free-energy},
  file = {~/Zotfiles/friston.k2010response Some free-energy puzzles resolved respo.pdf}
}

@article{friston.k:2012,
  title = {A Free Energy Principle for Biological Systems},
  author = {Friston, Karl},
  year = {2012},
  month = nov,
  journal = {Entropy},
  volume = {14},
  number = {11},
  pages = {2100--2121},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e14112100},
  urldate = {2022-06-10},
  abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian,ergodicity,free energy,random dynamical system,self-organization,surprise},
  file = {~/Zotfiles/friston.k2012 A free energy principle for biological s.pdf}
}

@article{fromkin.v:1971,
  title = {The Non-Anomalous Nature of Anomalous Utterances},
  author = {Fromkin, Victoria A.},
  year = {1971},
  journal = {Language},
  volume = {47},
  number = {1},
  eprint = {412187},
  eprinttype = {jstor},
  pages = {27--52},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/412187},
  urldate = {2022-06-24},
  abstract = {An analysis of speech errors provides evidence for the psychological reality of theoretical linguistic concepts such as distinctive features, morpheme structure constraints, abstract underlying forms, phonological rules, and syntactic and semantic features. Furthermore, such errors reveal that linguistic performance is highly rule-governed, and that in many cases it is grammatical rules which constrain or monitor actual speech production. While a model of linguistic competence is independent of temporal constraints, a model of linguistic performance must provide information as to the sequencing of events in real time. To explain the occurrence of particular kinds of errors, a specific ordering of rules is posited, which ordering may or may not coincide with the organization of a grammar.},
  keywords = {speech errors}
}

@misc{fu.z:2023arxiv,
  title = {Decoder-Only or Encoder-Decoder? {{Interpreting}} Language Model as a Regularized Encoder-Decoder},
  shorttitle = {Decoder-Only or Encoder-Decoder?},
  author = {Fu, Zihao and Lam, Wai and Yu, Qian and So, Anthony Man-Cho and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04052},
  eprint = {2304.04052},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-25},
  abstract = {The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/fu.z2023 Decoder-only or encoder-decoder Interpr.pdf}
}

@incollection{fung.r:1990,
  title = {Weighing and {{Integrating Evidence}} for {{Stochastic Simulation}} in {{Bayesian Networks}}},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Fung, Robert and Chang, Kuo-Chu},
  editor = {Henrion, Max and Shachter, Ross D. and Kanal, Laveen N. and Lemmer, John F.},
  year = {1990},
  month = jan,
  series = {Machine {{Intelligence}} and {{Pattern Recognition}}},
  volume = {10},
  pages = {209--219},
  publisher = {North-Holland},
  doi = {10.1016/B978-0-444-88738-2.50023-3},
  urldate = {2025-02-19},
  abstract = {Stochastic simulation approaches perform probabilistic inference in Bayesian networks by estimating the probability of an event based on the frequency that the event occurs in a set of simulation trials. This paper describes the evidence weighting mechanism, for augmenting the logic sampling stochastic simulation algorithm [Henrion, 1986]. Evidence weighting modifies the logic sampling algorithm by weighting each simulation trial by the likelihood of a network's evidence given the sampled state node values for that trial. We also describe an enhancement to the basic algorithm which uses the evidential integration technique [Chin and Cooper, 1987]. A comparison of the basic evidence weighting mechanism with the Markov blanket algorithm [Pearl, 1987], the logic sampling algorithm, and the evidence integration algorithm is presented. The comparison is aided by analyzing the performance of the algorithms in a simple example network.},
  keywords = {likelihood weighting}
}

@inproceedings{futrell.r:2017,
  title = {Noisy-Context Surprisal as a Human Sentence Processing Cost Model},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Volume}} 1, Long Papers},
  author = {Futrell, Richard and Levy, Roger},
  year = {2017},
  pages = {688--698},
  publisher = {Association for Computational Linguistics},
  address = {Valencia, Spain}
}

@article{futrell.r:2017L2,
  title = {L2 Processing as Noisy Channel Language Comprehension},
  author = {Futrell, Richard and Gibson, Edward},
  year = {2017},
  month = aug,
  journal = {Bilingualism: Language and Cognition},
  volume = {20},
  number = {4},
  pages = {683--684},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S1366728916001061},
  urldate = {2024-03-06},
  abstract = {The thesis in this paper is that L2 speakers differ from L1 speakers in their ability to do memory storage and retrieval about linguistic structure. We would like to suggest it is possible to go farther than this thesis and develop a computational-level theory which explains why this mechanistic difference between L2 and L1 speakers exists. For this purpose, we believe a noisy channel model (Shannon, 1948; Levy, 2008; Levy, Bicknell, Slattery \& Rayner, 2009; Gibson, Bergen \& Piantadosi, 2013) could be a good start. Under the reasonable assumption that L2 speakers have a less precise probabilistic representation of the syntax of their L2 language than L1 speakers do, the noisy channel model straightforwardly predicts that L2 comprehenders will depend more on world knowledge and discourse factors when interpreting and recalling utterances (cf. Gibson, Sandberg, Fedorenko, Bergen \& Kiran, 2015, for this assumption applied to language processing for persons with aphasia). Also, under the assumption that L2 speakers assume a higher error rate than L1 speakers do, the noisy channel model predicts that they will be more affected by alternative parses which are not directly compatible with the form of an utterance.},
  langid = {english},
  keywords = {L2,multilingual,noisy channel},
  file = {~/Zotfiles/futrell.r2017L2 L2 processing as noisy channel language.pdf}
}

@phdthesis{futrell.r:2017phd,
  title = {Memory and Locality in Natural Language},
  author = {Futrell, Richard},
  year = {2017},
  eprint = {1721.1/114075},
  eprinttype = {hdl},
  abstract = {I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning.},
  date-added = {2022-04-11 23:46:38 -0400},
  date-modified = {2022-04-11 23:50:11 -0400},
  school = {Massachusetts Institute of Technology / Massachusetts Institute of Technology. Department of Brain and Cognitive Sciences},
  keywords = {information theory,noisy channel coding},
  file = {~/Zotfiles/futrell.r2017phd Memory and locality in natural language.pdf}
}

@inproceedings{futrell.r:2018,
  title = {The {{Natural Stories}} Corpus},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven and Fedorenko, Evelina},
  year = {2018},
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  date-modified = {2021-12-02 00:12:56 -0500}
}

@inproceedings{futrell.r:2019,
  title = {Syntactic Dependencies Correspond to Word Pairs with High Mutual Information},
  booktitle = {Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, {{SyntaxFest}} 2019)},
  author = {Futrell, Richard and Qian, Peng and Gibson, Edward and Fedorenko, Evelina and Blank, Idan},
  year = {2019},
  pages = {3--13},
  publisher = {Association for Computational Linguistics},
  address = {Paris, France},
  doi = {10.18653/v1/W19-7703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-7703},
  file = {~/Zotfiles/futrell.r2019 Syntactic dependencies correspond to wor.pdf}
}

@article{futrell.r:2020,
  title = {Lossy-Context Surprisal: {{An}} Information-Theoretic Model of Memory Effects in Sentence Processing},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {3},
  pages = {e12814},
  publisher = {Wiley Online Library},
  doi = {10.1111/cogs.12814},
  date-added = {2020-03-27 16:35:14 -0400},
  date-modified = {2022-04-20 13:48:30 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lossy context surprisal,mutual information,processing},
  file = {~/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 2.pdf;~/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 3.pdf;~/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 4.pdf;~/Zotfiles/futrell.r2020 Lossy-context surprisal An information-.pdf}
}

@article{futrell.r:2021,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2021},
  month = mar,
  journal = {Language Resources and Evaluation},
  volume = {55},
  number = {1},
  pages = {63--77},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09503-7},
  urldate = {2022-06-09},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  keywords = {cognitive modeling,psycholinguistics,reading time,self-paced reading time},
  file = {~/Zotfiles/futrell.r2021 The Natural Stories corpus a reading-ti.pdf}
}

@inproceedings{futrell.r:2023cogsci,
  title = {An Information-Theoretic Account of Availability Effects in Language Production},
  booktitle = {Proceedings of the 45th {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Futrell, Richard},
  year = {2023},
  urldate = {2023-07-28},
  abstract = {I present a computational-level model of language production in terms of a combination of information theory and control theory in which words are chosen incrementally in order to maximize communicative value subject to an information-theoretic capacity constraint. The theory generally predicts a tradeoff between ease of production and communicative accuracy. I apply the theory to two cases of apparent availability effects in language production, in which words are selected on the basis of their accessibility to a speaker who has not yet perfectly planned the rest of the utterance. Using corpus data on English relative clause complementizer dropping from Levy \&amp; Jaeger (2007) and experimental data on Mandarin noun classifier choice from Zhan \&amp; Levy (2019), I show that the theory reproduces the observed phenomena, providing an alternative account to Uniform Information Density (UID) and a promising general model of language production which is tightly linked to emerging theories in computational neuroscience.},
  langid = {english},
  file = {~/Zotfiles/futrell.r2023cogsci An information-theoretic account of avai.pdf}
}

@article{futrell.r:2023PNAS,
  title = {Information-Theoretic Principles in Incremental Language Production},
  author = {Futrell, Richard},
  year = {2023},
  month = sep,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {39},
  pages = {e2220593120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2220593120},
  urldate = {2025-05-12},
  abstract = {I apply a recently emerging perspective on the complexity of action selection, the rate--distortion theory of control, to provide a computational-level model of errors and difficulties in human language production, which is grounded in information theory and control theory. Language production is cast as the sequential selection of actions to achieve a communicative goal subject to a capacity constraint on cognitive control. In a series of calculations, simulations, corpus analyses, and comparisons to experimental data, I show that the model directly predicts some of the major known qualitative and quantitative phenomena in language production, including semantic interference and predictability effects in word choice; accessibility-based (``easy-first'') production preferences in word order alternations; and the existence and distribution of disfluencies including filled pauses, corrections, and false starts. I connect the rate--distortion view to existing models of human language production, to probabilistic models of semantics and pragmatics, and to proposals for controlled language generation in the machine learning and reinforcement learning literature.},
  file = {~/Zotfiles/futrell.r2023PNAS Information-theoretic principles in incr.pdf}
}

@misc{futrell.r:2024arxiv,
  title = {Linguistic {{Structure}} from a {{Bottleneck}} on {{Sequential Information Processing}}},
  author = {Futrell, Richard and Hahn, Michael},
  year = {2024},
  month = oct,
  number = {arXiv:2405.12109},
  eprint = {2405.12109},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.12109},
  urldate = {2025-03-06},
  abstract = {Human language is a unique form of communication in the natural world, distinguished by its structured nature. Most fundamentally, it is systematic, meaning that signals can be broken down into component parts that are individually meaningful -- roughly, words -- which are combined in a regular way to form sentences. Furthermore, the way in which these parts are combined maintains a kind of locality: words are usually concatenated together, and they form contiguous phrases, keeping related parts of sentences close to each other. We address the challenge of understanding how these basic properties of language arise from broader principles of efficient communication under information processing constraints. Here we show that natural-language-like systematicity arises in codes that are constrained by predictive information, a measure of the amount of information that must be extracted from the past of a sequence in order to predict its future. In simulations, we show that such codes approximately factorize their source distributions, and then express the resulting factors systematically and locally. Next, in a series of cross-linguistic corpus studies, we show that human languages are structured to have low predictive information at the levels of phonology, morphology, syntax, and semantics. Our result suggests that human language performs a sequential, discrete form of Independent Components Analysis on the statistical distribution over meanings that need to be expressed. It establishes a link between the statistical and algebraic structure of human language, and reinforces the idea that the structure of human language is shaped by communication under cognitive constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory,Mathematics - Information Theory,predictive information},
  file = {~/Zotfiles/futrell.r2024arxiv Linguistic Structure from a Bottleneck o.pdf}
}

@incollection{gale.w:1994,
  title = {What Is Wrong with Adding One?},
  booktitle = {Corpus-{{Based Research}} into {{Language}}},
  author = {Gale, William and Church, Kenneth},
  year = {1994},
  month = jan,
  series = {Language and {{Computers}}},
  volume = {12},
  pages = {189--198},
  publisher = {Brill},
  doi = {10.1163/9789004653566_015},
  urldate = {2024-08-02},
  langid = {english},
  keywords = {Applied Linguistics,Languages and Linguistics}
}

@incollection{gamut.l:1991,
  title = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  booktitle = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  author = {Gamut, L. T. F.},
  year = {1991},
  publisher = {University of Chicago Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@misc{gao.l:2020,
  title = {The Pile: {{An 800GB}} Dataset of Diverse Text for Language Modeling},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  eprint = {2101.00027},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-11-30 10:16:24 -0500},
  date-modified = {2021-11-30 10:19:58 -0500}
}

@misc{gao.l:2021,
  title = {On the Sizes of {{OpenAI API}} Models},
  author = {Gao, Leo},
  year = {2021},
  month = may,
  journal = {EleutherAI Blog},
  urldate = {2021-12-13},
  date-added = {2021-12-13 19:27:28 -0500},
  date-modified = {2021-12-13 19:29:08 -0500},
  howpublished = {Blog post}
}

@book{garner.w:1962book,
  title = {Uncertainty and Structure as Psychological Concepts},
  author = {Garner, Wendell R.},
  year = {1962},
  series = {Uncertainty and Structure as Psychological Concepts},
  publisher = {Wiley},
  address = {Oxford, England},
  abstract = {A mathematical basis stemming from information theory has been used "to develop ideas about and an understanding of some psychological problems." The 1st 4 chapters discuss uncertainty in reference to perceptual discrimination and information transmission. Chapter 5 deals with "the partitioning of structure and meaning." Pattern perception, language redundancy, other sequential behavior, and concept formation are considered in Chapters 6 through 10. Chapter 11 is a "Final Commentary." (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@misc{gastaldi.j:2024arxiv,
  title = {The Foundations of Tokenization: Statistical and Computational Concerns},
  shorttitle = {The Foundations of Tokenization},
  author = {Gastaldi, Juan Luis and Terilla, John and Malagutti, Luca and DuSell, Brian and Vieira, Tim and Cotterell, Ryan},
  year = {2024},
  month = jul,
  number = {arXiv:2407.11606},
  eprint = {2407.11606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.11606},
  urldate = {2024-08-06},
  abstract = {Tokenization - the practice of converting strings of characters over an alphabet into sequences of tokens over a vocabulary - is a critical yet under-theorized step in the NLP pipeline. Notably, it remains the only major step not fully integrated into widely used end-to-end neural models. This paper aims to address this theoretical gap by laying the foundations of tokenization from a formal perspective. By articulating and extending basic properties about the category of stochastic maps, we propose a unified framework for representing and analyzing tokenizer models. This framework allows us to establish general conditions for the use of tokenizers. In particular, we formally establish the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for the design and implementation of tokenizer models. The framework and results advanced in this paper represent a step toward a robust theoretical foundation for neural language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,tokenization},
  file = {~/Zotfiles/gastaldi.j2024arxiv The Foundations of Tokenization Statist.pdf}
}

@inproceedings{gauthier.j:2020,
  title = {{{SyntaxGym}}: {{An}} Online Platform for Targeted Evaluation of Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: {{System}} Demonstrations},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {70--76},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.10},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-demos.10}
}

@inproceedings{gauthier.j:2023,
  title = {The Neural Dynamics of Word Recognition and Integration},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gauthier, Jon and Levy, Roger},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {980--995},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  urldate = {2023-12-12},
  abstract = {Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the recognition and integration of words. The model reveals distinct neural processing of words depending on whether or not they can be quickly recognized. While all words trigger a neural response characteristic of probabilistic integration --- voltage modulations predicted by a word's surprisal in context --- these modulations are amplified for words which require more than roughly 150 ms of input to be recognized. We observe no difference in the latency of these neural responses according to words' recognition times. Our results support a two-part model of speech comprehension, combining an eager and rapid process of word recognition with a temporally independent process of word integration. However, we also developed alternative models of the scalp EEG signal not incorporating word recognition dynamics which showed similar performance improvements. We discuss potential future modeling steps which may help to separate these hypotheses.},
  file = {~/Zotfiles/gauthier.j2023 The neural dynamics of word recognition.pdf}
}

@incollection{gazdar.g:1985,
  title = {Generalized Phrase Structure Grammar},
  booktitle = {Generalized Phrase Structure Grammar},
  author = {Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K. and Sag, Ivan A.},
  year = {1985},
  publisher = {Harvard University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:14 -0400},
  keywords = {GPSG}
}

@inproceedings{geertzen.j:2014,
  title = {Automatic Linguistic Annotation of Large Scale {{L2}} Databases: {{The EF-Cambridge Open Language Database}} ({{EFCamDat}})},
  booktitle = {Selected {{Proceedings}} of the 2012 {{Second Language Research Forum}}: {{Building Bridges}} between {{Disciplines}}},
  author = {Geertzen, Jeroen and Alexopoulou, Theodora and Korhonen, Anna},
  year = {2014},
  pages = {240--254},
  publisher = {Cascadilla Proceedings Project},
  address = {University of Pittsburgh and Carnegie Mellon University},
  isbn = {978-1-57473-464-5},
  keywords = {dataset,error correction}
}

@article{geisler.w:2003,
  title = {A {{Bayesian}} Approach to the Evolution of Perceptual and Cognitive Systems},
  author = {Geisler, Wilson S. and Diehl, Randy L.},
  year = {2003},
  month = may,
  journal = {Cognitive Science},
  series = {2002 {{Rumelhart Prize Special Issue Honoring Richard Shiffrin}}},
  volume = {27},
  number = {3},
  pages = {379--402},
  issn = {0364-0213},
  doi = {10.1016/S0364-0213(03)00009-0},
  urldate = {2025-02-07},
  abstract = {We describe a formal framework for analyzing how statistical properties of natural environments and the process of natural selection interact to determine the design of perceptual and cognitive systems. The framework consists of two parts: a Bayesian ideal observer with a utility function appropriate for natural selection, and a Bayesian formulation of Darwin's theory of natural selection. Simulations of Bayesian natural selection were found to yield new insights, for example, into the co-evolution of camouflage, color vision, and decision criteria. The Bayesian framework captures and generalizes, in a formal way, many of the important ideas of other approaches to perception and cognition.},
  keywords = {Camouflage evolution,Color perception,ideal observer,Ideal observer,Natural selection,Scene statistics},
  file = {~/Zotfiles/geisler.w2003 A Bayesian approach to the evolution of.pdf}
}

@book{gelman.a:2006book,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  month = dec,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511790942},
  urldate = {2024-05-21},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-86706-1 978-0-521-68689-1 978-0-511-79094-2},
  file = {~/Zotfiles/gelman.a2006book1 Data analysis using regression and multi.pdf}
}

@book{gelman.a:2014book3,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2014},
  series = {Texts in Statistical Science Series},
  edition = {3},
  publisher = {{CRC Press, Taylor and Francis Group}},
  address = {Boca Raton, FL, USA},
  abstract = {Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book},
  isbn = {978-1-4398-4095-5},
  langid = {english}
}

@article{geman.s:1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  date-added = {2021-03-17 15:08:02 -0400},
  date-modified = {2021-03-17 15:09:10 -0400},
  keywords = {Additive noise,Annealing,bayesian,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,gibbs sampling,image restoration,Image restoration,line process,MAP estimate,Markov random field,markov random fields,Markov random fields,relaxation,scene modeling,spatial degradation,stochastic processes,Stochastic processes,Temperature distribution},
  file = {~/Zotfiles/geman.s1984 Stochastic Relaxation, Gibbs Distributio.pdf}
}

@misc{genewein.t:2013,
  title = {Abstraction in Decision-Makers with Limited Information Processing Capabilities},
  author = {Genewein, Tim and Braun, Daniel A.},
  year = {2013},
  month = dec,
  number = {arXiv:1312.4353},
  eprint = {1312.4353},
  primaryclass = {cs, math, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1312.4353},
  urldate = {2022-06-09},
  abstract = {A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Statistics - Machine Learning},
  annotation = {note: Presented at the NIPS 2013 Workshop on Planning with Information Constraints},
  file = {~/Zotfiles/genewein.t2013 Abstraction in decision-makers with limi.pdf}
}

@article{genewein.t:2015,
  title = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making: An Information-Theoretic Optimality Principle},
  shorttitle = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making},
  author = {Genewein, Tim and Leibfried, Felix and {Grau-Moya}, Jordi and Braun, Daniel Alexander},
  year = {2015},
  journal = {Frontiers in Robotics and AI},
  volume = {2},
  issn = {2296-9144},
  urldate = {2022-06-07},
  abstract = {Abstraction and hierarchical information processing are hallmarks of human and animal intelligence underlying the unrivaled flexibility of behavior in biological systems. Achieving such flexibility in artificial systems is challenging, even with more and more computational power. Here, we investigate the hypothesis that abstraction and hierarchical information processing might in fact be the consequence of limitations in information-processing power. In particular, we study an information-theoretic framework of bounded rational decision-making that trades off utility maximization against information-processing costs. We apply the basic principle of this framework to perception-action systems with multiple information-processing nodes and derive bounded-optimal solutions. We show how the formation of abstractions and decision-making hierarchies depends on information-processing costs. We illustrate the theoretical ideas with example simulations and conclude by formalizing a mathematically unifying optimization principle that could potentially be extended to more complex systems.},
  keywords = {bounded rationality,information theory,lossy compression,rate-distortion theory},
  file = {~/Zotfiles/genewein.t2015 Bounded rationality, abstraction, and hi 2.pdf;~/Zotfiles/genewein.t2015 Bounded rationality, abstraction, and hi.pdf}
}

@inproceedings{georgi.d:2011,
  title = {Deriving the Distribution of Person Portmanteaux by Relativized Probing},
  booktitle = {Proceedings of the North-Eastern Linguistic Society},
  author = {Georgi, Doreen},
  year = {2011},
  volume = {42},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:19 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@incollection{gershman.s:2012,
  title = {Perception, Action and Utility: {{The}} Tangled Skein},
  booktitle = {Principles of Brain Dynamics: {{Global}} State Interactions},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  series = {Computational {{Neuroscience Series}}},
  pages = {293--312},
  publisher = {MIT Press},
  doi = {10.7551/mitpress/9108.003.0015},
  isbn = {978-0-262-01764-0},
  file = {~/Zotfiles/gershman.s2012 Perception, action and utility The tang.pdf}
}

@article{gershman.s:2015,
  title = {Computational Rationality: {{A}} Converging Paradigm for Intelligence in Brains, Minds, and Machines},
  shorttitle = {Computational Rationality},
  author = {Gershman, Samuel J. and Horvitz, Eric J. and Tenenbaum, Joshua B.},
  year = {2015},
  month = jul,
  journal = {Science},
  volume = {349},
  number = {6245},
  pages = {273--278},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac6076},
  urldate = {2024-05-14},
  abstract = {After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.},
  copyright = {http://www.sciencemag.org/about/science-licenses-journal-article-reuse},
  langid = {english},
  file = {~/Zotfiles/gershman.s2015 Computational rationality A converging.pdf}
}

@article{gershman.s:2019,
  title = {What Does the Free Energy Principle Tell Us about the Brain?},
  author = {Gershman, Samuel J},
  year = {2019},
  month = oct,
  journal = {Neurons, Behavior, Data analysis, and Theory},
  volume = {2},
  number = {3},
  issn = {2690-2664},
  doi = {10.51628/001c.10839},
  urldate = {2025-04-03},
  abstract = {The free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
  langid = {english},
  keywords = {Bayesian brain,free energy principle,inference,predictive coding},
  file = {~/Zotfiles/gershman.s2019 What does the free energy principle tell.pdf}
}

@unpublished{gershman.s:2021ms,
  type = {Draft for the {{Oxford}} Handbook of Human Memory},
  title = {The Rational Analysis of Memory},
  author = {Gershman, Samuel J.},
  year = {2021},
  abstract = {This chapter surveys rational models of memory, which posit that memory is optimized to store information that will be needed in the future, subject to the constraint that information can only be stored with a limited amount of precision. This optimization problem can be formalized using the framework of rate-distortion theory, which addresses the trade-off between memory precision and task performance. The design principles that emerge from this framework shed light on numerous regularities of memory, as well as how cognitive and environmental factors shape these regularities.},
  langid = {english},
  file = {~/Zotfiles/gershman.s2021draft The rational analysis of memory.pdf}
}

@article{gershman.s:2023,
  title = {The Molecular Memory Code and Synaptic Plasticity: {{A}} Synthesis},
  shorttitle = {The Molecular Memory Code and Synaptic Plasticity},
  author = {Gershman, Samuel J.},
  year = {2023},
  month = feb,
  journal = {Biosystems},
  volume = {224},
  pages = {104825},
  issn = {0303-2647},
  doi = {10.1016/j.biosystems.2022.104825},
  urldate = {2023-08-18},
  abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.},
  keywords = {Free energy,Inference,Learning,Memory,Synaptic plasticity},
  file = {~/Zotfiles/gershman.s2023 The molecular memory code and synaptic p.pdf}
}

@article{gibbs.a:2002,
  title = {On Choosing and Bounding Probability Metrics},
  author = {Gibbs, Alison L. and Su, Francis Edward},
  year = {2002},
  journal = {International Statistical Review},
  volume = {70},
  number = {3},
  pages = {419--435},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2002.tb00178.x},
  urldate = {2022-12-22},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
  langid = {english},
  keywords = {Discrepancy,Hellinger distance,probability metrics,Probability metrics,Prokhorov metric,Rates of convergence,relative entropy,Relative entropy,Wasserstein distance},
  file = {~/Zotfiles/gibbs.a2002 On choosing and bounding probability met.pdf}
}

@phdthesis{gibson.e:1991phd,
  title = {A Computational Theory of Human Linguistic Processing: {{Memory}} Limitations and Processing Breakdown},
  shorttitle = {A Computational Theory of Human Linguistic Processing},
  author = {Gibson, Edward},
  year = {1991},
  address = {United States -- Pennsylvania},
  urldate = {2022-06-14},
  abstract = {This thesis gives a theory of sentence comprehension that attempts to explain a number of linguistic performance effects, including garden-path effects, preferred readings for ambiguous input and processing overload effects. It is hypothesized that the human parser heuristically determines its options based upon evaluation of possible representations with respect to lexical, syntactic, semantic and pragmatic properties, each of which is associated with a weight. Processing overload effects are explained by the assumption of the existence of a maximum load corresponding to the limited capacity of short term memory: a structure becomes unacceptable at a particular parse state if the combination of the processing weights associated with its properties at that state is greater than the available capacity. Furthermore, it is assumed that the language processor is an automatic device that maintains only the best of the set of all compatible representations for an input string. This thesis assumes a formulation of representational evaluation within a parallel framework: one structure is preferred over another if the processing load associated with the first structure is markedly lower than the processing load associated with the second. Thus a garden path effect results if the unpreferred structure is necessary for a successful parse of the input. Four properties of linguistic representations are presented within this framework. The first two--the Properties of Thematic Reception and Transmission--derivable from the \${\textbackslash}theta\$-Criterion from Government-Binding (GB) Theory (Chomsky (1981)); the third--the Property of Lexical Requirement--derivable from the Projection Principle of GB Theory; and the fourth--the Property of Recency Preference--prefers local attachments over more distant attachments (cf. Kimball (1973), Frazier (1979)). This thesis shows how these properties interact to give a partially unified theory of many performance effects.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798617014688},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {Applied sciences,computational linguistics,Language,literature and linguistics,memory,parsing,processing,psycholinguistics},
  file = {~/Zotfiles/gibson.e1991phd A computational theory of human linguist.pdf}
}

@article{gibson.e:1998,
  title = {Linguistic Complexity: Locality of Syntactic Dependencies},
  author = {Gibson, Edward},
  year = {1998},
  month = aug,
  journal = {Cognition},
  volume = {68},
  number = {1},
  pages = {1--76},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00034-1},
  abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory -- the Syntactic Prediction Locality Theory (SPLT) -- has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
  keywords = {Computational resources,Linguistic complexity,Sentence processing,Syntactic dependency}
}

@article{gibson.e:1999,
  title = {Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical},
  author = {Gibson, Edward and Thomas, James},
  year = {1999},
  month = jun,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {3},
  pages = {225--248},
  publisher = {Informa UK Limited},
  doi = {10.1080/016909699386293},
  bdsk-url-2 = {https://doi.org/10.1080/016909699386293},
  date-added = {2022-04-19 22:36:47 -0400},
  date-modified = {2022-04-19 22:36:48 -0400}
}

@incollection{gibson.e:2000,
  title = {The Dependency Locality Theory: {{A}} Distance-Based Theory of Linguistic Complexity},
  shorttitle = {The Dependency Locality Theory},
  booktitle = {Image, Language, Brain:  {{Papers}} from the First Mind Articulation Project Symposium},
  author = {Gibson, Edward},
  editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  year = {2000},
  pages = {94--126},
  publisher = {The MIT Press},
  address = {Cambridge, MA, US},
  abstract = {Discusses the dependency locality theory (DLT) of human computational resources in sentence parsing that relies on 2 kinds of resource use. One of the key ideas underlying the theory is locality, such that the cost of integrating 2 elements (such as a head and a dependent, or a pronominal referent to its antecedent) depends on the distance between the 2. The remainder of the chapter reviews some empirical observations regarding the proceeding difficulty associated with unambiguous structures. It is shown that the DLT accounts for the complexity effect in these structures as well as preferences in ambiguous structures.},
  isbn = {978-0-262-13371-5},
  keywords = {Linguistics,Psychological Theories,Sentence Comprehension,Sentence Structure,Syntax}
}

@article{gibson.e:2013,
  title = {A Noisy-Channel Account of Crosslinguistic Word-Order Variation},
  author = {Gibson, Edward and Piantadosi, Steven T. and Brink, Kimberly and Bergen, Leon and Lim, Eunice and Saxe, Rebecca},
  year = {2013},
  month = may,
  journal = {Psychological Science},
  volume = {24},
  number = {7},
  pages = {1079--1088},
  publisher = {SAGE Publications},
  doi = {10.1177/0956797612463705},
  bdsk-url-2 = {https://doi.org/10.1177/0956797612463705},
  date-added = {2022-04-19 22:26:57 -0400},
  date-modified = {2022-05-02 14:46:14 -0400},
  keywords = {noisy channel coding},
  file = {~/Zotfiles/gibson.e2013 A noisy-channel account of crosslinguist.pdf}
}

@article{gibson.e:2013PNAS,
  title = {Rational Integration of Noisy Evidence and Prior Semantic Expectations in Sentence Interpretation},
  author = {Gibson, Edward and Bergen, Leon and Piantadosi, Steven T.},
  year = {2013},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {20},
  pages = {8051--8056},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1216438110},
  keywords = {noisy channel coding},
  file = {~/Zotfiles/gibson.e2013pnas Rational integration of noisy evidence a.pdf}
}

@article{gibson.e:2017,
  title = {Don't Underestimate the Benefits of Being Misunderstood},
  author = {Gibson, Edward and Tan, Caitlin and Futrell, Richard and Mahowald, Kyle and Konieczny, Lars and Hemforth, Barbara and Fedorenko, Evelina},
  year = {2017},
  month = jun,
  journal = {Psychological Science},
  volume = {28},
  number = {6},
  pages = {703--712},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797617690277},
  urldate = {2024-12-22},
  abstract = {Being a nonnative speaker of a language poses challenges. Individuals often feel embarrassed by the errors they make when talking in their second language. However, here we report an advantage of being a nonnative speaker: Native speakers give foreign-accented speakers the benefit of the doubt when interpreting their utterances; as a result, apparently implausible utterances are more likely to be interpreted in a plausible way when delivered in a foreign than in a native accent. Across three replicated experiments, we demonstrated that native English speakers are more likely to interpret implausible utterances, such as ``the mother gave the candle the daughter,'' as similar plausible utterances (``the mother gave the candle to the daughter'') when the speaker has a foreign accent. This result follows from the general model of language interpretation in a noisy channel, under the hypothesis that listeners assume a higher error rate in foreign-accented than in nonaccented speech.},
  langid = {english},
  file = {~/Zotfiles/gibson.e2017 Dont Underestimate the Benefits of Bein.pdf}
}

@article{gigerenzer.g:1991,
  title = {From Tools to Theories: {{A}} Heuristic of Discovery in Cognitive Psychology},
  shorttitle = {From Tools to Theories},
  author = {Gigerenzer, Gerd},
  year = {1991},
  journal = {Psychological Review},
  volume = {98},
  number = {2},
  pages = {254--267},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.98.2.254},
  abstract = {The study of scientific discovery---where do new ideas come from?---has long been denigrated by philosophers as irrelevant to analyzing the growth of scientific knowledge. In particular, little is known about how cognitive theories are discovered, and neither the classical accounts of discovery as either probabilistic induction (e.g., H. Reichenbach, 1938) or lucky guesses (e.g., K. Popper, 1959), nor the stock anecdotes about sudden "eureka" moments deepen the insight into discovery. A heuristics approach is taken in this review, where heuristics are understood as strategies of discovery less general than a supposed unique logic discovery but more general than lucky guesses. This article deals with how scientists' tools shape theories of mind, in particular with how methods of statistical inference have turned into metaphors of mind. The tools-to-theories heuristic explains the emergence of a broad range of cognitive theories, from the cognitive revolution of the 1960s up to the present, and it can be used to detect both limitations and new lines of development in current cognitive theories that investigate the mind as an "intuitive statistician." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Cognitive Psychology,Heuristics},
  file = {~/Zotfiles/gigerenzer.g1991 From tools to theories A heuristic of d.pdf}
}

@inproceedings{gildea.d:2007,
  title = {Optimizing Grammars for Minimum Dependency Length},
  booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  author = {Gildea, Daniel and Temperley, David},
  year = {2007},
  pages = {184--191},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@article{gildea.d:2010,
  title = {Do Grammars Minimize Dependency Length?},
  author = {Gildea, Daniel and Temperley, David},
  year = {2010},
  journal = {Cognitive Science},
  volume = {34},
  number = {2},
  pages = {286--310},
  publisher = {Wiley Online Library},
  date-added = {2019-05-14 23:50:31 -0400},
  date-modified = {2019-06-17 21:56:52 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,projectivity}
}

@inproceedings{giulianelli.m:2023,
  title = {Information Value: Measuring Utterance Predictability as Distance from Plausible Alternatives},
  shorttitle = {Information Value},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Giulianelli, Mario and Wallbridge, Sarenne and Fern{\'a}ndez, Raquel},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {5633--5653},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.343},
  urldate = {2024-10-17},
  abstract = {We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates of token-level surprisal and it is complementary to surprisal for predicting eye-tracked reading times.},
  file = {~/Zotfiles/giulianelli.m2023 Information Value Measuring Utterance P.pdf}
}

@inproceedings{giulianelli.m:2024,
  title = {On the Proper Treatment of Tokenization in Psycholinguistics},
  booktitle = {Proceedings of the 2024 Conference on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Giulianelli, Mario and Malagutti, Luca and Gastaldi, Juan Luis and DuSell, Brian and Vieira, Tim and Cotterell, Ryan},
  year = {2024},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA}
}

@misc{giulianelli.m:2024arxiv,
  title = {On the Proper Treatment of Tokenization in Psycholinguistics},
  author = {Giulianelli, Mario and Malagutti, Luca and Gastaldi, Juan Luis and DuSell, Brian and Vieira, Tim and Cotterell, Ryan},
  year = {2024},
  month = oct,
  number = {arXiv:2410.02691},
  eprint = {2410.02691},
  publisher = {arXiv},
  urldate = {2024-11-12},
  abstract = {Language models are widely used in computational psycholinguistics to test theories that relate the negative log probability (the surprisal) of a region of interest (a substring of characters) under a language model to its cognitive cost experienced by readers, as operationalized, for example, by gaze duration on the region. However, the application of modern language models to psycholinguistic studies is complicated by the practice of using tokenization as an intermediate step in training a model. Doing so results in a language model over token strings rather than one over character strings. Vexingly, regions of interest are generally misaligned with these token strings. The paper argues that token-level language models should be (approximately) marginalized into character-level language models before they are used in psycholinguistic studies to compute the surprisal of a region of interest; then, the marginalized character-level language model can be used to compute the surprisal of an arbitrary character substring, which we term a focal area, that the experimenter may wish to use as a predictor. Our proposal of marginalizing a token-level model into a character-level one solves this misalignment issue independently of the tokenization scheme. Empirically, we discover various focal areas whose surprisal is a better psychometric predictor than the surprisal of the region of interest itself.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/giulianelli.m2024arxiva On the Proper Treatment of Tokenization.pdf}
}

@misc{giulianelli.m:2024arxiva,
  title = {Generalized Measures of Anticipation and Responsivity in Online Language Processing},
  author = {Giulianelli, Mario and Opedal, Andreas and Cotterell, Ryan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.10728},
  eprint = {2409.10728},
  publisher = {arXiv},
  urldate = {2024-10-13},
  abstract = {We introduce a generalization of classic information-theoretic measures of predictive uncertainty in online language processing, based on the simulation of expected continuations of incremental linguistic contexts. Our framework provides a formal definition of anticipatory and responsive measures, and it equips experimenters with the tools to define new, more expressive measures beyond standard next-symbol entropy and surprisal. While extracting these standard quantities from language models is convenient, we demonstrate that using Monte Carlo simulation to estimate alternative responsive and anticipatory measures pays off empirically: New special cases of our generalized formula exhibit enhanced predictive power compared to surprisal for human cloze completion probability as well as ELAN, LAN, and N400 amplitudes, and greater complementarity with surprisal in predicting reading times.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Theory,Mathematics - Information Theory},
  file = {~/Zotfiles/giulianelli.m2024arxiv Generalized Measures of Anticipation and.pdf}
}

@misc{giulianelli.m:2024psyarxiv,
  title = {Incremental Alternative Sampling as a Lens into the Temporal and Representational Resolution of Linguistic Prediction},
  author = {Giulianelli, Mario and Wallbridge, Sarenne and Cotterell, Ryan and Fern{\'a}ndez, Raquel},
  year = {2024},
  month = jul,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/fhp84},
  urldate = {2024-10-17},
  abstract = {This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS).  Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alternatives. Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender's predicted continuations of partial linguistic stimuli thanks to (i) an expressive representation function that captures different levels of linguistic processing and (ii) the bootstrapping of long-horizon prediction error. Our results show that IAS estimates of processing difficulty, computed with autoregressive language models via Monte Carlo estimation, have greater predictive power than surprisal extracted from the same language models for most neural and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.  Perhaps more importantly, IAS estimates provide insight into the nature of the predictive mechanisms that generate those responses during language comprehension.  Variability in neural and behavioural responses is well explained by different combinations of the representational and temporal resolution of prediction. Processing difficulty calculated at varying representational domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are determined by a combination of experimental task setup and naturalness of the stimulus. Beyond enriching psycholinguistic models, IAS can also provide insights into the information processing mechanisms of computational language models.  Our analysis of next-word surprisal under the lenses of IAS reveals that, despite the metric's seemingly narrow focus on the upcoming word, language model surprisal implicitly captures anticipatory processing of multiple future lexical items.},
  langid = {american},
  keywords = {Cognitive cost,Cognitive modelling,Incremental language processing,Information theory,Language comprehension,Language models,Linguistics,Predictive uncertainty}
}

@article{gleitman.l:2019,
  title = {The Impossibility of Language Acquisition (and How They Do It)},
  author = {Gleitman, Lila R. and Liberman, Mark Y. and McLemore, Cynthia A. and Partee, Barbara H.},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {1--24},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011718-011640},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011640},
  date-added = {2021-08-17 09:52:01 -0400},
  date-modified = {2021-08-17 09:52:03 -0400}
}

@misc{godfrey.j:1993switchboard,
  title = {Switchboard-1 Release 2},
  author = {Godfrey, John J. and Holliman, Edward},
  year = {1993},
  number = {LDC97S62},
  publisher = {Linguistic Data Consortium},
  doi = {10.35111/SW3H-RW02},
  bdsk-url-2 = {https://doi.org/10.35111/SW3H-RW02},
  date-added = {2022-05-06 14:21:03 -0400},
  date-modified = {2022-05-06 14:23:45 -0400},
  howpublished = {Web Download},
  keywords = {dataset,speech errors}
}

@techreport{goertzel.g:1949,
  type = {Technical Report},
  title = {Quota Sampling and Importance Functions in Stochastic Solution of Particle Problems},
  author = {Goertzel, Gerald},
  year = {1949},
  number = {AECD-2793},
  address = {Oak Ridge, Tennessee},
  institution = {Oak Ridge National Laboratory, U.S. Atomic Energy Commission, Technical Information Division},
  langid = {english}
}

@inproceedings{gogate.v:2007,
  title = {{{SampleSearch}}: A Scheme That Searches for Consistent Samples},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  author = {Gogate, Vibhav and Dechter, Rina},
  editor = {Meila, Marina and Shen, Xiaotong},
  year = {2007},
  month = mar,
  series = {Proceedings of Machine Learning Research},
  volume = {2},
  pages = {147--154},
  publisher = {PMLR},
  address = {San Juan, Puerto Rico},
  abstract = {Sampling from belief networks which have a substantial number of zero probabilities is problematic. MCMC algorithms like Gibbs sampling do not converge and importance sampling schemes generate many zero weight samples that are rejected, yielding an inefficient sampling process (the rejection problem). In this paper, we propose to augment importance sampling with systematic constraint-satisfaction search in order to overcome the rejection problem. The resulting SampleSearch scheme can be made unbiased by using a computationally expensive weighting scheme. To overcome this an approximation is proposed such that the resulting estimator is asymptotically unbiased. Our empirical results demonstrate the potential of our new scheme.},
  date-added = {2022-05-05 09:35:36 -0400},
  date-modified = {2022-05-05 09:37:37 -0400},
  pdf = {http://proceedings.mlr.press/v2/gogate07a/gogate07a.pdf},
  keywords = {sample search}
}

@book{goldberg.y:2017book,
  title = {Neural Network Methods for Natural Language Processing},
  author = {Goldberg, Yoav},
  year = {2017},
  publisher = {{Morgan and Claypool Publishers}},
  date-added = {2019-05-17 21:08:13 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  keywords = {machine learning,neural networks,recurrent neural networks,sequence to sequence models,word embeddings}
}

@misc{goldberg.y:2019,
  title = {Assessing {{BERT}}'s Syntactic Abilities},
  author = {Goldberg, Yoav},
  year = {2019},
  eprint = {1901.05287},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{goldstein.a:2021,
  title = {Thinking Ahead: Spontaneous Prediction in Context as a Keystone of Language in Humans and Machines},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  year = {2021},
  journal = {bioRxiv : the preprint server for biology},
  eprint = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477.full.pdf},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.12.02.403477},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM's next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM's contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.Competing Interest StatementThe authors have declared no competing interest.},
  bdsk-url-2 = {https://doi.org/10.1101/2020.12.02.403477},
  date-added = {2021-06-09 15:53:23 -0400},
  date-modified = {2021-06-09 15:53:24 -0400},
  elocation-id = {2020.12.02.403477},
  file = {~/Zotfiles/goldstein.a2021 Thinking ahead spontaneous prediction i.pdf}
}

@article{gomez.p:2008,
  title = {The Overlap Model: {{A}} Model of Letter Position Coding.},
  shorttitle = {The Overlap Model},
  author = {Gomez, Pablo and Ratcliff, Roger and Perea, Manuel},
  year = {2008},
  journal = {Psychological Review},
  volume = {115},
  number = {3},
  pages = {577--600},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0012667},
  urldate = {2023-12-13},
  langid = {english},
  file = {~/Zotfiles/gomez.p2008 The overlap model A model of letter pos.pdf}
}

@inproceedings{goodkind.a:2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}} ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/w18-0102},
  bdsk-url-2 = {https://doi.org/10.18653/v1/w18-0102},
  date-added = {2021-11-29 10:00:16 -0500},
  date-modified = {2021-11-29 10:00:18 -0500}
}

@misc{goodkind.a:2021,
  title = {Local Word Statistics Affect Reading Times Independently of Surprisal},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2103.04469},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2103.04469},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-09 17:20:07 -0400},
  date-modified = {2022-05-09 17:21:18 -0400},
  keywords = {causal bottleneck},
  file = {~/Zotfiles/goodkind.a2021 Local word statistics affect reading tim.pdf}
}

@article{goodman.j:1999,
  title = {Semiring Parsing},
  author = {Goodman, Joshua},
  year = {1999},
  journal = {Computational Linguistics},
  volume = {25},
  number = {4},
  pages = {573--606}
}

@article{goodman.j:2001,
  title = {A Bit of Progress in Language Modeling},
  author = {Goodman, Joshua T.},
  year = {2001},
  journal = {Computer Speech \& Language},
  volume = {15},
  number = {4},
  pages = {403--434},
  issn = {0885-2308},
  doi = {10.1006/csla.2001.0174},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-09 15:53:09 -0400},
  opturl = {http://www.sciencedirect.com/science/article/pii/S0885230801901743}
}

@inproceedings{goodwin.e:2020,
  title = {Probing {{Linguistic Systematicity}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  year = {2020},
  month = jul,
  pages = {1958--1969},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.177},
  urldate = {2022-05-19},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.},
  keywords = {natural language inference,natural logic,systematicity}
}

@inproceedings{gorla.j:2007,
  title = {Two Approaches for Building an Unsupervised Dependency Parser and Their Other Applications},
  booktitle = {{{PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE}}},
  author = {Gorla, Jagadeesh and Goyal, Amit and Sangal, Rajeev},
  year = {2007},
  volume = {22},
  pages = {1860},
  date-added = {2020-04-23 11:09:55 -0400},
  date-modified = {2020-04-23 11:10:41 -0400},
  organization = {Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised parsing},
  file = {~/Zotfiles/gorla.j2007 Two approaches for building an unsupervi.pdf}
}

@article{gottwald.s:2020,
  title = {The Two Kinds of Free Energy and the {{Bayesian}} Revolution},
  author = {Gottwald, Sebastian and Braun, Daniel A.},
  year = {2020},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008420},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008420},
  urldate = {2022-07-11},
  abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information processing of perception.},
  langid = {english},
  keywords = {Decision making,Entropy,Free energy,Helmholtz free energy,Information processing,Kullback Leibler divergence,Optimization,Probability distribution},
  file = {~/Zotfiles/gottwald.s2020 The two kinds of free energy and the Bay.pdf}
}

@inproceedings{goyal.k:2019,
  title = {An {{Empirical Investigation}} of {{Global}} and {{Local Normalization}} for {{Recurrent Neural Sequence Models Using}} a {{Continuous Relaxation}} to {{Beam Search}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Goyal, Kartik and Dyer, Chris and {Berg-Kirkpatrick}, Taylor},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {1724--1733},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1171},
  urldate = {2025-06-26},
  abstract = {Globally normalized neural sequence models are considered superior to their locally normalized equivalents because they may ameliorate the effects of label bias. However, when considering high-capacity neural parametrizations that condition on the whole input sequence, both model classes are theoretically equivalent in terms of the distributions they are capable of representing. Thus, the practical advantage of global normalization in the context of modern neural methods remains unclear. In this paper, we attempt to shed light on this problem through an empirical study. We extend an approach for search-aware training via a continuous relaxation of beam search (Goyal et al., 2017b) in order to enable training of globally normalized recurrent sequence models through simple backpropagation. We then use this technique to conduct an empirical study of the interaction between global normalization, high-capacity encoders, and search-aware optimization. We observe that in the context of inexact search, globally normalized neural models are still more effective than their locally normalized counterparts. Further, since our training approach is sensitive to warm-starting with pre-trained models, we also propose a novel initialization strategy based on self-normalization for pre-training globally normalized models. We perform analysis of our approach on two tasks: CCG supertagging and Machine Translation, and demonstrate the importance of global normalization under different conditions while using search-aware training.},
  file = {~/Zotfiles/goyal.k2019 An Empirical Investigation of Global and.pdf}
}

@misc{goyal.k:2021,
  title = {Exposing the Implicit Energy Networks behind Masked Language Models via {{Metropolis}}--{{Hastings}}},
  author = {Goyal, Kartik and Dyer, Chris and {Berg-Kirkpatrick}, Taylor},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2106.02736},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.02736},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 12:27:57 -0400},
  date-modified = {2022-04-09 00:57:41 -0400},
  keywords = {energy networks,masked language models,metropolis hastings},
  file = {~/Zotfiles/goyal.k2021 Exposing the implicit energy networks be.pdf}
}

@article{graf.t:2017,
  title = {Relative Clauses as a Benchmark for Minimalist Parsing},
  author = {Graf, Thomas and Monette, James and Zhang, Chong},
  year = {2017},
  month = jul,
  journal = {Journal of Language Modelling},
  volume = {5},
  number = {1},
  issn = {2299-8470, 2299-856X},
  doi = {10.15398/jlm.v5i1.157},
  urldate = {2022-09-30},
  abstract = {Minimalist grammars have been used recently in a series of papers to explain well-known contrasts in human sentence processing in terms of subtle structural differences. These proposals combine a top-down parser with complexity metrics that relate parsing difficulty to memory usage. So far, though, there has been no large-scale exploration of the space of viable metrics. Building on this earlier work, we compare the ability of 1600 metrics to derive several processing effects observed with relative clauses, many of which have been proven difficult to unify. We show that among those 1600 candidates, a few metrics (and only a few) can provide a unified account of all these contrasts. This is a welcome result for two reasons: First, it provides a novel account of extensively studied psycholinguistic data. Second, it significantly limits the number of viable metrics that may be applied to other phenomena, thus reducing theoretical indeterminacy.},
  file = {~/Zotfiles/graf.t2017 Relative clauses as a benchmark for mini.pdf}
}

@article{grainger.j:1996,
  title = {Orthographic Processing in Visual Word Recognition:  {{A}} Multiple Read-out Model},
  shorttitle = {Orthographic Processing in Visual Word Recognition},
  author = {Grainger, Jonathan and Jacobs, Arthur M.},
  year = {1996},
  journal = {Psychological Review},
  volume = {103},
  number = {3},
  pages = {518--565},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.103.3.518},
  abstract = {A model of orthographic processing is described that postulates read-out from different information dimensions, determined by variable response criteria set on these dimensions. Performance in a perceptual identification task is simulated as the percentage of trials on which a noisy criterion set on the dimension of single word detector activity is reached. Two additional criteria set on the dimensions of total lexical activity and time from stimulus onset are hypothesized to be operational in the lexical decision task. These additional criteria flexibly adjust to changes in stimulus material and task demands. thus accounting for strategic influences on performance in this task. The model unifies results obtained in response-limited and data-limited paradigms and helps resolve a number of inconsistencies in the experimental literature that cannot be accommodated by other current models of visual word recognition. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Models,Orthography,Visual Stimulation,Word Recognition},
  file = {~/Zotfiles/grainger.j1996 Orthographic processing in visual word r.pdf}
}

@inproceedings{gralinski.f:2025,
  title = {Oddballness: Universal Anomaly Detection with Language Models},
  shorttitle = {Oddballness},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Computational Linguistics}}},
  author = {Gralinski, Filip and Staruch, Ryszard and Jurkiewicz, Krzysztof},
  editor = {Rambow, Owen and Wanner, Leo and Apidianaki, Marianna and {Al-Khalifa}, Hend and Eugenio, Barbara Di and Schockaert, Steven},
  year = {2025},
  month = jan,
  pages = {2683--2689},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, UAE},
  urldate = {2025-06-11},
  abstract = {We present a new method to detect anomalies in texts (in general: in sequences of any data), using language models, in a totally unsupervised manner. The method considers probabilities (likelihoods) generated by a language model, but instead of focusing on low-likelihood tokens, it considers a new metric defined in this paper: oddballness. Oddballness measures how ``strange'' a given token is according to the language model. We demonstrate in grammatical error detection tasks (a specific case of text anomaly detection) that oddballness is better than just considering low-likelihood events, if a totally unsupervised setup is assumed.},
  file = {~/Zotfiles/gralinski.f2025 Oddballness universal anomaly detection.pdf}
}

@misc{graves.a:2017arxiv,
  title = {Adaptive {{Computation Time}} for {{Recurrent Neural Networks}}},
  author = {Graves, Alex},
  year = {2017},
  month = feb,
  number = {arXiv:1603.08983},
  eprint = {1603.08983},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1603.08983},
  urldate = {2025-04-17},
  abstract = {This paper introduces Adaptive Computation Time (ACT), an algorithm that allows recurrent neural networks to learn how many computational steps to take between receiving an input and emitting an output. ACT requires minimal changes to the network architecture, is deterministic and differentiable, and does not add any noise to the parameter gradients. Experimental results are provided for four synthetic problems: determining the parity of binary vectors, applying binary logic operations, adding integers, and sorting real numbers. Overall, performance is dramatically improved by the use of ACT, which successfully adapts the number of computational steps to the requirements of the problem. We also present character-level language modelling results on the Hutter prize Wikipedia dataset. In this case ACT does not yield large gains in performance; however it does provide intriguing insight into the structure of the data, with more computation allocated to harder-to-predict transitions, such as spaces between words and ends of sentences. This suggests that ACT or other adaptive computation methods could provide a generic method for inferring segment boundaries in sequence data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  file = {~/Zotfiles/graves.a2017arxiv Adaptive Computation Time for Recurrent.pdf}
}

@book{green.d:1966book,
  title = {Signal Detection Theory and Psychophysics},
  author = {Green, David Marvin and Swets, John A.},
  year = {1966},
  volume = {1},
  publisher = {Wiley New York},
  keywords = {ideal observer,ideal observer model}
}

@incollection{grice.h:1975,
  title = {Logic and Conversation},
  booktitle = {Speech {{Acts}}},
  author = {Grice, H. P.},
  editor = {Cole, Peter and Morgan, Jerry L.},
  year = {1975},
  month = dec,
  pages = {41--58},
  publisher = {BRILL},
  doi = {10.1163/9789004368811_003},
  urldate = {2022-09-30},
  isbn = {978-90-04-36881-1 978-90-04-36857-6}
}

@misc{griffith.v:2012,
  title = {Quantifying Synergistic Mutual Information},
  author = {Griffith, Virgil and Koch, Christof},
  year = {2012},
  eprint = {1205.4265},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2020-07-06 08:48:33 -0400},
  date-modified = {2020-07-06 08:49:07 -0400},
  project = {information-compositionality}
}

@incollection{griffiths.t:2008,
  title = {A Primer on Probabilistic Inference},
  booktitle = {The Probabilistic Mind: Prospects for {{Bayesian}} Cognitive Science},
  author = {Griffiths, Thomas L. and Yuille, Alan},
  editor = {Chater, Nick and Oaksford, Mike},
  year = {2008},
  month = mar,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199216093.003.0002},
  urldate = {2024-05-26},
  abstract = {This chapter provides the technical introduction to Bayesian methods. Probabilistic models of cognition are often referred to as Bayesian models, reflecting the central role that Bayesian inference plays in reasoning under uncertainty. It introduces the basic ideas of Bayesian inference and discusses how it can be used in different contexts. Probabilistic models provide a unique opportunity to develop a rational account of human cognition that combines statistical learning with structured representations. It recommends the EM algorithm and Markov chain Monte Carlo to estimate the parameters of models that incorporate latent variables, and to work with complicated probability distributions of the kind that often arise in Bayesian inference.},
  isbn = {978-0-19-921609-3}
}

@article{griffiths.t:2012,
  title = {Bridging Levels of Analysis for Probabilistic Models of Cognition},
  author = {Griffiths, Thomas L. and Vul, Edward and Sanborn, Adam N.},
  year = {2012},
  month = aug,
  journal = {Current Directions in Psychological Science},
  volume = {21},
  number = {4},
  pages = {263--268},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1177/0963721412447619},
  urldate = {2025-02-17},
  abstract = {Probabilistic models of cognition characterize the abstract computational problems underlying inductive inferences and identify their ideal solutions. This approach differs from traditional methods of investigating human cognition, which focus on identifying the cognitive or neural processes that underlie behavior and therefore concern alternative levels of analysis. To evaluate the theoretical implications of probabilistic models and increase their predictive power, we must understand the relationships between theories at these different levels of analysis. One strategy for bridging levels of analysis is to explore cognitive processes that have a direct link to probabilistic inference. Recent research employing this strategy has focused on the possibility that the Monte Carlo principle---which concerns sampling from probability distributions in order to perform computations---provides a way to link probabilistic models of cognition to more concrete cognitive and neural processes.},
  langid = {english},
  keywords = {sampling},
  file = {~/Zotfiles/griffiths.t2012 Bridging Levels of Analysis for Probabil.pdf}
}

@article{griffiths.t:2015,
  title = {Rational Use of Cognitive Resources: {{Levels}} of Analysis between the Computational and the Algorithmic},
  author = {Griffiths, Thomas L. and Lieder, Falk and Goodman, Noah D.},
  year = {2015},
  journal = {Topics in cognitive science},
  volume = {7},
  number = {2},
  pages = {217--229},
  publisher = {Wiley Online Library},
  doi = {10.1111/tops.12142},
  isbn = {1756-8757}
}

@book{griffiths.t:2024book,
  title = {Bayesian Models of Cognition: Reverse Engineering the Mind},
  shorttitle = {Bayesian Models of Cognition},
  author = {Griffiths, Thomas L. and Chater, Nick and Tenenbaum, Joshua},
  year = {2024},
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  abstract = {"The definitive work on the Bayesian approach to Cognitive Science and an important work in understanding the mind and the brain"--},
  isbn = {978-0-262-04941-2},
  lccn = {BF311 .G758 2024},
  keywords = {Cognition,Cognitive psychology,Cognitive science},
  file = {~/Zotfiles/griffiths.t2024book Bayesian models of cognition reverse en.pdf}
}

@incollection{griffiths.t:2024bridge,
  title = {Sampling as a Bridge across Levels of Analysis},
  booktitle = {Bayesian Models of Cognition: Reverse Engineering the Mind},
  author = {Griffiths, Thomas L. and Vul, Edward and Sanborn, Adam N. and Chater, Nick},
  year = {2024},
  pages = {285--297},
  publisher = {MIT Press},
  address = {Cambdridge, Mass},
  isbn = {978-0-262-38105-5},
  langid = {english}
}

@incollection{griffiths.t:2024oecs,
  title = {Bayesian Models of Cognition},
  booktitle = {Open {{Encyclopedia}} of {{Cognitive Science}}},
  author = {Griffiths, Thomas L.},
  editor = {Frank, Michael C. and Majid, Asifa},
  year = {2024},
  month = jul,
  publisher = {MIT Press},
  doi = {10.21428/e2759450.7b420317},
  urldate = {2025-03-04},
  langid = {english},
  file = {~/Zotfiles/griffiths.t2024openEncyc Bayesian models of cognition.pdf}
}

@article{grodner.d:2003,
  title = {Against Repair-Based Reanalysis in Sentence Comprehension},
  author = {Grodner, Daniel and Gibson, Edward and Argaman, Vered and Babyonyshev, Maria},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {141--166},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1022496223965},
  bdsk-url-2 = {https://doi.org/10.1023/a:1022496223965},
  date-added = {2021-03-18 11:28:51 -0400},
  date-modified = {2021-03-18 11:30:16 -0400},
  keywords = {reading time,self-paced reading}
}

@misc{groeneveld.d:2024arxiv,
  title = {{{OLMo}}: Accelerating the Science of Language Models},
  shorttitle = {{{OLMo}}},
  author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00838},
  eprint = {2402.00838},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@book{grune.d:2008book,
  title = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  year = {2008},
  publisher = {Springer New York},
  doi = {10.1007/978-0-387-68954-8},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-68954-8},
  date-added = {2022-03-31 10:23:54 -0400},
  date-modified = {2022-03-31 10:24:43 -0400},
  keywords = {book,parsing}
}

@misc{grunwald.p:2004,
  title = {Shannon Information and Kolmogorov Complexity},
  author = {Grunwald, Peter and Vitanyi, Paul},
  year = {2004},
  eprint = {cs/0410002},
  archiveprefix = {arXiv},
  date-added = {2019-09-13 08:19:19 -0400},
  date-modified = {2019-09-13 08:20:10 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity,mutual information,rate-distortion theory,shannon entropy}
}

@misc{grunwald.p:2004a,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  year = {2004},
  eprint = {math/0406077},
  archiveprefix = {arXiv},
  date-added = {2020-02-20 11:40:38 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {minimum description length,mutual information},
  file = {~/Zotfiles/grunwald.p2004a A tutorial introduction to the minimum d.pdf}
}

@article{gryszka.k:2021,
  title = {From Biased Coin to Any Discrete Distribution},
  author = {Gryszka, Karol},
  year = {2021},
  month = sep,
  journal = {Periodica Mathematica Hungarica},
  volume = {83},
  number = {1},
  pages = {71--80},
  issn = {1588-2829},
  doi = {10.1007/s10998-020-00363-w},
  urldate = {2023-12-24},
  abstract = {In this note we construct an algorithm generating any discrete distribution with an arbitrary coin (and, as a result, with arbitrary initial distribution). The coin need not be fair and the target distribution can be supported on a countable set.},
  langid = {english},
  keywords = {biased coin,coin flip,discrete distribution,Discrete probability measure,expected value,Primary 65C10,Secondary 60A10,simulation of distribution},
  file = {~/Zotfiles/gryszka.k2021 From biased coin to any discrete distrib.pdf}
}

@article{gull.s:1993,
  title = {Imaginary Numbers Are Not Real---{{The}} Geometric Algebra of Spacetime},
  author = {Gull, Stephen and Lasenby, Anthony and Doran, Chris},
  year = {1993},
  month = sep,
  journal = {Foundations of Physics},
  volume = {23},
  number = {9},
  pages = {1175--1201},
  issn = {1572-9516},
  doi = {10.1007/BF01883676},
  urldate = {2025-02-05},
  abstract = {This paper contains a tutorial introduction to the ideas of geometric algebra, concentrating on its physical applications. We show how the definition of a ``geometric product'' of vectors in 2-and 3-dimensional space provides precise geometrical interpretations of the imaginary numbers often used in conventional methods. Reflections and rotations are analyzed in terms of bilinear spinor transformations, and are then related to the theory of analytic functions and their natural extension in more than two dimensions (monogenics), Physics is greatly facilitated by the use of Hestenes' spacetime algebra, which automatically incorporates the geometric structure of spacetime. This is demonstrated by examples from electromagnetism. In the course of this purely classical exposition many surprising results are obtained---results which are usually thought to belong to the preserve of quantum theory. We conclude that geometric algebra is the most powerful and general language available for the development of mathematical physics.},
  langid = {english},
  keywords = {Analytic Function,Conventional Method,geometric algebra,Mathematical Physic,Physical Application,Reflection},
  file = {~/Zotfiles/gull.s1993 Imaginary numbers are not realThe geome.pdf}
}

@inproceedings{gulordava.k:2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1108},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1108}
}

@article{gutknecht.a:2021,
  title = {Bits and Pieces: Understanding Information Decomposition from Part-Whole Relationships and Formal Logic},
  author = {Gutknecht, A. J. and Wibral, M. and Makkeh, A.},
  year = {2021},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {477},
  number = {2251},
  pages = {20210110},
  publisher = {The Royal Society},
  doi = {10.1098/rspa.2021.0110},
  bdsk-url-2 = {https://doi.org/10.1098/rspa.2021.0110},
  date-added = {2022-04-18 11:14:51 -0400},
  date-modified = {2022-04-18 11:15:12 -0400},
  keywords = {partial information decomposition}
}

@book{hacking.i:2006book2,
  title = {The Emergence of Probability: {{A}} Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference},
  author = {Hacking, Ian},
  year = {2006},
  edition = {2},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511817557},
  date-added = {2021-02-16 15:11:17 -0500},
  date-modified = {2021-02-16 15:11:20 -0500}
}

@inproceedings{hagiwara.m:2020,
  title = {{{GitHub Typo Corpus}}: {{A Large-Scale Multilingual Dataset}} of {{Misspellings}} and {{Grammatical Errors}}},
  shorttitle = {{{GitHub Typo Corpus}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Hagiwara, Masato and Mita, Masato},
  year = {2020},
  month = may,
  pages = {6761--6768},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  urldate = {2023-08-24},
  abstract = {The lack of large-scale datasets has been a major hindrance to the development of NLP tasks such as spelling correction and grammatical error correction (GEC). As a complementary new resource for these tasks, we present the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, a large and popular platform for hosting and sharing git repositories. The dataset, which we have made publicly available, contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date. We also describe our process for filtering true typo edits based on learned classifiers on a small annotated subset, and demonstrate that typo edits can be identified with F1 0.9 using a very simple classifier with only three features. The detailed analyses of the dataset show that existing spelling correctors merely achieve an F-measure of approx. 0.5, suggesting that the dataset serves as a new, rich source of spelling errors that complement existing datasets.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  keywords = {grammatical error correction,spelling correction,typos},
  file = {~/Zotfiles/hagiwara.m2020 GitHub Typo Corpus A Large-Scale Multil.pdf}
}

@inproceedings{hahn.m:2018cogsci,
  title = {An Information-Theoretic Explanation of Adjective Ordering Preferences},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Hahn, Michael and Degen, Judith and Goodman, Noah and Jurafsky, Daniel and Futrell, Richard},
  editor = {Kalish, Charles and Rau, Martina and Zhu, Jerry and Rogers, Timothy},
  year = {2018},
  pages = {1766--1772},
  publisher = {Cognitive Science Society},
  address = {Madison, Wisconsin, USA},
  abstract = {Across languages, adjectives are subject to ordering restrictions. Recent research shows that these are predicted by adjective subjectivity, but the question remains open why this is the case. We first conduct a corpus study and not only replicate the subjectivity effect, but also find a previously undocumented effect of mutual information between adjectives and nouns. We then describe a rational model of adjective use in which listeners explicitly reason about judgments made by different speakers, formalizing the notion of subjectivity as agreement between speakers. We show that, once incremental processing is combined with memory limitations, our model predicts effects both of subjectivity and mutual information. We confirm the adequacy of our model by evaluating it on corpus data, finding that it correctly predicts ordering in unseen data with an accuracy of 96.2\%. This suggests that adjective ordering can be explained by general principles of human communication and language processing.},
  langid = {english},
  file = {~/Zotfiles/hahn.m2018cogsci An information-theoretic explanation of.pdf}
}

@inproceedings{hahn.m:2019cogsci,
  title = {Character-Based {{Surprisal}} as a {{Model}} of {{Reading Difficulty}} in the {{Presence}} of {{Errors}}.},
  booktitle = {Proceedings of the 41st {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Hahn, Michael and Keller, Frank and Bisk, Yonatan and Belinkov, Yonatan},
  year = {2019},
  pages = {401--407},
  address = {Montr{\'e}al, Canada},
  doi = {10.48550/arXiv.1902.00595},
  file = {~/Zotfiles/hahn.m2019cogsci Character-based Surprisal as a Model of.pdf}
}

@unpublished{hahn.m:2019ms,
  type = {Unpublished Manuscript},
  title = {Estimating Predictive Rate-Distortion Curves via Neural Variational Inference},
  author = {Hahn, Michael and Futrell, Richard},
  year = {2019},
  date-added = {2019-06-11 14:15:47 -0400},
  date-modified = {2019-06-17 21:56:11 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory,variational inference}
}

@article{hahn.m:2021,
  title = {Modeling Word and Morpheme Order in Natural Language as an Efficient Trade-off of Memory and Surprisal.},
  author = {Hahn, Michael and Degen, Judith and Futrell, Richard},
  year = {2021},
  month = apr,
  journal = {Psychological Review},
  volume = {128},
  number = {4},
  pages = {726},
  publisher = {US: American Psychological Association},
  issn = {1939-1471},
  doi = {10.1037/rev0000269},
  urldate = {2023-03-27}
}

@article{hahn.m:2022,
  title = {Morpheme Ordering across Languages Reflects Optimization for Processing Efficiency},
  author = {Hahn, Michael and Mathew, Rebecca and Degen, Judith},
  year = {2022},
  month = feb,
  journal = {Open Mind},
  volume = {5},
  pages = {208--232},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00051},
  urldate = {2023-03-27},
  abstract = {The ordering of morphemes in a word displays well-documented regularities across languages. Previous work has explained these in terms of notions such as semantic scope, relevance, and productivity. Here, we test a recently formulated processing theory of the ordering of linguistic units, the efficient tradeoff hypothesis (Hahn et al., 2021). The claim of the theory is that morpheme ordering can partly be explained by the optimization of a tradeoff between memory and surprisal. This claim has received initial empirical support from two languages. In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology, and by considering nouns in addition to verbs. We find that the efficient tradeoff hypothesis predicts ordering in most cases with high accuracy, and accounts for cross-linguistic regularities in noun and verb inflection. Our work adds to a growing body of work suggesting that many ordering properties of language arise from a pressure for efficient language processing.},
  file = {~/Zotfiles/hahn.m2022 Morpheme ordering across languages refle.pdf}
}

@article{hahn.m:2022PNAS,
  title = {A Resource-Rational Model of Human Processing of Recursive Linguistic Structure},
  author = {Hahn, Michael and Futrell, Richard and Levy, Roger and Gibson, Edward},
  year = {2022},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {43},
  pages = {e2122602119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2122602119},
  urldate = {2022-11-28},
  abstract = {A major goal of psycholinguistic theory is to account for the cognitive constraints limiting the speed and ease of language comprehension and production. Wide-ranging evidence demonstrates a key role for linguistic expectations: A word's predictability, as measured by the information-theoretic quantity of surprisal, is a major determinant of processing difficulty. But surprisal, under standard theories, fails to predict the difficulty profile of an important class of linguistic patterns: the nested hierarchical structures made possible by recursion in human language. These nested structures are better accounted for by psycholinguistic theories of constrained working memory capacity. However, progress on theory unifying expectation-based and memory-based accounts has been limited. Here we present a unified theory of a rational trade-off between precision of memory representations with ease of prediction, a scaled-up computational implementation using contemporary machine learning methods, and experimental evidence in support of the theory's distinctive predictions. We show that the theory makes nuanced and distinctive predictions for difficulty patterns in nested recursive structures predicted by neither expectation-based nor memory-based theories alone. These predictions are confirmed 1) in two language comprehension experiments in English, and 2) in sentence completions in English, Spanish, and German. More generally, our framework offers computationally explicit theory and methods for understanding how memory constraints and prediction interact in human language comprehension and production.},
  file = {~/Zotfiles/hahn.m2022PNAS A resource-rational model of human proce 2.pdf;~/Zotfiles/hahn.m2022PNAS A resource-rational model of human proce.pdf}
}

@article{hahn.m:2023,
  title = {Modeling Task Effects in Human Reading with Neural Network-Based Attention},
  author = {Hahn, Michael and Keller, Frank},
  year = {2023},
  month = jan,
  journal = {Cognition},
  volume = {230},
  pages = {105289},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105289},
  urldate = {2023-03-23},
  abstract = {Research on human reading has long documented that reading behavior shows task-specific effects, but it has been challenging to build general models predicting what reading behavior humans will show in a given task. We introduce NEAT, a computational model of the allocation of attention in human reading, based on the hypothesis that human reading optimizes a tradeoff between economy of attention and success at a task. Our model is implemented using contemporary neural network modeling techniques, and makes explicit and testable predictions about how the allocation of attention varies across different tasks. We test this in an eyetracking study comparing two versions of a reading comprehension task, finding that our model successfully accounts for reading behavior across the tasks. Our work thus provides evidence that task effects can be modeled as optimal adaptation to task demands.},
  langid = {english},
  keywords = {Computational modeling,Reading,Task effects},
  file = {~/Zotfiles/hahn.m2023cognition Modeling task effects in human reading w.pdf}
}

@misc{hahn.m:2023arxiv,
  title = {A Theory of Emergent In-Context Learning as Implicit Structure Induction},
  author = {Hahn, Michael and Goyal, Navin},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07971},
  eprint = {2303.07971},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-23},
  abstract = {Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{hale.j:2001,
  title = {A Probabilistic {{Earley}} Parser as a Psycholinguistic Model},
  booktitle = {Second Meeting of the {{North American}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Hale, John T.},
  year = {2001},
  date-modified = {2022-04-20 13:49:59 -0400},
  file = {~/Zotfiles/hale.j2001 A probabilistic Earley parser as a psych.pdf}
}

@article{hale.j:2003,
  title = {The Information Conveyed by Words in Sentences},
  author = {Hale, John T.},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {101--123},
  publisher = {Springer},
  doi = {10.1023/A:1022492123056},
  date-added = {2021-04-08 14:24:37 -0400},
  date-modified = {2022-04-20 13:49:43 -0400},
  keywords = {entropy reduction}
}

@phdthesis{hale.j:2003phd,
  title = {Grammar, Uncertainty and Sentence Processing},
  author = {Hale, John T.},
  year = {2003},
  address = {Baltimore, Maryland},
  abstract = {Toward a probabilistic theory of human sentence processing, this dissertation proposes a definition of computational work done in the course of analyzing sentences generated by formal grammars. It applies the idea of entropy from information theory to the set of derivations compatible with an initial substring of a sentence. Given a probabilistic grammar, this permits the set of such compatible derivations to be viewed as a random variable, and the change in uncertainty about the outcomes to be calculated. This definition of computational work is examined as a cognitive model of human sentence processing difficulty. To apply the model, a variety of existing syntactic proposals for English sentences are cast as probabilistic Generalized Phrase Structure Grammars (Gazdar et al., 1985) and probabilistic Minimalist Grammars (Stabler, 1997). It is shown that the amount of predicted processing effort in relative clauses correlates with the Accessibility Hierarchy of relativized grammatical relations (Keenan and Comrie, 1977) on a Kaynian (1994) view of relative clause structure. Results from three new on-line sentence reading experiments suggest that while genitivity has the role suggested by the Accessibility Hierarchy, extraction from oblique does not. Evidence is also found for a direct object/indirect object processing asymmetry, which can be derived from the proposed cognitive model under the assumption of a lexicalized probabilistic grammar.},
  date-added = {2022-04-14 15:27:00 -0400},
  date-modified = {2022-04-14 15:31:29 -0400},
  isbn = {978-0-496-55064-7},
  school = {Johns Hopkins University},
  file = {~/Zotfiles/hale.j2003phd Grammar, uncertainty and sentence proces.pdf}
}

@inproceedings{hale.j:2004,
  title = {The Information-Processing Difficulty of Incremental Parsing},
  booktitle = {Proceedings of the Workshop on Incremental Parsing: {{Bringing}} Engineering and Cognition Together},
  author = {Hale, John T.},
  year = {2004},
  month = jul,
  pages = {58--65},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  date-added = {2022-04-14 13:31:29 -0400},
  date-modified = {2022-04-20 13:50:05 -0400}
}

@article{hale.j:2006,
  title = {Uncertainty about the Rest of the Sentence},
  author = {Hale, John T.},
  year = {2006},
  journal = {Cognitive Science},
  volume = {30},
  number = {4},
  pages = {643--672},
  publisher = {Wiley},
  doi = {10.1207/s15516709cog0000_64},
  bdsk-url-2 = {https://doi.org/10.1207/s15516709cog0000{$_6$}4},
  date-added = {2021-03-18 10:37:45 -0400},
  date-modified = {2022-04-20 13:50:10 -0400},
  keywords = {entropy reduction,processing}
}

@article{hale.j:2011,
  title = {What a Rational Parser Would Do},
  author = {Hale, John T.},
  year = {2011},
  month = apr,
  journal = {Cognitive Science},
  volume = {35},
  number = {3},
  pages = {399--443},
  issn = {03640213},
  doi = {10.1111/j.1551-6709.2010.01145.x},
  urldate = {2022-10-24},
  abstract = {This article examines cognitive process models of human sentence comprehension based on the idea of informed search. These models are rational in the sense that they strive to find a good syntactic analysis quickly. Informed search derives a new account of garden pathing that handles traditional counterexamples. It supports a symbolic explanation for local coherence as well as an algorithmic account of entropy reduction. The models are expressed in a broad framework for theories of human sentence comprehension.},
  langid = {english},
  keywords = {entropy reduction,parsing algorithm,rational analysis,sentence processing}
}

@book{hale.j:2014book,
  title = {Automaton Theories of Human Sentence Comprehension},
  author = {Hale, John T.},
  year = {2014},
  month = sep,
  series = {{{CSLI Studies}} in {{Computational Linguistics}}},
  publisher = {{CSLI Publications, Center for the Study of Language and Information}},
  address = {Stanford, CA},
  urldate = {2022-07-01},
  abstract = {Different kinds of grammars may actually be used in models of perceptual processing. By relating grammars to cognitive architecture, John T. Hale shows step-by-step how incremental parsing works and how specific learning rules might lead to frequency-sensitive preferences. Along the way, this book reconsiders garden-pathing, the parallel/serial distinction and information-theoretical complexity metrics, such as surprisal. This book is a must for cognitive scientists of language.},
  langid = {english},
  file = {~/Zotfiles/hale.j2014 Automaton theories of human sentence com.pdf}
}

@article{hale.j:2016,
  title = {Information-Theoretical {{Complexity Metrics}}},
  author = {Hale, John T.},
  year = {2016},
  journal = {Language and Linguistics Compass},
  volume = {10},
  number = {9},
  pages = {397--412},
  issn = {1749-818X},
  doi = {10.1111/lnc3.12196},
  urldate = {2024-05-03},
  abstract = {Information-theoretical complexity metrics are auxiliary hypotheses that link theories of parsing and grammar to potentially observable measurements such as reading times and neural signals. This review article considers two such metrics, Surprisal and Entropy Reduction, which are respectively built upon the two most natural notions of `information value' for an observed event (Blachman ). This review sketches their conceptual background and touches on their relationship to other theories in cognitive science. It characterizes them as `lenses' through which theorists `see' the information-processing consequences of linguistic grammars. While these metrics are not themselves parsing algorithms, the review identifies candidate mechanisms that have been proposed for both of them.},
  copyright = {{\copyright} 2016 The Author Language and Linguistics Compass {\copyright} 2016 John Wiley \& Sons Ltd},
  langid = {english},
  file = {~/Zotfiles/hale.j2016 Information-theoretical Complexity Metri.pdf}
}

@inproceedings{hale.j:2018,
  title = {Finding Syntax in Human Encephalography with Beam Search},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hale, John T. and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan},
  year = {2018},
  month = jul,
  pages = {2727--2736},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1254},
  urldate = {2023-08-18},
  abstract = {Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
  file = {~/Zotfiles/hale.j2018 Finding syntax in human encephalography.pdf}
}

@article{hale.j:2022,
  type = {Journal {{Article}}},
  title = {Neurocomputational Models of Language Processing},
  author = {Hale, John T. and Campanelli, Luca and Li, Jixing and Bhattasali, Shohini and Pallier, Christophe and Brennan, Jonathan R.},
  year = {2022},
  journal = {Annual Review of Linguistics},
  volume = {8},
  number = {Volume 8, 2022},
  pages = {427--446},
  publisher = {Annual Reviews},
  issn = {2333-9691},
  doi = {10.1146/annurev-linguistics-051421-020803},
  abstract = {Efforts to understand the brain bases of language face the Mapping Problem: At what level do linguistic computations and representations connect to human neurobiology? We review one approach to this problem that relies on rigorously defined computational models to specify the links between linguistic features and neural signals. Such tools can be used to estimate linguistic predictions, model linguistic features, and specify a sequence of processing steps that may be quantitatively fit to neural signals collected while participants use language. Progress has been helped by advances in machine learning, attention to linguistically interpretable models, and openly shared data sets that allow researchers to compare and contrast a variety of models. We describe one such data set in detail in the Supplemental Appendix.},
  keywords = {brain,computational model,deep learning,lexicon,neurolinguistics,parsing},
  file = {~/Zotfiles/hale.j2022 Neurocomputational models of language pr.pdf}
}

@inproceedings{hall.d:2014,
  title = {On Substance in Phonology},
  booktitle = {Proceedings of the 2014 Annual Conference of the {{Canadian Linguistic Association}}},
  author = {Hall, Daniel Currie},
  year = {2014},
  date-added = {2019-06-17 08:36:30 -0400},
  date-modified = {2019-06-17 08:37:21 -0400},
  keywords = {substance free phonology}
}

@article{halle.m:1994,
  title = {Some Key Features of {{Distributed Morphology}}},
  author = {Halle, Morris and Marantz, Alec},
  year = {1994},
  journal = {MIT working papers in linguistics},
  volume = {21},
  number = {275},
  pages = {88},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{haller.p:2024,
  title = {On Language Models' Cognitive Biases in Reading Time Prediction},
  booktitle = {{{ICML}} 2024 Workshop on {{LLMs}} and Cognition},
  author = {Haller, Patrick and Bolliger, Lena Sophia and J{\"a}ger, Lena Ann},
  year = {2024},
  address = {Vienna, Austria}
}

@misc{hamilton.w:2017,
  title = {Representation Learning on Graphs: {{Methods}} and Applications},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2017},
  eprint = {1709.05584},
  primaryclass = {cs.SI},
  archiveprefix = {arXiv},
  date-added = {2021-08-03 10:09:17 -0400},
  date-modified = {2021-08-03 10:10:59 -0400},
  project = {syntactic embedding},
  keywords = {graph embedding}
}

@inproceedings{hao.y:2020,
  title = {Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling},
  shorttitle = {Probabilistic Predictions of People Perusing},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Hao, Yiding and Mendelsohn, Simon and Sterneck, Rachel and Martinez, Randi and Frank, Robert},
  year = {2020},
  month = nov,
  pages = {75--86},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.cmcl-1.10},
  urldate = {2022-10-13},
  abstract = {By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model's ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell's analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.},
  keywords = {psychometrics},
  file = {~/Zotfiles/hao.y2020 Probabilistic predictions of people peru.pdf}
}

@incollection{harb.b:2005,
  title = {Approximating the Best-Fit Tree under {{L}} p Norms},
  booktitle = {Approximation, Randomization and Combinatorial Optimization. {{Algorithms}} and Techniques},
  author = {Harb, Boulos and Kannan, Sampath and McGregor, Andrew},
  year = {2005},
  pages = {123--133},
  publisher = {Springer},
  date-added = {2019-07-17 17:56:30 -0400},
  date-modified = {2019-07-17 17:56:53 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{harley.h:2002,
  title = {Person and Number in Pronouns: {{A}} Feature-Geometric Analysis},
  author = {Harley, Heidi and Ritter, Elizabeth},
  year = {2002},
  journal = {Language},
  volume = {78},
  number = {3},
  pages = {482--526},
  publisher = {Linguistic Society of America},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:21:59 -0400},
  project = {Icelandic gluttony},
  keywords = {feature geometry,phi features,pronouns}
}

@inproceedings{harremoes.p:2007,
  title = {The Information Bottleneck Revisited or How to Choose a Good Distortion Measure},
  booktitle = {2007 {{IEEE}} International Symposium on Information Theory},
  author = {Harremo{\"e}s, Peter and Tishby, Naftali},
  year = {2007},
  pages = {566--570},
  doi = {10.1109/ISIT.2007.4557285},
  date-added = {2019-05-15 00:02:01 -0400},
  date-modified = {2020-07-22 11:17:35 -0400},
  organization = {IEEE},
  project = {syntactic embedding},
  keywords = {information bottleneck,KL divergence},
  file = {~/Zotfiles/harremoes.p2007 The information bottleneck revisited or.pdf}
}

@article{harringtonstack.c:2018,
  title = {A Failure to Replicate Rapid Syntactic Adaptation in Comprehension},
  author = {Harrington Stack, Caoimhe M. and James, Ariel N. and Watson, Duane G.},
  year = {2018},
  month = aug,
  journal = {Memory \& Cognition},
  volume = {46},
  number = {6},
  pages = {864--877},
  issn = {1532-5946},
  doi = {10.3758/s13421-018-0808-6},
  urldate = {2024-05-26},
  abstract = {Language comprehension requires successfully navigating linguistic variability. One hypothesis for how listeners manage variability is that they rapidly update their expectations of likely linguistic events in new contexts. This process, called adaptation, allows listeners to better predict the upcoming linguistic input. In previous work, Fine, Jaeger, Farmer, and Qian (PLoS ONE, 8, e77661, 2013) found evidence for syntactic adaptation. Subjects repeatedly encountered sentences in which a verb was temporarily ambiguous between main verb (MV) and reduced relative clause (RC) interpretations. They found that subjects who had higher levels of exposure to the unexpected RC interpretation of the sentences had an easier time reading the RC sentences but a more difficult time reading the MV sentences. They concluded that syntactic adaptation occurs rapidly in unexpected structures and also results in difficulty with processing the previously expected alternative structures. This article presents two experiments. Experiment 1 was designed as a follow-up to Fine et al.'s study and failed to find evidence of adaptation. A power analysis of Fine et al.'s raw data revealed that a similar study would need double the items and four times the subjects to reach 95\% power. In Experiment 2 we designed a close replication of Fine et al.'s experiment using these sample size guidelines. No evidence of rapid syntactic adaptation was found in this experiment. The failure to find evidence of adaptation in both experiments calls into question the robustness of the effect.},
  langid = {english},
  keywords = {Adaptation,Replication,Sentence processing,Syntax},
  file = {~/Zotfiles/harringtonstack.c2018 A failure to replicate rapid syntactic a.pdf}
}

@book{harris.z:1951book,
  title = {Structural Linguistics},
  author = {Harris, Zellig},
  year = {1951},
  publisher = {University of Chicago Press},
  file = {~/Zotfiles/harris.z1951 Structural linguistics.pdf}
}

@book{harris.z:1991book,
  title = {A {{Theory}} of {{Language}} and {{Information}}: {{A Mathematical Approach}}},
  shorttitle = {A {{Theory}} of {{Language}} and {{Information}}},
  author = {Harris, Zellig},
  year = {1991},
  month = feb,
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780198242246.001.0001},
  urldate = {2024-04-11},
  abstract = {Abstract. Professor Harris presents a formal theory of language structure, in which syntax is characterized as an orderly system of departures from random},
  isbn = {978-1-383-01348-1},
  langid = {english}
}

@inproceedings{hartmann.j:2016,
  title = {Evading Agreement: {{A}} New Perspective on Low Nominative Agreement in {{Icelandic}}},
  booktitle = {Proceedings of the 46th Annual Meeting of the North East Linguistic Society ({{NELS}})},
  author = {Hartmann, Jutta M and Heycock, Caroline},
  editor = {Hammerly, Christopher and Prickett, Brandon},
  year = {2016},
  volume = {2},
  pages = {67--80},
  publisher = {GLSA Publications},
  date-added = {2020-01-22 18:01:44 -0500},
  date-modified = {2020-02-01 19:41:52 -0500},
  project = {Icelandic gluttony},
  keywords = {invisible dative,low nominative}
}

@article{hartmann.j:2022,
  title = {Person Effects in Agreement with {{Icelandic}} Low Nominatives: {{An}} Experimental Investigation},
  shorttitle = {Person Effects in Agreement with {{Icelandic}} Low Nominatives},
  author = {Hartmann, Jutta M. and Heycock, Caroline},
  year = {2022},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  issn = {1573-0859},
  doi = {10.1007/s11049-022-09564-z},
  urldate = {2023-01-12},
  abstract = {This paper investigates agreement---in particular person agreement---in two configurations in Icelandic where there are two potential controllers of agreement and where at least in some cases agreement is with the lower of the two (a~``low nominative''). One case is the Dative-Nominative construction, where there is a dative subject and a lower nominative argument. The other is the Specificational Copular Clause (SCC) construction, where there are two nominative arguments. A much-discussed aspect of agreement in the former case is that agreement in number with the low nominative is generally possible, but agreement in person is at best highly restricted, leading in some cases to ineffability. This person effect has been claimed to be ameliorated by syncretism in the agreement paradigm, but there is limited data available substantiating this effect, which is however crucial to deciding between two recent types of account. This paper reports on a pair of experimental rating studies on the Dat-Nom and SCC configurations in Icelandic. We show that, taken together, the two sets of data provide evidence against the Person Licensing Condition and in favour of an account of the Dat-Nom construction in terms of morphological conflict arising from double agreement, although we show that the ameliorating effect of morphological syncretism, while real, is limited. Further, we show that there is no evidence of double agreement in the copular clauses investigated. We argue that full agreement with the low nominative here arises if the first nominal can move out of the domain of agreement entirely. The possibility of agreement with the initial nominal we suggest indicates that nominatives, unlike datives, cause the search of the agreement probe to terminate.},
  langid = {english},
  keywords = {Agreement,Copula,hierarchy effects,icelandic,Icelandic,Person Case Constraint,Person Licensing Condition,Syncretism},
  file = {~/Zotfiles/hartmann.j2022 Person effects in agreement with Iceland.pdf}
}

@article{hartshorne.j:2015,
  title = {When {{Does Cognitive Functioning Peak}}? {{The Asynchronous Rise}} and {{Fall}} of {{Different Cognitive Abilities Across}} the {{Life Span}}},
  shorttitle = {When {{Does Cognitive Functioning Peak}}?},
  author = {Hartshorne, Joshua K. and Germine, Laura T.},
  year = {2015},
  month = apr,
  journal = {Psychological Science},
  volume = {26},
  number = {4},
  pages = {433--443},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797614567339},
  urldate = {2024-03-19},
  abstract = {Understanding how and when cognitive change occurs over the life span is a prerequisite for understanding normal and abnormal development and aging. Most studies of cognitive change are constrained, however, in their ability to detect subtle, but theoretically informative life-span changes, as they rely on either comparing broad age groups or sparse sampling across the age range. Here, we present convergent evidence from 48,537 online participants and a comprehensive analysis of normative data from standardized IQ and memory tests. Our results reveal considerable heterogeneity in when cognitive abilities peak: Some abilities peak and begin to decline around high school graduation; some abilities plateau in early adulthood, beginning to decline in subjects' 30s; and still others do not peak until subjects reach their 40s or later. These findings motivate a nuanced theory of maturation and age-related decline, in which multiple, dissociable factors differentially affect different domains of cognition.},
  langid = {english},
  keywords = {cognitive aging},
  file = {~/Zotfiles/hartshorne.j2015 When Does Cognitive Functioning Peak Th.pdf}
}

@book{hastie.t:1990book,
  title = {Generalized Additive Models},
  author = {Hastie, T.J. and Tibshirani, R.J.},
  year = {1990},
  month = oct,
  publisher = {Routledge},
  doi = {10.1201/9780203753781},
  bdsk-url-2 = {https://doi.org/10.1201/9780203753781},
  date-added = {2022-01-05 22:06:57 -0500},
  date-modified = {2022-01-05 22:10:25 -0500}
}

@article{havelka.j:2007,
  title = {Mathematical Properties of Dependency Trees and Their Application to Natural Language Syntax},
  author = {Havelka, Ji{\v r}{\'i}},
  year = {2007},
  publisher = {Univerzita Karlova, Matematicko-fyzik{\'a}ln{\'i} fakulta},
  date-added = {2020-02-26 18:33:58 -0500},
  date-modified = {2020-02-26 18:36:25 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projectivity}
}

@inproceedings{heafield.k:2011,
  title = {{{KenLM}}: Faster and Smaller Language Model Queries},
  shorttitle = {Kenlm},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{Statistical Machine Translation}}},
  author = {Heafield, Kenneth},
  year = {2011},
  month = jul,
  pages = {187--197},
  publisher = {Association for Computational Linguistics},
  address = {Edinburgh, Scotland},
  urldate = {2023-03-03}
}

@inproceedings{heap.t:2023,
  title = {Massively Parallel Reweighted Wake-Sleep},
  booktitle = {Proceedings of the {{Thirty-Ninth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Heap, Thomas and Leech, Gavin and Aitchison, Laurence},
  year = {2023},
  month = jul,
  pages = {870--878},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-07-31},
  abstract = {Reweighted wake-sleep (RWS) is a machine learning method for performing Bayesian inference in a very general class of models. RWS draws \$K\$ samples from an underlying approximate posterior, then uses importance weighting to provide a better estimate of the true posterior. RWS then updates its approximate posterior towards the importance-weighted estimate of the true posterior. However, recent work [Chattergee and Diaconis, 2018] indicates that the number of samples required for effective importance weighting is exponential in the number of latent variables. Attaining such a large number of importance samples is intractable in all but the smallest models. Here, we develop massively parallel RWS, which circumvents this issue by drawing \$K\$ samples of all \$n\$ latent variables, and individually reasoning about all \$K{\textasciicircum}n\$ possible combinations of samples. While reasoning about \$K{\textasciicircum}n\$ combinations might seem intractable, the required computations can be performed in polynomial time by exploiting conditional independencies in the generative model. We show considerable improvements over standard ``global'' RWS, which draws \$K\$ samples from the full joint.},
  langid = {english},
  file = {~/Zotfiles/heap.t2023 Massively parallel reweighted wake-sleep.pdf}
}

@article{heathcote.a:2012,
  title = {Linear Deterministic Accumulator Models of Simple Choice},
  author = {Heathcote, Andrew and Love, Jonathon},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00292},
  urldate = {2022-08-13},
  file = {~/Zotfiles/heathcote.a2012 Linear deterministic accumulator models.pdf}
}

@article{heckerman.d:2000,
  title = {Dependency Networks for Inference, Collaborative Filtering, and Data Visualization},
  author = {Heckerman, David and Chickering, David Maxwell and Meek, Christopher and Rounthwaite, Robert and Kadie, Carl},
  year = {2000},
  month = oct,
  journal = {Journal of Machine Learning Research},
  volume = {1},
  number = {Oct},
  pages = {49--75},
  issn = {ISSN 1533-7928},
  urldate = {2023-12-27},
  abstract = {We describe a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships.},
  file = {~/Zotfiles/heckerman.d2000 Dependency networks for inference, colla.pdf}
}

@article{heilbron.m:2022PNAS,
  title = {A Hierarchy of Linguistic Predictions during Natural Language Comprehension},
  author = {Heilbron, Micha and Armeni, Kristijan and Schoffelen, Jan-Mathijs and Hagoort, Peter and {de Lange}, Floris P.},
  year = {2022},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {32},
  pages = {e2201968119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2201968119},
  urldate = {2022-10-29},
  abstract = {Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analyzing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable neural signatures of predictions about syntactic category (parts of speech), phonemes, and semantics. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.}
}

@book{heim.i:1998book,
  title = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {1998},
  publisher = {Blackwell},
  date-added = {2019-05-19 21:59:01 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {0-631-19712-5},
  keywords = {semantics}
}

@incollection{heim.i:2000,
  title = {Semantics in Generative Grammar},
  booktitle = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {2000},
  publisher = {Blackwell Publishing},
  address = {Malden, MA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:16:57 -0400},
  keywords = {Semantics}
}

@misc{heiss.a:2021,
  title = {A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel {{Bayesian}} Models},
  author = {Heiss, Andrew},
  year = {2021},
  month = nov,
  journal = {Andrew Heiss's blog},
  doi = {10.59350/wbn93-edb02},
  urldate = {2024-05-22},
  abstract = {At the end of my previous post on beta and zero-inflated-beta regression, I included an example of a multilevel model that predicted the proportion of women members of parliament based on whether a country implements gender-based quotas for their legislatures, along with a few different control variables. I also included random effects for year and region in order to capture time- and geography-specific trends.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english}
}

@article{heng.j:2020,
  title = {Controlled Sequential {{Monte Carlo}}},
  author = {Heng, Jeremy and Bishop, Adrian N. and Deligiannidis, George and Doucet, Arnaud},
  year = {2020},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  pages = {2904--2929},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1914},
  urldate = {2024-10-03},
  abstract = {Sequential Monte Carlo methods, also known as particle methods, are a popular set of techniques for approximating high-dimensional probability distributions and their normalizing constants. These methods have found numerous applications in statistics and related fields; for example, for inference in nonlinear non-Gaussian state space models, and in complex static models. Like many Monte Carlo sampling schemes, they rely on proposal distributions which crucially impact their performance. We introduce here a class of controlled sequential Monte Carlo algorithms, where the proposal distributions are determined by approximating the solution to an associated optimal control problem using an iterative scheme. This method builds upon a number of existing algorithms in econometrics, physics and statistics for inference in state space models, and generalizes these methods so as to accommodate complex static models. We provide a theoretical analysis concerning the fluctuation and stability of this methodology that also provides insight into the properties of related algorithms. We demonstrate significant gains over state-of-the-art methods at a fixed computational complexity on a variety of applications.},
  keywords = {annealed importance sampling,approximate dynamic programming,Normalizing constants,optimal control,reinforcement learning,state space models},
  file = {~/Zotfiles/heng.j2020 Controlled sequential Monte Carlo.pdf}
}

@article{hengeveld.k:2018,
  title = {Transparent and Non-Transparent Languages},
  author = {Hengeveld, Kees and Leufkens, Sterre},
  year = {2018},
  month = apr,
  journal = {Folia Linguistica},
  volume = {52},
  number = {1},
  pages = {139--175},
  publisher = {De Gruyter Mouton},
  issn = {1614-7308},
  doi = {10.1515/flin-2018-0003},
  urldate = {2025-02-05},
  abstract = {Languages differ widely from one another in the extent to which they are transparent, i.e. obey one-to-one relationships between meaning and form. Transparency, in turn, is an important factor in the learnability of languages. This paper first sets out a framework for the study of transparency and subsequently studies cross-linguistic differences in transparency, using the theory of Functional Discourse Grammar as its point of departure. Transparent and non-transparent features of languages are systematically defined using the multi-level architecture of this model of language, representing them as mappings between and within levels. In applying this framework to a sample of 30 languages it is shown that the (non-)transparent features investigated can be ordered into an implicational transparency hierarchy, and that as a result the languages of the sample can be ranked in terms of their degrees of transparency as well. Finally, the consequences of these findings for the learnability of languages are discussed.},
  copyright = {De Gruyter expressly reserves the right to use all content for commercial text and data mining within the meaning of Section 44b of the German Copyright Act.},
  langid = {english},
  keywords = {Functional Discourse Grammar,learnability,redundancy,transparency,typology},
  file = {~/Zotfiles/hengeveld.k2018 Transparent and non-transparent language.pdf}
}

@inproceedings{hewitt.j:2019,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  pages = {4129--4138},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1419},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1419}
}

@inproceedings{hewitt.j:2019a,
  title = {Designing and Interpreting Probes with Control Tasks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Hewitt, John and Liang, Percy},
  year = {2019},
  pages = {2733--2743},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1275},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1275}
}

@article{hick.w:1952,
  title = {On the {{Rate}} of {{Gain}} of {{Information}}},
  author = {Hick, W. E.},
  year = {1952},
  month = mar,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {4},
  number = {1},
  pages = {11--26},
  publisher = {SAGE Publications},
  issn = {0033-555X},
  doi = {10.1080/17470215208416600},
  urldate = {2024-05-03},
  abstract = {The analytical methods of information theory are applied to the data obtained in certain choice-reaction-time experiments. Two types of experiment were performed: (a) a conventional choice-reaction experiment, with various numbers of alternatives up to ten, and with a negligible proportion of errors, and (b) a ten-choice experiment in which the subjects deliberately reduced their reaction time by allowing themselves various proportions of errors. The principal finding is that the rate of gain of information is, on the average, constant with respect to time, within the duration of one perceptual-motor act, and has a value of the order of five ``bits'' per second. The distribution of reaction times among the ten stimuli in the second experiment is shown to be related to the objective uncertainty as to which response will be given to each stimulus. The distribution of reaction times among the responses is also related to the same uncertainty. This is further evidence that information is intimately concerned with reaction time. Some possible conceptual models of the process are considered, but tests against the data are inconclusive.},
  langid = {english},
  file = {~/Zotfiles/hick.w1952 On the Rate of Gain of Information.pdf}
}

@article{hidaka.s:2013,
  title = {A Computational Model Associating Learning Process, Word Attributes, and Age of Acquisition},
  author = {Hidaka, Shohei},
  year = {2013},
  month = nov,
  journal = {PLOS ONE},
  volume = {8},
  number = {11},
  pages = {e76242},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076242},
  urldate = {2022-09-28},
  abstract = {We propose a new model-based approach linking word learning to the age of acquisition (AoA) of words; a new computational tool for understanding the relationships among word learning processes, psychological attributes, and word AoAs as measures of vocabulary growth. The computational model developed describes the distinct statistical relationships between three theoretical factors underpinning word learning and AoA distributions. Simply put, this model formulates how different learning processes, characterized by change in learning rate over time and/or by the number of exposures required to acquire a word, likely result in different AoA distributions depending on word type. We tested the model in three respects. The first analysis showed that the proposed model accounts for empirical AoA distributions better than a standard alternative. The second analysis demonstrated that the estimated learning parameters well predicted the psychological attributes, such as frequency and imageability, of words. The third analysis illustrated that the developmental trend predicted by our estimated learning parameters was consistent with relevant findings in the developmental literature on word learning in children. We further discuss the theoretical implications of our model-based approach.},
  langid = {english},
  keywords = {Children,Learning,Learning curves,Linguistic morphology,Semantics,Social psychology,Statistical distributions,Vocabulary},
  file = {~/Zotfiles/hidaka.s2013 A computational model associating learni.pdf}
}

@inproceedings{hinton.g:1986cogsci,
  title = {Learning Distributed Representations of Concepts},
  booktitle = {Proceedings of the Eighth Annual Meeting of the Cognitive Science Society},
  author = {Hinton, Geoffrey E.},
  year = {1986},
  pages = {12},
  address = {Amherst, MA}
}

@article{hinton.g:1995,
  title = {The "Wake-Sleep" Algorithm for Unsupervised Neural Networks},
  author = {Hinton, Geoffrey E. and Dayan, Peter and Frey, Brendan J. and Neal, Radford M.},
  year = {1995},
  month = may,
  journal = {Science},
  volume = {268},
  number = {5214},
  pages = {1158--1161},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.7761831},
  urldate = {2025-08-08},
  abstract = {An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up "recognition" connections convert the input into representations in successive hidden layers, and top-down "generative" connections reconstruct the representation in one layer from the representation in the layer above. In the "wake" phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the "sleep" phase, neurons are driven by generative connections, and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.},
  file = {~/Zotfiles/hinton.g1995 The wake-sleep algorithm for unsupervi.pdf}
}

@inproceedings{ho.j:2020,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-07-07},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {~/Zotfiles/ho.j2020 Denoising diffusion probabilistic models.pdf}
}

@article{hochreiter.s:1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT Press - Journals},
  doi = {10.1162/neco.1997.9.8.1735},
  bdsk-url-2 = {https://doi.org/10.1162/neco.1997.9.8.1735},
  date-added = {2021-12-01 18:30:30 -0500},
  date-modified = {2021-12-01 18:30:32 -0500}
}

@article{hockett.c:1953,
  title = {Review of the Mathematical Theory of Communication},
  author = {Hockett, Charles F.},
  year = {1953},
  journal = {Language},
  volume = {29},
  number = {1},
  eprint = {410457},
  eprinttype = {jstor},
  pages = {69--93},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/410457},
  urldate = {2024-05-03},
  collaborator = {Shannon, Claude E. and Weaver, Warren},
  file = {~/Zotfiles/hockett.c1953 Review of the mathematical theory of com.pdf}
}

@book{hodges.w:2008book,
  title = {Model Theory},
  author = {Hodges, Wilfrid},
  year = {2008},
  month = jun,
  edition = {1},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-0-521-06636-5},
  langid = {english}
}

@article{hoffman.m:2013,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {40},
  pages = {1303--1347},
  issn = {1533-7928},
  urldate = {2022-06-30},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {~/Zotfiles/hoffman.m2013 Stochastic variational inference.pdf}
}

@incollection{hofmann.m:2017,
  title = {Benchmarking N-Grams, Topic Models and Recurrent Neural Networks by Cloze Completions, {{EEGs}} and Eye Movements},
  booktitle = {Cognitive {{Approach}} to {{Natural Language Processing}}},
  author = {Hofmann, Markus J. and Biemann, Chris and Remus, Steffen},
  year = {2017},
  pages = {197--215},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-78548-253-3.50010-X},
  urldate = {2022-08-28},
  isbn = {978-1-78548-253-3},
  langid = {english}
}

@article{hofmann.m:2022,
  title = {Language Models Explain Word Reading Times Better than Empirical Predictability},
  author = {Hofmann, Markus J. and Remus, Steffen and Biemann, Chris and Radach, Ralph and Kuchinke, Lars},
  year = {2022},
  month = feb,
  journal = {Frontiers in Artificial Intelligence},
  volume = {4},
  pages = {730570},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.730570},
  urldate = {2022-08-28},
  abstract = {Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.},
  file = {~/Zotfiles/hofmann.m2022 Language models explain word reading tim.pdf}
}

@article{holly.j:2001,
  title = {Pictures of Ultrametric Spaces, the p-Adic Numbers, and Valued Fields},
  author = {Holly, Jan E},
  year = {2001},
  journal = {The American Mathematical Monthly},
  volume = {108},
  number = {8},
  pages = {721--728},
  publisher = {Taylor \& Francis},
  date-added = {2019-06-11 08:49:52 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry,ultrametric}
}

@article{holmberg.a:2003,
  title = {Agreement and Movement in {{Icelandic}} Raising Constructions},
  author = {Holmberg, Anders and Hr{\'o}arsd{\'o}ttir, {\TH}orbj{\"o}rg},
  year = {2003},
  journal = {Lingua. International review of general linguistics. Revue internationale de linguistique g{\'e}n{\'e}rale},
  volume = {113},
  number = {10},
  pages = {997--1019},
  publisher = {Elsevier},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:24:18 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@inproceedings{hoogeboom.e:2021,
  title = {Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
  year = {2021},
  date-added = {2022-03-31 10:59:19 -0400},
  date-modified = {2022-03-31 11:00:13 -0400},
  keywords = {denoising}
}

@phdthesis{hoover.i:2022phd,
  title = {Effective Equidistribution on {{Hilbert}} Modular Varieties},
  author = {Hoover, Ian},
  year = {2022},
  month = aug,
  doi = {2345/bc-ir:109520},
  urldate = {2022-09-03},
  abstract = {We compute effective error rates for the equidistribution of translates of diagonal orbits on Hilbert modular varieties. The translation is determined by \$n\$ real parameters and our results require the assumption that all parameters are non-zero. The error rate is given in explicit polynomial terms of the translation parameters and Sobolev type norms of the test functions. The effective equidistribution is applied to give counting estimates for binary quadratic forms of square discriminant over real number rings.},
  langid = {english},
  school = {Boston College},
  file = {~/Zotfiles/hoover.i2022phd Effective equidistribution on Hilbert mo.pdf}
}

@misc{hoover.j:2020,
  type = {Handout},
  title = {Accounting for Variation in Number Agreement in {{Icelandic DAT-NOM}} Constructions},
  author = {Hoover, Jacob Louis},
  year = {2020},
  month = mar,
  doi = {10.14288/1.0389856},
  howpublished = {WCCFL2020 Talk handout},
  peerreview = {no},
  project = {Icelandic gluttony},
  keywords = {agreement,feature geometry}
}

@inproceedings{hoover.j:2021,
  title = {Accounting for Variation in Number Agreement in Icelandic Dative-Nominative Constructions},
  booktitle = {Proceedings of the 38th {{West Coast Conference}} on {{Formal Linguistics}}},
  author = {Hoover, Jacob Louis},
  editor = {Soo, Rachel and Chow, Una Y. and Nederveen, Sander},
  year = {2021},
  month = jun,
  pages = {231--241},
  publisher = {Cascadilla Proceedings Project},
  address = {Somerville, Mass., USA},
  abstract = {Icelandic dative-nominative constructions exhibit a syntactic hierarchy effect known as the Person Restriction: only third person nominatives may control agreement. In these constructions, there is variation between speakers in the extent to which the verb agrees with the nominative for number. Sigur{\dh}sson \& Holmberg (2008) explain this variation as arising due to differences between varieties in the timing of subject raising, using a split phi-probe. This paper revises their approach, using the feature gluttony mechanism for Agree developed in Coon \& Keine (2020), and a split phi-probe in which person probing precedes number probing. Within this framework, the observed variation can be captured by allowing variability two independent parameters: the timing of EPP subject raising, and the visibility of a number feature on dative DPs. The proposed mechanism describes the variation, including predicting the observed optional agreement in certain cases that previous literature had struggled to account for, and makes additional predictions about the differences between varieties in cases of syncretism within the verbal paradigm. An investigation into these predictions should allow this already well-studied area of Icelandic grammar to continue to be a useful test-case for crosslinguistic assumptions about the mechanism of Agree, and the status of dative arguments.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {no},
  annotation = {Note: lingref.com document 3568},
  file = {~/Zotfiles/hoover.j2021wccfl Accounting for variation in number agree.pdf}
}

@inproceedings{hoover.j:2021a,
  title = {Linguistic Dependencies and Statistical Dependence},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hoover, Jacob Louis and Du, Wenyu and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2021},
  month = nov,
  pages = {2941--2963},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.234},
  urldate = {2022-05-19},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \${\textbackslash}approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {db},
  keywords = {dependency grammar,linguistic dependencies},
  file = {~/Zotfiles/hoover.j2021emnlp Linguistic dependencies and statistical.pdf}
}

@misc{hoover.j:2021arxiv,
  title = {Linguistic Dependencies and Statistical Dependence},
  author = {Hoover, Jacob Louis and Sordoni, Alessandro and Du, Wenyu and O'Donnell, Timothy J.},
  year = {2021},
  number = {arXiv:2104.08685},
  eprint = {2104.08685},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08685},
  urldate = {2022-05-18},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \${\textbackslash}approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {~/Zotfiles/hoover.j2021arxiv Linguistic dependencies and statistical.pdf}
}

@misc{hoover.j:2022poster,
  type = {Poster},
  title = {With Better Language Models, Processing Time Is Superlinear in Surprisal},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and O'Donnell, Timothy J.},
  year = {2022},
  month = sep,
  address = {York, England},
  collaborator = {Piantadosi, Steven T.},
  copyright = {All rights reserved},
  peerreview = {no}
}

@misc{hoover.j:2022psyarxiv,
  title = {The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/qjnpv},
  abstract = {Words that are more surprising given context take longer to process. However, no incremental parsing algorithm has been shown to directly predict this phenomenon. In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisal---those that involve repeatedly sampling from the prior. Our first contribution is to show that simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase. These two predictions stand in contrast with literature on surprisal theory (Hale, 2001; Levy, 2008), which assumes that the expected processing cost increases linearly with surprisal, and makes no prediction about variance. In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal. We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.},
  copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
  langid = {american},
  peerreview = {db},
  keywords = {Linguistics,parsing algorithms,sampling,sentence processing,Social and Behavioral Sciences,surprisal},
  file = {~/Zotfiles/hoover.j2022psyarxiv The plausibility of sampling as an algor.pdf}
}

@misc{hoover.j:2023,
  title = {When Does Unpredictable Not Mean Difficult?},
  author = {Hoover, Jacob Louis},
  year = {2023},
  month = dec,
  address = {MIT Computational Psycholinguistics Laboratory},
  collaborator = {O'Donnell, Timothy J.}
}

@article{hoover.j:2023a,
  title = {The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  year = {2023},
  month = jul,
  journal = {Open Mind: Discoveries in Cognitive Science},
  volume = {7},
  pages = {350--391},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00086},
  urldate = {2023-07-21},
  abstract = {Words that are more surprising given context take longer to process. However, no incremental parsing algorithm has been shown to directly predict this phenomenon. In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisal---those that involve repeatedly sampling from the prior. Our first contribution is to show that simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase. These two predictions stand in contrast with literature on surprisal theory (Hale, 2001; Levy, 2008a) which assumes that the expected processing cost increases linearly with surprisal, and makes no prediction about variance. In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal. We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.},
  openaccess = {true},
  pmcid = {PMC10449406},
  keywords = {processing algorithms,sentence processing,surprisal theory},
  file = {~/Zotfiles/hoover.j2023 The plausibility of sampling as an algor.pdf}
}

@phdthesis{hoover.j:2024phd,
  title = {The Cost of Information: Looking beyond Predictability in Language Processing},
  shorttitle = {The Cost of Information},
  author = {Hoover, Jacob Louis},
  year = {2024},
  month = aug,
  address = {Montr{\'e}al, Canada},
  langid = {canadian},
  school = {McGill University},
  file = {/Users/j/MIT Dropbox/Jacob Vigly/Zotfiles/hoover.j2024phd prcis.pdf;~/Zotfiles/hoover.j2024phd The cost of information looking beyond.pdf}
}

@article{howes.d:1957,
  title = {On the Relation between the Intelligibility and Frequency of Occurrence of English Words},
  author = {Howes, Davis},
  year = {1957},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {29},
  number = {2},
  pages = {296--305},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.1908862},
  urldate = {2024-05-03},
  abstract = {The threshold of intelligibility for a word in a wide-spectrum noise is shown to be a decreasing function of the frequency with which the word occurs in general linguistic usage (word frequency). The drop in threshold is about 4.5 db per logarithmic unit of word frequency. This rate is independent of the length of the word, although the thresholds for words of given frequency of occurrence are lower for long words.             The effect of restricting the listener's alternatives in an intelligibility test to a specified number of words is calculated from this relationship. These calculations come within 1 db of published experimental data. Theoretical functions relating intelligibility threshold to word length are also calculated from the word-frequency effect, on the assumption that listeners can discriminate the length of a word at levels too low for it to be identified. These functions are in general agreement with the experimental results.             Implications for intelligibility testing procedures are discussed.},
  langid = {english},
  file = {~/Zotfiles/howes.d1957 On the relation between the intelligibil.pdf}
}

@book{howson.c:2006book3,
  title = {Scientific Reasoning: The {{Bayesian}} Approach},
  shorttitle = {Scientific Reasoning},
  author = {Howson, Colin and Urbach, Peter},
  year = {2006},
  edition = {3},
  publisher = {Open Court Publishing},
  abstract = {In this clearly reasoned defense of Bayes's Theorem -- that probability can be used to reasonably justify scientific theories -- Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.},
  googlebooks = {3JusAwAAQBAJ},
  isbn = {978-0-8126-9578-6},
  langid = {english},
  keywords = {Philosophy / General,Philosophy / History & Surveys / Modern,Science / Philosophy & Social Aspects}
}

@article{hoyer.p:2004,
  title = {Non-Negative Matrix Factorization with Sparseness Constraints},
  author = {Hoyer, Patrik O},
  year = {2004},
  journal = {Journal of machine learning research},
  volume = {5},
  number = {Nov},
  pages = {1457--1469},
  date-added = {2020-07-26 15:09:12 -0400},
  date-modified = {2020-07-26 15:10:47 -0400},
  project = {syntactic embedding},
  keywords = {sparseness}
}

@misc{htut.p:2019arxiv,
  title = {Do Attention Heads in {{BERT}} Track Syntactic Dependencies?},
  author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
  year = {2019},
  month = nov,
  number = {arXiv:1911.12246},
  eprint = {1911.12246},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-05},
  abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods---taking the maximum attention weight and computing the maximum spanning tree---to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the semantics-oriented MNLI---to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/htut.p2019arxiv Do attention heads in BERT track syntact.pdf}
}

@inproceedings{hu.j:2020,
  title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
  year = {2020},
  pages = {1725--1744},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.158},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.158}
}

@inproceedings{hu.j:2022,
  title = {Predicting Scalar Diversity with Context-Driven Uncertainty over Alternatives},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Hu, Jennifer and Levy, Roger and Schuster, Sebastian},
  year = {2022},
  month = may,
  pages = {68--74},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.cmcl-1.8},
  urldate = {2023-05-26},
  abstract = {Scalar implicature (SI) arises when a speaker uses an expression (e.g., ``some'') that is semantically compatible with a logically stronger alternative on the same scale (e.g., ``all''), leading the listener to infer that they did not intend to convey the stronger meaning. Prior work has demonstrated that SI rates are highly variable across scales, raising the question of what factors determine the SI strength for a particular scale. Here, we test the hypothesis that SI rates depend on the listener's confidence in the underlying scale, which we operationalize as uncertainty over the distribution of possible alternatives conditioned on the context. We use a T5 model fine-tuned on a text infilling task to estimate this distribution. We find that scale uncertainty predicts human SI rates, measured as entropy over the sampled alternatives and over latent classes among alternatives in sentence embedding space. Furthermore, we do not find a significant effect of the surprisal of the strong scalemate. Our results suggest that pragmatic inferences depend on listeners' context-driven uncertainty over alternatives.},
  file = {~/Zotfiles/hu.j2022 Predicting scalar diversity with context.pdf}
}

@misc{hu.j:2023arxiv,
  title = {Expectations over Unspoken Alternatives Predict Pragmatic Inferences},
  author = {Hu, Jennifer and Levy, Roger and Degen, Judith and Schuster, Sebastian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04758},
  eprint = {2304.04758},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.04758},
  urldate = {2023-04-28},
  abstract = {Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable -- both within instances of a single scale, and across different scales -- there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/hu.j2023arxiv Expectations over unspoken alternatives.pdf}
}

@phdthesis{hu.j:2023phd,
  title = {Neural Language Models and Human Linguistic Knowledge},
  author = {Hu, Jennifer},
  year = {2023},
  month = may,
  address = {Cambridge, MA, USA},
  urldate = {2024-05-15},
  abstract = {Language is one of the hallmarks of intelligence, demanding explanation in a theory of human cognition. However, language presents unique practical challenges for quantitative empirical research, making many linguistic theories difficult to test at naturalistic scales. Artificial neural network language models (LMs) provide a new tool for studying language with mathematical precision and control, as they exhibit remarkably sophisticated linguistic behaviors while being fully intervenable. While LMs differ from humans in many ways, the learning outcomes of these models can reveal the behaviors that may emerge through expressive statistical learning algorithms applied to linguistic input.     In this thesis, I demonstrate this approach through three case studies using LMs to investigate open questions in language acquisition and comprehension. First, I use LMs to perform controlled manipulations of language learning, and find that syntactic generalizations depend more on a learner's inductive bias than on training data size. Second, I use LMs to explain systematic variation in scalar inferences by approximating human listeners' expectations over unspoken alternative sentences (e.g., "The bill was supported overwhelmingly" implies that the bill was not supported unanimously). Finally, I show that LMs and humans exhibit similar behaviors on a set of non-literal comprehension tasks which are hypothesized to require social reasoning (e.g., inferring a speaker's intended meaning from ironic statements). These findings suggest that certain aspects of linguistic knowledge could emerge through domain-general prediction mechanisms, while other aspects may require specific inductive biases and conceptual structures.},
  copyright = {Attribution 4.0 International (CC BY 4.0)},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  file = {~/Zotfiles/hu.j2023phd Neural language models and human linguis.pdf}
}

@inproceedings{hu.x:2021,
  title = {{{R2D2}}: Recursive Transformer Based on Differentiable Tree for Interpretable Hierarchical Language Modeling},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and {de Melo}, Gerard},
  year = {2021},
  month = aug,
  pages = {4897--4908},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.379},
  abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.379},
  date-added = {2022-04-28 10:19:40 -0400},
  date-modified = {2022-04-28 10:19:58 -0400},
  keywords = {chart parsing,pruning}
}

@misc{hu.x:2022arxiv,
  title = {Fast-{{R2D2}}: A Pretrained Recursive Neural Network Based on Pruned {{CKY}} for Grammar Induction and Text Representation},
  shorttitle = {Fast-{{R2D2}}},
  author = {Hu, Xiang and Mi, Haitao and Li, Liang and {de Melo}, Gerard},
  year = {2022},
  month = nov,
  number = {arXiv:2203.00281},
  eprint = {2203.00281},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.00281},
  urldate = {2024-05-15},
  abstract = {Recently CKY-based models show great potential in unsupervised grammar induction thanks to their human-like encoding paradigm, which runs recursively and hierarchically, but requires \$O(n{\textasciicircum}3)\$ time-complexity. Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pre-training even with complex tree encoder by introducing a heuristic pruning method. However, the rule-based pruning approach suffers from local optimum and slow inference issues. In this paper, we fix those issues in a unified method. We propose to use a top-down parser as a model-based pruning method, which also enables parallel encoding during inference. Typically, our parser casts parsing as a split point scoring task, which first scores all split points for a given sentence, and then recursively splits a span into two by picking a split point with the highest score in the current span. The reverse order of the splits is considered as the order of pruning in R2D2 encoder. Beside the bi-directional language model loss, we also optimize the parser by minimizing the KL distance between tree probabilities from parser and R2D2. Our experiments show that our Fast-R2D2 improves performance significantly in grammar induction and achieves competitive results in downstream classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{huang.c:2021,
  title = {A Variational Perspective on Diffusion-Based Generative Models and Score Matching},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron C},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {22863--22876},
  publisher = {Curran Associates, Inc.},
  file = {~/Zotfiles/huang.c2021 A variational perspective on diffusion-b.pdf}
}

@article{huang.k:2021,
  title = {Using Eye Tracking to Investigate Failure to Notice Word Transpositions in Reading},
  author = {Huang, Kuan-Jung and Staub, Adrian},
  year = {2021},
  month = nov,
  journal = {Cognition},
  volume = {216},
  pages = {104846},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104846},
  urldate = {2023-10-29},
  abstract = {Previous research (Mirault, Snell, \& Grainger, 2018) has demonstrated that subjects sometimes incorrectly judge an ungrammatical sentence as grammatical when it is created by the transposition of two words in a grammatical sentence (e.g., The white was cat big). Here we present two eye-tracking experiments designed to assess the prevalence of this phenomenon in a more natural reading task, and to explore theoretical explanations. Readers failed to notice transpositions at about the same rate as in Mirault et al. (2018). Failure to notice the transposition was more common when both words were short, and when readers' eyes skipped, rather than directly fixated, one of the two words. The status of the transposed words as open- or closed-class did not have a reliable effect. The transposed words caused disruption in the eye movement record only on trials when participants ultimately judged the sentence to be ungrammatical, not when they judged the sentence to be grammatical. We argue that the results are not entirely consistent with the account offered by Mirault et al. (2018), which attributes failure to notice transpositions to parallel processing of adjacent words, or with a late, post-perceptual rational inference account (Gibson, Bergen, \& Piantadosi, 2013). We propose that word recognition is serial, but post-lexical integration of each word into its context may not be perfectly incremental.},
  keywords = {Eye movements,Incremental processing,noisy channel,Sentence comprehension,Word position coding,word transposition},
  file = {~/Zotfiles/huang.k2021 Using eye tracking to investigate failur.pdf}
}

@article{huang.k:2021LLC,
  title = {Why Do Readers Fail to Notice Word Transpositions, Omissions, and Repetitions? {{A}} Review of Recent Evidence and Theory},
  shorttitle = {Why Do Readers Fail to Notice Word Transpositions, Omissions, and Repetitions?},
  author = {Huang, Kuan-Jung and Staub, Adrian},
  year = {2021},
  journal = {Language and Linguistics Compass},
  volume = {15},
  number = {7},
  pages = {e12434},
  issn = {1749-818X},
  doi = {10.1111/lnc3.12434},
  urldate = {2024-12-27},
  abstract = {Most readers have had the experience of initially failing to notice an omission or repetition of a function word, or a transposition of two adjacent words. In the present article, we review recent research investigating this phenomenon. We emphasize that failure to notice such errors is of substantial theoretical interest, given what we have learned about how systematically and incrementally readers inspect and process text. We endorse the idea that a process of rational inference may play a critical role, while we cast doubt on the idea that failure to notice errors arises from parallel processing of multiple words. We review a number of recent studies from our own laboratory that have investigated the relationship between eye movements during reading and noticing, or failing to notice, an error. While the conclusions from these studies are broadly consistent with a rational inference account, we find that when readers fail to notice an error, their eye movements generally show no indication that the error was registered at all. On its surface, this finding may be viewed as inconsistent with the idea that the rational inference process that enables readers to overlook errors is genuinely post-perceptual. We suggest a mechanism by which eye movement control models could account for this finding.},
  langid = {english},
  file = {~/Zotfiles/huang.k2021LLC Why do readers fail to notice word trans.pdf}
}

@misc{huang.k:2022HSP,
  type = {Talk},
  title = {{{SPR}} Mega-Benchmark Shows Surprisal Tracks Construction- but Not Item-Level Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2022},
  address = {University of California, Santa Cruz},
  keywords = {eye-tracking,surprisal},
  file = {~/Zotfiles/huang.k2022HSP SPR mega-benchmark shows surprisal track 2.pdf;~/Zotfiles/huang.k2022HSP SPR mega-benchmark shows surprisal track.pdf}
}

@misc{huang.k:2023psyarxiv,
  title = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty: Evidence from a Large-Scale Benchmark},
  shorttitle = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2023},
  month = apr,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/z38u6},
  urldate = {2023-04-22},
  abstract = {Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences - one of the major concerns of psycholinguistics - be explained by predictability, as estimated using computational language models? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. We find that the predictions of language models with two different architectures sharply diverge from the reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that prediction is most likely insufficient on its own to explain human syntactic processing.},
  langid = {american},
  keywords = {Language models,Linguistics,Prediction,Psycholinguistics and Neurolinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {~/Zotfiles/huang.k2023psyarxiv Surprisal does not explain syntactic dis.pdf}
}

@article{huang.k:2024,
  title = {Large-Scale Benchmark Yields No Evidence That Language Model Surprisal Explains Syntactic Disambiguation Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2024},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {137},
  pages = {104510},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104510},
  urldate = {2024-03-03},
  abstract = {Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences -- one of the major concerns of psycholinguistics -- be explained by predictability, as estimated using computational language models, and operationalized as surprisal (negative log probability)? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. By estimating the function that relates surprisal to reading times from filler items included in the experiment, we find that the predictions of language models with two different architectures sharply diverge from the empirical reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that next-word prediction is most likely insufficient on its own to explain human syntactic processing.},
  keywords = {Language models,Prediction,Sentence processing,suprisal theory,Surprisal},
  file = {~/Zotfiles/huang.k2024JML Large-scale benchmark yields no evidence.pdf}
}

@phdthesis{huang.l:2008phd,
  title = {Forest-Based Algorithms in Natural Language Processing},
  author = {Huang, Liang},
  year = {2008},
  date-added = {2022-03-31 09:58:59 -0400},
  date-modified = {2022-04-26 21:20:55 -0400},
  school = {University of Pennsylvania},
  keywords = {parsing},
  file = {~/Zotfiles/huang.l2008phd Forest-based algorithms in natural langu.pdf}
}

@inproceedings{huang.l:2010,
  title = {Dynamic Programming for Linear-Time Incremental Parsing},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Huang, Liang and Sagae, Kenji},
  year = {2010},
  month = jul,
  pages = {1077--1086},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  date-added = {2022-04-26 21:01:34 -0400},
  date-modified = {2022-04-26 21:02:08 -0400},
  keywords = {dependency parsing,dynamic programming}
}

@inproceedings{huang.y:2014,
  title = {Neurons as {{Monte Carlo Samplers}}: {{Bayesian}} {{Inference}} and {{Learning}} in {{Spiking Networks}}},
  shorttitle = {Neurons as {{Monte Carlo Samplers}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Huang, Yanping and Rao, Rajesh PN},
  year = {2014},
  volume = {27},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-02-18},
  abstract = {We propose a two-layer spiking network capable of performing approximate inference and learning for a hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how this network can sequentially learn the hidden Markov model using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.},
  file = {~/Zotfiles/huang.y2014 Neurons as Monte Carlo Samplers Bayesia.pdf}
}

@article{huddleston.r:2002,
  title = {The Cambridge Grammar of English},
  author = {Huddleston, Rodney and Pullum, Geoffrey K and others},
  year = {2002},
  journal = {Language. Cambridge: Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{hudson.r:1984book,
  title = {Word Grammar},
  author = {Hudson, Richard A},
  year = {1984},
  publisher = {Blackwell Oxford},
  date-added = {2021-07-17 10:26:33 -0400},
  date-modified = {2021-07-17 10:48:55 -0400}
}

@article{huettig.f:2023,
  title = {The Myth of Normal Reading},
  author = {Huettig, Falk and Ferreira, Fernanda},
  year = {2023},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {18},
  number = {4},
  pages = {863--870},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/17456916221127226},
  urldate = {2024-10-23},
  abstract = {We argue that the educational and psychological sciences must embrace the diversity of reading rather than chase the phantom of normal reading behavior. We critically discuss the research practice of asking participants in experiments to read ``normally.'' We then draw attention to the large cross-cultural and linguistic diversity around the world and consider the enormous diversity of reading situations and goals. Finally, we observe that people bring a huge diversity of brains and experiences to the reading task. This leads to four implications: First, there are important lessons for how to conduct psycholinguistic experiments; second, we need to move beyond Anglocentric reading research and produce models of reading that reflect the large cross-cultural diversity of languages and types of writing systems; third, we must acknowledge that there are multiple ways of reading and reasons for reading, and none of them is normal or better or a ``gold standard''; and fourth, we must stop stigmatizing individuals who read differently and for different reasons, and there should be increased focus on teaching the ability to extract information relevant to the person's goals. What is important is not how well people decode written language and how fast people read but what people comprehend given their own stated goals.},
  langid = {english},
  file = {~/Zotfiles/huettig.f2023 The Myth of Normal Reading.pdf}
}

@article{hughes.b:2004,
  title = {Trees and Ultrametric Spaces: A Categorical Equivalence},
  author = {Hughes, Bruce},
  year = {2004},
  journal = {Advances in Mathematics},
  volume = {189},
  number = {1},
  pages = {148--191},
  publisher = {Elsevier},
  date-added = {2019-07-10 18:13:04 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{huijben.i:2022,
  title = {A Review of the {{Gumbel-max}} Trick and Its Extensions for Discrete Stochasticity in Machine Learning},
  author = {Huijben, Iris A.M. and Kool, Wouter and Paulus, Max Benedikt and Sloun, Ruud JG Van},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {45},
  number = {2},
  pages = {1353--1371},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tpami.2022.3157042},
  file = {~/Zotfiles/huijben.i2022 A review of the Gumbel-max trick and its.pdf}
}

@article{hupkes.d:2018,
  title = {Visualisation and'diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure},
  author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  year = {2018},
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {907--926},
  date-added = {2019-06-17 18:51:15 -0400},
  date-modified = {2019-06-17 18:52:12 -0400},
  project = {syntactic embedding},
  keywords = {implicit information probing}
}

@inproceedings{hwa.r:1999,
  title = {Supervised Grammar Induction Using Training Data with Limited Constituent Information},
  booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics},
  author = {Hwa, Rebecca},
  year = {1999},
  pages = {73--79},
  publisher = {Association for Computational Linguistics},
  address = {College Park, Maryland, USA},
  doi = {10.3115/1034678.1034699},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034699}
}

@inproceedings{icard.t:2014cogsci,
  title = {Toward Boundedly Rational Analysis},
  booktitle = {Proceedings of the 36th {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Icard, Thomas},
  year = {2014},
  number = {36}
}

@unpublished{icard.t:2023ms,
  type = {Book Draft},
  title = {Resource Rationality},
  author = {Icard, Thomas},
  year = {2023},
  month = sep
}

@article{itti.l:2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  year = {2009},
  month = jun,
  journal = {Vision Research},
  series = {Visual {{Attention}}: {{Psychophysics}}, Electrophysiology and Neuroimaging},
  volume = {49},
  number = {10},
  pages = {1295--1306},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2008.09.007},
  urldate = {2022-08-07},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  langid = {english},
  keywords = {Attention,Bayes theorem,Eye movements,Free viewing,Information theory,KL divergence,Natural vision,Novelty,Saliency,surprisal theory,Surprise,vision},
  file = {~/Zotfiles/itti.l2009 Bayesian surprise attracts human attenti.pdf}
}

@incollection{jackendoff.r:2002,
  title = {Foundations of Language},
  booktitle = {Foundations of Language},
  author = {Jackendoff, Ray},
  year = {2002},
  publisher = {Oxford University Press},
  address = {New York},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:52 -0400},
  group = {Cognitive Science, Language},
  readinglist = {Thesis},
  keywords = {Parallel Architecture}
}

@article{jacobs.c:2024,
  title = {Constraint Satisfaction in Large Language Models},
  author = {Jacobs, Cassandra L. and MacDonald, Maryellen C.},
  year = {2024},
  month = jun,
  journal = {Language, Cognition and Neuroscience},
  volume = {0},
  number = {0},
  pages = {1--18},
  publisher = {Routledge},
  issn = {2327-3798},
  doi = {10.1080/23273798.2024.2364339},
  urldate = {2024-10-11},
  abstract = {Constraint satisfaction theories were prominent in the late 20th century and emphasized continuous, rich interaction between many sources of information in a linguistic signal unfolding over time. A major challenge was rigorously capturing these highly interactive comprehension processes and yielding explicit predictions, because the important constraints were numerous and changed in prominence from one context to the next. Connectionist models were conceptually well-suited to this, but researchers had insufficient computing power and lacked sufficiently large corpora to bring these models to bear. These limitations no longer hold, and large language models (LLMs) offer an opportunity to test constraint satisfaction ideas about human language comprehension. We consider how LLMs can be applied to study interactive processes with lexical ambiguity resolution as a test case. We argue that further study of LLMs can advance theories of constraint satisfaction, though gaps remain in our understanding of how people and LLMs combine linguistic information.},
  keywords = {ambiguity,connectionism,constraint satisfaction,Language comprehension,large language models}
}

@phdthesis{jaeger.t:2006phd,
  title = {Redundancy and Syntactic Reduction in Spontaneous Speech},
  author = {Jaeger, T. Florian},
  year = {2006},
  address = {Stanford, CA},
  school = {Stanford University}
}

@article{jaeger.t:2010,
  title = {Redundancy and Reduction: {{Speakers}} Manage Syntactic Information Density},
  shorttitle = {Redundancy and Reduction},
  author = {Jaeger, T. Florian},
  year = {2010},
  month = aug,
  journal = {Cognitive Psychology},
  volume = {61},
  number = {1},
  pages = {23--62},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2010.02.002},
  urldate = {2025-02-05},
  abstract = {A principle of efficient language production based on information theoretic considerations is proposed: Uniform Information Density predicts that language production is affected by a preference to distribute information uniformly across the linguistic signal. This prediction is tested against data from syntactic reduction. A single multilevel logit model analysis of naturally distributed data from a corpus of spontaneous speech is used to assess the effect of information density on complementizer that-mentioning, while simultaneously evaluating the predictions of several influential alternative accounts: availability, ambiguity avoidance, and dependency processing accounts. Information density emerges as an important predictor of speakers' preferences during production. As information is defined in terms of probabilities, it follows that production is probability-sensitive, in that speakers' preferences are affected by the contextual probability of syntactic structures. The merits of a corpus-based approach to the study of language production are discussed as well.},
  keywords = {Complementizer -mentioning,Efficient language production,Rational cognition,redundancy,Syntactic production,Syntactic reduction},
  file = {~/Zotfiles/jaeger.t2010 Redundancy and reduction Speakers manag.pdf}
}

@article{jager.g:2012,
  title = {Formal Language Theory: Refining the {{Chomsky}} Hierarchy},
  shorttitle = {Formal Language Theory},
  author = {J{\"a}ger, Gerhard and Rogers, James},
  year = {2012},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  number = {1598},
  pages = {1956--1970},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2012.0077},
  urldate = {2023-03-09},
  abstract = {The first part of this article gives a brief overview of the four levels of the Chomsky hierarchy, with a special emphasis on context-free and regular languages. It then recapitulates the arguments why neither regular nor context-free grammar is sufficiently expressive to capture all phenomena in the natural language syntax. In the second part, two refinements of the Chomsky hierarchy are reviewed, which are both relevant to the extant research in cognitive science: the mildly context-sensitive languages (which are located between context-free and context-sensitive languages), and the sub-regular hierarchy (which distinguishes several levels of complexity within the class of regular languages).},
  keywords = {artificial grammar learning,complexity,formal language theory},
  file = {~/Zotfiles/jager.g2012 Formal language theory refining the Cho.pdf}
}

@article{jager.l:2015,
  title = {Teasing Apart Retrieval and Encoding Interference in the Processing of Anaphors},
  author = {J{\"a}ger, Lena A. and Benz, Lena and Roeser, Jens and Dillon, Brian W. and Vasishth, Shravan},
  year = {2015},
  month = jun,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00506},
  urldate = {2023-10-29}
}

@article{jager.l:2015a,
  title = {Retrieval Interference in Reflexive Processing: Experimental Evidence from {{Mandarin}}, and Computational Modeling},
  shorttitle = {Retrieval Interference in Reflexive Processing},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  urldate = {2022-10-12},
  abstract = {We conducted two eye-tracking experiments investigating the processing of the Mandarin reflexive ziji in order to tease apart structurally constrained accounts from standard cue-based accounts of memory retrieval. In both experiments, we tested whether structurally inaccessible distractors that fulfill the animacy requirement of ziji influence processing times at the reflexive. In Experiment 1, we manipulated animacy of the antecedent and a structurally inaccessible distractor intervening between the antecedent and the reflexive. In conditions where the accessible antecedent mismatched the animacy cue, we found inhibitory interference whereas in antecedent-match conditions, no effect of the distractor was observed. In Experiment 2, we tested only antecedent-match configurations and manipulated locality of the reflexive-antecedent binding (Mandarin allows non-local binding). Participants were asked to hold three distractors (animate vs. inanimate nouns) in memory while reading the target sentence. We found slower reading times when animate distractors were held in memory (inhibitory interference). Moreover, we replicated the locality effect reported in previous studies. These results are incompatible with structure-based accounts. However, the cue-based ACT-R model of Lewis and Vasishth (2005) cannot explain the observed pattern either. We therefore extend the original ACT-R model and show how this model not only explains the data presented in this article, but is also able to account for previously unexplained patterns in the literature on reflexive processing.},
  keywords = {ACT-R},
  file = {~/Zotfiles/jager.l2015 Retrieval interference in reflexive proc.pdf}
}

@article{jager.l:2017,
  title = {Similarity-Based Interference in Sentence Comprehension: {{Literature}} Review and {{Bayesian}} Meta-Analysis},
  shorttitle = {Similarity-Based Interference in Sentence Comprehension},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2017},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {94},
  pages = {316--339},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.004},
  urldate = {2023-01-08},
  abstract = {We report a comprehensive review of the published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies. We also provide a quantitative random-effects meta-analysis of eyetracking and self-paced reading studies. We show that the empirical evidence is only partly consistent with cue-based retrieval as implemented in the ACT-R-based model of sentence processing by Lewis and Vasishth (2005) (LV05) and that there are important differences between the reviewed dependency types. In non-agreement subject-verb dependencies, there is evidence for inhibitory interference in configurations where the correct dependent fully matches the retrieval cues. This is consistent with the LV05 cue-based retrieval account. By contrast, in subject-verb agreement as well as in reflexive-/reciprocal-antecedent dependencies, no evidence for inhibitory interference is found in configurations with a fully cue-matching subject/antecedent. In configurations with only a partially cue-matching subject or antecedent, the meta-analysis reveals facilitatory interference in subject-verb agreement and inhibitory interference in reflexives/reciprocals. The former is consistent with the LV05 account, but the latter is not. Moreover, the meta-analysis reveals that (i) interference type (proactive versus retroactive) leads to different effects in the reviewed dependency types and (ii) the prominence of the distractor strongly influences the interference effect. In sum, the meta-analysis suggests that the LV05 needs important modifications to account for the unexplained interference patterns and the differences between the dependency types. More generally, the meta-analysis provides a quantitative empirical basis for comparing the predictions of competing accounts of retrieval processes in sentence comprehension.},
  langid = {english},
  keywords = {Agreement,Bayesian meta-analysis,Cue-based retrieval,Interference,Reflexives,Syntactic dependency processing},
  file = {~/Zotfiles/jager.l2017 Similarity-based interference in sentenc.pdf}
}

@misc{jakulin.a:2003,
  title = {Quantifying and Visualizing Attribute Interactions},
  author = {Jakulin, Aleks and Bratko, Ivan},
  year = {2003},
  eprint = {cs/0308002},
  archiveprefix = {arXiv},
  date-added = {2021-07-19 22:11:58 -0400},
  date-modified = {2021-07-19 22:12:25 -0400},
  keywords = {co-information,interaction information,multivariate mututal information}
}

@inproceedings{jang.e:2017,
  title = {Categorical Reparameterization with {{Gumbel-softmax}}},
  booktitle = {5th International Conference on Learning Representations, Conference Track Proceedings},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  month = apr,
  publisher = {OpenReview.net},
  address = {Toulon, France},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  date-added = {2022-04-10 19:10:47 -0400},
  date-modified = {2022-04-10 19:20:19 -0400},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200}
}

@article{jarnik.v:1930,
  title = {{O jist{\'e}m probl{\'e}mu minim{\'a}ln{\'i}m. (Z dopisu panu O. Bor{\u u}vkovi) [On a certain problem of minimization. (From a letter to Mr. O. Bor{\u u}vka).]}},
  author = {Jarn{\'i}k, Vojt{\v e}ch},
  year = {1930},
  journal = {Pr{\'a}ce moravsk{\'e} p{\v r}{\'i}rodov{\v e}deck{\'e} spole{\v c}nosti},
  volume = {6},
  number = {4},
  pages = {57--63},
  doi = {10338.dmlcz/500726},
  urldate = {2021-02-05},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  langid = {czech}
}

@misc{jasra.a:2013,
  title = {The Alive Particle Filter},
  author = {Jasra, Ajay and Lee, Anthony and Yau, Christopher and Zhang, Xiaole},
  year = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1304.0151},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1304.0151},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:49:28 -0400},
  date-modified = {2022-05-05 09:49:46 -0400},
  keywords = {particle filtering},
  file = {~/Zotfiles/jasra.a2013 The alive particle filter.pdf}
}

@article{jaynes.e:1957,
  title = {Information Theory and Statistical Mechanics},
  author = {Jaynes, E. T.},
  year = {1957},
  month = may,
  journal = {Physical Review},
  volume = {106},
  number = {4},
  pages = {620--630},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRev.106.620},
  urldate = {2025-02-13},
  abstract = {Information theory provides a constructive criterion for setting up probability distributions on the basis of partial knowledge, and leads to a type of statistical inference which is called the maximum-entropy estimate. It is the least biased estimate possible on the given information; i.e., it is maximally noncommittal with regard to missing information. If one considers statistical mechanics as a form of statistical inference rather than as a physical theory, it is found that the usual computational rules, starting with the determination of the partition function, are an immediate consequence of the maximum-entropy principle. In the resulting "subjective statistical mechanics," the usual rules are thus justified independently of any physical argument, and in particular independently of experimental verification; whether or not the results agree with experiment, they still represent the best estimates that could have been made on the basis of the information available.},
  keywords = {information theory}
}

@misc{jefferson.r:2019blog,
  type = {Blog},
  title = {Free Energy, Variational Inference, and the Brain},
  author = {Jefferson, Ro},
  year = {2019},
  month = oct,
  journal = {Ro's blog},
  urldate = {2025-02-21},
  abstract = {In several recent posts, I explored various ideas that lie at the interface of physics, information theory, and machine learning:We've seen, {\`a} la Jaynes, how the concepts of entropy in statis{\dots}},
  langid = {english}
}

@incollection{jegerski.j:2013,
  title = {Self-Paced Reading},
  booktitle = {Research {{Methods}} in {{Second Language Psycholinguistics}}},
  author = {Jegerski, Jill},
  year = {2013},
  edition = {1},
  pages = {20--49},
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9780203123430},
  abstract = {Self-Paced Reading - 1},
  isbn = {978-0-203-12343-0}
}

@article{jelinek.f:1976,
  title = {Continuous Speech Recognition by Statistical Methods},
  author = {Jelinek, F.},
  year = {1976},
  month = apr,
  journal = {Proceedings of the IEEE},
  volume = {64},
  number = {4},
  pages = {532--556},
  issn = {1558-2256},
  doi = {10.1109/PROC.1976.10159},
  abstract = {Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.},
  keywords = {Acoustic devices,Automatic speech recognition,Decoding,Loudspeakers,Natural languages,noisy-channel,Signal processing,Speech processing,Speech recognition,Statistical analysis,Statistics}
}

@book{jespersen.o:1922book,
  title = {Language: Its Nature Development and Origin},
  author = {Jespersen, Otto},
  year = {1922},
  publisher = {George Allen \& Unwin Ltd.},
  address = {London},
  urldate = {2024-05-03}
}

@misc{jiang.a:2023arxiv,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06825},
  urldate = {2024-02-01},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{jiang.a:2024arxiv,
  title = {Mixtral of Experts},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.04088},
  urldate = {2024-02-01},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{jin.l:2020,
  title = {Memory-Bounded Neural Incremental Parsing for Psycholinguistic Prediction},
  booktitle = {Proceedings of the 16th International Conference on Parsing Technologies and the {{IWPT}} 2020 Shared Task on Parsing into Enhanced Universal Dependencies},
  author = {Jin, Lifeng and Schuler, William},
  year = {2020},
  month = jul,
  pages = {48--61},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.iwpt-1.6},
  abstract = {Syntactic surprisal has been shown to have an effect on human sentence processing, and can be predicted from prefix probabilities of generative incremental parsers. Recent state-of-the-art incremental generative neural parsers are able to produce accurate parses and surprisal values but have unbounded stack memory, which may be used by the neural parser to maintain explicit in-order representations of all previously parsed words, inconsistent with results of human memory experiments. In contrast, humans seem to have a bounded working memory, demonstrated by inhibited performance on word recall in multi-clause sentences (Bransford and Franks, 1971), and on center-embedded sentences (Miller and Isard,1964). Bounded statistical parsers exist, but are less accurate than neural parsers in predict-ing reading times. This paper describes a neural incremental generative parser that is able to provide accurate surprisal estimates and can be constrained to use a bounded stack. Results show that the accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to un-bounded working memory.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.iwpt-1.6},
  date-added = {2021-09-13 19:25:38 -0400},
  date-modified = {2021-09-13 19:25:40 -0400}
}

@book{johnson.d:1980book,
  title = {Arc Pair Grammar},
  author = {Johnson, David E. and Postal, Paul M.},
  year = {1980},
  publisher = {Princeton University Press},
  address = {Princeton, New Jersey},
  area = {Linguistics, Syntax},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{johnson.m:2004,
  title = {A {{TAG-based}} Noisy-Channel Model of Speech Repairs},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Johnson, Mark and Charniak, Eugene},
  year = {2004},
  month = jul,
  pages = {33--39},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1218960},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218960},
  date-added = {2022-05-03 15:15:11 -0400},
  date-modified = {2022-05-03 15:18:08 -0400},
  keywords = {noisy channel coding,tree adjoining grammars,tree transducers}
}

@phdthesis{johnson.m:2014phd,
  title = {Bayesian Time Series Models and Scalable Inference},
  author = {Johnson, Matthew James},
  year = {2014},
  urldate = {2022-07-05},
  abstract = {With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and their Bayesian nonparametric extensions. The HMM is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model (HDP-HMM), have been applied in many settings. HSMMs and HDP-HSMMs extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which HSMM message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference (SVI) framework to develop scalable inference for the HMM, HSMM, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models (LDA), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  file = {~/Zotfiles/johnson.m2014phd Bayesian time series models and scalable 2.pdf;~/Zotfiles/johnson.m2014phd Bayesian time series models and scalable.pdf}
}

@article{johnson.r:2007,
  title = {Transposed-Letter Effects in Reading: {{Evidence}} from Eye Movements and Parafoveal Preview.},
  shorttitle = {Transposed-Letter Effects in Reading},
  author = {Johnson, Rebecca L. and Perea, Manuel and Rayner, Keith},
  year = {2007},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {1},
  pages = {209--229},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.1.209},
  urldate = {2023-12-13},
  langid = {english},
  file = {~/Zotfiles/johnson.r2007 Transposed-letter effects in reading Ev.pdf}
}

@article{johnson.r:2009,
  title = {The Quiet Clam Is Quite Calm: {{Transposed-letter}} Neighborhood Effects on Eye Movements during Reading.},
  shorttitle = {The Quiet Clam Is Quite Calm},
  author = {Johnson, Rebecca L.},
  year = {2009},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {35},
  number = {4},
  pages = {943--969},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/a0015572},
  urldate = {2023-12-06},
  langid = {english},
  file = {~/Zotfiles/johnson.r2009 The quiet clam is quite calm Transposed.pdf}
}

@article{johnson.s:1967,
  title = {Hierarchical Clustering Schemes},
  author = {Johnson, Stephen C},
  year = {1967},
  journal = {Psychometrika},
  volume = {32},
  number = {3},
  pages = {241--254},
  publisher = {Springer-Verlag},
  date-added = {2019-06-15 10:38:05 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {1},
  keywords = {hierarchical clustering,ultrametric}
}

@article{jozefowicz.r:2016,
  title = {Exploring the Limits of Language Modeling},
  author = {J{\'o}zefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1602.02410},
  eprint = {1602.02410},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/JozefowiczVSSW16},
  date-added = {2019-06-23 21:20:27 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {convolutional,language modeling,LSTM},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  file = {~/Zotfiles/jozefowicz.r2016 Exploring the limits of language modelin.pdf}
}

@article{jurafsky.d:1996,
  title = {A Probabilistic Model of Lexical and Syntactic Access and Disambiguation},
  author = {Jurafsky, Daniel},
  year = {1996},
  journal = {Cognitive Science},
  volume = {20},
  number = {2},
  pages = {137--194},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2002_1},
  urldate = {2022-07-23},
  abstract = {The problems of access---retrieving linguistic structure from some mental grammar ---and disambiguation---choosing among these structures to correctly parse ambiguous linguistic input---are fundamental to language understanding. The literature abounds with psychological results on lexical access, the access of idioms, syntactic rule access, parsing preferences, syntactic disambiguation, and the processing of garden-path sentences. Unfortunately, it has been difficult to combine models which account for these results to build a general, uniform model of access and disambiguation at the lexical, idiomatic, and syntactic levels. For example, psycholinguistic theories of lexical access and idiom access and parsing theories of syntactic rule access have almost no commonality in methodology or coverage of psycholinguistic data. This article presents a single probabilistic algorithm which models both the access and disambiguation of linguistic knowledge. The algorithm is based on a parallel parser which ranks constructions for access, and interpretations for disambiguation, by their conditional probability. Low-ranked constructions and interpretations are pruned through beam-search; this pruning accounts, among other things, for the garden-path effect. I show that this motivated probabilistic treatment accounts for a wide variety of psycholinguistic results, arguing for a more uniform representation of linguistic knowledge and for the use of probabilistically-enriched grammars and interpreters as models of human knowledge of and processing of language.},
  langid = {english},
  file = {~/Zotfiles/jurafsky.d1996 A probabilistic model of lexical and syn.pdf}
}

@incollection{jurafsky.d:2003,
  title = {Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production},
  shorttitle = {Probabilistic Modeling in Psycholinguistics},
  booktitle = {Probabilistic {{Linguistics}}},
  author = {Jurafsky, Daniel},
  editor = {Bod, Rens and Hay, Jennifer and Jannedy, Stefanie},
  year = {2003},
  month = apr,
  pages = {39--96},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/5582.003.0006},
  urldate = {2024-05-15},
  isbn = {978-0-262-26885-1},
  langid = {english}
}

@book{jurafsky.d:2009book2,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2009},
  edition = {2},
  publisher = {Pearson Prentice Hall},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{jurafsky.d:2024,
  title = {N-Gram {{Language Models}}},
  booktitle = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2024},
  month = feb,
  edition = {3}
}

@book{jurafsky.d:2024book3,
  title = {Speech and Language Processing},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2024},
  edition = {3}
}

@inproceedings{kahane.s:1997,
  title = {Bubble Trees and Syntactic Representations},
  booktitle = {Proceedings of Mathematics of Language (Mol5) Meeting},
  author = {Kahane, Sylvain},
  year = {1997},
  pages = {70--76},
  date-added = {2021-07-17 10:51:30 -0400},
  date-modified = {2021-07-17 10:52:15 -0400},
  file = {~/Zotfiles/kahane.s1997 Bubble trees and syntactic representatio.pdf}
}

@misc{kahardipraja.p:2021,
  title = {Towards {{Incremental Transformers}}: {{An Empirical Analysis}} of {{Transformer Models}} for {{Incremental NLU}}},
  shorttitle = {Towards {{Incremental Transformers}}},
  author = {Kahardipraja, Patrick and Madureira, Brielen and Schlangen, David},
  year = {2021},
  month = sep,
  number = {arXiv:2109.07364},
  eprint = {2109.07364},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2109.07364},
  urldate = {2022-05-18},
  abstract = {Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/kahardipraja.p2021 Towards Incremental Transformers An Emp.pdf}
}

@techreport{kahn.h:1949,
  type = {Technical Report},
  title = {Stochastic ({{Monte Carlo}}) Attenuation Analysis},
  author = {Kahn, Herman},
  year = {1949},
  month = jun,
  number = {R-163},
  institution = {RAND Corporation}
}

@techreport{kahn.h:1954,
  type = {Technical Report},
  title = {Applications of {{Monte Carlo}}},
  author = {Kahn, Herman},
  year = {1954},
  month = apr,
  number = {AECU-3259; RM-1237-AEC},
  address = {United States},
  institution = {RAND Corp., Santa Monica, CA},
  doi = {10.2172/4353680}
}

@article{kakade.s:2001,
  title = {A Natural Policy Gradient},
  author = {Kakade, Sham M.},
  year = {2001},
  journal = {Advances in neural information processing systems},
  volume = {14}
}

@article{kalin.l:2018,
  title = {Licensing and {{Differential Object Marking}}: {{The}} View from {{Neo-Aramaic}}},
  author = {Kalin, Laura},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {2},
  pages = {112--159},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case}
}

@article{kamide.y:1999,
  title = {Incremental Pre-Head Attachment in {{Japanese}} Parsing},
  author = {Kamide, Yuki and Mitchell, Don C.},
  year = {1999},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {5-6},
  pages = {631--662},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/016909699386211},
  urldate = {2022-10-13},
  abstract = {The present study addresses the question of whether structural analyses of verb-arguments are postponed up until the head verb has been processed (head-driven parsing accounts) or initiated prior to the appearance of the verb (pre-head attachment accounts). To explore this question in relation to a head-final language, a Japanese dative argument attachment ambiguity was examined in both a questionnaire study (Experiment 1) and a self-paced reading test (Experiment 2). The data suggested that the dative argument attachment ambiguity is resolved in the manner predicted by pre-head attachment accounts. The results were incompatible with most variants of the head-driven parsing model, and were not of the form currently predicted by constraint-satisfaction models. We end by discussing the general theoretical implications of the findings.},
  keywords = {eager processing},
  file = {~/Zotfiles/kamide.y1999 Incremental pre-head attachment in Japan.pdf}
}

@article{kamide.y:2008,
  title = {Anticipatory {{Processes}} in {{Sentence Processing}}},
  author = {Kamide, Yuki},
  year = {2008},
  journal = {Language and Linguistics Compass},
  volume = {2},
  number = {4},
  pages = {647--670},
  issn = {1749-818X},
  doi = {10.1111/j.1749-818X.2008.00072.x},
  urldate = {2022-06-11},
  abstract = {Anticipation is an essential ability for the human cognitive system to survive in its surrounding environment. The present article will review previous research on anticipatory processes in sentence processing (comprehension). I start by pointing out past research carried out with inadequate methods, then move on to reviewing recent research with relatively new, more appropriate methods, specifically, the so-called `visual-world' eye-tracking paradigm, and neuropsychological techniques. I then discuss remaining unresolved issues, both methodological and theoretical.},
  langid = {english},
  file = {~/Zotfiles/kamide.y2008 Anticipatory Processes in Sentence Proce.pdf}
}

@article{kartsaklis.d:2019,
  title = {Linguistic Matrix Theory},
  author = {Kartsaklis, Dimitrios and Ramgoolam, Sanjaye and Sadrzadeh, Mehrnoosh},
  year = {2019},
  journal = {Annales de l'Institut Henri Poincar{\'e} D},
  publisher = {European Mathematical Publishing House},
  issn = {2308-5827},
  date-added = {2019-08-06 08:52:19 +0300},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {physics}
}

@article{katz.j:2019,
  title = {The Phonetics and Phonology of Lenition: A {{Campidanese Sardinian}} Case Study},
  author = {Katz, Jonah and Pitzanti, Gianmarco},
  year = {2019},
  month = sep,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {10},
  number = {1},
  pages = {16},
  publisher = {Open Library of the Humanities},
  doi = {10.5334/labphon.184},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.184},
  date-added = {2022-05-10 10:57:54 -0400},
  date-modified = {2022-05-10 10:58:06 -0400},
  keywords = {causality,lenition},
  file = {~/Zotfiles/katz.j2019 The phonetics and phonology of lenition.pdf}
}

@article{kawabata.t:1992,
  title = {The Structure of the {{I-measure}} of a {{Markov}} Chain},
  author = {Kawabata, T. and Yeung, R.W.},
  year = {1992},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {3},
  pages = {1146--1149},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.135658},
  bdsk-url-2 = {https://doi.org/10.1109/18.135658},
  date-added = {2021-09-21 17:47:00 -0400},
  date-modified = {2021-09-21 17:47:01 -0400}
}

@article{kawamoto.a:1994,
  title = {When Two Meanings Are Better than One: {{Modeling}} the Ambiguity Advantage Using a Recurrent Distributed Network},
  shorttitle = {When Two Meanings Are Better than One},
  author = {Kawamoto, Alan H. and Farrar, William T. and Kello, Christopher T.},
  year = {1994},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {20},
  number = {6},
  pages = {1233--1247},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1277},
  doi = {10.1037/0096-1523.20.6.1233},
  abstract = {Ambiguous words are processed more quickly than unambiguous words in a lexical decision task despite the fact that each sense of an ambiguous word is less frequent than the single sense of unambiguous words of equal frequency or familiarity. In this computer simulation study, we examined the effects of different assumptions of a fully recurrent connectionist model in accounting for this processing advantage for ambiguous words. We argue that the ambiguity advantage effect can be accounted for by distributed models if (a) the least mean square (LMS) error-correction algorithm rather than the Hebbian algorithm is used in training the network and (b) activation of the units representing the spelling rather than the meaning is used to index word recognition times. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Neural Networks,Simulation,Stimulus Ambiguity,Word Recognition,Words (Phonetic Units)},
  file = {~/Zotfiles/kawamoto.a1994 When two meanings are better than one M.pdf}
}

@incollection{kay.p:2005,
  title = {Argument Structure Constructions and the {{Argument}}--{{Adjunct}} Distinction},
  booktitle = {Grammatical {{Constructions}}: {{Back}} to the Roots},
  author = {Kay, Paul},
  editor = {Fried, Mirjam and Boas, Hans C.},
  year = {2005},
  volume = {4},
  pages = {71--98},
  publisher = {John Benjamins Publishing Company},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:32 -0400},
  readinglist = {Adjunction},
  keywords = {Argument/Modifier}
}

@inproceedings{kazantseva.a:2018,
  title = {Kawenn{\'o}n:Nis: The {{Wordmaker}} for {{Kanyen}}'k{\'e}ha},
  shorttitle = {Kawenn{\'o}n:Nis},
  booktitle = {Proceedings of the {{Workshop}} on {{Computational Modeling}} of {{Polysynthetic Languages}}},
  author = {Kazantseva, Anna and Maracle, Owennatekha Brian and Maracle, Ronkwe'tiy{\'o}hstha Josiah and Pine, Aidan},
  year = {2018},
  month = aug,
  pages = {53--64},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA},
  urldate = {2022-06-04},
  abstract = {In this paper we describe preliminary work on Kawenn{\'o}n:nis, a verb conjugator for Kanyen'k{\'e}ha (Ohsweken dialect). The project is the result of a collaboration between Onkwawenna Kentyohkwa Kanyen'k{\'e}ha immersion school and the Canadian National Research Council's Indigenous Language Technology lab. The purpose of Kawenn{\'o}n:nis is to build on the educational successes of the Onkwawenna Kentyohkwa school and develop a tool that assists students in learning how to conjugate verbs in Kanyen'k{\'e}ha; a skill that is essential to mastering the language. Kawenn{\'o}n:nis is implemented with both web and mobile front-ends that communicate with an application programming interface that in turn communicates with a symbolic language model implemented as a finite state transducer. Eventually, it will serve as a foundation for several other applications for both Kanyen'k{\'e}ha and other Iroquoian languages.},
  keywords = {computational revitalization,iroquoian},
  file = {~/Zotfiles/kazantseva.a2018 Kawennnnis the Wordmaker for Kanyen'k.pdf}
}

@article{kellert.o:2023,
  title = {Probing Sociodemographic Influence on Code-Switching and Language Choice in {{Quebec}} with Geolocation of Tweets},
  author = {Kellert, Olga},
  year = {2023},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {14},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1137038},
  urldate = {2024-07-20},
  abstract = {{$<$}p{$>$}This paper investigates the influence of the relative size of speech communities on language use in multilingual regions and cities. Due to peoples' everyday mobility inside a city, it is still unclear whether the size of a population matters for language use on a sub-city scale. By testing the correlation between the size of a population and language use on various spatial scales, this study will contribute to a better understanding of the extent to which sociodemographic factors influence language use. The present study investigates two particular phenomena that are common to multilingual speakers, namely language mixing or Code-Switching and using multiple languages without mixing. Demographic information from a Canadian census will make predictions about the intensity of Code-Switching and language use by multilinguals in cities of Quebec and neighborhoods of Montreal. Geolocated tweets will be used to identify where these linguistic phenomena occur the most and the least. My results show that the intensity of Code-Switching and the use of English by bilinguals is influenced by the size of anglophone and francophone populations on various spatial scales such as the city level, land use level (city center vs. periphery of Montreal), and large urban zones on the sub-city level, namely the western and eastern urban zones of Montreal. However, the correlation between population figures and language use is difficult to measure and evaluate on a much smaller sub-urban scale such as the city block scale due to factors such as population figures missing from the census and people's mobility. A qualitative evaluation of language use on a small spatial scale seems to suggest that other social influences such as the location context or topic of discussion are much more important predictors for language use than population figures. Methods will be suggested for testing this hypothesis in future research. I conclude that geographic space can provide us information about the relation between language use in multilingual cities and sociodemographic factors such as a speech community's size and that social media is a valuable alternative data source for sociolinguistic research that offers new insights into the mechanisms of language use such as Code-Switching.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Bilingualism -,code-switching,Geolocation,Language contact,Quebec,Twitter},
  file = {~/Zotfiles/kellert.o2023 Probing sociodemographic influence on co.pdf}
}

@phdthesis{kelley.p:2018phd,
  title = {More People Understand {{Eschers}} than the Linguist Does: {{The}} Causes and Effects of Grammatical Illusions},
  shorttitle = {More People Understand Eschers than the Linguist Does},
  author = {Kelley, Patrick},
  year = {2018},
  address = {East Lansing, Michigan},
  urldate = {2023-02-22},
  abstract = {A grammatical illusion can be defined as a sentence that seems acceptable, but structurally, the sentence is ungrammatical. Grammatical illusions provide a challenge for linguists to understand why we do not immediately reject illusions like we do for most ungrammatical sentences. One type of illusion that has stirred several ongoing debates is the Escher Sentence. This dissertation focuses on the source of the illusory effect, or the reason why people fail to consistently reject these sentences. This dissertation explores the properties of Escher Sentences, the reason why they are illusory in nature, and what this contributes to our understanding of the parser. Six Experiments were designed to test the acceptability judgments, interpretations, and neurophysiological responses to these sentences. I conclude that Escher Sentences are recognized by the parser as ungrammatical, but because of the structure of these sentences, the parser is tricked into using a coercive operation to force Escher Sentences to have an acceptable interpretation. Escher Sentences gives us potential insight into the constraints of the parser in processing language while at the same time highlighting the parser's strategies in resolving computations that are ungrammatical.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780355930733},
  langid = {english},
  school = {Michigan State University},
  keywords = {Escher,grammatical illusions,Illusion,Language,literature and linguistics,Parser,Processing,Psychology},
  file = {~/Zotfiles/kelley.p2018 More people understand Eschers than the.pdf}
}

@inproceedings{kennedy.a:2003,
  title = {The {{Dundee}} Corpus},
  booktitle = {Proceedings of the 12th {{European}} Conference on Eye Movement},
  author = {Kennedy, Alan and Hill, Robin and Pynte, Jo{\"e}l},
  year = {2003},
  date-added = {2021-06-02 17:24:08 -0400},
  date-modified = {2021-06-02 17:25:50 -0400}
}

@article{kennedy.a:2013,
  title = {Frequency and Predictability Effects in the {{Dundee Corpus}}: {{An}} Eye Movement Analysis},
  author = {Kennedy, Alan and Pynte, Jo{\"e}l and Murray, Wayne S. and Paul, Shirley-Anne},
  year = {2013},
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {3},
  pages = {601--618},
  publisher = {SAGE Publications},
  doi = {10.1080/17470218.2012.676054},
  bdsk-url-2 = {https://doi.org/10.1080/17470218.2012.676054},
  date-added = {2021-06-02 17:10:01 -0400},
  date-modified = {2021-06-02 17:10:34 -0400}
}

@article{keshev.m:2021,
  title = {Noisy Is Better than Rare: {{Comprehenders}} Compromise Subject-Verb Agreement to Form More Probable Linguistic Structures},
  shorttitle = {Noisy Is Better than Rare},
  author = {Keshev, Maayan and {Meltzer-Asscher}, Aya},
  year = {2021},
  month = feb,
  journal = {Cognitive Psychology},
  volume = {124},
  pages = {101359},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2020.101359},
  urldate = {2024-12-23},
  abstract = {Production and perception errors are common in everyday language use. Recent studies suggest that in order to overcome the flawed speech signal, comprehenders engage in rational noisy-channel processing, which can pull their interpretation towards more probable ``near-neighbor'' analyses, based on the assumption that an error may have occurred in the transmission of the sentence. We investigate this type of processing using subject/object relative clause ambiguity in Hebrew. In four self-paced reading experiments and a sentence completion experiment, we find that during online processing, readers apply elaborate knowledge regarding the distribution of structures in the language, and that they are willing to compromise subject-verb agreement to refrain from (grammatical but) highly improbable structures. The results suggest that the prior probability of alternative analyses modulates the interpretation of agreement.},
  keywords = {Noisy-channel,Rational inference,Relative clause,Sentence processing,Subject-verb agreement},
  file = {~/Zotfiles/keshev.m2021 Noisy is better than rare Comprehenders.pdf}
}

@book{keynes.j:1921book,
  title = {A Treatise on Probability},
  author = {Keynes, John Maynard},
  year = {1921},
  publisher = {Macmillan And Co.,},
  urldate = {2024-05-14},
  langid = {english},
  lccn = {21020432},
  file = {~/Zotfiles/keynes.j1921 A treatise on probability.pdf}
}

@misc{kim.t:2020,
  title = {Chart-Based Zero-Shot Constituency Parsing on Multiple Languages},
  author = {Kim, Taeuk and Li, Bowen and Lee, Sang-goo},
  year = {2020},
  eprint = {2004.13805},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kim.t:2020a,
  title = {Are Pre-Trained Language Models Aware of Phrases? {{Simple}} but Strong Baselines for Grammar Induction},
  booktitle = {8th International Conference on Learning Representations, {{ICLR}} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  author = {Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  year = {2020},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KimCEL20.bib},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200}
}

@inproceedings{kim.y:2016,
  title = {Character-Aware Neural Language Models},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {{USA}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David A. and Rush, Alexander M.},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  year = {2016},
  pages = {2741--2749},
  publisher = {AAAI Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aaai/KimJSR16.bib},
  timestamp = {Fri, 15 Nov 2019 00:00:00 +0100}
}

@inproceedings{kingma.d:2014,
  title = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Welling, Max},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
  file = {~/Zotfiles/kingma.d2013 Auto-encoding variational bayes.pdf}
}

@phdthesis{kingma.d:2017phd,
  title = {Variational Inference \& Deep Learning: {{A}} New Synthesis},
  author = {Kingma, Diederik P},
  year = {2017},
  date-added = {2019-10-08 21:58:23 -0400},
  date-modified = {2021-03-12 11:48:12 -0500},
  project = {syntactic embedding},
  school = {University of Amsterdam},
  keywords = {autoencoders,variational inference},
  file = {~/Zotfiles/kingma.d2017 Variational inference & deep learning A.pdf}
}

@inproceedings{kipf.t:2017,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{kitaev.n:2018,
  title = {Constituency Parsing with a Self-Attentive Encoder},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kitaev, Nikita and Klein, Dan},
  year = {2018},
  pages = {2676--2686},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1249},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1249},
  file = {~/Zotfiles/kitaev.n2018 Constituency parsing with a self-attenti.pdf}
}

@inproceedings{kitaev.n:2019,
  title = {Multilingual {{Constituency Parsing}} with {{Self-Attention}} and {{Pre-Training}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
  year = {2019},
  month = jul,
  pages = {3499--3505},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1340},
  urldate = {2022-05-18},
  abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2\% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
  keywords = {parsing},
  file = {~/Zotfiles/kitaev.n2019 Multilingual Constituency Parsing with S.pdf}
}

@inproceedings{kitaev.n:2022,
  title = {Learned {{Incremental Representations}} for {{Parsing}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kitaev, Nikita and Lu, Thomas and Klein, Dan},
  year = {2022},
  month = may,
  pages = {3086--3095},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  urldate = {2022-05-18},
  abstract = {We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.},
  file = {~/Zotfiles/kitaev.n2022 Learned Incremental Representations for.pdf}
}

@article{klafka.j:2021,
  title = {Characterizing the {{Typical Information Curves}} of {{Diverse Languages}}},
  author = {Klafka, Josef and Yurovsky, Daniel},
  year = {2021},
  month = oct,
  journal = {Entropy},
  volume = {23},
  number = {10},
  pages = {1300},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23101300},
  urldate = {2023-11-07},
  abstract = {Optimal coding theories of language predict that speakers will keep the amount of information in their utterances relatively uniform under the constraints imposed by their language, but how much do these constraints influence information structure, and how does this influence vary across languages? We present a novel method for characterizing the information structure of sentences across a diverse set of languages. While the structure of English is broadly consistent with the shape predicted by optimal coding, many languages are not consistent with this prediction. We proceed to show that the characteristic information curves of languages are partly related to a variety of typological features from phonology to word order. These results present an important step in the direction of exploring upper bounds for the extent to which linguistic codes can be optimal for communication.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {communication,dynamic time warping,language development,n-grams,surprisal,typical information curves,typology},
  file = {~/Zotfiles/klafka.j2021 Characterizing the Typical Information C.pdf}
}

@inproceedings{klein.d:2002,
  title = {Fast Exact Inference with a Factored Model for Natural Language Parsing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Klein, Dan and Manning, Christopher D},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2002},
  volume = {15},
  publisher = {MIT Press},
  date-added = {2022-05-06 15:57:44 -0400},
  date-modified = {2022-05-06 16:01:12 -0400},
  keywords = {stanford dependencies,stanford parser},
  file = {~/Zotfiles/klein.d2002parserFactored Fast exact inference with a factored mod.pdf}
}

@inproceedings{klein.d:2003parserPCFG,
  title = {Accurate Unlexicalized Parsing},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  author = {Klein, Dan and Manning, Christopher D.},
  year = {2003},
  month = jul,
  pages = {423--430},
  publisher = {Association for Computational Linguistics},
  address = {Sapporo, Japan},
  doi = {10.3115/1075096.1075150},
  bdsk-url-2 = {https://doi.org/10.3115/1075096.1075150},
  date-added = {2022-05-06 16:00:23 -0400},
  date-modified = {2022-05-06 16:00:53 -0400}
}

@inproceedings{klein.d:2004,
  title = {Corpus-Based Induction of Syntactic Structure: {{Models}} of Dependency and Constituency},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Klein, Dan and Manning, Christopher},
  year = {2004},
  pages = {478--485},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1219016},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1219016}
}

@article{kleinschmidt.d:2015,
  title = {Robust Speech Perception: {{Recognize}} the Familiar, Generalize to the Similar, and Adapt to the Novel},
  shorttitle = {Robust Speech Perception},
  author = {Kleinschmidt, Dave F. and Jaeger, T. Florian},
  year = {2015},
  journal = {Psychological Review},
  volume = {122},
  number = {2},
  pages = {148--203},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/a0038695},
  abstract = {Successful speech perception requires that listeners map the acoustic signal to linguistic categories. These mappings are not only probabilistic, but change depending on the situation. For example, one talker's /p/ might be physically indistinguishable from another talker's /b/ (cf. lack of invariance). We characterize the computational problem posed by such a subjectively nonstationary world and propose that the speech perception system overcomes this challenge by (a) recognizing previously encountered situations, (b) generalizing to other situations based on previous similar experience, and (c) adapting to novel situations. We formalize this proposal in the ideal adapter framework: (a) to (c) can be understood as inference under uncertainty about the appropriate generative model for the current talker, thereby facilitating robust speech perception despite the lack of invariance. We focus on 2 critical aspects of the ideal adapter. First, in situations that clearly deviate from previous experience, listeners need to adapt. We develop a distributional (belief-updating) learning model of incremental adaptation. The model provides a good fit against known and novel phonetic adaptation data, including perceptual recalibration and selective adaptation. Second, robust speech recognition requires that listeners learn to represent the structured component of cross-situation variability in the speech signal. We discuss how these 2 aspects of the ideal adapter provide a unifying explanation for adaptation, talker-specificity, and generalization across talkers and groups of talkers (e.g., accents and dialects). The ideal adapter provides a guiding framework for future investigations into speech perception and adaptation, and more broadly language comprehension. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognitive Generalization,Comprehension,Learning,Linguistics,Speech Perception,Statistics},
  file = {~/Zotfiles/kleinschmidt.d2015 Robust speech perception Recognize the.pdf}
}

@article{klepousniotou.e:2007,
  title = {Disambiguating the Ambiguity Advantage Effect in Word Recognition: {{An}} Advantage for Polysemous but Not Homonymous Words},
  shorttitle = {Disambiguating the Ambiguity Advantage Effect in Word Recognition},
  author = {Klepousniotou, Ekaterini and Baum, Shari R.},
  year = {2007},
  month = jan,
  journal = {Journal of Neurolinguistics},
  volume = {20},
  number = {1},
  pages = {1--24},
  issn = {0911-6044},
  doi = {10.1016/j.jneuroling.2006.02.001},
  urldate = {2025-06-06},
  abstract = {Previous lexical decision studies reported a processing advantage for words with multiple meanings (i.e., the ``ambiguity advantage'' effect). The present study further specifies the source of this advantage by showing that it is based on the extent of meaning relatedness of ambiguous words. Four types of ambiguous words, balanced homonymous (e.g., ``panel''), unbalanced homonymous (e.g., ``port''), metaphorically polysemous (e.g., ``lip''), and metonymically polysemous (e.g., ``rabbit''), were used in auditory and visual simple lexical decision experiments. It was found that ambiguous words with multiple related senses (i.e., polysemous words) are processed faster than frequency-matched unambiguous control words, whereas ambiguous words with multiple unrelated meanings (i.e., homonymous words) do not show such an advantage. In addition, a distinction within polysemy (into metaphor and metonymy) is demonstrated experimentally. These results call for a re-evaluation of models of word recognition, so that the advantage found for polysemous, but not homonymous, words can be accommodated.},
  keywords = {ambiguity advantage,Homonymy,Lexical ambiguity,Metaphor,Metonymy,Polysemy,Word recognition},
  file = {~/Zotfiles/klepousniotou.e2007 Disambiguating the ambiguity advantage e.pdf}
}

@article{kliegl.r:2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  year = {2004},
  journal = {European Journal of Cognitive Psychology},
  volume = {16},
  number = {1-2},
  pages = {262--284},
  publisher = {Informa UK Limited},
  doi = {10.1080/09541440340000213},
  bdsk-url-2 = {https://doi.org/10.1080/09541440340000213},
  date-added = {2021-06-02 17:15:22 -0400},
  date-modified = {2021-06-02 17:15:24 -0400}
}

@inproceedings{kneser.r:1995,
  title = {Improved Backing-off for {{M-gram}} Language Modeling},
  booktitle = {1995 {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Kneser, R. and Ney, H.},
  year = {1995},
  month = may,
  volume = {1},
  pages = {181-184 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1995.479394},
  urldate = {2024-08-02},
  abstract = {In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10\% in terms of perplexity and 5\% in terms of word error rate.},
  keywords = {Error analysis,History,Interpolation,Laboratories,Probability distribution,Smoothing methods,Stochastic processes,Training data}
}

@article{kolbel.m:2004,
  title = {Faultless {{Disagreement}}},
  author = {K{\"o}lbel, Max},
  year = {2004},
  journal = {Proceedings of the Aristotelian Society},
  volume = {104},
  eprint = {4545405},
  eprinttype = {jstor},
  pages = {53--73},
  publisher = {[Aristotelian Society, Wiley]},
  issn = {0066-7374},
  urldate = {2025-08-13},
  abstract = {There seem to be topics on which people can disagree without fault. For example, you and I might disagree on whether Picasso was a better artist than Matisse, without either of us being at fault. Is this a genuine possibility or just apparent? In this paper I pursue two aims: I want to provide a systematic map of available responses to this question. Simultaneously, I want to assess these responses. I start by introducing and defining the notion of a faultless disagreement. Then I present a simple argument to the conclusion that faultless disagreement is not possible. Those who accept the argument have to explain away apparent cases of faultless disagreement. Those who want to maintain the possibility of faultless disagreement must deny one of the argument's premisses. The position I want to promote belongs to the latter category and is a form of genuine relativism.}
}

@article{kollar.t:2017,
  title = {Generalized Grounding Graphs: {{A}} Probabilistic Framework for Understanding Grounded Commands},
  author = {Kollar, Thomas and Tellex, Stefanie and Walter, Matthew and Huang, Albert and Bachrach, Abraham and Hemachandra, Sachi and Brunskill, Emma and Banerjee, Ashis and Roy, Deb and Teller, Seth and others},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.01097},
  eprint = {1712.01097},
  archiveprefix = {arXiv},
  date-added = {2020-07-28 16:14:45 -0400},
  date-modified = {2020-07-28 16:15:35 -0400},
  project = {syntactic embedding},
  keywords = {robotics,semantics}
}

@article{kolmogorov.a:1968,
  title = {Three Approaches to the Quantitative Definition of Information},
  author = {Kolmogorov, Andrei Nikolaevich},
  year = {1968},
  journal = {International journal of computer mathematics},
  volume = {2},
  number = {1-4},
  pages = {157--168},
  publisher = {Taylor \& Francis},
  date-added = {2019-09-13 08:14:37 -0400},
  date-modified = {2019-09-13 08:15:41 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@article{kolmogorov.a:1968a,
  title = {Logical Basis for Information Theory and Probability Theory},
  author = {Kolmogorov, Andrei},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {5},
  pages = {662--664},
  publisher = {IEEE},
  date-added = {2019-09-13 08:11:08 -0400},
  date-modified = {2019-09-13 08:11:46 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@techreport{kong.a:1992,
  type = {Technical Report},
  title = {A Note on Importance Sampling Using Standardized Weights},
  author = {Kong, Augustine},
  year = {1992},
  month = jul,
  number = {348},
  institution = {Department of Statistics, University of Chicago},
  file = {~/Zotfiles/kong.a1992 A note on importance sampling using stan.pdf}
}

@article{kong.a:1994,
  title = {Sequential Imputations and {{Bayesian}} Missing Data Problems},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1994.10476469},
  pages = {278--288},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1994.10476469},
  abstract = {Abstract For missing data problems, Tanner and Wong have described a data augmentation procedure that approximates the actual posterior distribution of the parameter vector by a mixture of complete data posteriors. Their method of constructing the complete data sets is closely related to the Gibbs sampler. Both required iterations, and, similar to the EM algorithm, convergence can be slow. We introduce in this article an alternative procedure that involves imputing the missing data sequentially and computing appropriate importance sampling weights. In many applications this new procedure works very well without the need for iterations. Sensitivity analysis, influence analysis, and updating with new data can be performed cheaply. Bayesian prediction and model selection can also be incorporated. Examples taken from a wide range of applications are used for illustration.},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1994.10476469},
  date-added = {2022-05-07 10:36:04 -0400},
  date-modified = {2022-05-07 10:36:22 -0400},
  keywords = {sequential importance sampling,sequential imputation,sequential Monte Carlo}
}

@inproceedings{konieczny.l:2003,
  title = {Anticipation of Clause-Final Heads. {{Evidence}} from Eye-Tracking and {{SRNs}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Cognitive Science}}},
  author = {Konieczny, Lars and D{\"o}ring, Philipp},
  year = {2003},
  month = jun,
  pages = {13--17},
  publisher = {Springer Berlin Heidelberg},
  address = {Melbourne, Australia and St. Petersburg, Russia},
  abstract = {In a Simple Recurrent Network simulation and an eye- tracking study, we investigated the processing of clause- final verbs. Following the integration cost hypothesis (Gibson, 1998), processing verbs should be the harder, the more complement integrations have to take place. In contrast, probabilistic prediction-based models, like Simple Recurrent Networks (SRNs, Elman, 1990), might anticipate verbs the better, the more dependents have been encountered beforehand. We trained SRNs with a subset of the German language to establish basic dependency relationships between verbs and their arguments in both verb-second and verb-final constructions. The test results established a clear anticipation hypothesis: the more arguments precede the verb, the lower the prediction error and hence, predicted reading times. The data from an eye-tracking experiment confirm the anticipation hypothesis: Clause final verbs are read faster when an additional Dative, instead of a noun-modifying Genitive, is read beforehand. Adverbial PP-adjuncts, in contrast to Noun-modifying PPs, however, did not affect reading times. In general, the results support a restricted anticipation hypothesis.}
}

@inproceedings{koo.t:2007,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  year = {2007},
  pages = {141--150},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@inproceedings{koo.t:2024,
  title = {Automata-Based Constraints for Language Model Decoding},
  booktitle = {First {{Conference}} on {{Language Modeling}}},
  author = {Koo, Terry and Liu, Frederick and He, Luheng},
  year = {2024},
  month = aug,
  address = {Philadelphia, PA, USA},
  urldate = {2024-10-10},
  abstract = {Language models (LMs) are often expected to generate strings in some formal language; for example, structured data, API calls, or code snippets. Although LMs can be tuned to improve their adherence to formal syntax, this does not *guarantee* conformance, especially with smaller LMs suitable for large-scale deployment. In addition, tuning requires significant resources, making it impractical for uncommon or task-specific formats. To prevent downstream parsing errors we would ideally *constrain* the LM to only produce valid output, but this is severely complicated by tokenization, which is typically both ambiguous and misaligned with the formal grammar. We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the *regular languages*, a broad class of formal languages with many practical applications, including API calls or schema-guided JSON and YAML. We also discuss pragmatic extensions for coping with the issue of high branching factor, and extend our techniques to *deterministic context-free languages*, which similarly admit an efficient closed-form solution. Previous work on this topic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading to problems with speed, correctness, and extensibility. Instead, we reformulate the entire task in terms of automata so we can leverage well-studied and well-optimized algorithms. Our system compiles constraints {\textasciitilde}7,000x faster, is provably correct, and can be extended in a modular fashion.},
  langid = {english}
}

@inproceedings{kool.w:2019,
  title = {Stochastic Beams and Where to Find Them: {{The Gumbel-top-k}} Trick for Sampling Sequences without Replacement},
  shorttitle = {Stochastic Beams and Where to Find Them},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kool, Wouter and Hoof, Herke Van and Welling, Max},
  year = {2019},
  month = may,
  pages = {3499--3508},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-11-06},
  abstract = {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample {$k$}kk elements without replacement. We show how to implicitly apply this 'Gumbel-Top-{$k$}kk' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in {$k$}kk and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.},
  langid = {english},
  file = {~/Zotfiles/kool.w2019 Stochastic beams and where to find them 2.pdf;~/Zotfiles/kool.w2019 Stochastic beams and where to find them.pdf}
}

@article{kool.w:2019ICLR,
  title = {Buy 4 {{REINFORCE Samples}}, {{Get}} a {{Baseline}} for {{Free}}!},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  year = {2019},
  month = apr,
  urldate = {2025-09-12},
  abstract = {REINFORCE can be used to train models in structured prediction settings to directly optimize the test-time objective. However, the common case of sampling one prediction per datapoint (input) is data-inefficient. We show that by drawing multiple samples (predictions) per datapoint, we can learn with significantly less data, as we freely obtain a REINFORCE baseline to reduce variance. Additionally we derive a REINFORCE estimator with baseline, based on sampling without replacement. Combined with a recent technique to sample sequences without replacement using Stochastic Beam Search, this improves the training procedure for a sequence model that predicts the solution to the Travelling Salesman Problem.},
  langid = {english},
  file = {/Users/v/Zotfiles/kool.w2019a Buy 4 REINFORCE Samples, Get a Baseline.pdf}
}

@article{kool.w:2020,
  title = {Ancestral {{Gumbel-Top-k Sampling}} for {{Sampling Without Replacement}}},
  author = {Kool, Wouter and van Hoof, Herke and Welling, Max},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {47},
  pages = {1--36},
  issn = {1533-7928},
  urldate = {2025-02-24},
  abstract = {We develop ancestral Gumbel-Top-kkk sampling: a generic and efficient method for sampling without replacement from discrete-valued Bayesian networks, which includes multivariate discrete distributions, Markov chains and sequence models. The method uses an extension of the Gumbel-Max trick to sample without replacement by finding the top kkk of perturbed log-probabilities among all possible configurations of a Bayesian network. Despite the exponentially large domain, the algorithm has a complexity linear in the number of variables and sample size kkk. Our algorithm allows to set the number of parallel processors mmm, to trade off the number of iterations versus the total cost (iterations times mmm) of running the algorithm. For m=1m=1m = 1 the algorithm has minimum total cost, whereas for m=km=km = k the number of iterations is minimized, and the resulting algorithm is known as Stochastic Beam Search. We provide extensions of the algorithm and discuss a number of related algorithms. We analyze the properties of ancestral Gumbel-Top-kkk sampling and compare against alternatives on randomly generated Bayesian networks with different levels of connectivity. In the context of (deep) sequence models, we show its use as a method to generate diverse but high-quality translations and statistical estimates of translation quality and entropy.},
  file = {~/Zotfiles/kool.w2020 Ancestral Gumbel-Top-k Sampling for Samp.pdf}
}

@book{kozen.d:1997book,
  title = {Automata and Computability},
  author = {Kozen, Dexter C.},
  year = {1997},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-1844-9},
  urldate = {2023-01-24},
  isbn = {978-1-4612-7309-7 978-1-4612-1844-9},
  langid = {english},
  file = {~/Zotfiles/kozen.d1997 Automata and computability.pdf}
}

@book{kracht.m:2003book,
  title = {The Mathematics of Language},
  author = {Kracht, Marcus},
  year = {2003},
  series = {Studies in Generative Grammar},
  number = {63},
  publisher = {Mouton De Gruyter},
  date-added = {2019-05-19 21:51:49 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {3-11-017620-3 978-3-11-017620-9},
  keywords = {automata,complexity,formal languages,mathematical linguistics,model theory}
}

@article{kubler.s:2009,
  title = {Dependency Parsing},
  author = {K{\"u}bler, Sandra and McDonald, Ryan and Nivre, Joakim},
  year = {2009},
  journal = {Synthesis lectures on human language technologies},
  volume = {1},
  number = {1},
  pages = {1--127},
  publisher = {Morgan \& Claypool Publishers},
  date-added = {2020-02-26 14:44:36 -0500},
  date-modified = {2020-02-26 14:45:01 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm}
}

@article{kucerova.i:2016,
  title = {Long-Distance Agreement in {{Icelandic}}: Locality Restored},
  author = {Ku{\v c}erov{\'a}, Ivona},
  year = {2016},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {19},
  number = {1},
  pages = {49--74},
  publisher = {Springer},
  date-added = {2020-02-26 09:11:49 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,object shift,phi features},
  file = {~/Zotfiles/kucerova.i2016 Long-distance agreement in Icelandic lo.pdf}
}

@book{kuhlmann.m:2010book,
  title = {Dependency Structures and Lexicalized Grammars: {{An}} Algebraic Approach},
  author = {Kuhlmann, Marco},
  year = {2010},
  volume = {6270},
  publisher = {Springer},
  date-added = {2020-02-26 18:37:01 -0500},
  date-modified = {2021-07-16 11:22:03 -0400},
  isbn = {978-3-642-14568-1},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projective dependencies,projectivity}
}

@article{kullback.s:1951,
  title = {On Information and Sufficiency},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  urldate = {2022-10-11},
  abstract = {The Annals of Mathematical Statistics},
  file = {~/Zotfiles/kullback.s1951 On information and sufficiency.pdf}
}

@book{kullback.s:1959book,
  title = {Information Theory and Statistics},
  author = {Kullback, Solomon},
  year = {[1968] 1959},
  publisher = {Peter Smith},
  address = {New York, NY, USA},
  isbn = {978-0-8446-5625-0},
  langid = {english},
  keywords = {Information theory},
  file = {~/Zotfiles/kullback.s1959 Information theory and statistics.djvu}
}

@article{kumar.m:2023,
  title = {Bayesian Surprise Predicts Human Event Segmentation in Story Listening},
  author = {Kumar, Manoj and Goldstein, Ariel and Michelmann, Sebastian and Zacks, Jeffrey M. and Hasson, Uri and Norman, Kenneth A.},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {10},
  pages = {e13343},
  issn = {1551-6709},
  doi = {10.1111/cogs.13343},
  urldate = {2024-02-10},
  abstract = {Event segmentation theory posits that people segment continuous experience into discrete events and that event boundaries occur when there are large transient increases in prediction error. Here, we set out to test this theory in the context of story listening, by using a deep learning language model (GPT-2) to compute the predicted probability distribution of the next word, at each point in the story. For three stories, we used the probability distributions generated by GPT-2 to compute the time series of prediction error. We also asked participants to listen to these stories while marking event boundaries. We used regression models to relate the GPT-2 measures to the human segmentation data. We found that event boundaries are associated with transient increases in Bayesian surprise but not with a simpler measure of prediction error (surprisal) that tracks, for each word in the story, how strongly that word was predicted at the previous time point. These results support the hypothesis that prediction error serves as a control mechanism governing event segmentation and point to important differences between operational definitions of prediction error.},
  langid = {english},
  keywords = {Bayesian surprise,Entropy,Event segmentation,GPT-2,Narratives,Surprise},
  file = {~/Zotfiles/kumar.m2023 Bayesian surprise predicts human event s.pdf}
}

@inproceedings{kuncoro.a:2018,
  title = {{{LSTMs}} Can Learn Syntax-Sensitive Dependencies Well, but Modeling Structure Makes Them Better},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John T. and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  year = {2018},
  pages = {1426--1436},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1132},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1132},
  date-modified = {2022-04-20 13:50:17 -0400}
}

@article{kuperberg.g:2016,
  title = {What Do We Mean by Prediction in Language Comprehension?},
  author = {Kuperberg, Gina R. and Jaeger, T. Florian},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  eprint = {https://doi.org/10.1080/23273798.2015.1102299},
  pages = {32--59},
  publisher = {Routledge},
  doi = {10.1080/23273798.2015.1102299},
  date-modified = {2021-06-05 22:29:28 -0400},
  file = {~/Zotfiles/kuperberg.g2016 What do we mean by prediction in languag.pdf}
}

@inproceedings{kuribayashi.t:2021,
  title = {Lower Perplexity Is Not Always Human-Like},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.acl-long.405},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.405},
  date-added = {2021-12-02 19:40:09 -0500},
  date-modified = {2021-12-02 19:40:25 -0500},
  file = {~/Zotfiles/kuribayashi.t2021 Lower perplexity is not always human-lik.pdf}
}

@inproceedings{kuribayashi.t:2022,
  title = {Context Limitations Make Neural Language Models More Human-Like},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Brassard, Ana and Inui, Kentaro},
  year = {2022},
  month = dec,
  pages = {10421--10436},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-04-30},
  abstract = {Language models (LMs) have been used in cognitive modeling as well as engineering studies---they compute information-theoretic complexity metrics that simulate humans' cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.Our results showed that constraining the LMs' context access improved their simulation of human reading behavior.We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs' context access might enhance their cognitive plausibility.},
  file = {~/Zotfiles/kuribayashi.t2022 Context limitations make neural language.pdf}
}

@misc{kuribayashi.t:2025arxiv,
  title = {Large Language Models Are Human-like Internally},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Taieb, Souhaib Ben and Inui, Kentaro and Baldwin, Timothy},
  year = {2025},
  month = feb,
  number = {arXiv:2502.01615},
  eprint = {2502.01615},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.01615},
  urldate = {2025-02-15},
  abstract = {Recent cognitive modeling studies have reported that larger language models (LMs) exhibit a poorer fit to human reading behavior, leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of LMs. Our analysis reveals that next-word probabilities derived from internal layers of larger LMs align with human sentence processing data as well as, or better than, those from smaller LMs. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, MAZE task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger LMs has been underestimated. Furthermore, we first identify an intriguing relationship between LM layers and human measures: earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and MAZE processing times. Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/kuribayashi.t2025arxiv Large Language Models Are Human-Like Int.pdf}
}

@inproceedings{kurihara.k:2004,
  title = {An Application of the Variational {{Bayesian}} Approach to Probabilistic Context-Free Grammars},
  booktitle = {In {{International Joint Conference}} on {{Natural Language Processing Workshop Beyond Shallow Analyses}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  year = {2004},
  abstract = {We present an efficient learning algorithm for probabilistic context-free grammars based on the variational Bayesian approach. Although the maximum likelihood method has traditionally been used for learning probabilistic language models, Bayesian learning is, in principle, less likely to cause overfitting problems than the maximum likelihood method. We show that the computational complexity of our algorithm is equal to that of the Inside-Outside algorithm. We also report results of experiments to compare precisions of the Inside-Outside algorithm and our algorithm. 1},
  file = {~/Zotfiles/kurihara.k2004 An application of the variational Bayesi.pdf}
}

@incollection{kurihara.k:2006,
  title = {Variational {{Bayesian}} Grammar Induction for Natural Language},
  booktitle = {Grammatical {{Inference}}: {{Algorithms}} and {{Applications}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  editor = {Sakakibara, Yasubumi and Kobayashi, Satoshi and Sato, Kengo and Nishino, Tetsuro and Tomita, Etsuji},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--96},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11872436_8},
  abstract = {This paper presents a new grammar induction algorithm for probabilistic context-free grammars (PCFGs). There is an approach to PCFG induction that is based on parameter estimation. Following this approach, we apply the variational Bayes to PCFGs. The variational Bayes (VB) is an approximation of Bayesian learning. It has been empirically shown that VB is less likely to cause overfitting. Moreover, the free energy of VB has been successfully used in model selection. Our algorithm can be seen as a generalization of PCFG induction algorithms proposed before. In the experiments, we empirically show that induced grammars achieve better parsing results than those of other PCFG induction algorithms. Based on the better parsing results, we give examples of recursive grammatical structures found by the proposed algorithm.},
  isbn = {978-3-540-45265-2},
  langid = {english},
  keywords = {Bayesian Learning,Noun Phrase,Parse Tree,Training Corpus,Wall Street Journal},
  file = {~/Zotfiles/kurihara.k2006 Variational Bayesian grammar induction f.pdf}
}

@article{kutas.m:1984,
  title = {Brain Potentials during Reading Reflect Word Expectancy and Semantic Association},
  author = {Kutas, M. and Hillyard, S. A.},
  year = {1984},
  month = jan,
  journal = {Nature},
  volume = {307},
  number = {5947},
  pages = {161--163},
  issn = {0028-0836},
  doi = {10.1038/307161a0},
  abstract = {The neuroelectric activity of the human brain that accompanies linguistic processing can be studied through recordings of event-related potentials (e.r.p. components) from the scalp. The e.r.ps triggered by verbal stimuli have been related to several different aspects of language processing. For example, the N400 component, peaking around 400 ms post-stimulus, appears to be a sensitive indicator of the semantic relationship between a word and the context in which it occurs. Words that complete sentences in a nonsensical fashion elicit much larger N400 waves than do semantically appropriate words or non-semantic irregularities in a text. In the present study, e.r.ps were recorded in response to words that completed meaningful sentences. The amplitude of the N400 component of the e.r.p. was found to be an inverse function of the subject's expectancy for the terminal word as measured by its 'Cloze probability'. In addition, unexpected words that were semantically related to highly expected words elicited lower N400 amplitudes. These findings suggest N400 may reflect processes of semantic priming or activation.},
  langid = {english},
  pmid = {6690995},
  keywords = {Brain,ERP,Evoked Potentials,Humans,N400,Reading,surprisal theory,Verbal Learning}
}

@article{kuznetsova.a:2017,
  title = {{{{\textbf{lmerTest}}}} Package: {{Tests}} in {{Linear Mixed Effects Models}}},
  shorttitle = {{{{\textbf{lmerTest}}}} {{Package}}},
  author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {82},
  number = {13},
  issn = {1548-7660},
  doi = {10.18637/jss.v082.i13},
  urldate = {2024-02-25},
  langid = {english}
}

@article{lago.s:2021,
  title = {The {{Reading Signatures}} of {{Agreement Attraction}}},
  author = {Lago, Sol and Acu{\~n}a Fari{\~n}a, Carlos and Meseguer, Enrique},
  year = {2021},
  month = nov,
  journal = {Open Mind},
  volume = {5},
  pages = {132--153},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00047},
  urldate = {2023-10-29},
  abstract = {The comprehension of subject-verb agreement shows ``attraction effects,'' which reveal that number computations can be derailed by nouns that are grammatically unlicensed to control agreement with a verb. However, previous results are mixed regarding whether attraction affects the processing of grammatical and ungrammatical sentences alike. In a large-sample eye-tracking replication of Lago et al. (2015), we support this ``grammaticality asymmetry'' by showing that the reading profiles associated with attraction depend on sentence grammaticality. In ungrammatical sentences, attraction affected both fixation durations and regressive eye-movements at the critical disagreeing verb. Meanwhile, both grammatical and ungrammatical sentences showed effects of the attractor noun number prior to the verb, in the first- and second-pass reading of the subject phrase. This contrast suggests that attraction effects in comprehension have at least two different sources: the first reflects verb-triggered processes that operate mainly in ungrammatical sentences. The second source reflects difficulties in the encoding of the subject phrase, which disturb comprehension in both grammatical and ungrammatical sentences.},
  keywords = {agreement attraction},
  file = {~/Zotfiles/lago.s2021 The Reading Signatures of Agreement Attr.pdf}
}

@incollection{lai.l:2021,
  title = {Policy Compression: {{An}} Information Bottleneck in Action Selection},
  shorttitle = {Chapter {{Five}} - {{Policy}} Compression},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Lai, Lucy and Gershman, Samuel J.},
  editor = {Federmeier, Kara D.},
  year = {2021},
  month = jan,
  series = {The {{Psychology}} of {{Learning}} and {{Motivation}}},
  volume = {74},
  pages = {195--232},
  publisher = {Academic Press},
  doi = {10.1016/bs.plm.2021.02.004},
  urldate = {2022-11-30},
  abstract = {The brain has evolved to produce a diversity of behaviors under stringent computational resource constraints. Given this limited capacity, how do biological agents balance reward maximization against the costs of representing complex action policies? In this chapter, we examine behavioral evidence for this reward-complexity trade-off. First, we introduce a theoretical framework that formalizes the idea of policy compression, or the reduction in cognitive cost of representing action policies by making them simpler. We then describe how a wide range of behavioral phenomena, including stochasticity, perseveration, response time, state and action chunking, and navigation are brought together under this framework. Finally, we discuss how our model can be used to probe the neural underpinnings of policy compression and their dysfunction in psychiatric illness.},
  langid = {english},
  keywords = {Action selection,Rational behavior,Reinforcement learning,Resource-rationality}
}

@article{lake.b:2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017},
  month = jan,
  journal = {Behavioral and Brain Sciences},
  volume = {40},
  pages = {e253},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X16001837},
  urldate = {2024-06-04},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {~/Zotfiles/lake.b2017 Building machines that learn and think l.pdf}
}

@article{lambek.j:1958,
  title = {The Mathematics of Sentence Structure},
  author = {Lambek, Joachim},
  year = {1958},
  journal = {The American Mathematical Monthly},
  volume = {65},
  number = {3},
  eprint = {2310058},
  eprinttype = {jstor},
  pages = {154--170},
  publisher = {Taylor \& Francis, Ltd. on behalf of the Mathematical Association of America},
  doi = {10.1080/00029890.1958.11989160},
  bdsk-url-2 = {https://doi.org/10.1080/00029890.1958.11989160},
  date-added = {2019-08-26 14:46:48 -0400},
  date-modified = {2021-06-25 00:48:42 -0400},
  keywords = {category theory,pregroup grammar}
}

@inproceedings{lambek.j:1999,
  title = {Type Grammar Revisited},
  booktitle = {International Conference on Logical Aspects of Computational Linguistics},
  author = {Lambek, Joachim},
  year = {1999},
  pages = {1--27},
  date-added = {2019-08-26 22:09:20 -0400},
  date-modified = {2019-08-26 22:09:55 -0400},
  organization = {Springer},
  keywords = {pregroup grammar}
}

@article{lambek.j:2001,
  title = {Type Grammars as Pregroups},
  author = {Lambek, Joachim},
  year = {2001},
  journal = {Grammars},
  volume = {4},
  pages = {21--39},
  date-added = {2019-08-26 21:51:00 -0400},
  date-modified = {2019-08-26 21:51:54 -0400},
  keywords = {pregroup grammar}
}

@article{lambek.j:2012,
  title = {Logic and Grammar},
  author = {Lambek, Joachim},
  year = {2012},
  journal = {Studia Logica: An International Journal for Symbolic Logic},
  volume = {100},
  number = {4},
  eprint = {23262129},
  eprinttype = {jstor},
  pages = {667--681},
  publisher = {Springer},
  issn = {00393215, 15728730},
  abstract = {Grammar can be formulated as a kind of substructural propositional logic. In support of this claim, we survey bare Gentzen style deductive systems and two kinds of non-commutative linear logic: intuitionistic and compact bilinear logic. We also glance at their categorical refinements.},
  date-added = {2019-08-26 21:59:20 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {pregroup grammar}
}

@incollection{lanchier.n:2017,
  title = {Basics of Measure and Probability Theory},
  booktitle = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  pages = {3--24},
  publisher = {Springer International},
  doi = {10.1007/978-3-319-50038-6_1},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6{$_{1}$}},
  date-added = {2022-04-07 10:02:11 -0400},
  date-modified = {2022-04-14 10:20:52 -0400}
}

@book{lanchier.n:2017book,
  title = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  publisher = {Springer International},
  doi = {10.1007/978-3-319-50038-6},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6},
  date-added = {2022-04-07 10:01:23 -0400},
  date-modified = {2022-04-14 10:21:03 -0400}
}

@article{laplace.p:1986,
  title = {Memoir on the Probability of the Causes of Events},
  author = {Laplace, Pierre Simon},
  translator = {Stiegler, S. M.},
  year = {1986},
  journal = {Statistical Science},
  volume = {1},
  number = {3},
  eprint = {2245476},
  eprinttype = {jstor},
  pages = {364--378},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2024-05-15},
  file = {~/Zotfiles/laplace.p1986trans Memoir on the probability of the causes.pdf}
}

@article{lari.k:1991,
  title = {Applications of Stochastic Context-Free Grammars Using the inside-Outside Algorithm},
  author = {Lari, K. and Young, S. J.},
  year = {1991},
  month = jul,
  journal = {Computer Speech \& Language},
  volume = {5},
  number = {3},
  pages = {237--257},
  issn = {0885-2308},
  doi = {10.1016/0885-2308(91)90009-F},
  urldate = {2022-07-04},
  abstract = {This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.},
  langid = {english},
  file = {~/Zotfiles/lari.k1991 Applications of stochastic context-free.pdf}
}

@inproceedings{lassiter.d:2011,
  title = {Vagueness as {{Probabilistic Linguistic Knowledge}}},
  booktitle = {Vagueness in {{Communication}}},
  author = {Lassiter, Daniel},
  editor = {Nouwen, Rick and {van Rooij}, Robert and Sauerland, Uli and Schmitz, Hans-Christian},
  year = {2011},
  pages = {127--150},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/978-3-642-18446-8_8},
  abstract = {Consideration of the metalinguistic effects of utterances involving vague terms has led Barker [1] to treat vagueness using a modified Stalnakerian model of assertion. I present a sorites-like puzzle for factual beliefs in the standard Stalnakerian model [28] and show that it can be resolved by enriching the model to make use of probabilistic belief spaces. An analogous problem arises for metalinguistic information in Barker's model, and I suggest that a similar enrichment is needed here as well. The result is a probabilistic theory of linguistic representation that retains a classical metalanguage but avoids the undesirable divorce between meaning and use inherent in the epistemic theory [34]. I also show that the probabilistic approach provides a plausible account of the sorites paradox and higher-order vagueness and that it fares well empirically and conceptually in comparison to leading competitors.},
  isbn = {978-3-642-18446-8},
  langid = {english},
  keywords = {higher-order vagueness,lexical representation,probability,Vagueness},
  file = {~/Zotfiles/lassiter.d2011 Vagueness as Probabilistic Linguistic Kn.pdf}
}

@article{laszlo.s:2012,
  title = {A Neurally Plausible {{Parallel Distributed Processing}} Model of {{Event-Related Potential}} Word Reading Data},
  author = {Laszlo, Sarah and Plaut, David C.},
  year = {2012},
  month = mar,
  journal = {Brain and Language},
  volume = {120},
  number = {3},
  pages = {271--281},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2011.09.001},
  urldate = {2025-04-15},
  abstract = {The Parallel Distributed Processing (PDP) framework has significant potential for producing models of cognitive tasks that approximate how the brain performs the same tasks. To date, however, there has been relatively little contact between PDP modeling and data from cognitive neuroscience. In an attempt to advance the relationship between explicit, computational models and physiological data collected during the performance of cognitive tasks, we developed a PDP model of visual word recognition which simulates key results from the ERP reading literature, while simultaneously being able to successfully perform lexical decision---a benchmark task for reading models. Simulations reveal that the model's success depends on the implementation of several neurally plausible features in its architecture which are sufficiently domain-general to be relevant to cognitive modeling more generally.},
  keywords = {Computational modeling,Event-Related Potentials,N400,Parallel Distributed Processing,Visual word recognition},
  file = {~/Zotfiles/laszlo.s2012 A neurally plausible Parallel Distribute.pdf}
}

@article{lau.j:2017,
  title = {Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge},
  shorttitle = {Grammaticality, Acceptability, and Probability},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2017},
  journal = {Cognitive Science},
  volume = {41},
  number = {5},
  pages = {1202--1241},
  issn = {1551-6709},
  doi = {10.1111/cogs.12414},
  urldate = {2024-05-26},
  abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
  copyright = {Copyright {\copyright} 2016 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Grammaticality,Probabilistic modeling,Syntactic knowledge},
  file = {~/Zotfiles/lau.j2017 Grammaticality, acceptability, and proba.pdf}
}

@article{laurinavichyute.a:2024,
  title = {Agreement Attraction in Grammatical Sentences and the Role of the Task},
  author = {Laurinavichyute, Anna and {von der Malsburg}, Titus},
  year = {2024},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {137},
  pages = {104525},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104525},
  urldate = {2024-10-25},
  abstract = {This study evaluates two broad classes of language processing accounts that make predictions for sentences like ``The admirer of the singer(s) apparently thinks...''. Feature distortion accounts predict increased processing difficulty at the verb in sentences with a plural distractor noun (singers) while similarity-based interference accounts predict the opposite: increased difficulty in sentences with a singular distractor noun (singer). Neither of these effects was reliably observed in earlier research, and the Bayesian meta-analysis of 31 published studies reported here is almost perfectly inconclusive. An explanation may be that both effects occur simultaneously and therefore mask each other. To test this idea, we conducted three single-trial self-paced reading experiments (N1=4,296, N2=3,920, N3=3,559) which orthogonally manipulated agreement attraction and inhibitory interference. Surprisingly, all three experiments produced evidence for agreement attraction but none for inhibitory interference, which supports feature distortion but not similarity-based interference accounts. Experiment 4 (N4=3,535) tested the role of the expected task by preparing participants for a comprehension question (vs. acceptability judgment in Experiments 1--3). It showed neither agreement attraction nor inhibitory interference effects. Our findings demonstrate that agreement attraction effects can arise in grammatical sentences -- contra earlier research -- but also that these effects crucially depend on the task. This explains inconsistent results in prior research and supports feature distortion as the driving force behind attraction effects in grammatical sentences.},
  keywords = {Adaptation,Agreement attraction,Grammaticality bias,Illusion of ungrammaticality,Sentence comprehension,Similarity-based interference,Task effects},
  file = {~/Zotfiles/laurinavichyute.a2024 Agreement attraction in grammatical sent.pdf}
}

@inproceedings{laverghetta.a:2022,
  title = {Predicting Human Psychometric Properties Using Computational Language Models},
  booktitle = {Quantitative {{Psychology}}},
  author = {Laverghetta, Antonio and Nighojkar, Animesh and Mirzakhalov, Jamshidbek and Licato, John},
  editor = {Wiberg, Marie and Molenaar, Dylan and Gonz{\'a}lez, Jorge and Kim, Jee-Seon and Hwang, Heungsun},
  year = {2022},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  pages = {151--169},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-04572-1_12},
  abstract = {Transformer-based language models (LMs) continue to achieve state-of-the-art performance on natural language processing (NLP) benchmarks, including tasks designed to mimic human-inspired ``commonsense'' competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts from psychometrics. But to what extent can benefits flow in the other direction? In other words, can LMs be of use in predicting the psychometric properties of test items, when those items are given to human participants? If so, the benefit for psychometric practitioners is enormous, as it can reduce the need for multiple rounds of empirical testing. We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the human responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions correlate. We find that transformer-based LMs predict the human psychometric data consistently well across most categories, suggesting that they can be used to gather human-like psychometric data without the need for extensive human trials.},
  isbn = {978-3-031-04572-1},
  langid = {english},
  keywords = {Classical test theory,Item response theory,Natural language processing,psychometrics},
  file = {~/Zotfiles/laverghetta.a2022 Predicting human psychometric properties.pdf}
}

@article{lavi-rotbain.o:2023,
  title = {Zipfian Distributions in Child-Directed Speech},
  author = {{Lavi-Rotbain}, Ori and Arnon, Inbal},
  year = {2023},
  month = jan,
  journal = {Open Mind},
  volume = {7},
  pages = {1--30},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00070},
  urldate = {2023-05-26},
  abstract = {Across languages, word frequency and rank follow a power law relation, forming a distribution known as the Zipfian distribution. There is growing experimental evidence that this well-studied phenomenon may be beneficial for language learning. However, most investigations of word distributions in natural language have focused on adult-to-adult speech: Zipf's law has not been thoroughly evaluated in child-directed speech (CDS) across languages. If Zipfian distributions facilitate learning, they should also be found in CDS. At the same time, several unique properties of CDS may result in a less skewed distribution. Here, we examine the frequency distribution of words in CDS in three studies. We first show that CDS is Zipfian across 15 languages from seven language families. We then show that CDS is Zipfian from early on (six-months) and across development for five languages with sufficient longitudinal data. Finally, we show that the distribution holds across different parts of speech: Nouns, verbs, adjectives and prepositions follow a Zipfian distribution. Together, the results show that the input children hear is skewed in a particular way from early on, providing necessary (but not sufficient) support for the postulated learning advantage of such skew. They highlight the need to study skewed learning environments experimentally.}
}

@inproceedings{lawson.d:2022,
  title = {{{SIXO}}: Smoothing Inference with Twisted Objectives},
  shorttitle = {{{SIXO}}},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Neural Information Processing Systems}}},
  author = {Lawson, Dieterich and Ravent{\'o}s, Allan and Warrington, Andrew and Linderman, Scott},
  year = {2022},
  month = nov,
  series = {{{NIPS}} '22},
  pages = {38844--38858},
  publisher = {Curran Associates Inc.},
  address = {Red Hook, NY, USA},
  urldate = {2025-07-30},
  abstract = {Sequential Monte Carlo (SMC) is an inference algorithm for state space models that approximates the posterior by sampling from a sequence of target distributions. The target distributions are often chosen to be the filtering distributions, but these ignore information from future observations, leading to practical and theoretical limitations in inference and model learning. We introduce SIXO, a method that instead learns target distributions that approximate the smoothing distributions, incorporating information from all observations. The key idea is to use density ratio estimation to fit functions that warp the filtering distributions into the smoothing distributions. We then use SMC with these learned targets to define a variational objective for model and proposal learning. SIXO yields provably tighter log marginal lower bounds and offers more accurate posterior inferences and parameter estimates in a variety of domains.},
  isbn = {978-1-7138-7108-8},
  file = {~/Zotfiles/lawson.d2022 SIXO smoothing inference with twisted o.pdf}
}

@book{lazore.d:1993book,
  title = {The {{Mohawk}} Language Standardisation Project Conference Report, Aug. 9-10, 1993},
  author = {Lazore, Dorothy Karihw{\'e}nhawe},
  editor = {Jacobs, Annette Kaia'tit{\'a}hkhe and Thompson, Nancy Kahawin{\'o}nkie and Leaf, Minnie Kai{\`a}:khons},
  year = {1993},
  month = aug,
  publisher = {{Literacy and Basic Skills Section, Ministry of Education and Training}},
  address = {Toronto},
  date-added = {2022-05-03 17:07:15 -0400},
  date-modified = {2022-05-03 17:17:25 -0400},
  isbn = {0-7778-6105-4},
  langid = {english},
  organization = {Mohawk Language Standardisation Conference (1993 : Tyendinaga Indian Reserve)},
  keywords = {kanien'keha,mohawk language,standardization}
}

@inproceedings{le.t:2020,
  title = {Revisiting {{Reweighted Wake-Sleep}} for {{Models}} with {{Stochastic Control Flow}}},
  booktitle = {Proceedings of {{The}} 35th {{Uncertainty}} in {{Artificial Intelligence Conference}}},
  author = {Le, Tuan Anh and Kosiorek, Adam R. and Siddharth, N. and Teh, Yee Whye and Wood, Frank},
  year = {2020},
  month = aug,
  pages = {1039--1049},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-08-09},
  abstract = {Stochastic control-flow models (SCFMs) are a class of generative models that involve branching on choices from discrete random variables. Amortized gradient-based learning of SCFMs is challenging as most approaches targeting discrete variables rely on their continuous relaxations---which can be intractable in SCFMs, as branching on relaxations requires evaluating all (exponentially many) branching paths. Tractable alternatives mainly combine REINFORCE with complex control-variate schemes to improve the variance of naive estimators. Here, we revisit the reweighted wake-sleep (RWS) [5] algorithm, and through extensive evaluations, show that it outperforms current state-of-the-art methods in learning SCFMs. Further, in contrast to the importance weighted autoencoder, we observe that RWS learns better models and inference networks with increasing numbers of particles. Our results suggest that RWS is a competitive, often preferable, alternative for learning SCFMs.},
  langid = {english},
  file = {~/Zotfiles/le.t2020 Revisiting Reweighted Wake-Sleep for Mod.pdf}
}

@article{lebesgue.h:1902,
  title = {{Int{\'e}grale, Longueur, Aire}},
  author = {Lebesgue, H.},
  year = {1902},
  month = dec,
  journal = {Annali di Matematica Pura ed Applicata (1898-1922)},
  volume = {7},
  number = {1},
  pages = {231--359},
  issn = {0373-3114},
  doi = {10.1007/BF02420592},
  urldate = {2022-06-22},
  langid = {french}
}

@inproceedings{lebrun.b:2022,
  title = {Evaluating Distributional Distortion in Neural Language Modeling},
  booktitle = {International Conference on Learning Representations},
  author = {LeBrun, Benjamin and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022}
}

@article{leemis.l:2008,
  title = {Univariate {{Distribution Relationships}}},
  author = {Leemis, Lawrence M. and McQueston, Jacquelyn T.},
  year = {2008},
  month = feb,
  journal = {The American Statistician},
  volume = {62},
  number = {1},
  pages = {45--53},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313008X270448},
  urldate = {2022-06-20},
  abstract = {Probability distributions are traditionally treated separately in introductory mathematical statistics textbooks. A figure is presented here that shows properties that individual distributions possess and many of the relationships between these distributions.},
  keywords = {Asymptotic relationships,Distribution properties,Limiting distributions,Stochastic parameters,Transformations},
  file = {~/Zotfiles/leemis.l2008 Univariate Distribution Relationships.pdf}
}

@book{legate.j:2014book,
  title = {Voice and v: {{Lessons}} from Acehnese},
  author = {Legate, Julie Anne},
  year = {2014},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9780262028141.001.0001},
  bdsk-url-2 = {https://doi.org/10.7551/mitpress/9780262028141.001.0001},
  date-added = {2021-03-22 00:34:12 -0400},
  date-modified = {2021-03-22 13:11:36 -0400},
  keywords = {argument structure,voice}
}

@article{legate.j:2020,
  title = {On Passives of Passives},
  author = {Legate, Julie Anne and Akku{\c s}, Faruk and {\v S}ereikait{\.e}, Milena and Ringe, Don},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {4},
  pages = {771--818},
  publisher = {Project Muse},
  doi = {10.1353/lan.2020.0062},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0062},
  date-added = {2021-03-20 12:21:19 -0400},
  date-modified = {2021-03-20 12:21:35 -0400},
  keywords = {argument structure,passives}
}

@article{leivada.e:2020,
  title = {Language {{Processing}} at {{Its Trickiest}}: {{Grammatical Illusions}} and {{Heuristics}} of {{Judgment}}},
  shorttitle = {Language {{Processing}} at {{Its Trickiest}}},
  author = {Leivada, Evelina},
  year = {2020},
  month = sep,
  journal = {Languages},
  volume = {5},
  number = {3},
  pages = {29},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2226-471X},
  doi = {10.3390/languages5030029},
  urldate = {2023-10-20},
  abstract = {Humans are intuitively good at providing judgments about what forms part of their native language and what does not. Although such judgments are robust, consistent, and reliable, human cognition is demonstrably fallible to illusions of various types. Language is no exception. In the linguistic domain, several types of sentences have been shown to trick the parser into giving them a high acceptability judgment despite their ill-formedness. One example is the so-called comparative illusion (`More people have been to Troms{\o} than I have'). To this day, comparative illusions have been tested mainly with monolingual, neurotypical speakers of English. The present research aims to broaden our understanding of this phenomenon by putting it to test in two populations that differ in one crucial factor: the number of languages they speak. A timed acceptability judgment task was administered to monolingual speakers of Standard Greek and bi(dia)lectal speakers of Standard and Cypriot Greek. The results are not fully in line with any of the semantic re-analyses proposed for the illusion so far, hence a new proposal is offered about what interpretation induces the illusion, appreciating the influence of both grammatical processing and cognitive heuristics. Second, the results reveal an effect of developmental trajectory. This effect may be linked to an enhanced ability to spot the illusion in bi(dia)lectals, but several factors can be identified as possible culprits behind this result. After discussing each of them, it is argued that having two grammars may facilitate the setting of a higher processing threshold, something that would entail decreased fallibility to grammatical illusions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {acceptability judgments,bilectalism,grammatical illusions,parsing,processing,reaction times},
  file = {~/Zotfiles/leivada.e2020 Language Processing at Its Trickiest Gr.pdf}
}

@article{lemoine.n:2019,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {1600-0706},
  doi = {10.1111/oik.05985},
  urldate = {2024-05-23},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  copyright = {{\copyright} 2019 The Authors},
  langid = {english},
  keywords = {Bayesian statistics,frequentist statistics,Markov chain Monte Carlo,vague priors},
  file = {~/Zotfiles/lemoine.n2019 Moving beyond noninformative priors why.pdf}
}

@misc{lenth.r:2024,
  title = {{{{\textbf{emmeans}}}}: Estimated Marginal Means, Aka Least-Squares Means},
  author = {Lenth, Russell V.},
  year = {2024}
}

@book{levelt.w:1974book,
  title = {Formal Grammars in Linguistics and Psycholinguistics: {{Volume}} 3: {{Psycholinguistic}} Applications},
  author = {Levelt, Willem JM},
  year = {1974},
  series = {{{JANUA LINGUARUM}}},
  volume = {192},
  publisher = {Mouton},
  address = {The Hague},
  date-added = {2019-06-11 14:51:49 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{levenshtein.v:1966,
  title = {{Binary codes capable of correcting deletions, insertions and reversals}},
  author = {Levenshtein, V. I.},
  translator = {Novikov, P. S.},
  year = {1966},
  month = feb,
  journal = {Soviet Physics Doklady},
  volume = {10},
  number = {8},
  pages = {707--710},
  urldate = {2025-02-04},
  langid = {russian}
}

@inproceedings{leviathan.y:2023,
  title = {Fast {{Inference}} from {{Transformers}} via {{Speculative Decoding}}},
  booktitle = {Proceedings of the 40th {{International Conference}} on {{Machine Learning}}},
  author = {Leviathan, Yaniv and Kalman, Matan and Matias, Yossi},
  year = {2023},
  month = jul,
  pages = {19274--19286},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-05-23},
  abstract = {Inference from large autoregressive models like Transformers is slow - decoding K tokens takes K serial runs of the model. In this work we introduce speculative decoding - an algorithm to sample from autoregressive models faster without any changes to the outputs, by computing several tokens in parallel. At the heart of our approach lie the observations that (1) hard language-modeling tasks often include easier subtasks that can be approximated well by more efficient models, and (2) using speculative execution and a novel sampling method, we can make exact decoding from the large models faster, by running them in parallel on the outputs of the approximation models, potentially generating several tokens concurrently, and without changing the distribution. Our method can accelerate existing off-the-shelf models without retraining or architecture changes. We demonstrate it on T5-XXL and show a 2X-3X acceleration compared to the standard T5X implementation, with identical outputs.},
  langid = {english},
  file = {~/Zotfiles/leviathan.y2023 Fast Inference from Transformers via Spe.pdf}
}

@incollection{levin.b:2005,
  title = {Argument Realization: {{Research}} Surveys in Linguistics},
  booktitle = {Argument Realization: {{Research}} Surveys in Linguistics},
  author = {Levin, Beth and Rappaport Hovav, Malka},
  year = {2005},
  publisher = {Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:20 -0400},
  readinglist = {Thesis}
}

@inproceedings{levshina.n:2017,
  title = {Communicative Efficiency and Syntactic Predictability: {{A}} Cross-Linguistic Study Based on the {{Universal Dependencies}} Corpora},
  booktitle = {Proceedings of the {{NoDaLiDa}} 2017 Workshop on Universal Dependencies, 22 May, Gothenburg Sweden},
  author = {Levshina, Natalia},
  year = {2017},
  number = {135},
  pages = {72--78},
  date-added = {2020-04-05 12:14:22 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  organization = {Link{\"o}ping University Electronic Press},
  project = {syntactic embedding},
  keywords = {dependency structures,information theory,random forests}
}

@inproceedings{levy.o:2014,
  title = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27 ({{NIPS}} 2014)},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  year = {2014},
  month = dec,
  pages = {2177--2185},
  address = {Montr{\'e}al, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyG14.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{levy.o:2014a,
  title = {Dependency-Based Word Embeddings},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Levy, Omer and Goldberg, Yoav},
  year = {2014},
  pages = {302--308},
  publisher = {Association for Computational Linguistics},
  address = {Baltimore, Maryland},
  doi = {10.3115/v1/P14-2050},
  bdsk-url-2 = {https://doi.org/10.3115/v1/P14-2050}
}

@phdthesis{levy.r:2005phd,
  title = {Probabilistic Models of Word Order and Syntactic Discontinuity},
  author = {Levy, Roger},
  year = {2005},
  abstract = {This thesis takes up the problem of syntactic comprehension, or parsing---how an agent (human or machine) with knowledge of a specific language goes about inferring the hierarchical structural relationships underlying a surface string in the language. I take the position that probabilistic models of combining evidential information are cognitively plausible and practically useful for syntactic comprehension. In particular, the thesis applies probabilistic methods in investigating the relationship between word order and psycholinguistic models of comprehension; and in the practical problems of accuracy and efficiency in parsing sentences with syntactic discontinuity. On the psychological side, the thesis proposes a theory of expectation-based processing difficulty as a consequence of probabilistic syntactic disambiguation: the ease of processing a word during comprehension is determined primarily by the degree to which that word is expected. I identify a class of syntactic phenomena, associated primarily with verb-final clause order, where the predictions of expectation-based processing diverge most sharply from more established locality-based theories of processing difficulty. Using existing probabilistic parsing algorithms and syntactically annotated data sources, I show that the expectation-based theory matches a range of established experimental psycholinguistic results better than locality-based theories. The comparison of probabilistic- and locality-driven processing theories is a crucial area of psycholinguistic research due to its implications for the relationship between linguistic production and comprehension, and more generally for theories of modularity in cognitive science. The thesis also takes up the problem of probabilistic models for discontinuous constituency, when phrases do not consist of continuous substrings of a sentence. Discontinuity poses a computational challenge in parsing, because it expands the set of possible substructures in a sentence beyond the bound, quadratic in sentence length, on the set of possible continuous constituents. For discontinuous constituency, I investigate the problem of accuracy employing discriminative classifiers organized on principles of syntactic theory and used to introduce discontinuous relationships into otherwise strictly context-free phrase structure trees; and the problem of efficiency in joint inference over both continuous and discontinuous structures, using probabilistic instantiations of mildly context-sensitive grammatical formalisms and factorizing grammatical generalizations into probabilistic components of dominance and linear order.},
  date-added = {2021-09-18 22:16:45 -0400},
  date-modified = {2022-04-04 13:25:27 -0400},
  isbn = {978-0-542-28638-4},
  school = {Stanford University},
  keywords = {Applied sciences,Cognitive psychology,Cognitive therapy,Computer science,Language,Linguistics,literature and linguistics,Natural language processing,Parsing,Probabilistic,Psychology,Syntactic discontinuity,Word order},
  file = {~/Zotfiles/levy.r2005phd Probabilistic models of word order and s.pdf}
}

@inproceedings{levy.r:2006,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems},
  author = {Levy, Roger and Jaeger, T. Florian},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John C. and Hofmann, Thomas},
  year = {2006},
  pages = {849--856},
  publisher = {MIT Press},
  address = {Vancouver, British Columbia, Canada},
  biburl = {https://dblp.org/rec/conf/nips/LevyJ06.bib},
  keywords = {uniform information density},
  file = {~/Zotfiles/levy.r2006 Speakers optimize information density th.pdf}
}

@article{levy.r:2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159--166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2007.05.006},
  date-added = {2021-01-14 13:02:24 -0500},
  date-modified = {2021-03-09 22:53:26 -0500},
  keywords = {Frequency,Information theory,Parsing,Prediction,processing,Sentence processing,surprisal,Syntactic complexity,Syntax,Word order},
  file = {~/Zotfiles/levy.r2008 Expectation-based syntactic comprehensio.pdf}
}

@inproceedings{levy.r:2008noisy,
  title = {A Noisy-Channel Model of Human Sentence Comprehension under Uncertain Input},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Levy, Roger},
  year = {2008},
  month = oct,
  pages = {234--243},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  date-added = {2022-04-11 23:17:10 -0400},
  date-modified = {2022-04-11 23:17:30 -0400},
  file = {~/Zotfiles/levy.r2008noisy A noisy-channel model of human sentence.pdf}
}

@inproceedings{levy.r:2008particle,
  title = {Modeling the Effects of Memory on Human Online Sentence Processing with Particle Filters},
  booktitle = {Proceedings of the Twenty-Second Annual {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Levy, Roger and Reali, Florencia and Griffiths, Thomas L.},
  editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, L{\'e}on},
  year = {2008},
  month = dec,
  pages = {937--944},
  publisher = {Curran Associates, Inc.},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyRG08.bib},
  date-modified = {2022-05-12 19:43:45 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {~/Zotfiles/levy.r2008particle Modeling the effects of memory on human.pdf}
}

@article{levy.r:2009PNAS,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  year = {2009},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {50},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0907664106},
  pages = {21086--21090},
  doi = {10.1073/pnas.0907664106},
  abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.0907664106},
  date-added = {2022-04-27 22:17:38 -0400},
  date-modified = {2022-04-27 22:18:16 -0400},
  keywords = {memory,noisy channel coding}
}

@inproceedings{levy.r:2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Levy, Roger},
  year = {2011},
  month = jun,
  pages = {1055--1065},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  date-added = {2022-04-27 08:47:55 -0400},
  date-modified = {2022-04-27 08:49:23 -0400},
  keywords = {noisy channel coding},
  file = {~/Zotfiles/levy.r2011 Integrating surprisal and uncertain-inpu.pdf}
}

@article{levy.r:2012,
  title = {The Processing of Extraposed Structures in {{English}}},
  author = {Levy, Roger and Fedorenko, Evelina and Breen, Mara and Gibson, Edward},
  year = {2012},
  month = jan,
  journal = {Cognition},
  volume = {122},
  number = {1},
  pages = {12--36},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2011.07.012},
  urldate = {2022-10-24},
  abstract = {In most languages, most of the syntactic dependency relations found in any given sentence are projective: the word--word dependencies in the sentence do not cross each other. Some syntactic dependency relations, however, are non-projective: some of their word--word dependencies cross each other. Non-projective dependencies are both rarer and more computationally complex than projective dependencies; hence, it is of natural interest to investigate whether there are any processing costs specific to non-projective dependencies, and whether factors known to influence processing of projective dependencies also affect non-projective dependency processing. We report three self-paced reading studies, together with corpus and sentence completion studies, investigating the comprehension difficulty associated with the non-projective dependencies created by the extraposition of relative clauses in English. We find that extraposition over either verbs or prepositional phrases creates comprehension difficulty, and that this difficulty is consistent with probabilistic syntactic expectations estimated from corpora. Furthermore, we find that manipulating the expectation that a given noun will have a postmodifying relative clause can modulate and even neutralize the difficulty associated with extraposition. Our experiments rule out accounts based purely on derivational complexity and/or dependency locality in terms of linear positioning. Our results demonstrate that comprehenders maintain probabilistic syntactic expectations that persist beyond projective-dependency structures, and suggest that it may be possible to explain observed patterns of comprehension difficulty associated with extraposition entirely through probabilistic expectations.},
  langid = {english},
  keywords = {Frequency,Memory and language,Parsing,Prediction,Self-paced reading,Sentence comprehension,surprisal,surprisal theory,Syntactic complexity,Word order}
}

@article{levy.r:2013a,
  title = {The Syntactic Complexity of {{Russian}} Relative Clauses},
  author = {Levy, Roger and Fedorenko, Evelina and Gibson, Edward},
  year = {2013},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {69},
  number = {4},
  pages = {461--495},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.10.005},
  urldate = {2023-03-09},
  abstract = {Although syntactic complexity has been investigated across dozens of studies, the available data still greatly underdetermine relevant theories of processing difficulty. Memory-based and expectation-based theories make opposite predictions regarding fine-grained time course of processing difficulty in syntactically constrained contexts, and each class of theory receives support from results on some constructions in some languages. Here we report four self-paced reading experiments on the online comprehension of Russian relative clauses together with related corpus studies, taking advantage of Russian's flexible word order to disentangle predictions of competing theories. We find support for key predictions of memory-based theories in reading times at RC verbs, and for key predictions of expectation-based theories in processing difficulty at RC-initial accusative noun phrase (NP) objects, which corpus data suggest should be highly unexpected. These results suggest that a complete theory of syntactic complexity must integrate insights from both expectation-based and memory-based theories.},
  langid = {english},
  keywords = {Expectation-based processing,Memory limitations in language processing,Parsing,Russian,Sentence comprehension,Syntax},
  file = {~/Zotfiles/levy.r2013jml The syntactic complexity of Russian rela.pdf}
}

@article{levy.r:2013b,
  title = {Surprisal, the {{PDC}}, and the Primary Locus of Processing Difficulty in Relative Clauses},
  author = {Levy, Roger and Gibson, Edward},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  urldate = {2023-03-08},
  file = {~/Zotfiles/levy.r2013opinion Surprisal, the PDC, and the primary locu.pdf}
}

@incollection{levy.r:2013c,
  title = {Memory and Surprisal in Human Sentence Comprehension},
  booktitle = {Sentence Processing},
  author = {Levy, Roger},
  editor = {{van Gompel}, Roger P. G.},
  year = {2013},
  month = sep,
  edition = {1},
  pages = {78--114},
  publisher = {Psychology Press},
  address = {London},
  isbn = {978-0-203-48845-4},
  annotation = {note: Corrected version of 4 October 2015},
  file = {~/Zotfiles/levy.r2013 Memory and surprisal in human sentence c.pdf}
}

@unpublished{levy.r:2013ms,
  type = {Unpublished Manuscript},
  title = {Why Grammar Is Probabilistic},
  author = {Levy, Roger},
  year = {2013},
  file = {~/Zotfiles/levy.r2013ms Why grammar is probabilistic.pdf}
}

@inproceedings{levy.r:2018cogsci,
  title = {Communicative Efficiency, Uniform Information Density, and the Rational Speech Act Theory},
  booktitle = {Proceedings of the 40th Annual Meeting of the Cognitive Science Society},
  author = {Levy, Roger},
  editor = {Kalish, Chuck and Martina Rau, Jerry Zhu and Rogers, Timothy},
  year = {2018},
  month = jul,
  pages = {684--689},
  address = {Madison, Wisconsin, USA},
  file = {~/Zotfiles/levy.r2018cogsci Communicative efficiency, uniform inform.pdf}
}

@unpublished{levy.r:2020ms,
  type = {Unpublished Manuscript},
  title = {How Structural Commitments Magnify Surprisal},
  author = {Levy, Roger},
  year = {2020},
  langid = {english},
  file = {~/Zotfiles/levy.r2020ms How Structural Commitments Magnify Surpr.pdf}
}

@inproceedings{lew.a:2020cogsci,
  title = {Leveraging {{Unstructured Statistical Knowledge}} in a {{Probabilistic Language}} of {{Thought}}},
  booktitle = {Proceedings of the 42nd {{Annual}} Meeting of the {{Cognitive Science Society}}},
  author = {Lew, Alexander K. and Tessler, Michael Henry and Mansinghka, Vikash K. and Tenenbaum, Joshua B.},
  year = {2020},
  month = jan,
  urldate = {2023-08-04},
  abstract = {One hallmark of human reasoning is that we can bring to bear a diverse web of common-sense knowledge in any situation. The vastness of our knowledge poses a challenge for the practical implementation of reasoning systems as well as for our cognitive theories -- how do people represent their common-sense knowledge? On the one hand, our best models of sophisticated reasoning are top-down, making use primarily of symbolically-encoded knowledge. On the other, much of our understanding of the statistical properties of our environment may arise in a bottom-up fashion, for example through asso- ciationist learning mechanisms. Indeed, recent advances in AI have enabled the development of billion-parameter language models that can scour for patterns in gigabytes of text from the web, picking up a surprising amount of common-sense knowledge along the way---but they fail to learn the structure of coherent reasoning. We propose combining these approaches, by embedding language-model-backed primitives into a state- of-the-art probabilistic programming language (PPL). On two open-ended reasoning tasks, we show that our PPL models with neural knowledge components characterize the distribution of human responses more accurately than the neural language models alone, raising interesting questions about how people might use language as an interface to common-sense knowledge, and suggesting that building probabilistic models with neural language-model components may be a promising approach for more human-like AI.},
  langid = {english},
  file = {~/Zotfiles/lew.a2020cogsci Leveraging Unstructured Statistical Know.pdf}
}

@inproceedings{lew.a:2022RAVI,
  title = {Recursive {{Monte Carlo}} and Variational Inference with Auxiliary Variables},
  booktitle = {Proceedings of the {{Thirty-Eighth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Lew, Alexander K. and {Cusumano-Towner}, Marco and Mansinghka, Vikash K.},
  year = {2022},
  month = aug,
  pages = {1096--1106},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-06},
  abstract = {A key design constraint when implementing Monte Carlo and variational inference algorithms is that it must be possible to cheaply and exactly evaluate the marginal densities of proposal distributions and variational families. This takes many interesting proposals off the table, such as those based on involved simulations or stochastic optimization. This paper broadens the design space, by presenting a framework for applying Monte Carlo and variational inference algorithms when proposal densities cannot be exactly evaluated. Our framework, recursive auxiliary-variable inference (RAVI), instead approximates the necessary densities using meta-inference: an additional layer of Monte Carlo or variational inference, that targets the proposal, rather than the model. RAVI generalizes and unifies several existing methods for inference with expressive approximating families, which we show correspond to specific choices of meta-inference algorithm, and provides new theory for analyzing their bias and variance. We illustrate RAVI's design framework and theorems by using them to analyze and improve upon Salimans et al.'s Markov Chain Variational Inference, and to design a novel sampler for Dirichlet process mixtures, achieving state-of-the-art results on a standard benchmark dataset from astronomy and on a challenging datacleaning task with Medicare hospital data.},
  langid = {english},
  file = {~/Zotfiles/lew.a2022RAVI Recursive Monte Carlo and variational in 2.pdf;~/Zotfiles/lew.a2022RAVI Recursive Monte Carlo and variational in.pdf}
}

@misc{lew.a:2023LLaMPPL,
  title = {Sequential {{Monte Carlo}} Steering of Large Language Models Using Probabilistic Programs},
  author = {Lew, Alexander K. and {Zhi-Xuan}, Tan and Grand, Gabriel and Mansinghka, Vikash K.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.03081},
  eprint = {2306.03081},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03081},
  urldate = {2023-06-08},
  abstract = {Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (https://github.com/probcomp/LLaMPPL), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Programming Languages,Statistics - Computation},
  annotation = {note: Presented at ICML 2023 Workshop: Sampling and Optimization in Discrete Space},
  file = {~/Zotfiles/lew.a2023LLaMPPL Sequential Monte Carlo steering of large 2.pdf;~/Zotfiles/lew.a2023LLaMPPL Sequential Monte Carlo steering of large.pdf}
}

@misc{lew.a:2023SMCP3,
  title = {{{SMCP}}{\textsuperscript{3}}: {{Sequential Monte Carlo}} with {{Probabilistic Program Proposals}}},
  shorttitle = {Smcp3},
  author = {Lew, Alexander K. and Matheos, George and Ghavamizadeh, Matin and Gothoskar, Nishad and Russell, Stuart and Mansinghka, Vikash K.},
  year = {2023},
  abstract = {There is a widespread need for sound, flexible frameworks for Monte Carlo inference. This paper introduces SMCP3, a sequential Monte Carlo framework that broadens the class of strategies practitioners can employ to update particles from iteration to iteration, relative to existing frameworks like resample-move SMC (Gilks \& Berzuini, 2001) and SMC samplers (Del Moral et al., 2006). In SMCP3, proposal kernels can be general probabilistic programs, which differ from traditional proposal densities in that they may sample many auxiliary variables, and may apply deterministic post-processing to calculate a proposed update. We have implemented our framework in the Gen probabilistic programming platform: given probabilistic programs that specify target distributions, forward kernels, and reverse kernels, our implementation fully automates the sound computation of incremental importance weights. To illustrate the effectiveness of SMCP3 algorithms, we apply our framework in two domains. First, we use it for online state-estimation, using proposal programs based on Langevin ascent to reduce the bias in log marginal likelihood estimates relative to resample-move SMC with Langevin rejuvenation. Second, we demonstrate an SMCP3 algorithm that yields more robust online clustering in Dirichlet process mixture models than strong SMC baselines.},
  file = {~/Zotfiles/lew.a2023SMCP3 SMCP3 Sequential Monte Carlo 2.pdf;~/Zotfiles/lew.a2023SMCP3 SMCP3 Sequential Monte Carlo.pdf}
}

@phdthesis{lew.a:2025phd,
  title = {Automatic {{Integration}} and {{Differentiation}} of  {{Probabilistic Programs}}},
  author = {Lew, Alexander K.},
  year = {2025},
  month = may,
  school = {Massachusetts Institute of Technology},
  file = {~/Zotfiles/lew.a2025phd Automatic Integration and Differentiatio.pdf}
}

@article{lewandowski.d:2009,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  year = {2009},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.04.008},
  urldate = {2024-05-21},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276--294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177--2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
  keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation}
}

@article{lewis.m:2013,
  title = {Combined Distributional and Logical Semantics},
  author = {Lewis, Mike and Steedman, Mark},
  year = {2013},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {179--192},
  doi = {10.1162/tacl_a_00219},
  file = {~/Zotfiles/lewis.m2014 Combined distributional and logical sema.pdf}
}

@inproceedings{lewis.m:2020,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  pages = {7871--7880},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.703}
}

@article{lewis.r:2005,
  title = {An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval},
  author = {Lewis, Richard L. and Vasishth, Shravan},
  year = {2005},
  journal = {Cognitive Science},
  volume = {29},
  number = {3},
  pages = {375--419},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_25},
  urldate = {2022-07-15},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated control structure that subserve sentence comprehension. The theory is derived from the application of independently motivated principles of memory and cognitive skill to the specialized task of sentence parsing. The resulting theory construes sentence processing as a series of skilled associative memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive principles are formalized in computational form in the Adaptive Control of Thought--Rational (ACT--R) architecture, and our process model is realized in ACT--R. We present the results of 6 sets of simulations: 5 simulation sets provide quantitative accounts of the effects of length and structural interference on both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-embedding difficulty is a novel one that derives from the model' complete reliance on discriminating retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT--R defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference are the key factors shaping working memory in sentence processing. We contrast the theory and empirical predictions with several related accounts of sentence-processing complexity.},
  langid = {english},
  keywords = {ACT-R,Activation,Cognitive architectures,Cognitive modeling,Decay,Interference,Parsing,Sentence processing,Syntax,Working memory},
  file = {~/Zotfiles/lewis.r2005 An activation-based model of sentence pr.pdf}
}

@article{lewis.r:2006,
  title = {Computational Principles of Working Memory in Sentence Comprehension},
  author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
  year = {2006},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {10},
  pages = {447--454},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.08.007},
  urldate = {2022-09-08},
  abstract = {Understanding a sentence requires a working memory of the partial products of comprehension, so that linguistic relations between temporally distal parts of the sentence can be rapidly computed. We describe an emerging theoretical framework for this working memory system that incorporates several independently motivated principles of memory: a sharply limited attentional focus, rapid retrieval of item (but not order) information subject to interference from similar items, and activation decay (forgetting over time). A computational model embodying these principles provides an explanation of the functional capacities and severe limitations of human processing, as well as accounts of reading times. The broad implication is that the detailed nature of crosslinguistic sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension.},
  langid = {english},
  keywords = {ACT-R,comprehension,memory,sentence processing},
  file = {~/Zotfiles/lewis.r2006 Computational principles of working memo.pdf}
}

@article{lewis.r:2014,
  title = {Computational Rationality: {{Linking}} Mechanism and Behavior through Bounded Utility Maximization},
  author = {Lewis, Richard L. and Howes, Andrew and Singh, Satinder},
  year = {2014},
  journal = {Topics in cognitive science},
  volume = {6},
  number = {2},
  pages = {279--311},
  publisher = {Wiley Online Library},
  doi = {10.1111/tops.12086},
  isbn = {1756-8757}
}

@article{lewis.s:2015,
  title = {Aligning {{Grammatical Theories}} and {{Language Processing Models}}},
  author = {Lewis, Shevaun and Phillips, Colin},
  year = {2015},
  month = feb,
  journal = {Journal of Psycholinguistic Research},
  volume = {44},
  number = {1},
  pages = {27--46},
  issn = {1573-6555},
  doi = {10.1007/s10936-014-9329-z},
  urldate = {2023-08-01},
  abstract = {We address two important questions about the relationship between theoretical linguistics and psycholinguistics. First, do grammatical theories and language processing models describe separate cognitive systems, or are they accounts of different aspects of the same system? We argue that most evidence is consistent with the one-system view. Second, how should we relate grammatical theories and language processing models to each other?},
  langid = {english},
  keywords = {Abstraction,Cognitive architecture of language,grammatical illusions,Grammatical theories,Parsing},
  file = {~/Zotfiles/lewis.s2015 Aligning Grammatical Theories and Langua.pdf}
}

@inproceedings{li.b:2020,
  title = {Heads-up! {{Unsupervised}} Constituency Parsing via Self-Attention Heads},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  author = {Li, Bowen and Kim, Taeuk and Amplayo, Reinald Kim and Keller, Frank},
  year = {2020},
  pages = {409--424},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China}
}

@phdthesis{li.b:2022phd,
  title = {Integrating Linguistic Theory and Neural Language Models},
  author = {Li, Bai},
  year = {2022},
  month = jul,
  eprint = {2207.09643},
  primaryclass = {cs},
  address = {Toronto},
  urldate = {2022-07-22},
  abstract = {Transformer-based language models have recently achieved remarkable results in many natural language tasks. However, performance on leaderboards is generally achieved by leveraging massive amounts of training data, and rarely by encoding explicit linguistic knowledge into neural models. This has led many to question the relevance of linguistics for modern natural language processing. In this dissertation, I present several case studies to illustrate how theoretical linguistics and neural language models are still relevant to each other. First, language models are useful to linguists by providing an objective tool to measure semantic distance, which is difficult to do using traditional methods. On the other hand, linguistic theory contributes to language modelling research by providing frameworks and sources of data to probe our language models for specific aspects of language understanding. This thesis contributes three studies that explore different aspects of the syntax-semantics interface in language models. In the first part of my thesis, I apply language models to the problem of word class flexibility. Using mBERT as a source of semantic distance measurements, I present evidence in favour of analyzing word class flexibility as a directional process. In the second part of my thesis, I propose a method to measure surprisal at intermediate layers of language models. My experiments show that sentences containing morphosyntactic anomalies trigger surprisals earlier in language models than semantic and commonsense anomalies. Finally, in the third part of my thesis, I adapt several psycholinguistic studies to show that language models contain knowledge of argument structure constructions. In summary, my thesis develops new connections between natural language processing, linguistic theory, and psycholinguistics to provide fresh perspectives for the interpretation of language models.},
  archiveprefix = {arXiv},
  school = {University of Toronto},
  keywords = {Computer Science - Computation and Language}
}

@misc{li.j:2016a,
  title = {Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author = {Li, Jiwei and Jurafsky, Dan},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1601.00372},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1601.00372},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:37:17 -0400},
  date-modified = {2022-05-15 15:40:00 -0400},
  keywords = {beam search,diversity},
  file = {~/Zotfiles/li.j2016a Mutual information and diverse decoding.pdf}
}

@misc{li.j:2016b,
  title = {A Simple, Fast Diverse Decoding Algorithm for Neural Generation},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1611.08562},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.08562},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:39:25 -0400},
  date-modified = {2022-05-15 15:40:09 -0400},
  keywords = {beam search,diversity},
  file = {~/Zotfiles/li.j2016b A simple, fast diverse decoding algorith.pdf}
}

@article{li.j:2023,
  title = {Heuristic Interpretation as Rational Inference: {{A}} Computational Model of the {{N400}} and {{P600}} in Language Processing},
  shorttitle = {Heuristic Interpretation as Rational Inference},
  author = {Li, Jiaxuan and Ettinger, Allyson},
  year = {2023},
  month = apr,
  journal = {Cognition},
  volume = {233},
  pages = {105359},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105359},
  urldate = {2025-05-12},
  abstract = {Much inquiry in psycholinguistics has focused on evidence from the N400 and P600 components of the event-related potential (ERP) signal---and a central theoretical challenge in this area is accounting for the so-called ``semantic P600'', which involves unexpected patterns in these components relative to traditional theories of the underlying mechanisms. In this paper we present a computational model of the language processing mechanisms underlying these ERP components, which builds on existing psycholinguistic theories in positing a heuristic interpretation stage of processing, but which deviates from existing theories in formulating this heuristic interpretation process as probabilistic selection via a noisy channel model, and in quantifying and accounting for fine-grained variation in statistical and representational properties of individual stimuli. Our model successfully simulates N400 and P600 patterns from eight psycholinguistic experiments, reflecting the full range of N400-only, P600-only, and biphasic N400-P600 effects, and its behaviors shed light on a number of key patterns that have presented challenges for existing theories. The model's success indicates that a strong account for the processing mechanisms underlying these effects is one in which language comprehension involves a probabilistic heuristic interpretation stage resembling a noisy channel process, feeding into subsequent processes that assess target word fit and reconcile between heuristic and literal interpretations. The model's success also indicates that these mechanisms are critically sensitive to statistical variation in individual stimuli, and that modeling the effects of this variation is essential to account for the full range of observed effects in language processing.},
  keywords = {Computational psycholinguistics,ERPs,N400,Noisy channel model,Psycholinguistics,Semantic anomaly,Semantic P600},
  file = {~/Zotfiles/li.j2023 Heuristic interpretation as rational inf.pdf}
}

@inproceedings{li.j:2023cogsci,
  title = {A Decomposition of Surprisal Tracks the {{N400}} and {{P600}} Brain Potentials},
  booktitle = {Proceedings of the 45th {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Li, Jiaxuan and Futrell, Richard},
  year = {2023},
  volume = {45},
  urldate = {2025-05-09},
  abstract = {The functional interpretation of language-related ERP components has been a central debate in psycholinguistics for decades. We advance an information-theoretic model of human language processing in the brain, in which incoming linguistic input is processed at two levels, in terms of a heuristic interpretation and in terms of error correction. We propose that these two kinds of information processing have distinct electroencephalographic signatures, corresponding to the well-documented N400 and P600 components of language-related event-related potentials (ERPs). Formally, we show that the information content (surprisal) of a word in context can be decomposed into two quantities: (A) heuristic surprise, which signals the processing difficulty of word given its inferred context, and corresponds with the N400 signal; and (B) discrepancy signal, which reflects divergence between the true context and the inferred context, and corresponds to the P600 signal. Both of these quantities can be estimated using modern NLP techniques. We validate our theory by successfully simulating ERP patterns elicited by a variety of linguistic manipulations in previously-reported experimental data from four experiments. Our theory is in principle compatible with traditional cognitive theories assuming the existence of a `good-enough' heuristic interpretation, but with a precise information-theoretic formulation.},
  langid = {english},
  file = {~/Zotfiles/li.j2023cogsci A decomposition of surprisal tracks the.pdf}
}

@misc{li.j:2024arxiv,
  title = {Decomposition of Surprisal: {{Unified}} Computational Model of {{ERP}} Components in Language Processing},
  shorttitle = {Decomposition of Surprisal},
  author = {Li, Jiaxuan and Futrell, Richard},
  year = {2024},
  month = nov,
  number = {arXiv:2409.06803},
  eprint = {2409.06803},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.06803},
  urldate = {2025-05-07},
  abstract = {The functional interpretation of language-related ERP components has been a central debate in psycholinguistics for decades. We advance an information-theoretic model of human language processing in the brain in which incoming linguistic input is processed at first shallowly and later with more depth, with these two kinds of information processing corresponding to distinct electroencephalographic signatures. Formally, we show that the information content (surprisal) of a word in context can be decomposed into two quantities: (A) shallow surprisal, which signals shallow processing difficulty for a word, and corresponds with the N400 signal; and (B) deep surprisal, which reflects the discrepancy between shallow and deep representations, and corresponds to the P600 signal and other late positivities. Both of these quantities can be estimated straightforwardly using modern NLP models. We validate our theory by successfully simulating ERP patterns elicited by a variety of linguistic manipulations in previously-reported experimental data from six experiments, with successful novel qualitative and quantitative predictions. Our theory is compatible with traditional cognitive theories assuming a `good-enough' shallow representation stage, but with a precise information-theoretic formulation. The model provides an information-theoretic model of ERP components grounded on cognitive processes, and brings us closer to a fully-specified neuro-computational model of language processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory,Mathematics - Information Theory},
  file = {/Users/j/MIT Dropbox/Jacob Vigly/Zotfiles/li.j2024arxiv Decomposition of surprisal Unified comp.pdf}
}

@inproceedings{li.j:2024cogsci,
  title = {An Information-Theoretic Model of Shallow and Deep Language Comprehension},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Li, Jiaxuan and Futrell, Richard},
  year = {2024},
  volume = {46},
  eprint = {2405.08223},
  primaryclass = {cs, math},
  urldate = {2024-10-21},
  abstract = {A large body of work in psycholinguistics has focused on the idea that online language comprehension can be shallow or `good enough': given constraints on time or available computation, comprehenders may form interpretations of their input that are plausible but inaccurate. However, this idea has not yet been linked with formal theories of computation under resource constraints. Here we use information theory to formulate a model of language comprehension as an optimal trade-off between accuracy and processing depth, formalized as bits of information extracted from the input, which increases with processing time. The model provides a measure of processing effort as the change in processing depth, which we link to EEG signals and reading times. We validate our theory against a large-scale dataset of garden path sentence reading times, and EEG experiments featuring N400, P600 and biphasic ERP effects. By quantifying the timecourse of language processing as it proceeds from shallow to deep, our model provides a unified framework to explain behavioral and neural signatures of language comprehension.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {~/Zotfiles/li.j2024cogsci An information-theoretic model of shallo.pdf;~/Zotfiles/li.j2024cogsci An information-theoretic model of shallo.pdf}
}

@misc{li.j:2025HSP,
  type = {Poster},
  title = {{{SPACER}}: {{A Parallel Dataset}} of {{Speech Production And Comprehension}} of {{Error Repairs}}},
  author = {Li, Jiaxuan and Upadhye, Shiva and Gandhasri, Rutvik and Futrell, Richard},
  year = {2025},
  month = mar,
  address = {University of Maryland, College Park},
  file = {~/Zotfiles/li.j2025 SPACER A Parallel Dataset of Speech Pro.pdf}
}

@article{li.m:2004,
  title = {The Similarity Metric},
  author = {Li, M. and Chen, X. and Li, X. and Ma, B. and Vitanyi, P.M.B.},
  year = {2004},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {50},
  number = {12},
  pages = {3250--3264},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.838101},
  urldate = {2023-01-19},
  langid = {english},
  file = {~/Zotfiles/li.m2004 The similarity metric.pdf}
}

@book{li.m:2008book3,
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  author = {Li, Ming and Vit{\'a}nyi, Paul},
  year = {2008},
  edition = {3},
  publisher = {Springer},
  doi = {10.1007/978-0-387-49820-1},
  date-added = {2019-09-13 08:17:22 -0400},
  date-modified = {2019-09-13 08:17:36 -0400},
  isbn = {978-0-387-49820-1},
  project = {information-entropy},
  keywords = {kolmogorov complexity},
  file = {~/Zotfiles/li.m2008 An introduction to Kolmogorov complexity.pdf}
}

@book{li.m:2019book4,
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  author = {Li, Ming and Vit{\'a}nyi, Paul},
  year = {2019},
  edition = {4},
  publisher = {Springer},
  address = {Cham, Switzerland},
  doi = {10.1007/978-3-030-11298-1},
  date-added = {2019-09-13 08:17:22 -0400},
  date-modified = {2019-09-13 08:17:36 -0400},
  isbn = {978-3-030-11298-1},
  project = {information-entropy},
  keywords = {kolmogorov complexity},
  file = {~/Zotfiles/li.m2019book4 An introduction to Kolmogorov complexity.pdf}
}

@inproceedings{li.x:2019,
  title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Li, Xiang Lisa and Eisner, Jason},
  year = {2019},
  pages = {2744--2754},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1276}
}

@misc{li.x:2022,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {2205.14217},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.14217},
  urldate = {2022-06-13},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/li.x2022 Diffusion-LM Improves Controllable Text.pdf}
}

@inproceedings{liang.d:2018,
  title = {Variational Autoencoders for Collaborative Filtering},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  year = {2018},
  month = apr,
  series = {{{WWW}} '18},
  pages = {689--698},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Republic and Canton of Geneva, CHE},
  doi = {10.1145/3178876.3186150},
  urldate = {2022-11-29},
  abstract = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  isbn = {978-1-4503-5639-8},
  keywords = {bayesian models,collaborative filtering,implicit feedback,recommender systems,variational autoencoder},
  file = {~/Zotfiles/liang.d2018 Variational autoencoders for collaborati.pdf}
}

@article{liberti.l:2016,
  title = {Six Mathematical Gems from the History of Distance Geometry},
  author = {Liberti, Leo and Lavor, Carlile},
  year = {2016},
  journal = {International Transactions in Operational Research},
  volume = {23},
  number = {5},
  pages = {897--920},
  publisher = {Wiley Online Library},
  date-added = {2019-06-11 11:26:58 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry}
}

@article{lieder.f:2018,
  title = {Overrepresentation of Extreme Events in Decision Making Reflects Rational Use of Cognitive Resources.},
  author = {Lieder, Falk and Griffiths, Thomas L. and Hsu, Ming},
  year = {2018},
  month = jan,
  journal = {Psychological Review},
  volume = {125},
  number = {1},
  pages = {1--32},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000074},
  urldate = {2025-02-17},
  langid = {english},
  file = {~/Zotfiles/lieder.f2018 Overrepresentation of extreme events in.pdf}
}

@article{lieder.f:2020,
  title = {Resource-Rational Analysis: {{Understanding}} Human Cognition as the Optimal Use of Limited Computational Resources},
  shorttitle = {Resource-Rational Analysis},
  author = {Lieder, Falk and Griffiths, Thomas L.},
  year = {2020},
  journal = {Behavioral and Brain Sciences},
  volume = {43},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1900061X},
  urldate = {2022-11-28},
  abstract = {Modeling human cognition is challenging because there are infinitely many mechanisms that can generate any given observation. Some researchers address this by constraining the hypothesis space through assumptions about what the human mind can and cannot do, while others constrain it through principles of rationality and adaptation. Recent work in economics, psychology, neuroscience, and linguistics has begun to integrate both approaches by augmenting rational models with cognitive constraints, incorporating rational principles into cognitive architectures, and applying optimality principles to understanding neural representations. We identify the rational use of limited resources as a unifying principle underlying these diverse approaches, expressing it in a new cognitive modeling paradigm called resource-rational analysis. The integration of rational principles with realistic cognitive constraints makes resource-rational analysis a promising framework for reverse-engineering cognitive mechanisms and representations. It has already shed new light on the debate about human rationality and can be leveraged to revisit classic questions of cognitive psychology within a principled computational framework. We demonstrate that resource-rational models can reconcile the mind's most impressive cognitive skills with people's ostensive irrationality. Resource-rational analysis also provides a new way to connect psychological theory more deeply with artificial intelligence, economics, neuroscience, and linguistics.},
  langid = {english},
  pmid = {30714890},
  keywords = {Artificial Intelligence,bounded rationality,Cognition,cognitive biases,cognitive mechanisms,cognitive modeling,Decision Making,Humans,Models Theoretical,Problem Solving,Psychological Theory,representations,resource rationality,Thinking},
  file = {~/Zotfiles/lieder.f2020 Resource-rational analysis Understandin.pdf}
}

@inproceedings{lin.c:2018,
  title = {Neural Particle Smoothing for Sampling from Conditional Sequence Models},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Lin, Chu-Cheng and Eisner, Jason},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/n18-1085},
  keywords = {parsing,sampling}
}

@article{lindley.d:1956,
  title = {On a Measure of the Information Provided by an Experiment},
  author = {Lindley, D. V.},
  year = {1956},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {27},
  number = {4},
  pages = {986--1005},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177728069},
  urldate = {2024-05-14},
  langid = {english},
  file = {~/Zotfiles/lindley.d1956 On a measure of the information provided.pdf}
}

@inproceedings{linzen.t:2014,
  title = {Investigating the Role of Entropy in Sentence Processing},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Linzen, Tal and Jaeger, Florian},
  editor = {Demberg, Vera and O'Donnell, Timothy},
  year = {2014},
  month = jun,
  pages = {10--18},
  publisher = {Association for Computational Linguistics},
  address = {Baltimore, Maryland, USA},
  doi = {10.3115/v1/W14-2002},
  urldate = {2024-07-27},
  file = {~/Zotfiles/linzen.t2014 Investigating the role of entropy in sen.pdf}
}

@article{linzen.t:2015,
  title = {Uncertainty and Expectation in Sentence Processing: {{Evidence}} from Subcategorization Distributions},
  author = {Linzen, Tal and Jaeger, T. Florian},
  year = {2015},
  journal = {Cognitive Science},
  volume = {40},
  number = {6},
  pages = {1382--1411},
  publisher = {Wiley},
  doi = {10.1111/cogs.12274},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12274},
  date-added = {2021-03-18 10:32:01 -0400},
  date-modified = {2021-03-18 10:37:45 -0400},
  keywords = {expectation,processing}
}

@article{linzen.t:2016,
  title = {Assessing the Ability of {{LSTMs}} to Learn Syntax-Sensitive Dependencies},
  author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  year = {2016},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {521--535},
  doi = {10.1162/tacl_a_00115},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0115},
  file = {~/Zotfiles/linzen.t2016 Assessing the ability of LSTMs to learn.pdf}
}

@article{linzen.t:2018,
  title = {What Can Linguistics and Deep Learning Contribute to Each Other?},
  author = {Linzen, Tal},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.04179},
  eprint = {1809.04179},
  archiveprefix = {arXiv},
  date-added = {2019-06-13 08:03:15 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {recurrent neural networks}
}

@misc{lipkin.b:2025arxiv,
  title = {Fast {{Controlled Generation}} from {{Language Models}} with {{Adaptive Weighted Rejection Sampling}}},
  author = {Lipkin, Benjamin and LeBrun, Benjamin and Vigly, Jacob Hoover and Loula, Jo{\~a}o and MacIver, David R. and Du, Li and Eisner, Jason and Cotterell, Ryan and Mansinghka, Vikash and O'Donnell, Timothy J. and Lew, Alexander K. and Vieira, Tim},
  year = {2025},
  month = apr,
  number = {arXiv:2504.05410},
  eprint = {2504.05410},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2504.05410},
  abstract = {The dominant approach to generating from language models subject to some constraint is locally constrained decoding (LCD), incrementally sampling tokens at each time step such that the constraint is never violated. Typically, this is achieved through token masking: looping over the vocabulary and excluding non-conforming tokens. There are two important problems with this approach. (i) Evaluating the constraint on every token can be prohibitively expensive -- LM vocabularies often exceed \$100,000\$ tokens. (ii) LCD can distort the global distribution over strings, sampling tokens based only on local information, even if they lead down dead-end paths. This work introduces a new algorithm that addresses both these problems. First, to avoid evaluating a constraint on the full vocabulary at each step of generation, we propose an adaptive rejection sampling algorithm that typically requires orders of magnitude fewer constraint evaluations. Second, we show how this algorithm can be extended to produce low-variance, unbiased estimates of importance weights at a very small additional cost -- estimates that can be soundly used within previously proposed sequential Monte Carlo algorithms to correct for the myopic behavior of local constraint enforcement. Through extensive empirical evaluation in text-to-SQL, molecular synthesis, goal inference, pattern matching, and JSON domains, we show that our approach is superior to state-of-the-art baselines, supporting a broader class of constraints and improving both runtime and performance. Additional theoretical and empirical analyses show that our method's runtime efficiency is driven by its dynamic use of computation, scaling with the divergence between the unconstrained and constrained LM, and as a consequence, runtime improvements are greater for better models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/lipkin.b2025arxiv Fast Controlled Generation from Language.pdf}
}

@article{lipman.b:1995,
  title = {Information {{Processing}} and {{Bounded Rationality}}: {{A Survey}}},
  shorttitle = {Information {{Processing}} and {{Bounded Rationality}}},
  author = {Lipman, Barton L.},
  year = {1995},
  journal = {The Canadian Journal of Economics / Revue canadienne d'Economique},
  volume = {28},
  number = {1},
  eprint = {136022},
  eprinttype = {jstor},
  pages = {42--67},
  publisher = {[Wiley, Canadian Economics Association]},
  issn = {0008-4085},
  doi = {10.2307/136022},
  urldate = {2022-06-14},
  abstract = {This paper surveys recent attempts to formulate a plausible and tractable model of bounded rationality. I focus in particular on models that view bounded rationality as stemming from limited information processing. I discuss partitional models (such as computability, automata, perceptrons, and optimal networks), non-partitional models, and axiomatic approaches. /// Transformation de l'information et rationalit{\'e} limit{\'e}e: une revue de la litt{\'e}rature. Ce m{\'e}moire examine certaines tentatives r{\'e}centes pour formuler un mod{\`e}le plausible et utilisable de la rationalit{\'e} limit{\'e}e. L'auteur s'attache en particulier aux mod{\`e}les qui pr{\'e}sentent la rationalit{\'e} limit{\'e}e comme un ph{\'e}nom{\`e}ne {\'e}manant de la limitation dans la capacit{\'e} {\`a} transformer l'information. L'auteur discute les mod{\`e}les qu'on appelle `partitionnels' (computabilit{\'e}, automates, perceptrons, r{\'e}seaux optimaux), les mod{\`e}les `non partitionnels' ainsi que les approches axiomatiques.},
  file = {~/Zotfiles/lipman.b1995 Information Processing and Bounded Ratio.pdf}
}

@article{liu.j:1998,
  title = {Rejection Control and Sequential Importance Sampling},
  author = {Liu, Jun S. and Chen, Rong and Wong, Wing Hung},
  year = {1998},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {443},
  pages = {1022--1031},
  publisher = {Informa UK Limited},
  doi = {10.1080/01621459.1998.10473764},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1998.10473764},
  date-added = {2022-05-05 09:40:36 -0400},
  date-modified = {2022-05-05 09:42:57 -0400},
  keywords = {importance sampling,rejection controlled sequential importance sampling,sequential importance sampling,sequential monte carlo}
}

@book{liu.j:2004book,
  title = {Monte {{Carlo}} Strategies in Scientific Computing},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-76371-2},
  urldate = {2022-12-07},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  keywords = {convergence of random variables,Excel,Markov chain,Markov Chains,mathematical statistics,modeling,Monte Carlo Method,optimization,Potential,Probability theory,Random variable,Scientific Computing,statistics}
}

@article{liu.j:2017,
  title = {In-Order Transition-Based Constituent Parsing},
  author = {Liu, Jiangming and Zhang, Yue},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {413--424},
  doi = {10.1162/tacl_a_00070},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0070},
  file = {~/Zotfiles/liu.j2017 In-order transition-based constituent pa.pdf}
}

@inproceedings{liu.p:2023,
  title = {Generating {{Wikipedia}} by Summarizing Long Sequences},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  year = {2023},
  month = may,
  urldate = {2023-05-25},
  abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
  langid = {english},
  file = {~/Zotfiles/liu.p2023 Generating Wikipedia by summarizing long.pdf}
}

@misc{liu.q:2020,
  title = {A Survey on Contextual Embeddings},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  year = {2020},
  eprint = {2003.07278},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {word embeddings}
}

@inproceedings{liu.z:2021,
  title = {Morphological {{Segmentation}} for {{Seneca}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}}},
  author = {Liu, Zoey and Jimerson, Robert and Prud'hommeaux, Emily},
  year = {2021},
  month = jun,
  pages = {90--101},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.americasnlp-1.10},
  urldate = {2022-06-06},
  abstract = {This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning.},
  keywords = {computational revitalization,iroquoian,morphology},
  file = {~/Zotfiles/liu.z2021 Morphological Segmentation for Seneca.pdf}
}

@techreport{llamateam.:2024,
  title = {The {{Llama}} 3 Herd of Models},
  author = {{Llama team}},
  year = {2024},
  month = jul,
  institution = {AI@Meta},
  urldate = {2024-07-25}
}

@article{lo.s:2015,
  title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
  author = {Lo, Steson and Andrews, Sally},
  year = {2015},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {Frontiers Media SA},
  doi = {10.3389/fpsyg.2015.01171},
  bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2015.01171},
  date-added = {2022-02-23 22:30:34 -0500},
  date-modified = {2022-02-23 22:30:36 -0500},
  file = {~/Zotfiles/lo.s2015 To transform or not to transform using.DOCX;~/Zotfiles/lo.s2015 To transform or not to transform using.pdf}
}

@article{logacev.p:2016,
  title = {Understanding Underspecification: {{A}} Comparison of Two Computational Implementations},
  shorttitle = {Understanding Underspecification},
  author = {Loga{\v c}ev, Pavel and Vasishth, Shravan},
  year = {2016},
  month = may,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {5},
  pages = {996--1012},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1080/17470218.2015.1134602},
  urldate = {2023-08-01},
  abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201--216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of lexical ambiguity resolution. Journal of Memory and Language, 39(4), 558--592], which has been explained in terms of the Unrestricted Race Model, can equally well be explained by assuming underspecification in ambiguous conditions driven by task-demands. Specifically, if comprehension questions require that ambiguities be resolved, the parser tends to make an attachment: when questions are about superficial aspects of the target sentence, readers tend to pursue an underspecification strategy. It is reasonable to assume that individual differences in strategy will play a significant role in the application of such strategies, so that studying average behaviour may not be informative. In order to study the predictions of the good-enough processing theory, we implemented two versions of underspecification: the partial specification model (PSM), which is an implementation of the Swets et al. proposal, and a more parsimonious version, the non-specification model (NSM). We evaluate the relative fit of these two kinds of underspecification to Swets et al.'s data; as a baseline, we also fitted three models that assume no underspecification. We find that a model without underspecification provides a somewhat better fit than both underspecification models, while the NSM model provides a better fit than the PSM. We interpret the results as lack of unambiguous evidence in favour of underspecification; however, given that there is considerable existing evidence for good-enough processing in the literature, it is reasonable to assume that some underspecification might occur. Under this assumption, the results can be interpreted as tentative evidence for NSM over PSM. More generally, our work provides a method for choosing between models of real-time processes in sentence comprehension that make qualitative predictions about the relationship between several dependent variables. We believe that sentence processing research will greatly benefit from a wider use of such methods.},
  langid = {english},
  keywords = {underspecification}
}

@article{lomashvili.l:2011,
  title = {Phases and Templates in {{Georgian}} Agreement},
  author = {Lomashvili, Leila and Harley, Heidi},
  year = {2011},
  journal = {Studia Linguistica},
  volume = {65},
  number = {3},
  pages = {233--267},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:50 -0400},
  project = {Icelandic gluttony},
  keywords = {phase theory,phi features}
}

@misc{lou.p:2018,
  title = {Disfluency Detection Using a Noisy Channel Model and a Deep Neural Language Model},
  author = {Lou, Paria Jamshid and Johnson, Mark},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1808.09091},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.09091},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-27 10:29:36 -0400},
  date-modified = {2022-04-27 10:29:37 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {~/Zotfiles/lou.p2018 Disfluency detection using a noisy chann.pdf}
}

@inproceedings{loula.j:2024,
  title = {Syntactic and {{Semantic Control}} of {{Large Language Models}} via {{Sequential Monte Carlo}}},
  booktitle = {The {{Thirteenth International Conference}} on {{Learning Representations}}},
  author = {Loula, Jo{\~a}o and LeBrun, Benjamin and Du, Li and Lipkin, Ben and Pasti, Clemente and Grand, Gabriel and Liu, Tianyu and Emara, Yahya and Freedman, Marjorie and Eisner, Jason and Cotterell, Ryan and Mansinghka, Vikash and Lew, Alexander K. and Vieira, Tim and O'Donnell, Timothy J.},
  year = {2024},
  month = oct,
  urldate = {2025-03-13},
  abstract = {A wide range of LM applications require generating text that conforms to syntactic or semantic constraints. Imposing such constraints can be naturally framed as probabilistic conditioning, but exact generation from the resulting distribution---which can differ substantially from the LM's base distribution---is generally intractable. In this work, we develop an architecture for controlled LM generation based on sequential Monte Carlo (SMC). This SMC framework allows us to flexibly incorporate domain- and problem-specific constraints at inference time, and efficiently reallocate computational resources in light of new information during the course of generation. By comparing to a number of alternatives and ablations on four challenging domains---Python code generation for data science, text-to-SQL, goal inference, and molecule synthesis---we demonstrate that, with little overhead, our approach allows small open-source language models to outperform models over 8{\texttimes} larger, as well as closed-source, fine-tuned ones. In support of the probabilistic perspective, we show that these performance improvements are driven by better approximation to the posterior distribution. [Our system](https://github.com/probcomp/gen-parse) builds on the framework of Lew et al. (2023) and integrates with its language model probabilistic programming language, giving users a simple, programmable way to apply SMC to a broad variety of controlled generation problems.},
  langid = {english},
  file = {~/Zotfiles/loula.j2024genparse Syntactic and Semantic Control of Large.pdf}
}

@incollection{lounsbury.f:1954,
  title = {Transitional Probability, Linguistic Structure, and Systems of Habit-Family Hierarchies},
  booktitle = {Psycholinguistics},
  author = {Lounsbury, Floyd G},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {93--101},
  publisher = {Waverly Press Baltimore},
  chapter = {5.1},
  date-added = {2022-04-14 23:38:02 -0400},
  date-modified = {2022-04-14 23:51:56 -0400},
  keywords = {entropy reduction}
}

@article{lowder.m:2018,
  title = {Lexical Predictability during Natural Reading: {{Effects}} of Surprisal and Entropy Reduction},
  shorttitle = {Lexical Predictability during Natural Reading},
  author = {Lowder, Matthew W. and Choi, Wonil and Ferreira, Fernanda and Henderson, John M.},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1166--1183},
  issn = {1551-6709},
  doi = {10.1111/cogs.12597},
  urldate = {2022-10-13},
  abstract = {What are the effects of word-by-word predictability on sentence processing times during the natural reading of a text? Although information complexity metrics such as surprisal and entropy reduction have been useful in addressing this question, these metrics tend to be estimated using computational language models, which require some degree of commitment to a particular theory of language processing. Taking a different approach, this study implemented a large-scale cumulative cloze task to collect word-by-word predictability data for 40 passages and compute surprisal and entropy reduction values in a theory-neutral manner. A separate group of participants read the same texts while their eye movements were recorded. Results showed that increases in surprisal and entropy reduction were both associated with increases in reading times. Furthermore, these effects did not depend on the global difficulty of the text. The findings suggest that surprisal and entropy reduction independently contribute to variation in reading times, as these metrics seem to capture different aspects of lexical predictability.},
  langid = {english},
  keywords = {entropy reduction,Entropy reduction,Eyetracking,Prediction,Sentence processing,Surprisal,surprisal theory},
  file = {~/Zotfiles/lowder.m2018 Lexical predictability during natural re.pdf}
}

@book{luce.r:1959book,
  title = {Individual Choice Behavior: A Theoretical Analysis},
  shorttitle = {Individual Choice Behavior},
  author = {Luce, R. Duncan},
  year = {1959},
  publisher = {Wiley},
  abstract = {And conclusions. Summary ; Conclusions -- Appendix 1. Alternative forms of Axiom 1 -- Appendix 2. Form of latency distribution -- Appendix 3. Maximum likelihood equations for the two-alternative, two-outcome beta learning model -- Appendix 4. Open problems. Conceptual and empirical ; Mathematical.},
  langid = {english}
}

@article{luce.r:2003,
  title = {Whatever Happened to Information Theory in Psychology?},
  author = {Luce, R. Duncan},
  year = {2003},
  month = jun,
  journal = {Review of General Psychology},
  volume = {7},
  number = {2},
  pages = {183--188},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1037/1089-2680.7.2.183},
  urldate = {2024-05-03},
  abstract = {Although Shannon's information theory is alive and well in a number of fields, after an initial fad in psychology during the 1950s and 1960s it no longer is much of a factor, beyond the word bit, in psychological theory. The author discusses what seems to him (and others) to be the root causes of an actual incompatibility between information theory and the psychological phenomena to which it has been applied.},
  langid = {english},
  file = {~/Zotfiles/luce.r2003 Whatever happened to information theory.pdf}
}

@book{luce.r:2005book,
  title = {Individual Choice Behavior: {{A}} Theoretical Analysis.},
  shorttitle = {Individual Choice Behavior},
  author = {Luce, R. Duncan},
  year = {2005},
  publisher = {Dover Publications},
  address = {Mineola},
  doi = {10.1037/14396-000},
  urldate = {2025-02-23},
  isbn = {978-0-486-44136-8},
  langid = {english},
  file = {~/Zotfiles/luce.r2005 Individual choice behavior A theoretica.pdf}
}

@article{ludecke.d:2018,
  title = {{{{\textbf{ggeffects}}}}: Tidy Data Frames of Marginal Effects from Regression Models},
  shorttitle = {Ggeffects},
  author = {L{\"u}decke, Daniel},
  year = {2018},
  month = jun,
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {26},
  pages = {772},
  issn = {2475-9066},
  doi = {10.21105/joss.00772},
  urldate = {2024-01-24}
}

@inproceedings{lueckmann.j:2021,
  title = {Benchmarking Simulation-Based Inference},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {{AISTATS}} 2021, April 13-15, 2021, Virtual Event},
  author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David S. and Gon{\c c}alves, Pedro J. and Macke, Jakob H.},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021},
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {343--351},
  publisher = {PMLR},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aistats/LueckmannBGGM21.bib},
  timestamp = {Wed, 14 Apr 2021 01:00:00 +0200}
}

@misc{lugosch.l:2020arxiv,
  title = {Surprisal-Triggered Conditional Computation with Neural Networks},
  author = {Lugosch, Loren and Nowrouzezahrai, Derek and Meyer, Brett H.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.01659},
  eprint = {2006.01659},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-03-19},
  abstract = {Autoregressive neural network models have been used successfully for sequence generation, feature extraction, and hypothesis scoring. This paper presents yet another use for these models: allocating more computation to more difficult inputs. In our model, an autoregressive model is used both to extract features and to predict observations in a stream of input observations. The surprisal of the input, measured as the negative log-likelihood of the current observation according to the autoregressive model, is used as a measure of input difficulty. This in turn determines whether a small, fast network, or a big, slow network, is used. Experiments on two speech recognition tasks show that our model can match the performance of a baseline in which the big network is always used with 15\% fewer FLOPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{luke.s:2017,
  title = {The {{Provo Corpus}}: {{A}} Large Eye-Tracking Corpus with Predictability Norms},
  author = {Luke, Steven G. and Christianson, Kiel},
  year = {2017},
  month = may,
  volume = {50},
  number = {2},
  pages = {826--833},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-017-0908-4},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-017-0908-4},
  date-added = {2021-10-19 00:07:37 -0400},
  date-modified = {2021-10-19 00:07:38 -0400}
}

@inproceedings{luong.t:2015,
  title = {Evaluating Models of Computation and Storage in Human Sentence Processing},
  booktitle = {Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning},
  author = {Luong, Thang and O'Donnell, Timothy and Goodman, Noah},
  year = {2015},
  month = sep,
  pages = {14--21},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/W15-2403},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W15-2403},
  date-added = {2022-05-02 11:30:20 -0400},
  date-modified = {2022-05-17 08:07:37 -0400},
  keywords = {fragment grammars,incrementality,parsing}
}

@article{lupker.s:2008,
  title = {Transposed-Letter Effects: {{Consonants}}, Vowels and Letter Frequency},
  shorttitle = {Transposed-Letter Effects},
  author = {Lupker, Stephen J. and Perea, Manuel and Davis, Colin J.},
  year = {2008},
  month = jan,
  journal = {Language and Cognitive Processes},
  volume = {23},
  number = {1},
  pages = {93--116},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/01690960701579714},
  urldate = {2023-12-13},
  abstract = {There is now considerable evidence (e.g., Perea \& Lupker, 2003a, 2003b) that transposed-letter nonword primes (e.g., jugde for JUDGE) are more effective primes than replacement-letter nonword primes (e.g., jupte for JUDGE). Recently, Perea and Lupker (2004) demonstrated that, in Spanish, this transposed-letter prime advantage exists only when the transposed letters are consonants (C-C transpositions) and not when they are vowels (V-V transpositions). This vowel-consonant difference causes problems even for models that can successfully explain transposed-letter effects (e.g., SOLAR, Davis, 1999). In Experiment 1 in the present paper, we demonstrated a parallel result in a language with a different syllabic structure (English) in both a masked priming experiment and an unprimed lexical decision task in which the transposed letter strings (e.g., ADACEMY, ACEDAMY) were used as the nonwords. Results in Experiment 2 suggest that at least part of the reason for the vowel-consonant difference is because of the higher letter frequencies of the vowels. Possible alternative interpretations of the vowel-consonant difference are discussed.},
  file = {~/Zotfiles/lupker.s2008 Transposed-letter effects Consonants, v.pdf}
}

@book{lurie.j:2009book,
  title = {Higher Topos Theory (Preprint)},
  author = {Lurie, Jacob},
  year = {2009},
  publisher = {Princeton University Press},
  date-added = {2019-08-24 09:21:19 -0400},
  date-modified = {2019-08-24 09:23:18 -0400},
  keywords = {category theory,topos theory}
}

@article{mackay.d:1992,
  title = {Information-Based Objective Functions for Active Data Selection},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {590--604},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.590},
  urldate = {2024-05-15},
  abstract = {Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.},
  file = {~/Zotfiles/mackay.d1992 Information-based objective functions fo.pdf}
}

@book{mackay.d:2003book,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  publisher = {Cambridge university press},
  date-added = {2020-02-16 21:05:41 -0500},
  date-modified = {2020-04-29 12:55:23 -0400},
  project = {information-entropy},
  keywords = {information theory},
  file = {~/Zotfiles/mackay.d2003 Information theory, inference and learni.pdf}
}

@inproceedings{maddison.c:2017,
  title = {Filtering {{Variational Objectives}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Maddison, Chris J and Lawson, John and Tucker, George and Heess, Nicolas and Norouzi, Mohammad and Mnih, Andriy and Doucet, Arnaud and Teh, Yee},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-07-31},
  abstract = {When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.},
  file = {~/Zotfiles/maddison.c2017 Filtering Variational Objectives.pdf}
}

@inproceedings{madureira.b:2020,
  title = {Incremental Processing in the Age of Non-Incremental Encoders: {{An}} Empirical Assessment of Bidirectional Models for Incremental {{NLU}}},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Madureira, Brielen and Schlangen, David},
  year = {2020},
  pages = {357--374},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.26},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.26}
}

@article{maehara.h:2013,
  title = {Euclidean Embeddings of Finite Metric Spaces},
  author = {Maehara, Hiroshi},
  year = {2013},
  journal = {Discrete Mathematics},
  volume = {313},
  number = {23},
  pages = {2848--2856},
  publisher = {Elsevier},
  date-added = {2019-06-13 07:52:44 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {euclidean space,geometry}
}

@inproceedings{magerman.d:1990,
  title = {Parsing a Natural Language Using Mutual Information Statistics.},
  booktitle = {{{AAAI}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1990},
  volume = {90},
  pages = {984--989},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:27:23 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,word association}
}

@inproceedings{magerman.d:1991,
  title = {{\emph{P}}earl: A Probabilistic Chart Parser},
  shorttitle = {{\emph{P}}earl},
  booktitle = {Proceedings of the Fifth Conference on {{European}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1991},
  month = apr,
  series = {{{EACL}} '91},
  pages = {15--20},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/977180.977184},
  urldate = {2022-06-13},
  abstract = {This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.},
  file = {~/Zotfiles/magerman.d1991 Pearl a probabilistic chart pars.pdf}
}

@article{mahowald.k:2023,
  title = {Grammatical Cues to Subjecthood Are Redundant in a Majority of Simple Clauses across Languages},
  author = {Mahowald, Kyle and Diachek, Evgeniia and Gibson, Edward and Fedorenko, Evelina and Futrell, Richard},
  year = {2023},
  month = dec,
  journal = {Cognition},
  volume = {241},
  pages = {105543},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2023.105543},
  urldate = {2025-02-05},
  abstract = {Grammatical cues are sometimes redundant with word meanings in natural language. For instance, English word order rules constrain the word order of a sentence like ``The dog chewed the bone'' even though the status of ``dog'' as subject and ``bone'' as object can be inferred from world knowledge and plausibility. Quantifying how often this redundancy occurs, and how the level of redundancy varies across typologically diverse languages, can shed light on the function and evolution of grammar. To that end, we performed a behavioral experiment in English and Russian and a cross-linguistic computational analysis measuring the redundancy of grammatical cues in transitive clauses extracted from corpus text. English and Russian speakers (n~=~484) were presented with subjects, verbs, and objects (in random order and with morphological markings removed) extracted from naturally occurring sentences and were asked to identify which noun is the subject of the action. Accuracy was high in both languages ({$\sim$}89\% in English, {$\sim$}87\% in Russian). Next, we trained a neural network machine classifier on a similar task: predicting which nominal in a subject-verb-object triad is the subject. Across 30 languages from eight language families, performance was consistently high: a median accuracy of 87\%, comparable to the accuracy observed in the human experiments. The conclusion is that grammatical cues such as word order are necessary to convey subjecthood and objecthood in a minority of naturally occurring transitive clauses; nevertheless, they can (a) provide an important source of redundancy and (b) are crucial for conveying intended meaning that cannot be inferred from the words alone, including descriptions of human interactions, where roles are often reversible (e.g., Ray helped Lu/Lu helped Ray), and expressing non-prototypical meanings (e.g., ``The bone chewed the dog.'').},
  keywords = {Computational modeling,corpus linguistics,Grammatical cues,Linguistic efficiency,Psycholinguistics,redundancy,Syntax},
  file = {~/Zotfiles/mahowald.k2023 Grammatical cues to subjecthood are redu.pdf}
}

@article{makkeh.a:2021,
  title = {Introducing a Differentiable Measure of Pointwise Shared Information},
  author = {Makkeh, Abdullah and Gutknecht, Aaron J. and Wibral, Michael},
  year = {2021},
  month = mar,
  journal = {Physical Review E},
  volume = {103},
  number = {3},
  publisher = {American Physical Society (APS)},
  doi = {10.1103/physreve.103.032149},
  bdsk-url-2 = {https://doi.org/10.1103/physreve.103.032149},
  date-added = {2022-04-18 11:15:53 -0400},
  date-modified = {2022-04-18 11:16:04 -0400},
  keywords = {partial information decomposition}
}

@book{malchukov.a:2012book,
  title = {The Oxford Handbook of Case},
  author = {Malchukov, Andrej L. and Spencer, Andrew},
  year = {2012},
  publisher = {Oxford University Press},
  date-added = {2020-02-03 16:08:35 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  isbn = {978-0-19-920647-6},
  project = {Icelandic gluttony},
  keywords = {case}
}

@article{manasterramer.a:1992,
  title = {Mathematical Methods in Linguistics},
  author = {Manaster Ramer, Alexis},
  year = {1992},
  journal = {Computational Linguistics},
  volume = {18},
  number = {1}
}

@article{manning.c:2020PNAS,
  title = {Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision},
  author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30046--30054},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1907367117},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1907367117},
  date-added = {2021-07-16 19:46:55 -0400},
  date-modified = {2021-07-16 19:46:57 -0400}
}

@inproceedings{mansinghka.v:2009,
  title = {Exact and Approximate Sampling by Systematic Stochastic Search},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  author = {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},
  editor = {{van Dyk}, David and Welling, Max},
  year = {2009},
  month = apr,
  series = {Proceedings of Machine Learning Research},
  volume = {5},
  pages = {400--407},
  publisher = {PMLR},
  address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  abstract = {We introduce \textsubscript{a}daptive sequential rejection sampling\textsubscript{,} an algorithm for generating exact samples from high-dimensional, discrete distributions, building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions, sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation, and a randomized version of the directed arc consistency algorithm with backtracking when applied to deterministic problems. In this paper, we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on ferromagnetic Isings and other probabilistic graphical models, obtaining exact and approximate samples in a range of situations.},
  date-added = {2022-05-05 09:38:21 -0400},
  date-modified = {2022-05-05 09:39:35 -0400},
  pdf = {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},
  keywords = {adaptive sequential rejection sampling}
}

@article{marcus.g:1999,
  title = {Rule Learning by Seven-Month-Old Infants},
  author = {Marcus, G. F. and Vijayan, S. and Bandi Rao, S. and Vishton, P. M.},
  year = {1999},
  month = jan,
  journal = {Science},
  volume = {283},
  number = {5398},
  pages = {77--80},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.283.5398.77},
  urldate = {2024-05-26},
  abstract = {A fundamental task of language acquisition is to extract abstract algebraic rules. Three experiments show that 7-month-old infants attend longer to sentences with unfamiliar structures than to sentences with familiar structures. The design of the artificial language task used in these experiments ensured that this discrimination could not be performed by counting, by a system that is sensitive only to transitional probabilities, or by a popular class of simple neural network models. Instead, these results suggest that infants can represent, extract, and generalize abstract algebraic rules.}
}

@phdthesis{marcus.m:1978phd,
  title = {A Theory of Syntactic Recognition for Natural Language},
  author = {Marcus, Mitchell P.},
  year = {1978},
  eprint = {1721.1/16176},
  eprinttype = {hdl},
  date-added = {2022-03-31 11:14:02 -0400},
  date-modified = {2022-04-26 21:21:13 -0400},
  school = {Massachusetts Institute of Technology},
  file = {~/Zotfiles/marcus.m1978phd A theory of syntactic recognition for na.pdf}
}

@book{marcus.m:1980book,
  title = {Theory of Syntactic Recognition for Natural Languages},
  author = {Marcus, Mitchell P.},
  year = {1980},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  date-added = {2022-03-31 11:15:48 -0400},
  date-modified = {2022-04-26 21:21:21 -0400},
  isbn = {0-262-13149-8}
}

@inproceedings{marcus.m:1994,
  title = {The {{Penn Treebank}}: {{Annotating}} Predicate Argument Structure},
  booktitle = {Human {{Language Technology}}: {{Proceedings}} of a {{Workshop}} Held at {{Plainsboro}}, {{New Jersey}}, {{March}} 8-11, 1994},
  author = {Marcus, Mitchell P. and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  year = {1994}
}

@phdthesis{marecek.d:2012,
  title = {Unsupervised Dependency Parsing},
  author = {Mare{\v c}ek, David},
  year = {2012},
  address = {Prague, Czech Republic},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-09-07 16:53:21 -0400},
  project = {syntactic embedding},
  school = {Charles University},
  keywords = {dependency parsing,unsupervised parsing},
  file = {~/Zotfiles/marecek.d2012 Unsupervised dependency parsing.pdf}
}

@inproceedings{marecek.d:2018,
  title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
  booktitle = {Proceedings of the 2018 {{EMNLP}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2018},
  month = nov,
  pages = {347--349},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/W18-5444},
  abstract = {This is a work in progress about extracting the sentence tree structures from the encoder's self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W18-5444},
  date-added = {2021-09-08 00:32:46 -0400},
  date-modified = {2021-09-08 00:32:47 -0400}
}

@inproceedings{marecek.d:2019,
  title = {From Balustrades to {{Pierre Vinken}}: Looking for Syntax in Transformer Self-Attentions},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2019},
  pages = {263--275},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4827},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4827}
}

@article{markov.a:1913,
  title = {{Essai d'une recherche statistique sur le texte du roman "Eug{\`e}ne On{\v e}gin", illustrant la liaison des {\'e}preuves en cha{\^i}ne}},
  author = {Markov, A. A.},
  year = {1913},
  journal = {Bulletin de l'Acad{\'e}mie Imp{\'e}riale des Sciences de St.-P{\'e}tersbourg. VI s{\'e}rie},
  series = {{6}},
  volume = {7},
  number = {3},
  pages = {153--162},
  langid = {russian}
}

@article{marr.d:1976,
  title = {From Understanding Computation to Understanding Neural Circuitry},
  author = {Marr, D. and Poggio, T.},
  year = {1976},
  month = may,
  urldate = {2022-06-06},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  langid = {american},
  file = {~/Zotfiles/marr.d1976 From understanding computation to unders.pdf}
}

@book{marr.d:1982book,
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  shorttitle = {Vision},
  author = {Marr, David},
  year = {1982},
  publisher = {W.H. Freeman},
  address = {San Francisco, CA},
  isbn = {978-0-262-51462-0},
  langid = {english},
  file = {~/Zotfiles/marr.d1982vision Vision a computational investigation in.pdf}
}

@article{marsden.e:2018,
  title = {A Methodological Synthesis of Self-Paced Reading in Second Language Research},
  author = {Marsden, Emma and Thompson, Sophie and Plonsky, Luke},
  year = {2018},
  month = sep,
  journal = {Applied Psycholinguistics},
  volume = {39},
  number = {5},
  pages = {861--904},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716418000036},
  urldate = {2024-05-26},
  abstract = {Self-paced reading tests (SPRs) are being increasingly adopted by second language (L2) researchers. Using SPR with L2 populations presents specific challenges, and its use is still evolving in L2 research (as well as in first language research, in many respects). Although the topic of several narrative overviews (Keating \& Jegerski, 2015; Roberts, 2016), we do not have a comprehensive picture of its usage in L2 research. Building on the growing body of systematic reviews of research practices in applied linguistics (e.g., Liu \& Brown, 2015; Plonsky, 2013), we report a methodological synthesis of the rationales, study contexts, and methodological decision making in L2 SPR research. Our comprehensive search yielded 74 SPRs used in L2 research. Each instrument was coded along 121 parameters, including: reported rationales and study characteristics, indicating the scope and nature of L2 SPR research agendas; design and analysis features and reporting practices, determining instrument validity and reliability; and materials transparency, affecting reproducibility and systematicity of agendas. Our findings indicate an urgent need to standardize the use and reporting of this technique, requiring empirical investigation to inform methodological decision making. We also identify several areas (e.g., study design, sample demographics, instrument construction, data analysis, and transparency) where SPR research could be improved to enrich our understanding of L2 processing, reading, and learning.},
  langid = {english},
  keywords = {foreign language learning,moving window,open science,research design,research methodology,second language learning,self-paced reading,synthesis,systematic review},
  file = {~/Zotfiles/marsden.e2018 A methodological synthesis of self-paced.pdf}
}

@article{marslen-wilson.w:1973,
  title = {Linguistic Structure and Speech Shadowing at Very Short Latencies},
  author = {{Marslen-Wilson}, William D.},
  year = {1973},
  month = aug,
  journal = {Nature},
  volume = {244},
  number = {5417},
  pages = {522--523},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/244522a0},
  bdsk-url-2 = {https://doi.org/10.1038/244522a0},
  date-added = {2022-04-14 13:38:15 -0400},
  date-modified = {2022-05-02 14:45:30 -0400},
  keywords = {incrementality},
  file = {~/Zotfiles/marslen-wilson.w1973 Linguistic structure and speech shadowin.pdf}
}

@article{marslen-wilson.w:1975,
  title = {Sentence Perception as an Interactive Parallel Process},
  author = {{Marslen-Wilson}, William D.},
  year = {1975},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {189},
  number = {4198},
  pages = {226--228},
  publisher = {American Association for the Advancement of Science (AAAS)},
  doi = {10.1126/science.189.4198.226},
  bdsk-url-2 = {https://doi.org/10.1126/science.189.4198.226},
  date-added = {2022-04-14 13:38:57 -0400},
  date-modified = {2022-05-02 14:45:37 -0400},
  keywords = {incrementality},
  file = {~/Zotfiles/marslen-wilson.w1975 Sentence perception as an interactive pa.pdf}
}

@article{martino.l:2017,
  title = {Effective Sample Size for Importance Sampling Based on Discrepancy Measures},
  author = {Martino, Luca and Elvira, V{\'i}ctor and Louzada, Francisco},
  year = {2017},
  month = feb,
  journal = {Signal Processing},
  volume = {131},
  pages = {386--401},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2016.08.025},
  urldate = {2022-12-16},
  abstract = {The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation ESS{\textasciicircum} of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, ESS{\textasciicircum}, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression ESS{\textasciicircum} is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.},
  langid = {english},
  keywords = {Bayesian Inference,Effective Sample Size,Importance Sampling,Particle Filtering,Perplexity,Sequential Monte Carlo},
  file = {~/Zotfiles/martino.l2017 Effective sample size for importance sam.pdf}
}

@incollection{martino.l:2018,
  title = {Adaptive Rejection Sampling Methods},
  booktitle = {Independent {{Random Sampling Methods}}},
  author = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  editor = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  year = {2018},
  series = {Statistics and {{Computing}}},
  pages = {115--157},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-72634-2_4},
  urldate = {2022-07-05},
  abstract = {This chapter is devoted to describing the class of the adaptive rejection sampling (ARS) schemes. These (theoretically) universal methods are very efficient samplers that update the proposal density whenever a generated sample is rejected in the RS test. In this way, they can produce i.i.d. samples from the target with an increasing acceptance rate that can converge to 1. As a by-product, these techniques also generate a sequence of proposal pdfs converging to the true shape of the target density. Another advantage of the ARS samplers is that, when they can be applied, the user only has to select a set of initial conditions. After the initialization, they are completely automatic, self-tuning algorithms (i.e., no parameters need to be adjusted by the user) regardless of the specific target density. However, the need to construct a suitable sequence of proposal densities restricts the practical applicability of this methodology. As a consequence, ARS schemes are often tailored to specific classes of target distributions. Indeed, the construction of the proposal is particularly hard in multidimensional spaces. Hence, ARS algorithms are usually designed only for drawing from univariate densities.},
  isbn = {978-3-319-72634-2},
  langid = {english},
  file = {~/Zotfiles/martino.l2018 Adaptive rejection sampling methods.pdf}
}

@inproceedings{marvin.r:2018,
  title = {Targeted Syntactic Evaluation of Language Models},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Marvin, Rebecca and Linzen, Tal},
  year = {2018},
  pages = {1192--1202},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1151},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1151},
  file = {~/Zotfiles/marvin.r2018 Targeted syntactic evaluation of languag 2.pdf;~/Zotfiles/marvin.r2018 Targeted syntactic evaluation of languag.pdf}
}

@article{mazur.b:2008,
  title = {When Is One Thing Equalto Some Other Thing?},
  author = {Mazur, Barry},
  year = {2008},
  journal = {Proof and other dilemmas: Mathematics and philosophy},
  volume = {59},
  pages = {221},
  publisher = {MAA},
  date-added = {2019-08-24 09:29:47 -0400},
  date-modified = {2019-08-24 09:29:57 -0400},
  keywords = {category theory}
}

@inproceedings{mccann.b:2017,
  title = {Learned in Translation: {{Contextualized}} Word Vectors},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, {{CA}}, {{USA}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {6294--6305},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/McCannBXS17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{mcclelland.j:1989,
  title = {Sentence Comprehension: {{A}} Parallel Distributed Processing Approach},
  shorttitle = {Sentence Comprehension},
  author = {McClelland, J. L. and St. John, Mark and Taraban, Roman},
  year = {1989},
  month = sep,
  journal = {Language and Cognitive Processes},
  volume = {4},
  number = {3-4},
  pages = {SI287-SI335},
  issn = {0169-0965},
  doi = {10.1080/01690968908406371},
  urldate = {2025-04-15},
  abstract = {In this paper, we review basic aspects of conventional approaches to sentence comprehension and point out some of the difficulties faced by models that take these approaches. We then describe an alternative approach, based on the principles of parallel distributed processing, and show how it offers different answers to basic questions about the nature of the language processing mechanism. We describe an illustrative simulation model that captures the key characteristics of the approach, and illustrate how it can cope with the difficulties faced by conventional models. We describe alternative ways of conceptualising basic aspects of language processing within the framework of this approach, consider how it can address several arguments that might be brought to bear against it, and suggest avenues for future development.},
  file = {~/Zotfiles/mcclelland.j1989 Sentence comprehension A parallel distr.pdf}
}

@incollection{mccloskey.j:2017,
  title = {Resumption},
  booktitle = {The {{Wiley Blackwell Companion}} to {{Syntax}}, {{Second Edition}}},
  author = {McCloskey, James},
  year = {2017},
  pages = {1--30},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118358733.wbsyncom105},
  urldate = {2023-04-10},
  abstract = {In many languages and in a range of circumstances, a pronominal element may appear in a position in which one might have expected to see a gap bound by a clause-peripheral element (in relative clauses, constituent questions, cleft constructions, and the like). Such elements (often, but not exclusively, personal pronouns) are known as resumptive elements. Since they are formally pronouns but serve many of the core semantic functions associated with movement constructions, the study of resumptive elements raises fundamental questions about the nature of movement, about the nature of anaphoric elements and relationships, and about the nature of the interaction between these two spheres. These questions have been the focus of a great deal of work, beginning especially in the middle 1970s; this chapter surveys that work -- the questions that have been asked, the phenomena that have been discovered, and the current state of thinking about the relevant questions. Consideration of the theoretical questions is organized around the asking of three questions: (i) what are the properties of resumptive elements; (ii) in what ways do they share, or fail to share, properties of movement constructions (island phenomena, reconstruction effects); and (iii) in what ways do they share, or fail to share, properties of anaphoric elements and interactions? In recent years especially, the study of resumption has been enriched by a great deal of experimental work, aimed not only at establishing some basic properties of resumptive elements, but also at better understanding the mechanisms involved in their production and comprehension. The chapter ends with a consideration of that work and its implications. Here, the central question that emerges quickly is that of how theories of competence and theories of performance interact with one another.},
  isbn = {978-1-118-35873-3},
  langid = {english},
  keywords = {anaphora,Chomsky,Noam,production,resumptive pronouns,syntax,theoretical linguistics},
  file = {~/Zotfiles/mccloskey.j2017 Resumption.pdf}
}

@incollection{mcconnell-ginet.s:2000,
  title = {Meaning and Grammar: {{An}} Introduction to Semantics},
  booktitle = {Meaning and Grammar: {{An}} Introduction to Semantics},
  author = {{McConnell-Ginet}, Sally and Chierchia, Gennaro},
  year = {2000},
  publisher = {MIT Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{mccurdy.k:2024,
  title = {Lossy Context Surprisal Predicts Task-Dependent Patterns in Relative Clause Processing},
  booktitle = {Proceedings of the 28th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {McCurdy, Kate and Hahn, Michael},
  editor = {Barak, Libby and Alikhani, Malihe},
  year = {2024},
  month = nov,
  pages = {36--45},
  publisher = {Association for Computational Linguistics},
  address = {Miami, FL, USA},
  doi = {10.18653/v1/2024.conll-1.4},
  urldate = {2025-02-14},
  abstract = {English relative clauses are a critical test case for theories of syntactic processing. Expectation- and memory-based accounts make opposing predictions, and behavioral experiments have found mixed results. We present a technical extension of Lossy Context Surprisal (LCS) and use it to model relative clause processing in three behavioral experiments. LCS predicts key results at distinct retention rates, showing that task-dependent memory demands can account for discrepant behavioral patterns in the literature.},
  keywords = {lossy-context surprisal,noisy channel processing},
  file = {~/Zotfiles/mccurdy.k2024 Lossy Context Surprisal Predicts Task-De.pdf}
}

@book{mcdonald.m:1977book,
  title = {Iontenwennaweienstahkhwa' {{Mohawk}} Spelling Dictionary},
  author = {McDonald, Mary and Barnes, Ann and Cook, Louise and Herne, Jean and Jacobs, Rita and Jock, Louise and LaFrance, Harriett and Ransom, Elaine and Sinclair, Winnie and Tarbell, Elizabeth},
  editor = {Mithun, Marianne},
  year = {1977},
  month = sep,
  number = {Bulletin 429},
  publisher = {The University of the State of New York, State Education Department},
  address = {Albany, N.Y.},
  date-added = {2022-05-11 11:20:30 -0400},
  date-modified = {2022-05-11 11:26:42 -0400},
  keywords = {kanien'keha},
  file = {~/Zotfiles/mcdonald.m1977 Iontenwennaweienstahkhwa' Mohawk spellin.pdf}
}

@inproceedings{mcdonald.r:2005,
  title = {Non-Projective Dependency Parsing Using Spanning Tree Algorithms},
  booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Haji{\v c}, Jan},
  year = {2005},
  pages = {523--530},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, British Columbia, Canada}
}

@inproceedings{mcdonald.r:2005a,
  title = {Online Large-Margin Training of Dependency Parsers},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({{ACL}}'05)},
  author = {McDonald, Ryan and Crammer, Koby and Pereira, Fernando},
  year = {2005},
  pages = {91--98},
  publisher = {Association for Computational Linguistics},
  address = {Ann Arbor, Michigan},
  doi = {10.3115/1219840.1219852},
  bdsk-url-2 = {https://doi.org/10.3115/1219840.1219852}
}

@article{mcdonald.s:2003,
  title = {Eye Movements Reveal the On-Line Computation of Lexical Probabilities during Reading},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = nov,
  journal = {Psychological Science},
  volume = {14},
  number = {6},
  pages = {648--652},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1046/j.0956-7976.2003.psci_1480.x},
  urldate = {2022-10-13},
  abstract = {Skilled readers are able to derive meaning from a stream of visual input with remarkable efficiency. In this article, we present the first evidence that statistical information latent in the linguistic environment can contribute to an account of reading behavior. In two eye-tracking studies, we demonstrate that the transitional probabilities between words have a measurable influence on fixation durations, and using a simple Bayesian statistical model, we show that lexical probabilities derived by combining transitional probability with the prior probability of a word's occurrence provide the most parsimonious account of the eye movement data. We suggest that the brain is able to draw upon statistical information in order to rapidly estimate the lexical probabilities of upcoming words: a computationally inexpensive mechanism that may underlie proficient reading.},
  langid = {english},
  file = {~/Zotfiles/mcdonald.s2003a Eye movements reveal the on-line computa.pdf}
}

@article{mcdonald.s:2003a,
  title = {Low-Level Predictive Inference in Reading: The Influence of Transitional Probabilities on Eye Movements},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = jul,
  journal = {Vision Research},
  volume = {43},
  number = {16},
  pages = {1735--1751},
  publisher = {Elsevier BV},
  doi = {10.1016/s0042-6989(03)00237-2}
}

@book{mcelreath.r:2020book2,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {2},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  isbn = {978-0-367-13991-9},
  langid = {english}
}

@article{mcfadden.t:2018,
  title = {What the {{EPP}} and Comp-Trace Effects Have in Common: {{Constraining}} Silent Elements at the Edge},
  author = {McFadden, Thomas and Sundaresan, Sandhya},
  year = {2018},
  journal = {Glossa: a journal of general linguistics},
  volume = {3},
  number = {1},
  publisher = {Ubiquity Press},
  date-added = {2020-02-02 08:05:04 -0500},
  date-modified = {2020-02-02 08:05:55 -0500},
  keywords = {EPP}
}

@article{mcgill.w:1954,
  title = {Multivariate Information Transmission},
  author = {McGill, W.},
  year = {1954},
  month = sep,
  journal = {Transactions of the IRE Professional Group on Information Theory},
  volume = {4},
  number = {4},
  pages = {93--111},
  issn = {2168-2704},
  doi = {10.1109/TIT.1954.1057469},
  urldate = {2024-05-03},
  abstract = {A multivariate analysis based on transmitted information is presented. It is shown that sample transmitted information provides a simple method for measuring and testing association in multidimensional contingency tables. Relations with analysis of variance are pointed out, and statistical tests are described.},
  keywords = {Analysis of variance,Communication systems,Contracts,Humans,Information analysis,Laboratories,Multidimensional systems,Organisms,Psychology,Testing},
  file = {~/Zotfiles/mcgill.w1954 Multivariate information transmission.pdf}
}

@misc{mcguffie.k:2020,
  title = {The Radicalization Risks of {{GPT-3}} and Advanced Neural Language Models},
  author = {McGuffie, Kris and Newhouse, Alex},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2009.06807},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.06807},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-15 11:01:44 -0400},
  date-modified = {2022-03-15 11:01:46 -0400},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences},
  file = {~/Zotfiles/mcguffie.k2020 The radicalization risks of GPT-3 and ad.pdf}
}

@article{meehl.p:1957,
  title = {When Shall We Use Our Heads Instead of the Formula?},
  author = {Meehl, Paul E.},
  year = {1957},
  journal = {Journal of Counseling Psychology},
  volume = {4},
  number = {4},
  pages = {268--273},
  publisher = {Wm. C. Brown Co.},
  address = {US},
  issn = {1939-2168},
  doi = {10.1037/h0047554},
  abstract = {The statistical vs. clinical prediction issue as applied to daily clinical decisions. The problem of pragmatic decisions, the theoretical derivation of novel patterns, and the relationship of nonfrequentist probability and rational action are considered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Clinical Judgment (Not Diagnosis),Decision Making,Statistics},
  file = {~/Zotfiles/meehl.p1957 When shall we use our heads instead of t.pdf}
}

@article{mehta.p:2019,
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  year = {2019},
  month = may,
  journal = {Physics Reports},
  series = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  volume = {810},
  pages = {1--124},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.03.001},
  urldate = {2022-07-11},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias--variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton--proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
  langid = {english},
  keywords = {boltzmann machines,energy models},
  file = {~/Zotfiles/mehta.p2019 A high-bias, low-variance introduction t.pdf}
}

@inproceedings{meister.c:2020,
  title = {If Beam Search Is the Answer, What Was the Question?},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
  year = {2020},
  pages = {2173--2185},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.170},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.170},
  date-modified = {2022-03-31 09:40:23 -0400},
  keywords = {beam search,parsing}
}

@article{meister.c:2020tacl,
  title = {Best-First Beam Search},
  author = {Meister, Clara and Vieira, Tim and Cotterell, Ryan},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {795--809},
  publisher = {MIT Press - Journals},
  doi = {10.1162/tacl_a_00346},
  date-added = {2022-03-31 09:48:44 -0400},
  date-modified = {2022-03-31 09:55:48 -0400},
  keywords = {beam search,memory,parsing,space-complexity,time-complexity},
  file = {~/Zotfiles/meister.c2020tacl Best-first beam search.pdf}
}

@inproceedings{meister.c:2021,
  title = {Revisiting the Uniform Information Density Hypothesis},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Meister, Clara and Pimentel, Tiago and Haller, Patrick and J{\"a}ger, Lena and Cotterell, Ryan and Levy, Roger},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.emnlp-main.74},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.74},
  date-added = {2022-04-15 16:06:48 -0400},
  date-modified = {2022-04-15 16:06:50 -0400},
  file = {~/Zotfiles/meister.c2021 Revisiting the uniform information densi.pdf}
}

@inproceedings{meister.c:2022,
  title = {On the Probability--Quality Paradox in Language Generation},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Meister, Clara and Wiher, Gian and Pimentel, Tiago and Cotterell, Ryan},
  year = {2022},
  month = may,
  pages = {36--45},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-short.5},
  urldate = {2023-07-24},
  abstract = {When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text---covering multiple tasks and common decoding strategies---suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.},
  file = {~/Zotfiles/meister.c2022 On the probabilityquality paradox in la.pdf}
}

@article{meister.c:2023,
  title = {Locally {{Typical Sampling}}},
  author = {Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  year = {2023},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {102--121},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00536},
  urldate = {2023-07-24},
  abstract = {Today's probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process---which allows for an information-theoretic analysis---can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.},
  file = {~/Zotfiles/meister.c2023 Locally Typical Sampling.pdf}
}

@book{melcuk.i:1988book,
  title = {Dependency Syntax : {{Theory}} and Practice},
  author = {Mel'{\v c}uk, Igor A.},
  year = {1988},
  series = {{{SUNY}} Series in Linguistics},
  publisher = {State University of New York Press},
  address = {Albany, N.Y.},
  abstract = {This work presents the first sustained examination of Dependency Syntax. In clear and stimulating analyses Mel'cuk promotes syntactic description in terms of dependency rather than in terms of more familiar phrase-structure. The notions of dependency relations and dependency structure are introduced and substantiated, and the advantages of dependency representation are demonstrated by applying it to a number of popular linguistic problems, e.g. grammatical subject and ergative construction. A wide array of linguistic data is used -- the well-known (Dyirbal), the less known (Lezgian), and the more recent (Alutor). Several "exotic" cases of Russian are discussed to show how dependency can be used to solve difficult technical problems. The book is not only formal and rigorous, but also strongly theory-oriented and data-based. Special attention is paid to linguistic terminology, specifically to its logical consistency. The dependency formalism is presented within the framework of a new semantics-oriented general linguistic theory, Meaning-Text theory.},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-17 10:33:46 -0400},
  isbn = {978-0-88706-450-0},
  keywords = {Dependency Grammar}
}

@article{menne.m:2012,
  title = {An Overview of the Global Historical Climatology Network-Daily Database},
  author = {Menne, Matthew J. and Durre, Imke and Vose, Russell S. and Gleason, Byron E. and Houston, Tamara G.},
  year = {2012},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {29},
  number = {7},
  pages = {897--910},
  publisher = {American Meteorological Society},
  doi = {10.1175/jtech-d-11-00103.1},
  bdsk-url-2 = {https://doi.org/10.1175/jtech-d-11-00103.1},
  date-added = {2021-12-03 19:02:07 -0500},
  date-modified = {2021-12-03 19:02:09 -0500}
}

@inproceedings{merkx.d:2021,
  title = {Human Sentence Processing: {{Recurrence}} or Attention?},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Merkx, Danny and Frank, Stefan L.},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.cmcl-1.2},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.2},
  date-added = {2021-11-29 10:15:11 -0500},
  date-modified = {2021-11-29 10:15:12 -0500},
  file = {~/Zotfiles/merkx.d2021 Human sentence processing Recurrence or.pdf}
}

@article{metropolis.n:1949,
  title = {The {{Monte Carlo}} Method},
  author = {Metropolis, Nicholas and Ulam, S.},
  year = {1949},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {44},
  number = {247},
  pages = {335--341},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.1949.10483310},
  urldate = {2025-02-18},
  langid = {english}
}

@article{metropolis.n:1953,
  title = {Equation of State Calculations by Fast Computing Machines},
  author = {Metropolis, Nicholas and Rosenbluth, Arianna W. and Rosenbluth, Marshall N. and Teller, Augusta H. and Teller, Edward},
  year = {1953},
  month = jun,
  journal = {The Journal of Chemical Physics},
  volume = {21},
  number = {6},
  pages = {1087--1092},
  issn = {0021-9606, 1089-7690},
  doi = {10.1063/1.1699114},
  urldate = {2025-02-18},
  abstract = {A general method, suitable for fast computing machines, for investigating such properties as equations of state for substances consisting of interacting individual molecules is described. The method consists of a modified Monte Carlo integration over configuration space. Results for the two-dimensional rigid-sphere system have been obtained on the Los Alamos MANIAC and are presented here. These results are compared to the free volume equation of state and to a four-term virial coefficient expansion.},
  langid = {english}
}

@article{meylan.s:2021,
  title = {The {{Challenges}} of {{Large-Scale}}, {{Web-Based Language Datasets}}: {{Word Length}} and {{Predictability Revisited}}},
  shorttitle = {The {{Challenges}} of {{Large-Scale}}, {{Web-Based Language Datasets}}},
  author = {Meylan, Stephan C. and Griffiths, Thomas L.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12983},
  issn = {1551-6709},
  doi = {10.1111/cogs.12983},
  urldate = {2025-03-06},
  abstract = {Language research has come to rely heavily on large-scale, web-based datasets. These datasets can present significant methodological challenges, requiring researchers to make a number of decisions about how they are collected, represented, and analyzed. These decisions often concern long-standing challenges in corpus-based language research, including determining what counts as a word, deciding which words should be analyzed, and matching sets of words across languages. We illustrate these challenges by revisiting ``Word lengths are optimized for efficient communication'' (Piantadosi, Tily, \& Gibson, 2011), which found that word lengths in 11 languages are more strongly correlated with their average predictability (or average information content) than their frequency. Using what we argue to be best practices for large-scale corpus analyses, we find significantly attenuated support for this result and demonstrate that a stronger relationship obtains between word frequency and length for a majority of the languages in the sample. We consider the implications of the results for language research more broadly and provide several recommendations to researchers regarding best practices.},
  copyright = {{\copyright} 2021 Cognitive Science Society LLC},
  langid = {english},
  keywords = {Compression,Corpus linguistics,Information theory,Linguistic universals,n-Gram models,Noisy channel communication,Uniform information density},
  file = {~/Zotfiles/meylan.s2021a The Challenges of Large-Scale, Web-Based.pdf}
}

@article{meylan.s:2021a,
  title = {The Challenges of Large-Scale, Web-Based Language Datasets: {{Word}} Length and Predictability Revisited},
  author = {Meylan, Stephan C. and Griffiths, Thomas L.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  publisher = {Wiley},
  doi = {10.1111/cogs.12983},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12983},
  date-added = {2021-07-25 10:58:38 -0400},
  date-modified = {2021-07-25 10:58:39 -0400}
}

@article{michaelov.j:2021cogsci,
  title = {Different Kinds of Cognitive Plausibility: Why Are Transformers Better than {{RNNs}} at Predicting {{N400}} Amplitude?},
  shorttitle = {Different Kinds of Cognitive Plausibility},
  author = {Michaelov, James A. and Bardolph, Megan D. and Coulson, Seana and Bergen, Benjamin},
  year = {2021},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {43},
  number = {43},
  urldate = {2023-08-18},
  abstract = {Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation---their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.},
  langid = {english},
  file = {~/Zotfiles/michaelov.j2021 Different kinds of cognitive plausibilit.pdf}
}

@inproceedings{michaelov.j:2022,
  title = {Collateral Facilitation in Humans and Language Models},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Michaelov, James and Bergen, Benjamin},
  year = {2022},
  pages = {13--26},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates (Hybrid)},
  doi = {10.18653/v1/2022.conll-1.2},
  urldate = {2024-03-01},
  abstract = {Are the predictions of humans and language models affected by similar things? Research suggests that while comprehending language, humans make predictions about upcoming words, with more predictable words being processed more easily. However, evidence also shows that humans display a similar processing advantage for highly anomalous words when these words are semantically related to the preceding context or to the most probable continuation. Using stimuli from 3 psycholinguistic experiments, we find that this is also almost always also the case for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa, XLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of this phenomenon for our understanding of both human language comprehension and the predictions made by language models.},
  langid = {english},
  file = {~/Zotfiles/michaelov.j2022 Collateral facilitation in humans and la.pdf}
}

@article{michaelov.j:2023,
  title = {Strong {{Prediction}}: {{Language Model Surprisal Explains Multiple N400 Effects}}},
  shorttitle = {Strong {{Prediction}}},
  author = {Michaelov, James A. and Bardolph, Megan D. and Van Petten, Cyma K. and Bergen, Benjamin K. and Coulson, Seana},
  year = {2023},
  month = jun,
  journal = {Neurobiology of Language},
  pages = {1--29},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00105},
  urldate = {2023-09-08},
  abstract = {Theoretical accounts of the N400 are divided as to whether the amplitude of the N400 response to a stimulus reflects the extent to which the stimulus was predicted, the extent to which the stimulus is semantically similar to its preceding context, or both. We use state-ofthe-art machine learning tools to investigate which of these three accounts is best supported by the evidence. GPT-3, a neural language model trained to compute the conditional probability of any word based on the words that precede it, was used to operationalize contextual predictability. In particular, we used an information-theoretic construct known as surprisal (the negative logarithm of the conditional probability). Contextual semantic similarity was operationalized by using two high-quality co-occurrence-derived vector-based meaning representations for words: GloVe and fastText. The cosine between the vector representation of the sentence frame and final word was used to derive contextual cosine similarity estimates. A series of regression models were constructed, where these variables, along with cloze probability and plausibility ratings, were used to predict single trial N400 amplitudes recorded from healthy adults as they read sentences whose final word varied in its predictability, plausibility, and semantic relationship to the likeliest sentence completion. Statistical model comparison indicated GPT-3 surprisal provided the best account of N400 amplitude and suggested that apparently disparate N400 effects of expectancy, plausibility, and contextual semantic similarity can be reduced to variation in the predictability of words. The results are argued to support predictive coding in the human language network.},
  langid = {english},
  file = {~/Zotfiles/michaelov.j2023 Strong Prediction Language Model Surpri.pdf}
}

@article{michaelov.j:2024,
  title = {On the Mathematical Relationship between Contextual Probability and {{N400}} Amplitude},
  author = {Michaelov, James A. and Bergen, Benjamin K.},
  year = {2024},
  month = jun,
  journal = {Open Mind},
  volume = {8},
  pages = {859--897},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00150},
  urldate = {2024-07-20},
  abstract = {Accounts of human language comprehension propose different mathematical relationships between the contextual probability of a word and how difficult it is to process, including linear, logarithmic, and super-logarithmic ones. However, the empirical evidence favoring any of these over the others is mixed, appearing to vary depending on the index of processing difficulty used and the approach taken to calculate contextual probability. To help disentangle these results, we focus on the mathematical relationship between corpus-derived contextual probability and the N400, a neural index of processing difficulty. Specifically, we use 37 contemporary transformer language models to calculate the contextual probability of stimuli from 6 experimental studies of the N400, and test whether N400 amplitude is best predicted by a linear, logarithmic, super-logarithmic, or sub-logarithmic transformation of the probabilities calculated using these language models, as well as combinations of these transformed metrics. We replicate the finding that on some datasets, a combination of linearly and logarithmically-transformed probability can predict N400 amplitude better than either metric alone. In addition, we find that overall, the best single predictor of N400 amplitude is sub-logarithmically-transformed probability, which for almost all language models and datasets explains all the variance in N400 amplitude otherwise explained by the linear and logarithmic transformations. This is a novel finding that is not predicted by any current theoretical accounts, and thus one that we argue is likely to play an important role in increasing our understanding of how the statistical regularities of language impact language comprehension.},
  file = {~/Zotfiles/michaelov.j2024 On the Mathematical Relationship Between.pdf}
}

@phdthesis{michaelov.j:2024phd,
  title = {Understanding the Role of Statistics in the Predictive Processing of Language},
  author = {Michaelov, James A.},
  year = {2024},
  journal = {ProQuest Dissertations and Theses},
  address = {San Diego, California},
  abstract = {In recent years, converging evidence has suggested that prediction plays a role in language comprehension, as it appears to do in information processing in a range of cognitive domains. Much of the evidence for this comes from the N400, a neural index of the processing of meaningful stimuli which has been argued to index the extent to which a word was predicted before it was encountered. The main aim of this thesis is to investigate the extent to which this prediction can be explained as arising from the statistics of the linguistic inputs we receive over the course of our lives, in line with predictive processing in other cognitive domains. To do this, I turn to language models---computational systems that can calculate the probability of a word given its context based on the statistics of language---and investigate how well their predictions correlate with the N400. The results show that probabilities calculated using language models are highly correlated with N400 amplitude, in many cases better than human-derived metrics such as cloze probability and plausibility, previously the best predictors of the N400. I also show that language model probabilities are able to qualitatively model a wide range of effects, showing significant differences based on the same experimental manipulations that lead to significant differences in N400 amplitude. In addition, the results show that language models that are better able to predict the next word in a sequence are better able to model N400 amplitude in both of these ways, showing both a closer fit to the data and more of the qualitative effects. Taken together, these results show a high degree of correlation between the N400 and predictions based on the statistics of language, consistent with the idea that the predictions indexed by the N400 are at least partly based on language statistics.},
  collaborator = {Bergen, Benjamin K.},
  isbn = {9798384424857},
  langid = {english},
  school = {University of California, San Diego},
  keywords = {Artificial intelligence,Cloze probability,Cognitive psychology,Experimental manipulations,Language comprehension,Language statistics,Linguistic inputs,Linguistics,Neurosciences},
  annotation = {31335087},
  file = {~/Zotfiles/michaelov.j2024phd Understanding the Role of Statistics in.pdf}
}

@article{michel.j:2024,
  title = {Distributions for Compositionally Differentiating Parametric Discontinuities},
  author = {Michel, Jesse and Mu, Kevin and Yang, Xuanda and Bangaru, Sai Praveen and Collins, Elias Rojas and Bernstein, Gilbert and {Ragan-Kelley}, Jonathan and Carbin, Michael and Li, Tzu-Mao},
  year = {2024},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {8},
  number = {OOPSLA1},
  pages = {126:893--126:922},
  doi = {10.1145/3649843},
  urldate = {2024-05-09},
  abstract = {Computations in physical simulation, computer graphics, and probabilistic inference often require the differentiation of discontinuous processes due to contact, occlusion, and changes at a point in time. Popular differentiable programming languages, such as PyTorch and JAX, ignore discontinuities during differentiation. This is incorrect for parametric discontinuities---conditionals containing at least one real-valued parameter and at least one variable of integration. We introduce Potto, the first differentiable first-order programming language to soundly differentiate parametric discontinuities. We present a denotational semantics for programs and program derivatives and show the two accord. We describe the implementation of Potto, which enables separate compilation of programs. Our prototype implementation overcomes previous compile-time bottlenecks achieving an 88.1x and 441.2x speed up in compile time and a 2.5x and 7.9x speed up in runtime, respectively, on two increasingly large image stylization benchmarks. We showcase Potto by implementing a prototype differentiable renderer with separately compiled shaders.},
  keywords = {Denotational Semantics,Differentiable Programming,Differentiable Rendering,Distribution Theory,Probabilistic Programming},
  file = {~/Zotfiles/michel.j2024 Distributions for compositionally differ.pdf}
}

@book{michelson.k:2016book,
  title = {Iroquoian {{Languages}}},
  author = {Michelson, Karin},
  year = {2016},
  month = aug,
  publisher = {Oxford University Press},
  doi = {10.1093/acrefore/9780199384655.013.47},
  urldate = {2022-05-30},
  abstract = {The Iroquoian languages are spoken today in New York State, Ontario, Quebec, Wisconsin, North Carolina, and Oklahoma. The languages share a relatively small segment inventory, a challenging accentual system, polysynthetic morphology, a complex system of pronominal affixes, an unusual kinship terminology, and a syntax that functions almost exclusively to combine the meaning of two expressions. Some of the languages have been documented since contact with Europeans in the 16th century. There exists substantial scholarly linguistic work on most of the languages, and solid teaching materials continue to be developed.},
  isbn = {978-0-19-938465-5},
  langid = {english},
  keywords = {iroquoian}
}

@inproceedings{mikolov.t:2013,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Mikolov, Tom{\'a}s and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  pages = {746--751},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, Georgia}
}

@inproceedings{mikolov.t:2013a,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26: 27th Annual Conference on Neural Information Processing Systems 2013. {{Proceedings}} of a Meeting Held {{December}} 5-8, 2013, {{Lake Tahoe}}, {{Nevada}}, {{United States}}},
  author = {Mikolov, Tom{\'a}s and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
  editor = {Burges, Christopher J. C. and Bottou, L{\'e}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  year = {2013},
  pages = {3111--3119},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{miller.g:1957,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information.},
  shorttitle = {The Magical Number Seven, plus or Minus Two},
  author = {Miller, George A.},
  year = {1957},
  month = feb,
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {81},
  publisher = {US: American Psychological Association},
  issn = {1939-1471},
  doi = {10.1037/h0043158},
  urldate = {2022-09-25},
  file = {~/Zotfiles/miller.g1957 The magical number seven, plus or minus.pdf}
}

@incollection{miller.g:1963,
  title = {Finitary Models of Language Users},
  booktitle = {Handbook of Mathematical Psychology},
  author = {Miller, George A. and Chomsky, Noam},
  editor = {Luce, D.},
  year = {1963},
  pages = {2--419},
  publisher = {John Wiley \& Sons.},
  date-added = {2022-03-31 11:48:29 -0400},
  date-modified = {2022-03-31 11:48:31 -0400}
}

@misc{milligan.s:2025HSP,
  type = {Talk},
  title = {Linguistic and Oculomotor Causes and Consequences of Word Skipping: {{Insights}} from Parafoveal {{N400}} Fixation-Related Potentials},
  author = {Milligan, Sara and Schotter, Elizabeth R.},
  year = {2025},
  month = mar,
  address = {University of Maryland, College Park}
}

@inproceedings{milward.d:1995,
  title = {Incremental Interpretation of Categorial Grammar},
  booktitle = {Seventh Conference of the {{European}} Chapter of the Association for Computational Linguistics},
  author = {Milward, David},
  year = {1995},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland}
}

@inproceedings{min.s:2022,
  title = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  pages = {5316--5330},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.365},
  urldate = {2022-10-25},
  abstract = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  file = {~/Zotfiles/min.s2022 Noisy channel language model prompting f.pdf}
}

@inproceedings{minka.t:2001,
  title = {Expectation Propagation for Approximate {{Bayesian}} Inference},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas Peter},
  year = {2001},
  month = aug,
  series = {{{UAI}}'01},
  pages = {362--369},
  publisher = {Morgan Kaufmann Publishers Inc., San Francisco, CA},
  address = {Seattle, WA},
  urldate = {2023-03-30},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation," unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  isbn = {978-1-55860-800-9},
  file = {~/Zotfiles/minka.t2001 Expectation propagation for approximate.pdf}
}

@phdthesis{minka.t:2001phd,
  title = {A Family of Algorithms for Approximate {{Bayesian}} Inference},
  author = {Minka, Thomas Peter},
  year = {2001},
  address = {Cambridge, MA},
  urldate = {2023-03-30},
  abstract = {Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {expectation propogation},
  file = {~/Zotfiles/minka.t2001phd A family of algorithms for approximate B.pdf}
}

@incollection{mitchell.d:1984,
  title = {An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading                      1},
  booktitle = {New {{Methods}} in {{Reading Comprehension Research}}},
  author = {Mitchell, Don C.},
  year = {1984},
  publisher = {Routledge},
  abstract = {Over the last 5-6 years my colleagues and I have made use of three main experimental techniques for investigating immediate processing. In chronological order these were the Rapid Sequential Visual Presentation (RSVP) task, the Subject-Paced Reading Task and various types of priming tasks.},
  isbn = {978-0-429-50537-9},
  file = {~/Zotfiles/mitchell.d1984 An evaluation of subject-paced reading t.pdf}
}

@inproceedings{mitchell.j:2010,
  title = {Syntactic and Semantic Factors in Processing Difficulty: {{An}} Integrated Measure},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Mitchell, Jeff and Lapata, Mirella and Demberg, Vera and Keller, Frank},
  year = {2010},
  pages = {196--206},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  file = {~/Zotfiles/mitchell.j2010 Syntactic and semantic factors in proces.pdf}
}

@incollection{mithun.m:2014,
  title = {Syntactic and Prosodic Structures: {{Segmentation}}, Integration, and in Between},
  booktitle = {Spoken {{Corpora}} and {{Linguistic Studies}}},
  author = {Mithun, Marianne},
  editor = {Raso, Tommaso and Mello, Heliana},
  year = {2014},
  series = {Studies in {{Corpus Linguistics}}},
  volume = {61},
  pages = {297--330},
  publisher = {John Benjamins Publishing Company},
  abstract = {In this paper the focus is on syntactic and prosodic structures in a language that is typologically quite different from the majority languages of Europe and Asia. Mohawk, a language of the Iroquoian family, is indigenous to northeastern North America. Examples cited here are drawn from unscripted conversations. Though much of the grammatical structure of Mohawk differs substantially from that of European languages, many of the devices exploited by speakers to shape the flow of information converge.},
  langid = {english},
  keywords = {iroquoian},
  file = {~/Zotfiles/mithun.m2014 Syntactic and prosodic structures Segme.pdf}
}

@incollection{mithun.m:2020,
  title = {Discourse Particle Position and Information Structure},
  booktitle = {Information-{{Structural Perspectives}} on {{Discourse Particles}}},
  author = {Mithun, Marianne},
  editor = {Modicom, Pierre-Yves and Dupl{\^a}tre, Olivier},
  year = {2020},
  month = mar,
  series = {Studies in {{Language Companion Series}}},
  number = {213},
  pages = {27--46},
  publisher = {John Benjamins Publishing Company},
  doi = {10.1075/slcs.213.01mit},
  urldate = {2022-05-30},
  abstract = {Discourse markers differ cross-linguistically not only in their functions but also in their positions within the sentence. Some are sentence-initial, some are sentence-final, and some occur in what has been termed the `middle-field'. But many appear simply in second position in the sentence. In many cases the positions of the markers can be explained in terms of the source constructions from which they emerged. Here one likely pathway of development is traced in Mohawk, indigenous to North America, illustrated with a pervasive marker of discourse coherence. Patterns in the modern language suggest that it and others emerged from marked information structures, which, over time, evolved into basic clause structures via familiar mechanisms of grammaticalization.},
  langid = {english},
  keywords = {iroquoian}
}

@article{mollica.f:2017,
  title = {How Data Drive Early Word Learning: A Cross-Linguistic Waiting Time Analysis},
  author = {Mollica, Francis and Piantadosi, Steven T.},
  year = {2017},
  month = sep,
  journal = {Open Mind},
  volume = {1},
  number = {2},
  pages = {67--77},
  issn = {2470-2986},
  doi = {10.1162/OPMI_a_00006},
  urldate = {2022-09-28},
  abstract = {The extent to which word learning is delayed by maturation as opposed to accumulating data is a longstanding question in language acquisition. Further, the precise way in which data influence learning on a large scale is unknown---experimental results reveal that children can rapidly learn words from single instances as well as by aggregating ambiguous information across multiple situations. We analyze Wordbank, a large cross-linguistic dataset of word acquisition norms, using a statistical waiting time model to quantify the role of data in early language learning, building off Hidaka (2013). We find that the model both fits and accurately predicts the shape of children's growth curves. Further analyses of model parameters suggest a primarily data-driven account of early word learning. The parameters of the model directly characterize both the amount of data required and the rate at which informative data occurs. With high statistical certainty, words require on the order of {$\sim$} 10 learning instances, which occur on average once every two months. Our method is extremely simple, statistically principled, and broadly applicable to modeling data-driven learning effects in development.},
  file = {~/Zotfiles/mollica.f2017 How data drive early word learning a cr.pdf}
}

@article{montague.r:1970,
  title = {Universal Grammar},
  author = {Montague, Richard},
  year = {1970},
  journal = {Theoria: a Swedish journal of philosophy and psychology},
  volume = {36},
  number = {3},
  pages = {373--398},
  doi = {10.1111/j.1755-2567.1970.tb00434.x},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@phdthesis{montalbetti.m:1984phd,
  title = {After Binding : On the Interpretation of Pronouns},
  shorttitle = {After Binding},
  author = {Montalbetti, Mario M.},
  year = {1984},
  urldate = {2023-08-01},
  abstract = {This thesis is a study of the interpretation of pronouns, in particular, of the interpretive differences between overt and empty pronouns in certain configurations involving binding phenomena. We have captured these differences by means of a constraint which we have called the Overt Pronoun Constraint (Ope) and which is operative at the level of Logical Form. Informally, the ope states that overt pronouns that are in contrastive distribution with empty ones cannot link to formal variables (where by formal variable we roughly mean WH and QR traces). Some theoretically interesting consequences follow from the ope. For one thing, it shows that the lexical realization (or not) of a pronoun carries with it important interpretive consequences hence arguing for the view that the so called Null Subject Parameter has relevant LF properties. Indeed, if overt pronouns Cof the type nlentioned) cannot link to formal variables then they cannot be interpreted as bound variables. However, there are certain configurations in which overt pronouns can act as bound variables, and these configurations involve the presence of an extra bound pronoun which serves as a gate for binding. We will show that these cases present us with empirical evidence in favor of a Linking theory of binding (as outlined in Higginbotham 1983). Furthermore we use the ope as a diagnostic for both the existence and nature of certain controversial empty categories that occur in constructions such as clitic constructions, restructuring constructions, empty operator binding constructions, etc. The case of sloppy identity is also analyzed in terms of the ope. Although our analysis is based on the behavior of Spanish pronouns, we extend it to cover the behavior of pronouns in other Romance languages (Italian, Portuguese, Catalan) as well as in languages like Japanese and Chinese. The hope is thus parametrized to account for the subtle differences which underlie the striking similarities between the languages studied.},
  copyright = {M.I.T. theses are protected by  copyright. They may be viewed from this source for any purpose, but  reproduction or distribution in any format is prohibited without written  permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {comparative illusions}
}

@incollection{moortgat.m:1997,
  title = {Categorial Type Logics},
  booktitle = {Handbook of Logic and Language},
  author = {Moortgat, M.},
  year = {1997},
  pages = {93--177},
  publisher = {Elsevier},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:01 -0400}
}

@incollection{muller.h:2020,
  title = {Negative Polarity Illusions},
  booktitle = {The {{Oxford Handbook}} of {{Negation}}},
  author = {Muller, Hanna and Phillips, Colin},
  editor = {D{\'e}prez, Viviane and Espinal, M. Teresa},
  year = {2020},
  month = mar,
  pages = {656--676},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780198830528.013.42},
  urldate = {2023-02-22},
  abstract = {Although decades of research have illuminated the licensing requirements, both syntactic and semantic, of negative polarity items, the matter of how these licensing requirements are satisfied in real time, as a sentence is being processed, remains an ill-understood problem. Grammatical illusions---cases where native speakers, as they comprehend an ungrammatical sentence, experience a fleeting perception of acceptability---offer a window into online computations like NPI licensing. This chapter reviews the findings on negative polarity illusions, their parallels (and, in some cases, the lack of parallels) with other grammaticality illusions, and the implications of this line of research for understanding the incremental processing of negative sentences as well as negative polarity phenomena more broadly.},
  isbn = {978-0-19-883052-8},
  keywords = {grammaticality illusions,negative polarity illusions},
  file = {~/Zotfiles/muller.h2020 Negative polarity illusions.pdf}
}

@article{murray.w:2004,
  title = {Serial Mechanisms in Lexical Access: The Rank Hypothesis.},
  author = {Murray, Wayne S and Forster, Kenneth I},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {3},
  pages = {721},
  publisher = {American Psychological Association},
  doi = {10.1037/0033-295X.111.3.721},
  date-added = {2021-02-16 16:15:20 -0500},
  date-modified = {2021-02-16 16:16:45 -0500},
  keywords = {frequency effects,psycholinguistics,rank hypothesis,word access},
  file = {~/Zotfiles/murray.w2004 Serial mechanisms in lexical access the.pdf}
}

@article{nadas.a:1984,
  title = {Estimation of Probabilities in the Language Model of the {{IBM}} Speech Recognition System},
  author = {N{\'a}das, Arthur},
  year = {1984},
  month = aug,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {32},
  number = {4},
  pages = {859--861},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1984.1164378},
  urldate = {2024-08-02},
  abstract = {The language model probabilities are estimated by an empirical Bayes approach in which a prior distribution for the unknown probabilities is itself estimated through a novel choice of data. The predictive power of the model thus fitted is compared by means of its experimental perplexity [1] to the model as fitted by the Jelinek-Mercer deleted estimator and as fitted by the Turing-Good formulas for probabilities of unseen or rarely seen events.},
  keywords = {Bayesian methods,Cities and towns,Natural languages,Power system modeling,Predictive models,Probability,Smoothing methods,Speech recognition,Vocabulary}
}

@inproceedings{naesseth.c:2018VSMC,
  title = {Variational Sequential Monte Carlo},
  booktitle = {Proceedings of the {{Twenty-First International Conference}} on {{Artificial Intelligence}} and {{Statistics}}},
  author = {Naesseth, Christian and Linderman, Scott and Ranganath, Rajesh and Blei, David},
  year = {2018},
  month = mar,
  pages = {968--977},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-07-30},
  abstract = {Many recent advances in large scale probabilistic inference rely on variational methods. The success of variational approaches depends on (i) formulating a flexible parametric family of distributions, and (ii) optimizing the parameters to find the member of this family that most closely approximates the exact posterior. In this paper we present a new approximating family of distributions, the variational sequential Monte Carlo (VSMC) family, and show how to optimize it in variational inference. VSMC melds variational inference (VI) and sequential Monte Carlo (SMC), providing practitioners with flexible, accurate, and powerful Bayesian inference. The VSMC family is a variational family that can approximate the posterior arbitrarily well, while still allowing for efficient optimization of its parameters. We demonstrate its utility on state space models, stochastic volatility models for financial data, and deep Markov models of brain neural circuits.},
  langid = {english},
  file = {~/Zotfiles/naesseth.c2018 Variational Sequential Monte Carlo.pdf}
}

@article{naesseth.c:2019,
  title = {Elements of Sequential {{Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2019},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {3},
  pages = {307--392},
  publisher = {Now Publishers},
  issn = {1935-8237},
  doi = {10.1561/2200000074},
  keywords = {Bayesian learning,Learning and statistical methods,Sampling},
  annotation = {note: Accessed as e-print, arXiv:1903.04797},
  file = {~/Zotfiles/naesseth.c2019 Elements of sequential Monte Carlo.pdf}
}

@inproceedings{naesseth.c:2020,
  title = {Markovian Score Climbing: Variational Inference with {{KL}}(P{\textbar}{\textbar}q)},
  shorttitle = {Markovian Score Climbing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Naesseth, Christian and Lindsten, Fredrik and Blei, David},
  year = {2020},
  volume = {33},
  pages = {15499--15510},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-04-18},
  abstract = {Modern variational inference (VI) uses stochastic gradients to avoid intractable expectations, enabling large-scale probabilistic inference in complex models. VI posits a family of approximating distributions q and then finds the member of that family that is closest to the exact posterior p. Traditionally, VI algorithms minimize the ``exclusive Kullback-Leibler (KL)'' KL(q{\textbar}{\textbar}p), often for computational convenience. Recent research, however, has also focused on the ``inclusive KL'' KL(p{\textbar}{\textbar}q), which has good statistical properties that makes it more appropriate for certain inference problems. This paper develops a simple algorithm for reliably minimizing the inclusive KL using stochastic gradients with vanishing bias. This method, which we call Markovian score climbing (MSC), converges to a local optimum of the inclusive KL. It does not suffer from the systematic errors inherent in existing methods, such as Reweighted Wake-Sleep and Neural Adaptive Sequential Monte Carlo, which lead to bias in their final estimates. We illustrate convergence on a toy model and demonstrate the utility of MSC on Bayesian probit regression for classification as well as a stochastic volatility model for financial data.},
  file = {~/Zotfiles/naesseth.c2020 Markovian Score Climbing Variational In.pdf}
}

@inproceedings{nair.s:2023,
  title = {Words, Subwords, and Morphemes: What Really Matters in the Surprisal-Reading Time Relationship?},
  shorttitle = {Words, Subwords, and Morphemes},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Nair, Sathvik and Resnik, Philip},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {11251--11260},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.752},
  urldate = {2024-12-13},
  abstract = {An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that *in the aggregate*, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.},
  file = {~/Zotfiles/nair.s2023 Words, Subwords, and Morphemes What Rea.pdf}
}

@misc{nair.s:2023arxiv,
  title = {Words, {{Subwords}}, and {{Morphemes}}: {{What Really Matters}} in the {{Surprisal-Reading Time Relationship}}?},
  shorttitle = {Words, {{Subwords}}, and {{Morphemes}}},
  author = {Nair, Sathvik and Resnik, Philip},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17774},
  eprint = {2310.17774},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,surprisal,tokenization},
  file = {~/Zotfiles/nair.s2023arxiv Words, Subwords, and Morphemes What Rea.pdf}
}

@article{nalborczyk.l:2019,
  title = {An Introduction to {{Bayesian}} Multilevel Models Using {{{\textbf{brms}}}}: A Case Study of Gender Effects on Vowel Variability in Standard Indonesian},
  shorttitle = {An Introduction to Bayesian Multilevel Models Using Brms},
  author = {Nalborczyk, Ladislas and Batailler, C{\'e}dric and L{\oe}venbruck, H{\'e}l{\`e}ne and Vilain, Anne and B{\"u}rkner, Paul-Christian},
  year = {2019},
  month = may,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {62},
  number = {5},
  pages = {1225--1242},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2018_JSLHR-S-18-0006},
  urldate = {2024-05-21},
  abstract = {Purpose  Bayesian multilevel models are increasingly used to overcome the limitations of frequentist approaches in the analysis of complex structured data. This tutorial introduces Bayesian multilevel modeling for the specific analysis of speech data, using the brms package developed in R. Method  In this tutorial, we provide a practical introduction to Bayesian multilevel modeling by reanalyzing a phonetic data set containing formant (F1 and F2) values for 5 vowels of standard Indonesian (ISO 639-3:ind), as spoken by 8 speakers (4 females and 4 males), with several repetitions of each vowel. Results  We first give an introductory overview of the Bayesian framework and multilevel modeling. We then show how Bayesian multilevel models can be fitted using the probabilistic programming language Stan and the R package brms, which provides an intuitive formula syntax. Conclusions  Through this tutorial, we demonstrate some of the advantages of the Bayesian framework for statistical modeling and provide a detailed case study, with complete source code for full reproducibility of the analyses (https://osf.io/dpzcb/). Supplemental Material  https://doi.org/10.23641/asha.7973822},
  file = {~/Zotfiles/nalborczyk.l2019 An introduction to Bayesian multilevel m.pdf}
}

@inproceedings{narayanan.s:1998cogsci,
  title = {Bayesian Models of Human Sentence Processing},
  booktitle = {Procedings of Twentieth Annual Meeting of the Cognitive Science Society: {{University}} of Wisconsin-Madison},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  editor = {Gernsbacher, Morton Ann and Derry, Sharon J.},
  year = {1998},
  pages = {752--757},
  publisher = {Lawrence Erlbaum Associates},
  address = {Mahwah, NJ},
  date-added = {2021-03-09 22:52:27 -0500},
  date-modified = {2021-03-09 22:52:27 -0500},
  keywords = {bayesian,processing}
}

@inproceedings{narayanan.s:2001,
  title = {A {{Bayesian}} Model Predicts Human Parse Preference and Reading Times in Sentence Processing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  year = {2001},
  volume = {14},
  publisher = {MIT Press},
  urldate = {2022-06-28},
  abstract = {Narayanan and Jurafsky (1998) proposed that human language compre- hension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian de- cision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.},
  file = {~/Zotfiles/narayanan.s2001 A Bayesian model predicts human parse pr.pdf}
}

@unpublished{narayanan.s:2004ms,
  type = {Unpublished Manuscript},
  title = {A {{Bayesian}} Model of Human Sentence Processing},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  year = {2004},
  month = nov,
  file = {~/Zotfiles/narayanan.s2004 A Bayesian model of human sentence proce.pdf}
}

@inproceedings{naseem.t:2012,
  title = {Selective Sharing for Multilingual Dependency Parsing},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Naseem, Tahira and Barzilay, Regina and Globerson, Amir},
  year = {2012},
  month = jul,
  pages = {629--637},
  publisher = {Association for Computational Linguistics},
  address = {Jeju Island, Korea},
  date-added = {2022-04-04 12:41:51 -0400},
  date-modified = {2022-04-04 12:41:52 -0400}
}

@article{neal.r:2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2022-06-10},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation,rejection sampling,sampling,slice sampling},
  file = {~/Zotfiles/neal.r2003 Slice sampling.pdf}
}

@article{nevins.a:2011,
  title = {Multiple Agree with Clitics: {{Person}} Complementarity vs. Omnivorous Number},
  author = {Nevins, Andrew},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {939--971},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:19 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,omnivorous agree}
}

@inproceedings{newell.a:1959,
  title = {Report on a General Problem Solving Program},
  booktitle = {Proceedings of the {{International Conference}} on {{Information Processing}}},
  author = {Newell, Allen and Shaw, John C. and Simon, Herbert A.},
  year = {1959},
  pages = {256--264},
  publisher = {Pittsburgh, PA},
  file = {~/Zotfiles/newell.a1959 Report on a general problem solving prog.pdf}
}

@book{newell.a:1972book,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert Alexander},
  year = {1972},
  volume = {104},
  publisher = {Prentice-hall Englewood Cliffs, NJ}
}

@incollection{newell.a:1973,
  title = {Production Systems: Models of Control Structures},
  shorttitle = {Production Systems},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  editor = {Chase, William G.},
  year = {1973},
  month = jan,
  pages = {463--526},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-170150-5.50016-0},
  urldate = {2022-07-15},
  abstract = {This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works---on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.},
  isbn = {978-0-12-170150-5},
  langid = {english}
}

@article{newell.a:1981,
  title = {The Knowledge Level: Presidential Address},
  author = {Newell, Allen},
  year = {1981},
  month = sep,
  journal = {AI Magazine},
  volume = {2},
  number = {2},
  pages = {1},
  doi = {10.1609/aimag.v2i2.99},
  urldate = {2024-05-15},
  abstract = {This is the first presidential address of AAAI, the American Association for Artificial Intelligence. In the grand scheme of history of artificial intelligence (AI), this is surely a minor event. The field this scientific society represents has been thriving for quite some time. No doubt the society itself will make solid contributions to the health of our field. But it is too much to expect a presidential address to have a major impact. So what is the role of the presidential address and what is the significance of the first one? I believe its role is to set a tone, to provide an emphasis. I think the role of the first address is to take a stand about what that tone and emphasis should be-set expectations for future addresses and to communicate to my fellow presidents. Only two foci are really possible for a presidential address: the state of the society or the state of the science. I believe the latter to be correct focus. AAAI itself, its nature and its relationship to the larger society that surrounds it, are surely important. However, our main business is to help AI become a science -- albeit a science with a strong engineering flavor. Thus, though a president's address cannot be narrow or highly technical, it can certainly address a substantive issue. That is what I propose to do.},
  chapter = {Articles},
  annotation = {Also published as: Arificial intelligence, 18(1), 87--127, 1982},
  file = {~/Zotfiles/newell.a1981address The knowledge level presidential addres.pdf}
}

@incollection{newell.a:1981a,
  title = {Mechanisms of Skill Acquisition and the Law of Practice},
  shorttitle = {Mechanisms of Skill Acquisition and the Law of Practice},
  booktitle = {Cognitive {{Skills}} and {{Their Acquisition}}},
  author = {Newell, Allen and Paul, Rosenbloom},
  editor = {Anderson, John R.},
  year = {1981},
  publisher = {Psychology Press},
  doi = {10.4324/9780203728178-6},
  abstract = {Practice makes perfect. Correcting the overstatement of a maxim: Almost always, practice brings improvement, and more practice brings more improvement. We all expect improvement with practice to be ubiquitous, though obviously limits exist both in scope and extent. Take only the experimental laboratory: We do not expect people to perform an experimental task correctly without at least some practice; and we design all our psychology experiments with one eye to the confounding influence of practice effects.},
  isbn = {978-0-203-72817-8}
}

@book{newell.a:1994book,
  title = {Unified Theories of Cognition},
  author = {Newell, Allen},
  year = {1994},
  publisher = {Harvard University Press},
  abstract = {Psychology is now ready for unified theories of cognition--so says Allen Newell, a leading investigator in computer science and cognitive psychology. Not everyone will agree on a single set of mechanisms that will explain the full range of human cognition, but such theories are within reach and we should strive to articulate them.In this book, Newell makes the case for unified theories by setting forth a candidate. After reviewing the foundational concepts of cognitive science--knowledge, representation, computation, symbols, architecture, intelligence, and search--Newell introduces Soar, an architecture for general cognition. A pioneer system in artificial intelligence, Soar is the first problem solver to create its own subgoals and learn continuously from its own experience.Newell shows how Soar's ability to operate within the real-time constraints of intelligent behavior, such as immediate-response and item-recognition tasks, illustrates important characteristics of the human cognitive structure. Throughout, Soar remains an exemplar: we know only enough to work toward a fully developed theory of cognition, but Soar's success so far establishes the viability of the enterprise.Given its integrative approach, Unified Theories of Cognition will be of tremendous interest to researchers in a variety of fields, including cognitive science, artificial intelligence, psychology, and computer science. This exploration of the nature of mind, one of the great problems of philosophy, should also transcend disciplines and attract a large scientific audience.},
  googlebooks = {1lbY14DmV2cC},
  isbn = {978-0-674-92101-6},
  langid = {english},
  keywords = {Psychology / General}
}

@article{newton.m:1994,
  title = {Approximate {{Bayesian}} Inference with the Weighted Likelihood Bootstrap},
  author = {Newton, Michael A. and Raftery, Adrian E.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {56},
  number = {1},
  pages = {3--26},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1994.tb01956.x},
  urldate = {2024-06-25},
  abstract = {We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling-importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR-adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second-order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.},
  copyright = {{\copyright} 1994 Royal Statistical Society},
  langid = {english},
  keywords = {bayes factor,bayesian inference,dirichlet weights,monte carlo methods}
}

@inproceedings{nguyen.l:2012,
  title = {Accurate Unbounded Dependency Recovery Using Generalized Categorial Grammars},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Nguyen, Luan and {van Schijndel}, Marten and Schuler, William},
  year = {2012},
  pages = {2125--2140},
  publisher = {The COLING 2012 Organizing Committee},
  address = {Mumbai, India}
}

@article{nicenboim.b:2016,
  title = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed up: Working Memory and Locality Effects},
  shorttitle = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed Up},
  author = {Nicenboim, Bruno and Loga{\v c}ev, Pavel and Gattei, Carolina and Vasishth, Shravan},
  year = {2016},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00280},
  urldate = {2022-08-13},
  file = {~/Zotfiles/nicenboim.b2016 When high-capacity readers slow down and.pdf}
}

@article{nicenboim.b:2018,
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  author = {Nicenboim, Bruno and Vasishth, Shravan},
  year = {2018},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {99},
  pages = {1--34},
  issn = {0749596X},
  doi = {10.1016/j.jml.2017.08.004},
  urldate = {2022-08-13},
  langid = {english},
  file = {~/Zotfiles/nicenboim.b2018 Models of retrieval in sentence comprehe.pdf}
}

@book{nicenboim.b:2024book,
  title = {An Introduction to {{Bayesian}} Data Analysis for Cognitive Science},
  author = {Nicenboim, Bruno and Schad, Daniel and Vasishth, Shravan},
  year = {2024},
  month = mar,
  publisher = {Bookdown},
  urldate = {2024-05-21},
  abstract = {An introduction to Bayesian data analysis for Cognitive Science.}
}

@inproceedings{nichol.a:2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  month = jul,
  pages = {8162--8171},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-07-07},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  langid = {english},
  keywords = {diffusion processes},
  file = {~/Zotfiles/nichol.a2021 Improved denoising diffusion probabilist 2.pdf;~/Zotfiles/nichol.a2021 Improved denoising diffusion probabilist.pdf}
}

@article{nicklin.c:2020,
  title = {Outliers in {{L2}} Research in Applied Linguistics: A Synthesis and Data Re-Analysis},
  shorttitle = {Outliers in {{L2}} Research in Applied Linguistics},
  author = {Nicklin, Christopher and Plonsky, Luke},
  year = {2020},
  month = mar,
  journal = {Annual Review of Applied Linguistics},
  volume = {40},
  pages = {26--55},
  issn = {0267-1905, 1471-6356},
  doi = {10.1017/S0267190520000057},
  urldate = {2024-05-26},
  abstract = {Data from self-paced reading (SPR) tasks are routinely checked for statistical outliers (Marsden, Thompson, \& Plonsky, 2018). Such data points can be handled in a variety of ways (e.g., trimming, data transformation), each of which may influence study results in a different manner. This two-phase study sought, first, to systematically review outlier handling techniques found in studies that involve SPR and, second, to re-analyze raw data from SPR tasks to understand the impact of those techniques. Toward these ends, in Phase I, a sample of 104 studies that employed SPR tasks was collected and coded for different outlier treatments. As found in Marsden et al. (2018), wide variability was observed across the sample in terms of selection of time and standard deviation (SD)-based boundaries for determining what constitutes a legitimate reading time (RT). In Phase II, the raw data from the SPR studies in Phase I were requested from the authors. Nineteen usable datasets were obtained and re-analyzed using data transformations, SD boundaries, trimming, and winsorizing, in order to test their relative effectiveness for normalizing SPR reaction time data. The results suggested that, in the vast majority of cases, logarithmic transformation circumvented the need for SD boundaries, which blindly eliminate or alter potentially legitimate data. The results also indicated that choice of SD boundary had little influence on the data and revealed no meaningful difference between trimming and winsorizing, implying that blindly removing data from SPR analyses might be unnecessary. Suggestions are provided for future research involving SPR data and the handling of outliers in second language (L2) research more generally.},
  langid = {english}
}

@article{nivre.j:2008,
  title = {Algorithms for {{Deterministic Incremental Dependency Parsing}}},
  author = {Nivre, Joakim},
  year = {2008},
  month = dec,
  journal = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {513--553},
  issn = {0891-2017},
  doi = {10.1162/coli.07-056-R1-07-027},
  urldate = {2022-06-19},
  abstract = {Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.},
  file = {~/Zotfiles/nivre.j2008 Algorithms for Deterministic Incremental.pdf}
}

@inproceedings{nivre.j:2016,
  title = {Universal {{Dependencies}} v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({{LREC}}'16)},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Haji{\v c}, Jan and Manning, Christopher D. and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
  year = {2016},
  pages = {1659--1666},
  publisher = {European Language Resources Association (ELRA)},
  address = {Portoro{\v z}, Slovenia}
}

@inproceedings{nivre.j:2017,
  title = {Universal {{Dependencies}}},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Tutorial}} Abstracts},
  author = {Nivre, Joakim and Zeman, Daniel and Ginter, Filip and Tyers, Francis},
  year = {2017},
  publisher = {Association for Computational Linguistics},
  address = {Valencia, Spain},
  file = {~/Zotfiles/zeman.d2020 Universal Dependencies.pdf}
}

@misc{nivre.j:2017a,
  title = {Universal Dependencies 2.0 -- {{CoNLL}} 2017 Shared Task Development and Test Data},
  author = {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Burchardt, Aljoscha and Candito, Marie and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Cinkov{\'a}, Silvie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and {de Marneffe}, Marie-Catherine and {de Paiva}, Valeria and {Diaz de Ilarraza}, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Eli, Marhaba and Elkahky, Ali and Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'i}na and Galbraith, Daniel and Garcia, Marcos and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni, Matias and Gr{\=u}ztis, Normunds and Guillaume, Bruno and Habash, Nizar and Haji{\v c}, Jan and {Haji{\v c} jr.}, Jan and H{\`a} M{\~y}, Linh and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hohle, Petter and Ion, Radu and Irimia, Elena and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and L{\^e} Hng, Phng and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'i}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and McDonald, Ryan and Mendon{\c c}a, Gustavo and Missil{\"a}, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shunsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Mustafina, Nina and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Nedoluzhko, Anna and Nguyn Th{\d i}, Lng and Nguyn Th{\d i} Minh, Huyn and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and {\O}vrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Popel, Martin and Pretkalni{\c n}a, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Real, Livy and Reddy, Siva and Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and Rosa, Rudolf and Rovati, Davide and Saleh, Shadi and Sanguinetti, Manuela and Saulte, Baiba and Sawanakunanon, Yanin and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shakurova, Lena and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Stella, Antonio and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and {van Noord}, Gertjan and Varga, Viktor and Vincze, Veronika and Washington, Jonathan North and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeman, Daniel and Zhu, Hanzhi},
  year = {2017},
  eprint = {11234/1-2184},
  eprinttype = {hdl},
  copyright = {Licence Universal Dependencies v2.0},
  date-added = {2021-04-30 13:00:05 -0400},
  date-modified = {2021-04-30 13:02:24 -0400},
  keywords = {dependency parsing,dependency structures,universal dependencies}
}

@inproceedings{nivre.j:2020,
  title = {Universal {{Dependencies}} v2: {{An}} Evergrowing Multilingual Treebank Collection},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Haji{\v c}, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
  year = {2020},
  pages = {4034--4043},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@article{norris.d:2006,
  title = {The {{Bayesian}} Reader: {{Explaining}} Word Recognition as an Optimal {{Bayesian}} Decision Process},
  shorttitle = {The {{Bayesian}} Reader},
  author = {Norris, Dennis},
  year = {2006},
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {327--357},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.113.2.327},
  abstract = {This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. This leads to the development of a computational model of word recognition, the Bayesian reader. The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Lexical Decision,Models,Reading,Semantics,Statistical Probability,Word Recognition},
  file = {~/Zotfiles/norris.d2006 The Bayesian reader Explaining word rec.pdf}
}

@article{norris.d:2008,
  title = {Shortlist {{B}}: {{A Bayesian}} Model of Continuous Speech Recognition},
  shorttitle = {Shortlist {{B}}},
  author = {Norris, Dennis and McQueen, James M.},
  year = {2008},
  journal = {Psychological Review},
  volume = {115},
  number = {2},
  pages = {357--395},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.115.2.357},
  abstract = {A Bayesian model of continuous speech recognition is presented. It is based on Shortlist (D. Norris, 1994; D. Norris, J. M. McQueen, A. Cutler, \& S. Butterfield, 1997) and shares many of its key assumptions: parallel competitive evaluation of multiple lexical hypotheses, phonologically abstract prelexical and lexical representations, a feedforward architecture with no online feedback, and a lexical segmentation algorithm based on the viability of chunks of the input as possible words. Shortlist B is radically different from its predecessor in two respects. First, whereas Shortlist was a connectionist model based on interactive-activation principles, Shortlist B is based on Bayesian principles. Second, the input to Shortlist B is no longer a sequence of discrete phonemes; it is a sequence of multiple phoneme probabilities over 3 time slices per segment, derived from the performance of listeners in a large-scale gating study. Simulations are presented showing that the model can account for key findings: data on the segmentation of continuous speech, word frequency effects, the effects of mispronunciations on word recognition, and evidence on lexical involvement in phonemic decision making. The success of Shortlist B suggests that listeners make optimal Bayesian decisions during spoken-word recognition. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Mathematical Modeling,Speech Perception,Statistical Probability,Word Recognition}
}

@article{norris.d:2009,
  title = {Putting It All Together: {{A}} Unified Account of Word Recognition and Reaction-Time Distributions},
  shorttitle = {Putting It All Together},
  author = {Norris, Dennis},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {1},
  pages = {207--219},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/a0014259},
  abstract = {R. Ratcliff, P. Gomez, and G. McKoon (2004) suggested much of what goes on in lexical decision is attributable to decision processes and may not be particularly informative about word recognition. They proposed that lexical decision should be characterized by a decision process, taking the form of a drift-diffusion model (R. Ratcliff, 1978), that operates on the output of lexical model. The present article argues that the distinction between perception and decision making is unnecessary and that it is possible to give a unified account of both lexical processing and decision making. This claim is supported by formal arguments and reinforced by simulations showing how the Bayesian Reader model (D. Norris, 2006) can be extended to fit the data on reaction time distributions collected by Ratcliff, Gomez, and McKoon simply by adding extra sources of noise. The Bayesian Reader gives an integrated explanation of both word recognition and decision making, using fewer parameters than the diffusion model. It can be thought of as a Bayesian diffusion model, which subsumes Ratcliff's drift-diffusion model as a special case. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Lexical Decision,Models,Reaction Time,Word Recognition},
  file = {~/Zotfiles/norris.d2009 Putting it all together A unified accou.pdf}
}

@incollection{noureddine.s:2022,
  title = {The {{N400}} {\emph{in Silico}}: {{A}} Review of Computational Models},
  shorttitle = {The {{N400}} {\emph{in Silico}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Nour Eddine, Samer and Brothers, Trevor and Kuperberg, Gina R.},
  editor = {Federmeier, Kara D.},
  year = {2022},
  month = jan,
  volume = {76},
  pages = {123--206},
  publisher = {Academic Press},
  doi = {10.1016/bs.plm.2022.03.005},
  urldate = {2025-04-17},
  abstract = {The N400 event-related brain potential is elicited by each word in a sentence and offers an important window into the mechanisms of real-time language comprehension. Since the 1980s, studies investigating the N400 have expanded our understanding of how bottom-up linguistic inputs interact with top-down contextual constraints. More recently, a growing body of computational modeling research has aimed to formalize theoretical accounts of the N400 to better understand the neural and functional basis of this component. Here, we provide a comprehensive review of this literature. We discuss ``word-level'' models that focus on the N400's sensitivity to lexical factors and simple priming manipulations, as well as more recent sentence-level models that explain its sensitivity to broader context. We discuss each model's insights and limitations in relation to a set of cognitive and biological constraints that have informed our understanding of language comprehension and the N400 over the past few decades. We then review a novel computational model of the N400 that is based on the principles of predictive coding, which can accurately simulate both word-level and sentence-level phenomena. In this predictive coding account, the N400 is conceptualized as the magnitude of lexico-semantic prediction error produced by incoming words during the process of inferring their meaning. Finally, we highlight important directions for future research, including a discussion of how these computational models can be expanded to explain language-related ERP effects outside the N400 time window, and variation in N400 modulation across different populations.},
  keywords = {Event,Language comprehension,Neural network,Predictive coding,Semantic}
}

@article{noureddine.s:2024,
  title = {A Predictive Coding Model of the {{N400}}},
  author = {Nour Eddine, Samer and Brothers, Trevor and Wang, Lin and Spratling, Michael and Kuperberg, Gina R.},
  year = {2024},
  month = may,
  journal = {Cognition},
  volume = {246},
  pages = {105755},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2024.105755},
  urldate = {2025-04-03},
  abstract = {The N400 event-related component has been widely used to investigate the neural mechanisms underlying real-time language comprehension. However, despite decades of research, there is still no unifying theory that can explain both its temporal dynamics and functional properties. In this work, we show that predictive coding -- a biologically plausible algorithm for approximating Bayesian inference -- offers a promising framework for characterizing the N400. Using an implemented predictive coding computational model, we demonstrate how the N400 can be formalized as the lexico-semantic prediction error produced as the brain infers meaning from the linguistic form of incoming words. We show that the magnitude of lexico-semantic prediction error mirrors the functional sensitivity of the N400 to various lexical variables, priming, contextual effects, as well as their higher-order interactions. We further show that the dynamics of the predictive coding algorithm provides a natural explanation for the temporal dynamics of the N400, and a biologically plausible link to neural activity. Together, these findings directly situate the N400 within the broader context of predictive coding research. More generally, they raise the possibility that the brain may use the same computational mechanism for inference across linguistic and non-linguistic domains.},
  keywords = {Bayesian inference,Language comprehension,Orthographic,Prediction,Prediction error,Semantic},
  file = {~/Zotfiles/noureddine.s2024 A predictive coding model of the N400.pdf}
}

@phdthesis{noureddine.s:2024phd,
  title = {A {{Predictive Coding Account}} of {{Lexico-Semantic Processing}}},
  author = {Nour Eddine, Samer A.},
  year = {2024},
  journal = {ProQuest LLC},
  urldate = {2025-04-08},
  abstract = {In this thesis, I use a combination of simulations and empirical data to demonstrate that a small set of structural and functional principles - the basic tenets of predictive coding theory - succinctly accounts for a very wide range of properties in the language processing system. Predictive coding approximates hierarchical Bayesian inference via a biologically plausible mechanism of competition whose unique properties (i) allow it to capture important asymmetries in bottom-up and top-down processing and (ii) offer a critical insight into the functional relationship between behavioral and neural measures of word processing. In addition to accounting for well-known properties of the language processing system, this mechanism of competition allows us to formulate and test a novel empirical prediction - that N400 attenuation can be observed in the presence of behavioral interference - which challenges a widely held assumption that N400 attenuation is essentially synonymous with facilitated processing. Importantly, the computational model that we used to simulate the above psycholinguistic phenomena was tightly constrained by predictive coding principles and the particular class of algorithm we employed (PC/BC-DIM; Spratling, De Meyer, \& Kompass 2009), emphasizing how a fundamental, biologically plausible perceptual mechanism that is shared across the cortex can also give rise to psycholinguistic phenomena. [The dissertation citations contained here are published with the permission of ProQuest LLC. Further reproduction is prohibited without permission. Copies of dissertations may be obtained by Telephone (800) 1-800-521-0600. Web page: http://www.proquest.com/en-US/products/dissertations/individuals.shtml.]},
  isbn = {9798382750095},
  langid = {english},
  school = {ProQuest LLC},
  keywords = {Bayesian Statistics,Brain Hemisphere Functions,Computational Linguistics,Correlation,Interference (Language),Language Processing,Linguistic Theory,Prediction,predictive coding,Psycholinguistics,Semantics,Simulation,Vocabulary},
  annotation = {ERIC Number: ED653848},
  file = {~/Zotfiles/noureddine.s2024phd A Predictive Coding Account of Lexico-Se.pdf}
}

@article{oaksford.m:1996,
  title = {Rational Explanation of the Selection Task},
  author = {Oaksford, Mike and Chater, Nick},
  year = {1996},
  month = apr,
  journal = {Psychological Review},
  volume = {103},
  number = {2},
  pages = {381--391},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.103.2.381},
  urldate = {2024-05-15},
  langid = {english}
}

@book{oaksford.m:1998book,
  title = {Rational Models of Cognition},
  editor = {Oaksford, Mike and Chater, Nick},
  year = {1998},
  publisher = {Oxford University Press},
  address = {Oxford, England},
  isbn = {0-19-852415-3}
}

@phdthesis{oconnor.e:2015phd,
  title = {Comparative Illusions at the Syntax-Semantics Interface},
  author = {O'Connor, Ellen},
  year = {2015},
  urldate = {2023-08-01},
  abstract = {Psycholinguistic research has focused much attention on the factors that influence structural ambiguity resolution, under the assumption that meaning is derived from a selected syntactic representation in a systematic, compositional way. Problematically, however, researchers have increasingly observed examples suggesting that perceptions of sentence acceptability and meaning are not always straightforwardly constrained by logical semantics. Most English speakers, for example, initially accept the sentence More people have been to Berlin than I have until asked to explain more clearly what it means, at which point its meaninglessness becomes obvious. Meanwhile, the sentence No head injury is too trivial to ignore is overwhelmingly perceived to mean exactly the opposite of its implausible grammar-based meaning, an error that is only readily detected with extended conscious effort. The goal of this thesis is uncover what these ``semantic illusions'' tell us about semantic processing by identifying the locus of nonveridical processing. In spite of appearances I argue that it is impossible to explain perceptions of and reactions to these illusion sentences without referencing properties that influence their logical form; this suggests, contrary to existing proposals, that the illusion is generated by online computations associated with, not external to, the grammar, and that nonveridical perceptions are induced by processing mechanisms responsible for navigating the logical form of sentences that contain probable speech errors and/or those that fall at the outer boundaries of computational tractability. Because the source of these illusions is not well understood, a more general goal of this work is to establish their fundamental properties, including the nature of their percept(s) and the properties that modulate that percept, to pave the way for future research.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  langid = {english},
  school = {University of Southern California},
  keywords = {Language,literature and linguistics}
}

@techreport{odonnell.t:2009,
  type = {Technical Report},
  title = {Fragment {{Grammars}}: {{Exploring Computation}} and {{Reuse}} in {{Language}}},
  shorttitle = {Fragment {{Grammars}}},
  author = {O'Donnell, Timothy J. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {2009},
  month = mar,
  number = {MIT-CSAIL-TR-2009-013},
  institution = {{MIT Computer Science and Artificial Intelligence Laboratory}},
  urldate = {2022-06-15},
  abstract = {Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP) and in particular using a probabilistic variant of Lisp known as the Church programming language (Goodman, Mansinghka, Roy, Bonawitz, \& Tenenbaum, 2008). We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them---adaptor grammars (Johnson, Griffiths, \& Goldwater, 2007). We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.},
  langid = {english},
  file = {~/Zotfiles/odonnell.t2009 Fragment Grammars Exploring Computation.pdf}
}

@inproceedings{odonnell.t:2011cogsci,
  title = {Productivity and {{Reuse}} in {{Language}}},
  booktitle = {Proceedings of the 33rd {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {O'Donnell, Timothy and Snedeker, Jesse and Tenenbaum, Joshua and Goodman, Noah},
  year = {2011},
  volume = {33},
  urldate = {2022-06-15},
  abstract = {Author(s): O'Donnell, Timothy; Snedeker, Jesse; Tenenbaum, Joshua; Goodman, Noah},
  langid = {english},
  file = {~/Zotfiles/odonnell.t2011cogsci Productivity and Reuse in Language.pdf}
}

@phdthesis{odonnell.t:2011phd,
  title = {Productivity and {{Reuse}} in {{Language}}},
  author = {O'Donnell, Timothy J.},
  year = {2011},
  month = may,
  abstract = {A much-celebrated aspect of language is the way in which it allows us to make "infinite use of finite means" (von Humboldt, 1836). This property is made possible because language is fundamentally a productive computational system: Novel expressions can be composed from a large inventory of stored, reusable parts. For any given language, however, there are many more potential ways of forming novel expressions than can actually be used in practice. For example, English contains suffixes that are highly productive (e.g., -ness ; Lady-Gagaesqueness, pine-scentedness), but also contains suffixes which can only be reused in specific, existing words (e.g., -th; truth, width, warmth). How are such differences in productivity and reusability represented? How can the child acquire this system of knowledge? This thesis presents a formal model of productivity and reuse which treats the problem as a structure-by-structure inference in a Bayesian framework. The model---Fragment Grammars, a generalization of Adaptor Grammars (Johnson et al., 2007a)---is built around two proposals. The first is that anything that can be computed can be stored. The specific computational mechanism by which this is accomplished, stochastic memoization, is inherited from Adaptor Grammars (Goodman et al., 2008; Johnson et al., 2007a). The second proposal is that any stored item can include subparts which must be computed productively. This is made possible by the computational mechanism of stochastically lazy evaluation, introduced in the thesis. Throughout the thesis, Fragment Grammars are systematically compared to four other probabilistic models of productivity and reuse which formalize historical proposals from the linguistics and psycholinguistics literatures. The five models are evaluated on two very different sub-systems of English morphology: the English past tense, which is characterized by a sharp dichotomy in productivity between regular (i.e., +ed) and irregular (e.g., sing/sang) forms, and English derivational morphology, which is characterized by a graded cline from very productive (e.g., -ness) to very unproductive (e.g., -th). The thesis examines many aspects of these two domains including: performance on past-tense inflection, past-tense processing phenomena, developmental overregularization, the productivity of derivational suffixes, ordering restrictions on derivational suffixes, and base-driven selectional restrictions on suffix combinations.},
  langid = {english},
  school = {Harvard University},
  file = {~/Zotfiles/odonnell.t2011phd Productivity and Reuse in Language.pdf}
}

@book{odonnell.t:2015book,
  title = {Productivity and {{Reuse}} in {{Language}}: {{A Theory}} of {{Linguistic Computation}} and {{Storage}}},
  author = {O'Donnell, Timothy J.},
  year = {2015},
  month = aug,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9780262028844.001.0001},
  urldate = {2022-06-16},
  abstract = {A proposal for a formal model, Fragment Grammars, that treats productivity and reuse as the target of inference in a probabilistic framework.Language allows us to express and comprehend an unbounded number of thoughts. This fundamental and much-celebrated property is made possible by a division of labor between a large inventory of stored items (e.g., affixes, words, idioms) and a computational system that productively combines these stored units on the fly to create a potentially unlimited array of new expressions. A language learner must discover a language's productive, reusable units and determine which computational processes can give rise to new expressions. But how does the learner differentiate between the reusable, generalizable units (for example, the affix -ness, as in coolness, orderliness, cheapness) and apparent units that do not actually generalize in practice (for example, -th, as in warmth but not coolth)? In this book, Timothy O'Donnell proposes a formal computational model, Fragment Grammars, to answer these questions. This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distribution of productive word-formation processes and reusable units in a given language.O'Donnell compares this model to a number of other theoretical and mathematical models, applying them to the English past tense and English derivational morphology, and showing that Fragment Grammars unifies a number of superficially distinct empirical phenomena in these domains and justifies certain seemingly ad hoc assumptions in earlier theories.},
  isbn = {978-0-262-32680-3}
}

@inproceedings{oh.b:2021,
  title = {Surprisal Estimators for Human Reading Times Need Character Models},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2021},
  month = aug,
  pages = {3746--3757},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.290},
  urldate = {2023-05-02},
  abstract = {While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.},
  file = {~/Zotfiles/oh.b2021 Surprisal estimators for human reading t.pdf}
}

@article{oh.b:2022,
  title = {Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  issn = {2624-8212},
  urldate = {2023-05-02},
  abstract = {Expectation-based theories of sentence processing posit that processing difficulty is determined by predictability in context. While predictability quantified via surprisal has gained empirical support, this representation-agnostic measure leaves open the question of how to best approximate the human comprehender's latent probability model. This article first describes an incremental left-corner parser that incorporates information about common linguistic abstractions such as syntactic categories, predicate-argument structure, and morphological rules as a computational-level model of sentence processing. The article then evaluates a variety of structural parsers and deep neural language models as cognitive models of sentence processing by comparing the predictive power of their surprisal estimates on self-paced reading, eye-tracking, and fMRI data collected during real-time language processing. The results show that surprisal estimates from the proposed left-corner processing model deliver comparable and often superior fits to self-paced reading and eye-tracking data when compared to those from neural language models trained on much more data. This may suggest that the strong linguistic generalizations made by the proposed processing model may help predict humanlike processing costs that manifest in latency-based measures, even when the amount of training data is limited. Additionally, experiments using Transformer-based language models sharing the same primary architecture and training data show a surprising negative correlation between parameter count and fit to self-paced reading and eye-tracking data. These findings suggest that large-scale neural language models are making weaker generalizations based on patterns of lexical items rather than stronger, more humanlike generalizations based on linguistic structure.},
  file = {~/Zotfiles/oh.b2022 Comparison of structural parsers and neu.pdf}
}

@inproceedings{oh.b:2023,
  title = {Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with about Two Billion Training Tokens},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Oh, Byung-Doh and Schuler, William},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {1915--1921},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.128},
  urldate = {2024-05-16},
  abstract = {Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a `tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.},
  file = {~/Zotfiles/oh.b2023emnlpfindings Transformer-based language model surpris.pdf}
}

@misc{oh.b:2023arxiv,
  title = {Transformer-Based {{LM}} Surprisal Predicts Human Reading Times Best with about Two Billion Training Tokens},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11389},
  eprint = {2304.11389},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11389},
  urldate = {2023-05-15},
  abstract = {Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@article{oh.b:2023tacl,
  title = {Why Does Surprisal from Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {336--350},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00548},
  urldate = {2023-04-30},
  abstract = {This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to `memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.},
  file = {~/Zotfiles/oh.b2023tacl Why does surprisal from larger transform.pdf}
}

@misc{oh.b:2024arxiv,
  title = {Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times},
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02255},
  eprint = {2402.02255},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{oh.b:2024eacl,
  title = {Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times},
  booktitle = {Proceedings of the 18th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  editor = {Graham, Yvette and Purver, Matthew},
  year = {2024},
  month = mar,
  pages = {2644--2663},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  urldate = {2024-05-16},
  abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
  file = {~/Zotfiles/oh.b2024eacl Frequency explains the inverse correlati.pdf}
}

@inproceedings{oh.b:2024emnlp,
  title = {Leading Whitespaces of Language Models' Subword Vocabulary Pose a Confound for Calculating Word Probabilities},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Oh, Byung-Doh and Schuler, William},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {3464--3472},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  urldate = {2024-11-20},
  file = {~/Zotfiles/oh.b2024emnlp Leading Whitespaces of Language Models'.pdf}
}

@article{oh.b:2025,
  title = {Dissociable Frequency Effects Attenuate as Large Language Model Surprisal Predictors Improve},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2025},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {143},
  pages = {104645},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2025.104645},
  urldate = {2025-05-18},
  abstract = {Recent psycholinguistic modeling work using surprisal from Transformer-based language models has reported separable effects of frequency and predictability on real-time processing difficulty. However, it has also been shown that as Transformer-based language models become larger and are trained on more data, they are able to predict low-frequency words more accurately, which has a deleterious effect on fit to reading times. This article examines the impact of this property of language models on the dissociability of frequency effects and predictability effects in naturalistic reading. Regression results show robust positive effects of language model size and training data amount on the ability of word frequency to explain variance in held-out reading times as the contribution due to surprisal declines, which suggests a strong compensatory relationship between frequency and language model surprisal. Additionally, an analysis of the learning trajectories of low-frequency tokens reveals that the influence of model size is strongest on the prediction of tokens that are not part of a bigram sequence observed earlier in the context that models can readily copy, which suggests that limitations in model size create pressures toward learning more general associations. Taken together, these results suggest that the observed frequency effects may be due to imperfect estimates of predictability, and may disappear entirely as better-fitting language models are discovered. This further highlights the importance of exploring additional language models as models of human sentence processing.},
  keywords = {Computational modeling,Frequency,Large language models,Sentence processing,Surprisal},
  file = {~/Zotfiles/oh.b2025 Dissociable frequency effects attenuate.pdf}
}

@article{ohlsson.e:1998,
  title = {Sequential Poisson Sampling},
  author = {Ohlsson, Esbj{\"o}rn},
  year = {1998},
  month = jun,
  journal = {Journal of Official Statistics},
  volume = {14},
  number = {2},
  pages = {149--162},
  publisher = {Statistics Sweden (SCB)},
  address = {Stockholm, Sweden},
  issn = {0282423X},
  urldate = {2025-02-24},
  abstract = {Poisson sampling is a simple way to draw a probability proportional to size (pps) sample from a finite population. It also offers an easy way to update a sample while retaining as many units as possible from the previous sample, and/or to minimize overlap of different samples. A drawback of Poisson sampling is the random sample size. We present a fixed size alteration of Poisson sampling, sequential Poisson sampling, designed for, and used in, the Swedish Consumer Price Index (CPI). We show that the respective estimators associated with ordinary and sequential Poisson sampling, are both asymptotically normally distributed and unbiased as well as equally efficient. Simulations on CPI data verify approximate unbiasedness and approximate equality of variances, plus equally good performance of associated estimators of variance. Therefore, sequential Poisson sampling is preferable, because of the fixed size.},
  copyright = {Copyright Statistics Sweden (SCB) Jun 1998},
  langid = {english},
  keywords = {asymptotic normality,consumer price index,Consumer Price Index,overlap control,Poisson,Poisson sampling,Probability proportional to size,sample coordination,Sample size,sample updating,Sampling,Sequence,Sequential analysis},
  file = {~/Zotfiles/ohlsson.e1998 Sequential poisson sampling.pdf}
}

@book{ontarioministryofeducation:2011book,
  title = {Native {{Languages}}: {{A Support Document}} for the {{Teaching}} of {{Language Patterns}}: {{Oneida}}, {{Cayuga}}, and {{Mohawk}}},
  author = {{Ontario Ministry of Education}},
  year = {2011},
  series = {The {{Ontario Curriculum}}: {{Grades}} 1 to 12},
  publisher = {Ontario: Queen's Printer for Ontario},
  langid = {english},
  file = {~/Zotfiles/ontario2011 Native Languages A Support Document for.pdf}
}

@inproceedings{opedal.a:2024,
  title = {On the {{Role}} of {{Context}} in {{Reading Time Prediction}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Opedal, Andreas and Chodroff, Eleanor and Cotterell, Ryan and Wilcox, Ethan},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {3042--3058},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.179},
  urldate = {2025-06-11},
  abstract = {We present a new perspective on how readers integrate context during real-time language comprehension. Our proposals build on surprisal theory, which posits that the processing effort of a linguistic unit (e.g., a word) is an affine function of its in-context information content. We first observe that surprisal is only one out of many potential ways that a contextual predictor can be derived from a language model. Another one is the pointwise mutual information (PMI) between a unit and its context, which turns out to yield the same predictive power as surprisal when controlling for unigram frequency. Moreover, both PMI and surprisal are correlated with frequency. This means that neither PMI nor surprisal contains information about context alone. In response to this, we propose a technique where we project surprisal onto the orthogonal complement of frequency, yielding a new contextual predictor that is uncorrelated with frequency. Our experiments show that the proportion of variance in reading times explained by context is a lot smaller when context is represented by the orthogonalized predictor. From an interpretability standpoint, this indicates that previous studies may have overstated the role that context has in predicting reading times.},
  file = {~/Zotfiles/opedal.a2024 On the Role of Context in Reading Time P.pdf}
}

@misc{openai.:2023GPT4,
  title = {{{GPT-4}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  urldate = {2023-03-15},
  abstract = {We've created GPT-4, the latest milestone in OpenAI's effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.},
  howpublished = {https://openai.com/research/gpt-4},
  langid = {american}
}

@techreport{openai.:2023report,
  type = {Technical Report},
  title = {{{GPT-4}} Technical Report},
  author = {OpenAI},
  year = {2023},
  month = mar,
  institution = {OpenAI},
  urldate = {2023-03-15},
  file = {~/Zotfiles/openai2023GPT4techreport GPT-4 technical report.pdf}
}

@incollection{opper.m:2015,
  title = {Expectation Propagation},
  booktitle = {Statistical {{Physics}}, {{Optimization}}, {{Inference}}, and {{Message-Passing Algorithms}}: {{Lecture Notes}} of the {{Les Houches School}} of {{Physics}}: {{Special Issue}}, {{October}} 2013},
  author = {Opper, Manfred},
  editor = {Krzakala, Florent and {Ricci-Tersenghi}, Federico and Zdeborova, Lenka and Zecchina, Riccardo and Tramel, Eric W. and Cugliandolo, Leticia F.},
  year = {2015},
  month = dec,
  pages = {263--292},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198743736.003.0009},
  urldate = {2024-05-09},
  abstract = {Variational inference is a powerful concept that underlies many iterative approximation algorithms: expectation propagation, mean-field methods, belief propagation, and TAP equations can all be perceived in terms of this unifying framework. This chapter introduces the archetypal example of expectation propagation; after following its original derivation, we describe some of its properties, introduce some examples, and establish connections with the other approximation methods. The Gibbs free energy and its relation to these approximations, as well as double-loop algorithms for its minimization, are briefly discussed. Corrections by expansion about expectation propagation are then explained and, finally, some advanced inference topics and applications, such as recommender systems, Gaussian regression models and continuous-time stochastic dynamics, are explored.},
  isbn = {978-0-19-874373-6},
  file = {~/Zotfiles/opper.m2015 Expectation propagation.pdf}
}

@article{ortega.p:2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information-Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  year = {2013},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {469},
  number = {2153},
  pages = {20120683},
  publisher = {Royal Society},
  doi = {10.1098/rspa.2012.0683},
  urldate = {2022-06-09},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
  keywords = {bounded rationality,decision-making,information processing},
  file = {~/Zotfiles/ortega.p2013 Thermodynamics as a theory of decision-m.pdf}
}

@article{orth.w:2021,
  title = {Negative Polarity Item ({{NPI}}) Illusion Is a Quantification Phenomenon.},
  author = {Orth, Wesley and Yoshida, Masaya and Sloggett, Shayne},
  year = {2021},
  month = jun,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {47},
  number = {6},
  pages = {906--947},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000957},
  urldate = {2023-08-03},
  langid = {english},
  file = {~/Zotfiles/orth.w2021 Negative polarity item (NPI) illusion is.pdf}
}

@article{os.m:2022,
  title = {Rational Speech Comprehension: {{Interaction}} between Predictability, Acoustic Signal, and Noise},
  shorttitle = {Rational Speech Comprehension},
  author = {{van Os}, Marjolein and Kray, Jutta and Demberg, Vera},
  year = {2022},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.914239},
  urldate = {2024-05-05},
  abstract = {{$<$}sec{$><$}title{$>$}Introduction{$<$}/title{$><$}p{$>$}During speech comprehension, multiple sources of information are available to listeners, which are combined to guide the recognition process. Models of speech comprehension posit that when the acoustic speech signal is obscured, listeners rely more on information from other sources. However, these models take into account only word frequency information and local contexts (surrounding syllables), but not sentence-level information. To date, empirical studies investigating predictability effects in noise did not carefully control the tested speech sounds, while the literature investigating the effect of background noise on the recognition of speech sounds does not manipulate sentence predictability. Additionally, studies on the effect of background noise show conflicting results regarding which noise type affects speech comprehension most. We address this in the present experiment.{$<$}/p{$><$}/sec{$><$}sec{$><$}title{$>$}Methods{$<$}/title{$><$}p{$>$}We investigate how listeners combine information from different sources when listening to sentences embedded in background noise. We manipulate top-down predictability, type of noise, and characteristics of the acoustic signal, thus creating conditions which differ in the extent to which a specific speech sound is masked in a way that is grounded in prior work on the confusability of speech sounds in noise. Participants complete an online word recognition experiment.{$<$}/p{$><$}/sec{$><$}sec{$><$}title{$>$}Results and discussion{$<$}/title{$><$}p{$>$}The results show that participants rely more on the provided sentence context when the acoustic signal is harder to process. This is the case even when interactions of the background noise and speech sounds lead to small differences in intelligibility. Listeners probabilistically combine top-down predictions based on context with noisy bottom-up information from the acoustic signal, leading to a trade-off between the different types of information that is dependent on the combination of a specific type of background noise and speech sound.{$<$}/p{$><$}/sec{$>$}},
  langid = {english},
  keywords = {background noise,Mishearing,Noisy channel,predictive context,rational processing,speech comprehension},
  file = {~/Zotfiles/vanos.m2022frontiers Rational speech comprehension Interacti.pdf}
}

@misc{os.m:2022amlap,
  type = {Poster},
  title = {Rational Speech Comprehension: Interaction between Predictability, Acoustic Signal, and Noise},
  author = {{van Os}, Marjolein and Kray, Jutta and Demberg, Vera},
  year = {2022},
  month = sep,
  address = {York, England},
  urldate = {2022-09-09},
  annotation = {link-abstract: https://oxford-abstracts.s3.amazonaws.com/2dbcb575-3c17-4635-a1b8-24e1e8b44d3d.pdf\\
link-poster: https://oxford-abstracts.s3.amazonaws.com/0e28630c-2678-413b-bb42-daa834276886.pdf}
}

@article{owsley.c:2011,
  title = {Aging and Vision},
  author = {Owsley, Cynthia},
  year = {2011},
  month = jul,
  journal = {Vision Research},
  series = {Vision {{Research}} 50th {{Anniversary Issue}}: {{Part}} 2},
  volume = {51},
  number = {13},
  pages = {1610--1622},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2010.10.020},
  urldate = {2024-03-19},
  abstract = {Given the increasing size of the older adult population in many countries, there is a pressing need to identify the nature of aging-related vision impairments, their underlying mechanisms, and how they impact older adults' performance of everyday visual tasks. The results of this research can then be used to develop and evaluate interventions to slow or reverse aging-related declines in vision, thereby improving quality of life. Here we summarize salient developments in research on aging and vision over the past 25years, focusing on spatial contrast sensitivity, vision under low luminance, temporal sensitivity and motion perception, and visual processing speed.},
  keywords = {aging,Aging,vision,Vision,Vision impairment},
  file = {~/Zotfiles/owsley.c2011 Aging and vision.pdf}
}

@article{oxford.w:2019,
  title = {Inverse Marking and Multiple Agree in Algonquin},
  author = {Oxford, Will},
  year = {2019},
  journal = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {3},
  pages = {955--996},
  doi = {10.1007/s11049-018-9428-x},
  abstract = {This paper shows that inverse marking and portmanteau agreement are in complementary distribution in Algonquin: inverse marking is possible only in contexts where portmanteau agreement is not. This correlation holds despite intralanguage variation in both phenomena. The paper proposes that the two phenomena pattern together because both are determined by the outcome of the Agree operation on Infl. When Infl enters a Multiple Agree relation with both arguments, the realization of portmanteau agreement morphology is possible. When Infl agrees only with the object, it duplicates the result of an earlier object agreement operation on Voice. The presence of identical features on Infl and Voice triggers an impoverishment operation that deletes the features of Voice, resulting in its spellout as an underspecified elsewhere form---which is the exponent that we know descriptively as the inverse marker. This analysis explains why inverse marking and portmanteau agreement never co-occur in Algonquin: the two phenomena are determined by alternative outcomes of the Agree operation on Infl. The analysis also enables a simple account of the intralanguage variation in the patterning of the two phenomena, which is shown to follow from variation in the specification of the probe on Infl.},
  da = {2019/08/01},
  date-added = {2020-06-16 10:51:08 -0400},
  date-modified = {2020-06-16 10:52:50 -0400},
  isbn = {1573-0859},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{paape.d:2020,
  title = {Quadruplex {{Negatio Invertit}}? {{The On-Line Processing}} of {{Depth Charge Sentences}}},
  shorttitle = {Quadruplex {{Negatio Invertit}}?},
  author = {Paape, Dario and Vasishth, Shravan and {von der Malsburg}, Titus},
  year = {2020},
  month = nov,
  journal = {Journal of Semantics},
  volume = {37},
  number = {4},
  pages = {509--555},
  issn = {0167-5133},
  doi = {10.1093/jos/ffaa009},
  urldate = {2023-08-01},
  abstract = {So-called ``depth charge'' sentences (No head injury is too trivial to be ignored) are interpreted by the vast majority of speakers to mean the opposite of what their compositional semantics would dictate. The semantic inversion that is observed for sentences of this type is the strongest and most persistent linguistic illusion known to the field ( Wason \&amp; Reich, 1979). However, it has recently been argued that the preferred interpretation arises not because of a prevailing failure of the processing system, but rather because the non-compositional meaning is grammaticalized in the form of a stored construction ( Cook \&amp; Stevenson, 2010; Fortuin, 2014). In a series of five experiments, we investigate whether the depth charge effect is better explained by processing failure due to memory overload (the overloading hypothesis) or by the existence of an underlying grammaticalized construction with two available meanings (the ambiguity hypothesis). To our knowledge, our experiments are the first to explore the on-line processing profile of depth charge sentences. Overall, the data are consistent with specific variants of the ambiguity and overloading hypotheses while providing evidence against other variants. As an extension of the overloading hypothesis, we suggest two heuristic processes that may ultimately yield the incorrect reading when compositional processing is suspended for strategic reasons.},
  keywords = {depth-charge illusions,grammatical illusions},
  file = {~/Zotfiles/paape.d2020 Quadruplex Negatio Invertit The On-Line.pdf}
}

@article{paape.d:2021,
  title = {Does {{Local Coherence Lead}} to {{Targeted Regressions}} and {{Illusions}} of {{Grammaticality}}?},
  author = {Paape, Dario and Vasishth, Shravan and Engbert, Ralf},
  year = {2021},
  month = jul,
  journal = {Open Mind},
  volume = {5},
  pages = {42--58},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00041},
  urldate = {2024-03-06},
  abstract = {Local coherence effects arise when the human sentence processor is temporarily misled by a locally grammatical but globally ungrammatical analysis (The coach smiled at the player tossed a frisbee by the opposing team). It has been suggested that such effects occur either because sentence processing occurs in a bottom-up, self-organized manner rather than under constant grammatical supervision, or because local coherence can disrupt processing due to readers maintaining uncertainty about previous input. We report the results of an eye-tracking study in which subjects read German grammatical and ungrammatical sentences that either contained a locally coherent substring or not and gave binary grammaticality judgments. In our data, local coherence affected on-line processing immediately at the point of the manipulation. There was, however, no indication that local coherence led to illusions of grammaticality (a prediction of self-organization), and only weak, inconclusive support for local coherence leading to targeted regressions to critical context words (a prediction of the uncertain-input approach). We discuss implications for self-organized and noisy-channel models of local coherence.},
  file = {~/Zotfiles/paape.d2021 Does Local Coherence Lead to Targeted Re.pdf}
}

@article{paape.d:2024,
  title = {How Do Linguistic Illusions Arise? {{Rational}} Inference and Good-Enough Processing as Competing Latent Processes within Individuals},
  shorttitle = {How Do Linguistic Illusions Arise?},
  author = {Paape, Dario},
  year = {2024},
  month = nov,
  journal = {Language, Cognition and Neuroscience},
  volume = {39},
  number = {10},
  pages = {1334--1365},
  publisher = {Routledge},
  issn = {2327-3798},
  doi = {10.1080/23273798.2024.2387226},
  urldate = {2025-02-14},
  abstract = {Non-literal interpretations of implausible sentences such as The mother gave the candle the daughter have been taken as evidence for a rational error-correction mechanism that reconstructs the intended utterance from the ill-formed input ({\dots}gave the daughter the candle). However, the good-enough processing framework offers an alternative explanation: readers sometimes miss problematic aspects of sentences because they are only processing them superficially, which leads to acceptability illusions. As a synthesis of these accounts, I propose that conscious rational inferences about errors on the one hand and good-enough processing on the other are competing latent processes that simultaneously occur within the same comprehender. In support of this view, I present data from a two-dimensional grammaticality/interpretability judgment task with different types of subtly ill-formed sentences. Both conscious rational inference and good-enough processing predict positive interpretability judgments for such sentences, but only good-enough processing also predicts positive grammaticality judgments. By fitting a lognormal race model jointly to judgments and response latencies, I show that conscious rational inference and good-enough processing, as well as purely grammar-driven processing, actively trade off with each other during reading. Furthermore, individual differences measures reveal that participant traits such as linguistic pedantry, interpretational charity, and analytic/intuitive cognitive styles contribute to variability in the processing patterns.},
  keywords = {good enough processing,individual differences,Linguistic illusions,linguistics illusions,race model,rational inference},
  file = {~/Zotfiles/paape.d2024 How do linguistic illusions arise Ratio.pdf}
}

@phdthesis{paille.m:2022phd,
  title = {Strengthening {{Predicates}}},
  author = {Paill{\'e}, Mathieu},
  year = {2022},
  month = aug,
  address = {Montr{\'e}al, Canada},
  urldate = {2024-04-30},
  abstract = {Sentences in natural language are routinely interpreted as stronger than would be expected from the lexical meanings of the overt lexical items alone. This has led to the postulation of exhaustification (strengthening) mechanisms in pragmatics and semantics. Such exhaustivity effects have largely been discussed for logical vocabulary, focused expressions, and predicates forming entailment scales with other predicates. Relying on recent work on additive particles, I argue that exhaustivity is at play in a significantly broader array of meanings than previously appreciated: all predicates are exhaustified, in all sentences. That is, the intuited meanings of predicates in sentences are stronger than their lexical--conceptual meanings. I focus on 'taxonomic' predicates, which do not form entailment scales with other predicates. I make this case first and foremost based on apparently banal contradictions like This comedy is a tragedy or The white flag is green. While these contradictions are intuitively due to the meanings of the predicates, the interaction of these predicates with additive particles (This comedy is also a tragedy) and conjunction (This play is both a comedy and a tragedy) is argued to show that the predicates are underlyingly consistent. As such, the contradiction observed in the basic case must result from exhaustification.},
  langid = {canadian},
  school = {McGill University},
  keywords = {additive particles,alternatives,exhaustivity,homogeneity,predicates,semantics}
}

@inproceedings{pal.c:2006,
  title = {Sparse Forward-Backward Using Minimum Divergence Beams for Fast Training of Conditional Random Fields},
  booktitle = {{{IEEE}} International Conference on Acoustics Speed and Signal Processing Proceedings},
  author = {Pal, C. and Sutton, C. and McCallum, A.},
  year = {2006},
  volume = {V},
  pages = {581--584},
  publisher = {IEEE},
  doi = {10.1109/icassp.2006.1661342},
  bdsk-url-2 = {https://doi.org/10.1109/icassp.2006.1661342},
  date-added = {2022-03-25 11:41:07 -0400},
  date-modified = {2022-03-25 11:45:04 -0400}
}

@book{palermo.d:1964book,
  title = {Word Association Norms: {{Grade}} School through College},
  author = {Palermo, David and Jenkins, James},
  year = {1964},
  publisher = {U. Minnesota Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@misc{park.k:2024arxiv,
  title = {Grammar-Aligned Decoding},
  author = {Park, Kanghee and Wang, Jiayu and {Berg-Kirkpatrick}, Taylor and Polikarpova, Nadia and D'Antoni, Loris},
  year = {2024},
  month = may,
  number = {arXiv:2405.21047},
  eprint = {2405.21047},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.21047},
  urldate = {2024-06-25},
  abstract = {Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/park.k2024arxiv Grammar-Aligned Decoding.pdf}
}

@inproceedings{park.y:2009,
  title = {Minimal-Length Linearizations for Mildly Context-Sensitive Dependency Trees},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2009},
  pages = {335--343},
  publisher = {Association for Computational Linguistics},
  address = {Boulder, Colorado}
}

@inproceedings{park.y:2011,
  title = {Automated Whole Sentence Grammar Correction Using a Noisy Channel Model},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2011},
  month = jun,
  pages = {934--944},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  date-added = {2022-04-11 23:09:22 -0400},
  date-modified = {2022-04-11 23:09:27 -0400}
}

@article{parker.d:2016,
  title = {Negative Polarity Illusions and the Format of Hierarchical Encodings in Memory},
  author = {Parker, Dan and Phillips, Colin},
  year = {2016},
  month = dec,
  journal = {Cognition},
  volume = {157},
  pages = {321--339},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2016.08.016},
  urldate = {2023-02-22},
  abstract = {Linguistic illusions have provided valuable insights into how we mentally navigate complex representations in memory during language comprehension. Two notable cases involve illusory licensing of agreement and negative polarity items (NPIs), where comprehenders fleetingly accept sentences with unlicensed agreement or an unlicensed NPI, but judge those same sentences as unacceptable after more reflection. Existing accounts have argued that illusions are a consequence of faulty memory access processes, and make the additional assumption that the encoding of the sentence remains fixed over time. This paper challenges the predictions made by these accounts, which assume that illusions should generalize to a broader set of structural environments and a wider range of syntactic and semantic phenomena. We show across seven reading-time and acceptability judgment experiments that NPI illusions can be reliably switched ``on'' and ``off'', depending on the amount of time from when the potential licensor is processed until the NPI is encountered. But we also find that the same profile does not extend to agreement illusions. This contrast suggests that the mechanisms responsible for switching the NPI illusion on and off are not shared across all illusions. We argue that the contrast reflects changes over time in the encoding of the semantic/pragmatic representations that can license NPIs. Just as optical illusions have been informative about the visual system, selective linguistic illusions are informative not only about the nature of the access mechanisms, but also about the nature of the encoding mechanisms.},
  langid = {english},
  keywords = {Agreement,Binding,Linguistic illusions,Memory,Negative polarity,Representation,Sentence processing}
}

@book{parr.t:2022book,
  title = {Active Inference: The Free Energy Principle in Mind, Brain, and Behavior},
  shorttitle = {Active Inference},
  author = {Parr, Thomas and Pezzulo, Giovanni and Friston, Karl J.},
  year = {2022},
  month = mar,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/12441.001.0001},
  urldate = {2025-01-15},
  abstract = {The first comprehensive treatment of active inference, an integrative perspective on brain, cognition, and behavior used across multiple disciplines.Active},
  isbn = {978-0-262-36997-8},
  langid = {english},
  file = {~/Zotfiles/parr.t2022 Active Inference The Free Energy Princi.pdf}
}

@inproceedings{paskin.m:2001,
  title = {Grammatical Bigrams},
  booktitle = {Advances in Neural Information Processing Systems 14 ({{NIPS}} 2001)},
  author = {Paskin, Mark A.},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  year = {2001},
  month = dec,
  pages = {91--97},
  publisher = {MIT Press},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/Paskin01.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{pennington.j:2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-10-29},
  file = {~/Zotfiles/pennington.j2014 GloVe Global Vectors for Word Represent.pdf}
}

@phdthesis{pentangelo.j:2020phd,
  title = {360{$^\circ$} {{Video}} and {{Language Documentation}}: {{Towards}} a {{Corpus}} of {{Kanien}}'k{\'e}ha ({{Mohawk}})},
  shorttitle = {360{$^\circ$} {{Video}} and {{Language Documentation}}},
  author = {Pentangelo, Joseph and link will open in a new window {Link to external site}, this},
  year = {2020},
  address = {United States -- New York},
  urldate = {2022-05-31},
  abstract = {Robust documentation is a major goal of documentary linguistics. Recognizing spoken language as a multimodal phenomenon, researchers working in this field broadly agree that video is an improvement over audio-only recording. At the same time, video is limited by the format's frame, which permits only a relatively small portion of the visual field to be recorded at any given time. This results in much data being lost, as the documenter must decide where to aim their camera, necessarily leaving out more than they record. In this dissertation, I apply 360 video to language documentation for the first time. 360 video, which is one variety of virtual reality, improves upon traditional video by drastically expanding the frame, recording in all directions surrounding the camera. In this way, a maximum of visual data is recorded, and there is no need for the camera to be redirected as participants take turns speaking or move around the space. I recorded over 10 hours of 360 video with ambisonic audio, containing mostly naturalistic conversation in the Akwesasne variety of Kanien'k{\'e}ha (Mohawk), an endangered Northern Iroquoian language spoken in New York State, Ontario, and Quebec. Most of the existing documentation of Kanien'k{\'e}ha outside of this corpus is formal or non-naturalistic. The resulting corpus thus serves a dual purpose: it is both a demonstration of the capabilities of 360 video for language documentation, and a contribution to the documentation of Kanien'k{\'e}ha. This dissertation includes a brief grammatical description of Kanien'k{\'e}ha phonology and morphology, a discussion of the interplay between technology and language documentation throughout North American history, an exploration of the significance of 360 video to documentary linguistics, a brief analysis of gesture and intonation in the present corpus, and an assessment of the suitability of ambisonic audio for linguistic analysis. Directions for potential future research are indicated throughout.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798678110015},
  langid = {english},
  school = {City University of New York},
  keywords = {Akwesasne,Documentary linguistics,Iroquoian,Language documentation,Virtual reality},
  file = {~/Zotfiles/pentangelo.j2020 360 Video and Language Documentation T.pdf}
}

@article{perea.m:2003,
  title = {Does Jugde Activate {{COURT}}? {{Transposed-letter}} Similarity Effects in Masked Associative Priming},
  shorttitle = {Does Jugde Activate {{COURT}}?},
  author = {Perea, Manuel and Lupker, Stephen J.},
  year = {2003},
  month = sep,
  journal = {Memory \& Cognition},
  volume = {31},
  number = {6},
  pages = {829--841},
  issn = {1532-5946},
  doi = {10.3758/BF03196438},
  urldate = {2023-10-29},
  abstract = {Transposed-letter (TL) nonwords (e.g.,jugde) can be easily misperceived as words, a fact that is somewhat inconsistent with the letter-position-coding schemes employed by most current models of visual word recognition. To examine this issue further, we conducted four masked semantic/associative priming experiments, using a lexical decision task. In Experiment 1, the related primes could be words, TL-internal nonwords, or replacement-letter (RL) nonwords (e.g.,judge, jugde, orjudpe, respectively; the target would be COURT). Relative to an unrelated condition, masked TL-internal primes produced a significant semantic/associative priming effect, an effect that was only slightly smaller than the priming effect for word primes. No effect, however, was observed for RL-nonword primes. In Experiment 2, the TL-nonword primes were created by switching the two final letters of the primes (e.g.,judeg). The results again showed a semantic/associative priming effect for word primes, but not for TL-final nonword primes or for RL-nonword primes. Experiment 3 replicated the associative/semantic priming effect for TL-internal nonword primes, with, again, no effect for TL-final nonword primes. Finally, Experiment 4 again failed to yield a priming effect for TL-final nonword primes. The implications of these results for the choice of a letter-position-coding scheme in visual word recognition models are discussed.},
  langid = {english},
  keywords = {Lexical Decision,Lexical Decision Task,priming,Priming Effect,transposed letter effects,Visual Word Recognition,Word Target}
}

@article{pereira.f:2000,
  title = {Formal Grammar and Information Theory: {{Together}} Again?},
  author = {Pereira, Fernando C. N.},
  year = {2000},
  journal = {Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences},
  volume = {358},
  number = {1769},
  pages = {1239--1253},
  doi = {10.1098/rsta.2000.0583},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {information theory},
  file = {~/Zotfiles/pereira.f2000 Formal grammar and information theory T.pdf}
}

@article{perfors.a:2011,
  title = {A Tutorial Introduction to {{Bayesian}} Models of Cognitive Development},
  author = {Perfors, Amy and Tenenbaum, Joshua B. and Griffiths, Thomas L. and Xu, Fei},
  year = {2011},
  month = sep,
  journal = {Cognition},
  series = {Probabilistic Models of Cognitive Development},
  volume = {120},
  number = {3},
  pages = {302--321},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2010.11.015},
  urldate = {2024-05-03},
  abstract = {We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.},
  keywords = {Bayesian models,Cognitive development},
  file = {~/Zotfiles/perfors.a2011 A tutorial introduction to Bayesian mode.pdf}
}

@article{perfors.a:2011a,
  title = {The Learnability of Abstract Syntactic Principles},
  author = {Perfors, Amy and Tenenbaum, Joshua B and Regier, Terry},
  year = {2011},
  journal = {Cognition},
  volume = {118},
  number = {3},
  pages = {306--338},
  publisher = {Elsevier},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{peters.m:2018,
  title = {Deep Contextualized Word Representations},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1202}
}

@inproceedings{peters.m:2018a,
  title = {Dissecting Contextual Word Embeddings: {{Architecture}} and Representation},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  pages = {1499--1509},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1179},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1179}
}

@inproceedings{petrov.s:2007,
  title = {Discriminative Log-Linear Grammars with Latent Variables},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Petrov, Slav and Klein, Dan},
  year = {2007},
  volume = {20},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-16},
  abstract = {We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be effi- ciently approximated in a gradient-based procedure. We compare L1 and L2 reg- ularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing exper- iments, the discriminative latent models outperform both the comparable genera- tive latent models as well as the discriminative non-latent baselines.},
  keywords = {pruning},
  file = {~/Zotfiles/petrov.s2007 Discriminative log-linear grammars with.pdf}
}

@incollection{phillips.c:2011,
  title = {Grammatical Illusions and Selective Fallibility in Real-Time Language Comprehension},
  shorttitle = {Chapter 5},
  booktitle = {Experiments at the {{Interfaces}}},
  author = {Phillips, Colin and Wagers, Matthew W. and Lau, Ellen F.},
  year = {2011},
  month = jan,
  series = {Syntax and {{Semantics}}},
  volume = {37},
  pages = {147--180},
  publisher = {Brill},
  doi = {10.1163/9781780523750_006},
  urldate = {2024-05-26},
  isbn = {978-1-78052-375-0},
  langid = {english},
  keywords = {Languages and Linguistics,Morphology & Syntax,Psycholinguistics & Language and Cognition,Semantics},
  file = {~/Zotfiles/phillips.c2011 Grammatical illusions and selective fall.pdf}
}

@article{piantadosi.s:2011PNAS,
  title = {Word Lengths Are Optimized for Efficient Communication},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = {2011},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3526--3529},
  doi = {10.1073/pnas.1012551108},
  file = {~/Zotfiles/piantadosi.s2011 Word lengths are optimized for efficient.pdf}
}

@misc{piantadosi.s:2013arxiv,
  title = {Information Content versus Word Length in Natural Language: {{A}} Reply to {{Ferrer-i-Cancho}} and {{Moscoso}} Del {{Prado Martin}} [{{arXiv}}:1209.1751]},
  shorttitle = {Information Content versus Word Length in Natural Language},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = {2013},
  month = jul,
  number = {arXiv:1307.6726},
  eprint = {1307.6726},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1307.6726},
  urldate = {2025-03-06},
  abstract = {Recently, Ferrer i Cancho and Moscoso del Prado Martin [arXiv:1209.1751] argued that an observed linear relationship between word length and average surprisal (Piantadosi, Tily, \& Gibson, 2011) is not evidence for communicative efficiency in human language. We discuss several shortcomings of their approach and critique: their model critically rests on inaccurate assumptions, is incapable of explaining key surprisal patterns in language, and is incompatible with recent behavioral results. More generally, we argue that statistical models must not critically rely on assumptions that are incompatible with the real system under study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Mathematics - Probability,Physics - Data Analysis Statistics and Probability},
  file = {~/Zotfiles/piantadosi.s2013arxiv Information content versus word length i.pdf}
}

@article{piantadosi.s:2014,
  title = {Zipf's Word Frequency Law in Natural Language: {{A}} Critical Review and Future Directions},
  shorttitle = {Zipf's Word Frequency Law in Natural Language},
  author = {Piantadosi, Steven T.},
  year = {2014},
  month = oct,
  journal = {Psychonomic bulletin \& review},
  volume = {21},
  number = {5},
  pages = {1112--1130},
  issn = {1069-9384},
  doi = {10.3758/s13423-014-0585-6},
  urldate = {2022-09-27},
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf ' s law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  pmcid = {PMC4176592},
  pmid = {24664880},
  file = {~/Zotfiles/piantadosi.s2014 Zipfs word frequency law in natural lan.pdf}
}

@article{pickering.m:2007,
  title = {Do People Use Language Production to Make Predictions during Comprehension?},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2007},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {11},
  number = {3},
  pages = {105--110},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2006.12.002},
  urldate = {2023-07-29},
  langid = {english},
  pmid = {17254833},
  file = {~/Zotfiles/pickering.m2007 Do people use language production to mak.pdf}
}

@article{pickering.m:2013,
  title = {An Integrated Theory of Language Production and Comprehension},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2013},
  journal = {Behavioral and Brain Sciences},
  volume = {36},
  number = {4},
  pages = {329--347},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0140525X12001495},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding},
  file = {~/Zotfiles/pickering.m2013 An integrated theory of language product.pdf}
}

@article{pickering.m:2018,
  title = {Predicting While Comprehending Language: {{A}} Theory and Review.},
  shorttitle = {Predicting While Comprehending Language},
  author = {Pickering, Martin J. and Gambi, Chiara},
  year = {2018},
  month = oct,
  journal = {Psychological Bulletin},
  volume = {144},
  number = {10},
  pages = {1002--1044},
  issn = {1939-1455, 0033-2909},
  doi = {10.1037/bul0000158},
  urldate = {2025-06-01},
  langid = {english},
  file = {~/Zotfiles/pickering.m2018 Predicting while comprehending language.pdf}
}

@article{pickering.m:2018a,
  title = {Predicting While Comprehending Language: {{A}} Theory and Review.},
  author = {Pickering, Martin J and Gambi, Chiara},
  year = {2018},
  journal = {Psychological Bulletin},
  volume = {144},
  number = {10},
  pages = {1002},
  publisher = {American Psychological Association},
  doi = {10.1037/bul0000158},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@inproceedings{piktus.a:2019,
  title = {Misspelling {{Oblivious Word Embeddings}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Piktus, Aleksandra and Edizel, Necati Bora and Bojanowski, Piotr and Grave, Edouard and Ferreira, Rui and Silvestri, Fabrizio},
  year = {2019},
  month = jun,
  pages = {3226--3234},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1326},
  urldate = {2023-08-25},
  abstract = {In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.},
  file = {~/Zotfiles/piktus.a2019 Misspelling Oblivious Word Embeddings.pdf}
}

@inproceedings{pimentel.t:2023emnlp,
  title = {Revisiting the {{Optimality}} of {{Word Lengths}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pimentel, Tiago and Meister, Clara and Wilcox, Ethan and Mahowald, Kyle and Cotterell, Ryan},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {2240--2255},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.137},
  urldate = {2025-03-06},
  abstract = {Zipf (1935) posited that wordforms are optimized to minimize utterances' communicative costs. Under the assumption that cost is given by an utterance`s length, he supported this claim by showing that words' lengths are inversely correlated with their frequencies. Communicative cost, however, can be operationalized in different ways. Piantadosi et al. (2011) claim that cost should be measured as the distance between an utterance`s information rate and channel capacity, which we dub the channel capacity hypothesis (CCH) here. Following this logic, they then proposed that a word`s length should be proportional to the expected value of its surprisal (negative log-probability in context). In this work, we show that Piantadosi et al.`s derivation does not minimize CCH`s cost, but rather a lower bound, which we term CCH-lower. We propose a novel derivation, suggesting an improved way to minimize CCH`s cost. Under this method, we find that a language`s word lengths should instead be proportional to the surprisal`s expectation plus its variance-to-mean ratio. Experimentally, we compare these three communicative cost functions: Zipf`s, CCH-lower , and CCH. Across 13 languages and several experimental settings, we find that length is better predicted by frequency than either of the other hypotheses. In fact, when surprisal`s expectation, or expectation plus variance-to-mean ratio, is estimated using better language models, it leads to worse word length predictions. We take these results as evidence that Zipf`s longstanding hypothesis holds.},
  file = {~/Zotfiles/pimentel.t2023 Revisiting the Optimality of Word Length.pdf}
}

@article{pimentel.t:2023tacl,
  title = {On the Effect of Anticipation on Reading Times},
  author = {Pimentel, Tiago and Meister, Clara and Wilcox, Ethan G. and Levy, Roger P. and Cotterell, Ryan},
  year = {2023},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1624--1642},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00603},
  urldate = {2024-05-26},
  abstract = {Abstract             Over the past two decades, numerous studies have demonstrated how less-predictable (i.e., higher surprisal) words take more time to read. In general, these studies have implicitly assumed the reading process is purely responsive: Readers observe a new word and allocate time to process it as required. We argue that prior results are also compatible with a reading process that is at least partially anticipatory: Readers could make predictions about a future word and allocate time to process it based on their expectation. In this work, we operationalize this anticipation as a word's contextual entropy. We assess the effect of anticipation on reading by comparing how well surprisal and contextual entropy predict reading times on four naturalistic reading datasets: two self-paced and two eye-tracking. Experimentally, across datasets and analyses, we find substantial evidence for effects of contextual entropy over surprisal on a word's reading time (RT): In fact, entropy is sometimes better than surprisal in predicting a word's RT. Spillover effects, however, are generally not captured by entropy, but only by surprisal. Further, we hypothesize four cognitive mechanisms through which contextual entropy could impact RTs---three of which we are able to design experiments to analyze. Overall, our results support a view of reading that is not just responsive, but also anticipatory.1},
  langid = {english},
  file = {~/Zotfiles/pimentel.t2023tacl On the effect of anticipation on reading.pdf}
}

@inproceedings{pimentel.t:2024,
  title = {How to Compute the Probability of a Word},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Pimentel, Tiago and Meister, Clara},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {18358--18375},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  urldate = {2024-11-20},
  abstract = {Language models (LMs) estimate a probability distribution over strings in a natural language; these distributions are crucial for computing perplexity and surprisal in linguistics research. While we are usually concerned with measuring these values for words, most LMs operate over subwords. Despite seemingly straightforward, accurately computing probabilities over one unit given probabilities over the other requires care. Indeed, we show here that many recent linguistic studies have been incorrectly computing these values. This paper derives the correct methods for computing word probabilities, highlighting issues when relying on language models that use beginning-of-word (bow)-marking tokenisers, e.g., the GPT family. Empirically, we show that correcting the widespread bug in probability computations affects measured outcomes in sentence comprehension and lexical optimisation analyses.},
  file = {~/Zotfiles/pimentel.t2024 How to Compute the Probability of a Word.pdf}
}

@phdthesis{pimentelmartinsdasilva.t:2024phd,
  title = {On the {{Optimality}} of the {{Lexicon}}},
  author = {Pimentel Martins Da Silva, Tiago},
  year = {2024},
  month = apr,
  urldate = {2024-04-29},
  abstract = {The principle of least effort posits that a pressure towards communicative efficiency shapes natural languages. In this thesis, we investigate the existence, the nature, and the impact of such a pressure in natural languages' lexicons. We investigate the existence of this pressure by (i) estimating what optimal word lengths would be using coding theory, (ii) proposing pressure-free baselines and estimating their word lengths, and then (iii) comparing natural lexicons to both these optimal and pressure-free artificial lexicons. We investigate the nature of this pressure by comparing multiple ways in which communicative efficiency can be operationalised; we formalise it as either a pressure to shorten utterances, or a pressure to keep information rates as close as possible to an unknown communication channel capacity. Finally, we study the impact of this pressure on cross-linguistic differences in word lengths and on the ratio of homophones in natural languages. Overall, our results support a Zipfian view of communicative efficiency, in which lexicons are pressured towards having utterances that are as short as possible. Our results, however, also highlight the existence of competing constraints and pressures in how lexicons are structured: (i) a language's phonotactic complexity seems to bottleneck the extent to which economy of expression can optimise a lexicon, and (ii) a pressure for clarity seems to keep the ratio of homophones in a language close to chance.},
  langid = {english},
  school = {University of Cambridge},
  file = {~/Zotfiles/pimentel.t2024phd On the Optimality of the Lexicon.pdf}
}

@inproceedings{pine.a:2022,
  title = {Requirements and {{Motivations}} of {{Low-Resource Speech Synthesis}} for {{Language Revitalization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pine, Aidan and Wells, Dan and Brinklow, Nathan and Littell, Patrick and Richmond, Korin},
  year = {2022},
  month = may,
  pages = {7346--7359},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.507},
  urldate = {2022-06-04},
  abstract = {This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien'k{\'e}ha, Gitksan \& SEN{\'C}O{\textTstroke}EN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.},
  keywords = {computational revitalization,iroquoian},
  file = {~/Zotfiles/pine.a2022 Requirements and Motivations of Low-Reso.pdf}
}

@incollection{polinsky.m:2013,
  title = {Resumption in {{English}}},
  booktitle = {Experimental {{Syntax}} and {{Island Effects}}},
  author = {Polinsky, Maria and Eby Clemens, Lauren and Milton Morgan, Adam and Xiang, Ming and Heestand, Dustin},
  editor = {Sprouse, Jon and Hornstein, Norbert},
  year = {2013},
  pages = {341--359},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139035309.017},
  urldate = {2023-04-05},
  isbn = {978-1-107-00870-0},
  keywords = {resumptive pronouns},
  file = {~/Zotfiles/polinsky.m2013 Resumption in English.pdf}
}

@book{polyanskiy.y:2024book,
  title = {Information Theory: From Coding to Learning},
  author = {Polyanskiy, Yury and Wu, Yihong},
  year = {2024},
  month = nov,
  edition = {1},
  isbn = {978-1-108-83290-8},
  file = {~/Zotfiles/polyanskiy.y2023 Information theory from coding to learn.pdf}
}

@article{poole.e:2016,
  title = {Deconstructing Subjecthood},
  author = {Poole, Ethan},
  year = {2016},
  journal = {Ms., UMass Amherst.},
  date-added = {2019-06-14 09:32:06 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,subject positions}
}

@inproceedings{poppels.t:2016cogsci,
  title = {Structure-Sensitive Noise Inference: Comprehenders Expect Exchange Errors},
  booktitle = {Proceedings of the 38th {{Annual}} Meeting of the {{Cognitive Science Society}}},
  author = {Poppels, Till and Levy, Roger},
  year = {2016},
  pages = {378--383},
  publisher = {Cognitive Science Society},
  address = {Austin, TX},
  abstract = {Previous research has found that comprehenders are willing to adopt non-literal interpretations of sentences whose literal reading is unlikely. Several studies found evidence that comprehenders decide whether a given utterance should be taken at face value in accordance with principles of Bayesian rationality, by weighing the prior probability of potential interpretations against the degree to which they are (in)consistent with the literal form of the utterance. While all of these results are consistent with string-edit noise models, many error processes are known to be sensitive to the underlying linguistic structure of the intended utterance. Here, we explore the case of exchange errors and provide experimental evidence that comprehenders' noise model is structure-sensitive. Our results add further support to the noisy-channel theory of language comprehension, extend the set of known noise operations to include positional exchanges, and show that comprehenders' noise models are well-adapted to structure-sensitive sources of signal corruption.},
  file = {~/Zotfiles/poppels.t2016 Structure-sensitive Noise Inference Com.pdf}
}

@misc{poppels.t:2021HSP,
  type = {Talk},
  title = {Bias against "She" Pronouns Can Be Rapidly Overcome by Changing Event Expectations},
  author = {Poppels, Till and Boyce, Veronica and Ajunwa, Chelsea and {von der Malsburg}, Titus and Levy, Roger},
  year = {2021},
  month = feb,
  urldate = {2024-12-21},
  langid = {american},
  file = {~/Zotfiles/poppels.t2021CUNY Bias against she pronouns can be rapid.pdf}
}

@article{post.e:1943,
  title = {Formal Reductions of the General Combinatorial Decision Problem},
  author = {Post, Emil L.},
  year = {1943},
  journal = {American Journal of Mathematics},
  volume = {65},
  number = {2},
  eprint = {2371809},
  eprinttype = {jstor},
  pages = {197--215},
  publisher = {Johns Hopkins University Press},
  issn = {0002-9327},
  doi = {10.2307/2371809},
  urldate = {2022-07-15},
  file = {~/Zotfiles/post.e1943 Formal reductions of the general combina.pdf}
}

@article{post.m:2013,
  title = {Bayesian Tree Substitution Grammars as a Usage-Based Approach},
  author = {Post, Matt and Gildea, Daniel},
  year = {2013},
  journal = {Language and Speech},
  volume = {56},
  number = {3},
  pages = {291--308},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:27 -0400},
  keywords = {Tree Substitution Grammar}
}

@article{pozniak.c:2021,
  title = {Failures of {{Gricean}} Reasoning and the Role of Stereotypes in the Production of Gender Marking in {{French}}},
  author = {Pozniak, C{\'e}line and Burnett, Heather},
  year = {2021},
  month = apr,
  journal = {Glossa: a journal of general linguistics},
  volume = {6},
  number = {1},
  issn = {2397-1835},
  doi = {10.5334/gjgl.1310},
  urldate = {2024-12-21},
  abstract = {We partly replicate von der Malsburg et al. (2020)'s recent experiments investigating the relationship between speaker expectations, gender stereotypes and language use in English on a grammatical gender language: French. The results of our experiment show how the linguistic particularities of the English and French gender marking systems interact with speaker expectations and stereotypes to create different patterns of gender marking production. They also raise a puzzle for current theoretical and computational frameworks that formalize Gricean pragmatics, particularly those in which informativity (Gricean Quantity) is assumed to play a driving role in linguisticproduction.},
  copyright = {https://creativecommons.org/licenses/by/4.0}
}

@misc{prasad.g:2019a,
  title = {Rapid Syntactic Adaptation in Self-Paced Reading: Detectable, but Requires Many Participants.},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  publisher = {Center for Open Science},
  doi = {10.31234/osf.io/9ptg4},
  bdsk-url-2 = {https://doi.org/10.31234/osf.io/9ptg4},
  date-added = {2021-03-18 11:13:22 -0400},
  date-modified = {2021-03-18 17:40:38 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing,reading time,self-paced reading}
}

@inproceedings{prasad.g:2019cogsci,
  title = {How Much Harder Are Hard Garden-Path Sentences than Easy Ones?},
  booktitle = {Proceedings of the 41st {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  abstract = {The advent of broad-coverage computational models of human sentence processing has made it possible to derive quantitative predictions for empirical phenomena of longstanding interest in psycholinguistics; one such case is the disambiguation difficulty in temporarily ambiguous sentences (garden-path sentences). Adequate evaluation of the accuracy of such quantitative predictions requires going beyond the classic binary distinction between "hard" and "easy" garden path sentences. It requires precise quantitative measurements of processing difficulty and statistical analyses that focus on more than just statistical significance. We evaluate how well a particular specification of surprisal theory predicts data from a self-paced reading study designed to estimate the magnitude of the disambiguation difficulty in two temporarily ambiguous sentence types (NP/Z and NP/S ambiguities). Using Bayesian analysis we conclude that our specification of surprisal theory cannot account for the observed NP/Z garden path effects. We have insufficient evidence to draw conclusions about whether it can account for the NP/S garden path effects.},
  keywords = {processing,reading time,self-paced reading}
}

@article{prasad.g:2021,
  title = {Rapid Syntactic Adaptation in Self-Paced Reading: Detectable, but Only with Many Participants.},
  shorttitle = {Rapid Syntactic Adaptation in Self-Paced Reading},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2021},
  month = jul,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {47},
  number = {7},
  pages = {1156--1172},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0001046},
  urldate = {2025-05-05},
  langid = {english},
  file = {~/Zotfiles/prasad.g2021 Rapid syntactic adaptation in self-paced.pdf}
}

@article{preminger.o:2011,
  title = {Asymmetries between Person and Number in Syntax: A Commentary on {{Baker}}'s {{SCOPA}}},
  author = {Preminger, Omer},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {917--937},
  publisher = {Springer},
  date-added = {2020-02-25 21:43:01 -0500},
  date-modified = {2020-02-26 09:11:06 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement,phi features}
}

@book{preminger.o:2014book,
  title = {Agreement and Its Failures},
  author = {Preminger, Omer},
  year = {2014},
  volume = {68},
  publisher = {MIT Press},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@article{prim.r:1957,
  title = {Shortest Connection Networks and Some Generalizations},
  author = {Prim, R. C.},
  year = {1957},
  journal = {The Bell System Technical Journal},
  volume = {36},
  number = {6},
  pages = {1389--1401},
  doi = {10.1002/j.1538-7305.1957.tb01515.x},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{prince.e:1990,
  title = {Syntax and Discourse: {{A}} Look at Resumptive Pronouns},
  booktitle = {Proceedings of the Sixteenth Annual Meeting of the {{Berkeley}} Linguistics Society},
  author = {Prince, Ellen F},
  year = {1990},
  volume = {16},
  pages = {482--497},
  isbn = {2377-1666},
  file = {~/Zotfiles/prince.e1990 Syntax and discourse A look at resumpti.pdf}
}

@article{priva.u:2020,
  title = {The Causal Structure of Lenition: A Case for the Causal Precedence of Durational Shortening},
  author = {Priva, Uriel Cohen and Gleason, Emily},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {2},
  pages = {413--448},
  publisher = {Project Muse},
  doi = {10.1353/lan.2020.0025},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0025},
  date-added = {2022-05-10 10:31:58 -0400},
  date-modified = {2022-05-10 10:32:14 -0400},
  keywords = {causality,lenition},
  file = {~/Zotfiles/priva.u2020 The causal structure of lenition a case.pdf}
}

@inproceedings{przepiorkowski.a:2018,
  title = {Arguments and Adjuncts in {{Universal Dependencies}}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {Przepi{\'o}rkowski, Adam and Patejuk, Agnieszka},
  year = {2018},
  pages = {3837--3852},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA}
}

@article{pustejovsky.j:1991,
  title = {The Generative Lexicon},
  author = {Pustejovsky, James},
  year = {1991},
  journal = {Computational Linguistics},
  volume = {17},
  number = {4},
  pages = {409--441},
  urldate = {2023-12-20},
  file = {~/Zotfiles/pustejovsky.j1991 The generative lexicon.pdf}
}

@book{pustejovsky.j:2002book,
  title = {The Generative Lexicon},
  author = {Pustejovsky, James},
  year = {2002},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-16158-9 978-0-262-66140-9},
  langid = {english},
  file = {~/Zotfiles/pustejovsky.j2002book The generative lexicon.pdf}
}

@book{pylyshyn.z:1984book,
  title = {Computation and Cognition: Toward a Foundation for Cognitive Science},
  shorttitle = {Computation and Cognition},
  author = {Pylyshyn, Zenon W.},
  year = {1984},
  publisher = {MIT Press},
  abstract = {This systematic investigation of computation and mental phenomena by a noted psychologist and computer scientist argues that cognition is a form of computation, that the semantic contents of mental states are encoded in the same general way as computer representations are encoded.},
  isbn = {978-0-262-16098-8},
  langid = {english},
  file = {~/Zotfiles/pylyshyn.z1984 Computation and cognition toward a foun.pdf}
}

@misc{qian.c:2025arxiv,
  title = {Demystifying {{Reasoning Dynamics}} with {{Mutual Information}}: {{Thinking Tokens}} Are {{Information Peaks}} in {{LLM Reasoning}}},
  shorttitle = {Demystifying {{Reasoning Dynamics}} with {{Mutual Information}}},
  author = {Qian, Chen and Liu, Dongrui and Wen, Haochen and Bai, Zhen and Liu, Yong and Shao, Jing},
  year = {2025},
  month = jun,
  number = {arXiv:2506.02867},
  eprint = {2506.02867},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.02867},
  urldate = {2025-07-24},
  abstract = {Large reasoning models (LRMs) have demonstrated impressive capabilities in complex problem-solving, yet their internal reasoning mechanisms remain poorly understood. In this paper, we investigate the reasoning trajectories of LRMs from an information-theoretic perspective. By tracking how mutual information (MI) between intermediate representations and the correct answer evolves during LRM reasoning, we observe an interesting MI peaks phenomenon: the MI at specific generative steps exhibits a sudden and significant increase during LRM's reasoning process. We theoretically analyze such phenomenon and show that as MI increases, the probability of model's prediction error decreases. Furthermore, these MI peaks often correspond to tokens expressing reflection or transition, such as ``Hmm'', ``Wait'' and ``Therefore,'' which we term as the thinking tokens. We then demonstrate that these thinking tokens are crucial for LRM's reasoning performance, while other tokens has minimal impacts. Building on these analyses, we propose two simple yet effective methods to improve LRM's reasoning performance, by delicately leveraging these thinking tokens. Overall, our work provides novel insights into the reasoning mechanisms of LRMs and offers practical ways to improve their reasoning capabilities. The code is available at https://github.com/ChnQ/MI-Peaks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/qian.c2025arxiv Demystifying Reasoning Dynamics with Mut.pdf}
}

@inproceedings{qian.p:2022,
  title = {Flexible Generation from Fragmentary Linguistic Input},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Qian, Peng and Levy, Roger},
  year = {2022},
  month = may,
  pages = {8176--8196},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.563},
  urldate = {2023-02-09},
  abstract = {The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models. But does direct specialization capture how humans approach novel language tasks? We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user. To test this hypothesis, we formulate a set of novel fragmentary text completion tasks, and compare the behavior of three direct-specialization models against a new model we introduce, GibbsComplete, which composes two basic computational motifs central to contemporary models: masked and autoregressive word prediction. We conduct three types of evaluation: human judgments of completion quality, satisfaction of syntactic constraints imposed by the input fragment, and similarity to human behavior in the structural statistics of the completions. With no task-specific parameter tuning, GibbsComplete performs comparably to direct-specialization models in the first two evaluations, and outperforms all direct-specialization models in the third evaluation. These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization.},
  file = {~/Zotfiles/qian.p2022 Flexible generation from fragmentary lin.pdf}
}

@misc{qian.p:2023psyarxiv,
  title = {Comprehenders' Error Correction Mechanisms Are Finely Calibrated to Language Production Statistics},
  author = {Qian, Peng and Levy, Roger Philip},
  year = {2023},
  month = sep,
  number = {e3v5b},
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/e3v5b},
  urldate = {2023-09-30},
  abstract = {An essential feature of human communication is robustness to errors: even when speakers and writers make mistakes, listeners and readers can usually determine the intended meaning. A leading hypothesis is that comprehenders achieve robust error correction by internally deploying a "noisy-channel" model of the language production process. Crucially, to best achieve error robustness, a comprehender's noisy-channel model should be finely calibrated to the environmental statistics of language production, incorporating not only prior expectations about communicative intents but also structure-sensitive likelihoods of different types of speaker error. This environmentally-calibrated noisy-channel hypothesis has been difficult to test, because the rates of most types of language production errors are hard to estimate. Here we test this hypothesis by experimentally investigating comprehenders' interpretations of subject-verb agreement errors, for which context-contingent language production error rates are feasible to estimate. In a novel free-form error correction task,  participants edited sentences with subject-verb agreement mismatches to indicate what they think was intended. Bayesian analysis of the resulting data shows that comprehenders' interpretations reflect both item-specific prior expectations and structure-sensitive error likelihoods that closely correspond to error rates in language production. These results provide quantitative evidence that humans closely track prior statistics over linguistic forms and deploy a noisy-channel model finely calibrated to statistics of the linguistic environment to achieve robust language understanding.},
  langid = {american},
  keywords = {Bayesian models of cognition,Cognitive Psychology,Language,language comprehension,noisy-channel language processing,psycholinguistics,rational adaptation,Social and Behavioral Sciences},
  file = {~/Zotfiles/qian.p2023psyarxiv Comprehenders error correction mechanis.pdf}
}

@article{rabagliati.h:2016,
  title = {Learning to Predict or Predicting to Learn?},
  author = {Rabagliati, Hugh and Gambi, Chiara and Pickering, J},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {94--105},
  doi = {10.1080/23273798.2015.1077979},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@misc{rabe.m:2023arxiv,
  title = {{{SEAM}}: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading},
  shorttitle = {Seam},
  author = {Rabe, Maximilian M. and Paape, Dario and Mertzen, Daniela and Vasishth, Shravan and Engbert, Ralf},
  year = {2023},
  month = mar,
  number = {arXiv:2303.05221},
  eprint = {2303.05221},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-06-05},
  abstract = {Models of eye-movement control during reading, developed largely within psychology, usually focus on visual, attentional, and motor processes but neglect post-lexical language processing; by contrast, models of sentence comprehension processes, developed largely within psycholinguistics, generally focus only on post-lexical language processes. We present a model that combines these two research threads, by integrating eye-movement control and sentence processing. Developing such an integrated model is extremely challenging and computationally demanding, but such an integration is an important step toward complete mathematical models of natural language comprehension in reading. We combine the SWIFT model of eye-movement control (Engbert et al., Psychological Review, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth sentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp. 375-419). This integration becomes possible, for the first time, due in part to recent advances in successful parameter identification in dynamical models, which allows us to investigate profile log-likelihoods for individual model parameters. We present a fully implemented proof-of-concept model demonstrating how such an integrated model can be achieved; our approach includes Bayesian model inference with Markov Chain Monte Carlo (MCMC) sampling as a key computational tool. The integrated model, SEAM, can successfully reproduce eye movement patterns that arise due to similarity-based interference in reading. To our knowledge, this is the first-ever integration of a complete process model of eye-movement control with linguistic dependency completion processes in sentence comprehension. In future work, this proof of concept model will need to be evaluated using a comprehensive set of benchmark data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,cue-based retrieval,eye-tracking,Quantitative Biology - Neurons and Cognition,sentence processing},
  file = {~/Zotfiles/rabe.m2023 SEAM an integrated activation-coupled m.pdf}
}

@book{rabinovich.m:2012book,
  title = {Principles of Brain Dynamics: Global State Interactions},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  month = jul,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9108.001.0001},
  urldate = {2022-07-08},
  abstract = {Experimental and theoretical approaches to global brain dynamics that draw on the latest research in the field.The consideration of time or dynamics is fundamental for all aspects of mental activity---perception, cognition, and emotion---because the main feature of brain activity is the continuous change of the underlying brain states even in a constant environment. The application of nonlinear dynamics to the study of brain activity began to flourish in the 1990s when combined with empirical observations from modern morphological and physiological observations. This book offers perspectives on brain dynamics that draw on the latest advances in research in the field. It includes contributions from both theoreticians and experimentalists, offering an eclectic treatment of fundamental issues.Topics addressed range from experimental and computational approaches to transient brain dynamics to the free-energy principle as a global brain theory. The book concludes with a short but rigorous guide to modern nonlinear dynamics and their application to neural dynamics.},
  isbn = {978-0-262-30558-7}
}

@article{rabovsky.m:2018,
  title = {Modelling the {{N400}} Brain Potential as Change in a Probabilistic Representation of Meaning},
  author = {Rabovsky, Milena and Hansen, Steven S. and McClelland, James L.},
  year = {2018},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {9},
  pages = {693--705},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0406-4},
  urldate = {2025-04-03},
  abstract = {The N400 component of the event-related brain potential has aroused much interest because it is thought to provide an online measure of meaning processing in the brain. However, the underlying process remains incompletely understood and actively debated. Here we present a computationally explicit account of this process and the emerging representation of sentence meaning. We simulate N400 amplitudes as the change induced by an incoming stimulus in an implicit and probabilistic representation of meaning captured by the hidden unit activation pattern in a neural network model of sentence comprehension, and we propose that the process underlying the N400 also drives implicit learning in the network. The model provides a unified account of 16 distinct findings from the N400 literature and connects human language comprehension with recent deep learning approaches to language processing.},
  copyright = {2018 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Language},
  file = {~/Zotfiles/rabovsky.m2018 Modelling the N400 brain potential as ch_1.pdf;~/Zotfiles/rabovsky.m2018 Modelling the N400 brain potential as ch.pdf}
}

@misc{radford.a:2018GPT,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {generative pre-training,GPT,GPT1,transfer learning}
}

@misc{radford.a:2018GPTwebpage,
  title = {Improving Language Understanding with Unsupervised Learning},
  shorttitle = {{{GPT}}},
  author = {Radford, Alec},
  year = {2018},
  month = jun,
  journal = {OpenAI},
  urldate = {2023-03-12},
  abstract = {We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas:~transformers~and~unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse~datasets.},
  howpublished = {https://openai.com/research/language-unsupervised},
  langid = {american}
}

@misc{radford.a:2019GPT2,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {GPT,GPT2}
}

@misc{radford.a:2019GPT2webpage,
  title = {Better Language Models and Their Implications},
  shorttitle = {{{GPT-2}}},
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniella and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  year = {2019},
  month = feb,
  journal = {OpenAI},
  urldate = {2023-03-12},
  abstract = {We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization---all without task-specific~training.},
  howpublished = {https://openai.com/research/better-language-models},
  langid = {american}
}

@article{raffel.c:2020T5,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text {{Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
  issn = {1533-7928},
  urldate = {2023-05-24},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  keywords = {unified LM},
  file = {~/Zotfiles/raffel.c2020T5 Exploring the limits of transfer learnin.pdf}
}

@misc{rahimi.h:2023arxiv,
  title = {Contextualized {{Topic Coherence Metrics}}},
  author = {Rahimi, Hamed and Hoover, Jacob Louis and Mimno, David and Naacke, Hubert and Constantin, Camelia and Amann, Bernd},
  year = {2023},
  month = may,
  number = {arXiv:2305.14587},
  eprint = {2305.14587},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-11},
  abstract = {The recent explosion in work on neural topic modeling has been criticized for optimizing automated topic evaluation metrics at the expense of actual meaningful topic identification. But human annotation remains expensive and time-consuming. We propose LLM-based methods inspired by standard human topic evaluations, in a family of metrics called Contextualized Topic Coherence (CTC). We evaluate both a fully automated version as well as a semi-automated CTC that allows human-centered evaluation of coherence while maintaining the efficiency of automated methods. We evaluate CTC relative to five other metrics on six topic models and find that it outperforms automated topic coherence methods, works well on short documents, and is not susceptible to meaningless but high-scoring topics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {~/Zotfiles/rahimi.h2023arxiv Contextualized Topic Coherence Metrics.pdf}
}

@inproceedings{rahimi.h:2024,
  title = {Contextualized {{Topic Coherence Metrics}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EACL}} 2024},
  author = {Rahimi, Hamed and Mimno, David and Hoover, Jacob and Naacke, Hubert and Constantin, Camelia and Amann, Bernd},
  editor = {Graham, Yvette and Purver, Matthew},
  year = {2024},
  month = mar,
  pages = {1760--1773},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  urldate = {2024-04-29},
  abstract = {This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.},
  file = {~/Zotfiles/rahimi.h2024 Contextualized Topic Coherence Metrics.pdf}
}

@inproceedings{rainforth.t:2018,
  title = {Tighter {{Variational Bounds}} Are {{Not Necessarily Better}}},
  booktitle = {Proceedings of the 35th {{International Conference}} on {{Machine Learning}}},
  author = {Rainforth, Tom and Kosiorek, Adam and Le, Tuan Anh and Maddison, Chris and Igl, Maximilian and Wood, Frank and Teh, Yee Whye},
  year = {2018},
  month = jul,
  pages = {4277--4285},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2025-08-12},
  abstract = {We provide theoretical and empirical evidence that using tighter evidence lower bounds (ELBOs) can be detrimental to the process of learning an inference network by reducing the signal-to-noise ratio of the gradient estimator. Our results call into question common implicit assumptions that tighter ELBOs are better variational objectives for simultaneous model learning and inference amortization schemes. Based on our insights, we introduce three new algorithms: the partially importance weighted auto-encoder (PIWAE), the multiply importance weighted auto-encoder (MIWAE), and the combination importance weighted autoencoder (CIWAE), each of which includes the standard importance weighted auto-encoder (IWAE) as a special case. We show that each can deliver improvements over IWAE, even when performance is measured by the IWAE target itself. Furthermore, our results suggest that PIWAE may be able to deliver simultaneous improvements in the training of both the inference and generative networks.},
  langid = {english},
  file = {~/Zotfiles/rainforth.t2018 Tighter Variational Bounds are Not Neces_1.pdf;~/Zotfiles/rainforth.t2018 Tighter Variational Bounds are Not Neces.pdf}
}

@article{ramgoolam.s:2019,
  title = {Permutation Invariant Gaussian Matrix Models},
  author = {Ramgoolam, Sanjaye},
  year = {2019},
  journal = {Nuclear Physics B},
  pages = {114682},
  publisher = {Elsevier},
  date-added = {2019-08-06 08:51:05 +0300},
  date-modified = {2019-08-06 08:52:06 +0300},
  project = {syntactic embedding},
  keywords = {gaussian matrix models,physics}
}

@article{rao.r:1999,
  title = {Predictive Coding in the Visual Cortex: A Functional Interpretation of Some Extra-Classical Receptive-Field Effects},
  shorttitle = {Predictive Coding in the Visual Cortex},
  author = {Rao, Rajesh P. N. and Ballard, Dana H.},
  year = {1999},
  month = jan,
  journal = {Nature Neuroscience},
  volume = {2},
  number = {1},
  pages = {79--87},
  publisher = {Nature Publishing Group},
  issn = {1546-1726},
  doi = {10.1038/4580},
  urldate = {2025-04-03},
  abstract = {We describe a model of visual processing in which feedback connections from a higher- to a lower-order visual cortical area carry predictions of lower-level neural activities, whereas the feedforward connections carry the residual errors between the predictions and the actual lower-level activities. When exposed to natural images, a hierarchical network of model neurons implementing such a model developed simple-cell-like receptive fields. A subset of neurons responsible for carrying the residual errors showed endstopping and other extra-classical receptive-field effects. These results suggest that rather than being exclusively feedforward phenomena, nonclassical surround effects in the visual cortex may also result from cortico-cortical feedback as a consequence of the visual system using an efficient hierarchical strategy for encoding natural images.},
  copyright = {1999 Nature America Inc.},
  langid = {english},
  keywords = {Animal Genetics and Genomics,Behavioral Sciences,Biological Techniques,Biomedicine,general,Neurobiology,Neurosciences},
  file = {~/Zotfiles/rao.r1999 Predictive coding in the visual cortex.pdf}
}

@article{rasmussen.n:2018,
  title = {Left-Corner Parsing with Distributed Associative Memory Produces Surprisal and Locality Effects},
  author = {Rasmussen, Nathan E. and Schuler, William},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1009--1042},
  issn = {1551-6709},
  doi = {10.1111/cogs.12511},
  urldate = {2022-06-13},
  abstract = {This article describes a left-corner parser implemented within a cognitively and neurologically motivated distributed model of memory. This parser's approach to syntactic ambiguity points toward a tidy account both of surprisal effects and of locality effects, such as the parsing breakdowns caused by center embedding. The model provides an algorithmic-level (Marr, 1982) account of these breakdowns: The structure of the parser's memory and the nature of incremental parsing produce a smooth degradation of processing accuracy for longer center embeddings, and a steeper degradation when they are nested, in line with recall observations by Miller and Isard (1964) and speed-accuracy trade-off observations by McElree et al. (2003). Modeling results show that this effect is distinct from the effects of ambiguity and exceeds the effect of mere sentence length.},
  langid = {english},
  keywords = {Computational modeling,Computer simulation,Language understanding,Linguistics,Locality,Memory,Sentence processing,Surprisal,Syntax},
  file = {~/Zotfiles/rasmussen.n2018 Left-corner parsing with distributed ass.pdf}
}

@misc{rasooli.m:2015arxiv,
  title = {Yara {{Parser}}: {{A Fast}} and {{Accurate Dependency Parser}}},
  shorttitle = {Yara {{Parser}}},
  author = {Rasooli, Mohammad Sadegh and Tetreault, Joel},
  year = {2015},
  month = mar,
  number = {arXiv:1503.06733},
  eprint = {1503.06733},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-05-17},
  abstract = {Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https://github.com/yahoo/YaraParser.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/rasooli.m2015 Yara Parser A Fast and Accurate Depende.pdf}
}

@article{ratcliff.r:1978,
  title = {A Theory of Memory Retrieval},
  author = {Ratcliff, Roger},
  year = {1978},
  journal = {Psychological Review},
  volume = {85},
  number = {2},
  pages = {59--108},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.85.2.59},
  abstract = {Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe--memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe--memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study--test, and continuous) and to speed--accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Memory,Theories}
}

@article{ratcliff.r:2008,
  title = {The {{Diffusion Decision Model}}: {{Theory}} and {{Data}} for {{Two-Choice Decision Tasks}}},
  shorttitle = {The {{Diffusion Decision Model}}},
  author = {Ratcliff, Roger and McKoon, Gail},
  year = {2008},
  month = apr,
  journal = {Neural Computation},
  volume = {20},
  number = {4},
  pages = {873--922},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.12-06-420},
  urldate = {2024-02-29},
  abstract = {The diffusion decision model allows detailed explanations of behavior in two-choice discrimination tasks. In this article, the model is reviewed to show how it translates behavioral data---accuracy, mean response times, and response time distributions---into components of cognitive processing. Three experiments are used to illustrate experimental manipulations of three components: stimulus difficulty affects the quality of information on which a decision is based; instructions emphasizing either speed or accuracy affect the criterial amounts of information that a subject requires before initiating a response; and the relative proportions of the two stimuli affect biases in drift rate and starting point. The experiments also illustrate the strong constraints that ensure the model is empirically testable and potentially falsifiable. The broad range of applications of the model is also reviewed, including research in the domains of aging and neurophysiology.},
  keywords = {drift diffusion},
  file = {~/Zotfiles/ratcliff.r2008 The Diffusion Decision Model Theory and.pdf}
}

@inproceedings{rathi.n:2021,
  title = {An {{Information-Theoretic Characterization}} of {{Morphological Fusion}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rathi, Neil and Hahn, Michael and Futrell, Richard},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {10115--10120},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.793},
  urldate = {2024-03-31},
  abstract = {Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.},
  file = {~/Zotfiles/rathi.n2021 An Information-Theoretic Characterizatio.pdf}
}

@phdthesis{ravishankar.m:1996,
  title = {Efficient Algorithms for Speech Recognition},
  author = {Ravishankar, Mosur K},
  year = {1996},
  month = may,
  date-added = {2022-03-25 23:17:15 -0400},
  date-modified = {2022-03-25 23:19:35 -0400},
  school = {Carnegie Mellon University, Department of Computer Science}
}

@article{rayner.k:1998,
  title = {Eye Movements in Reading and Information Processing: 20 Years of Research},
  author = {Rayner, Keith},
  year = {1998},
  journal = {Psychological Bulletin},
  volume = {124},
  pages = {372--422},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.124.3.372},
  abstract = {Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {cognitive processes,eye movements,Reading}
}

@manual{rcoreteam.:2021,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@article{redington.m:1997,
  title = {Probabilistic and Distributional Approaches to Language Acquisition},
  author = {Redington, Martin and Chater, Nick},
  year = {1997},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {1},
  number = {7},
  pages = {273--281},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(97)01081-4},
  urldate = {2024-05-26},
  langid = {english},
  pmid = {21223923},
  file = {~/Zotfiles/redington.m1997 Probabilistic and distributional approac.pdf}
}

@inproceedings{rehurek.r:2010,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r u}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  pages = {45--50},
  publisher = {ELRA},
  address = {Valletta, Malta},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@article{reichle.e:2003,
  title = {The {{E-Z Reader}} Model of Eye-Movement Control in Reading: {{Comparisons}} to Other Models},
  author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
  year = {2003},
  journal = {Behavioral and Brain Sciences},
  volume = {26},
  number = {4},
  pages = {445--476},
  publisher = {Cambridge University Press (CUP)},
  doi = {10.1017/s0140525x03000104},
  bdsk-url-2 = {https://doi.org/10.1017/s0140525x03000104},
  date-added = {2021-05-22 14:59:06 -0400},
  date-modified = {2022-05-06 15:37:48 -0400},
  keywords = {eye-tracking,processing},
  file = {~/Zotfiles/reichle.e2003 The E-Z Reader model of eye-movement con.pdf}
}

@inproceedings{reif.e:2019,
  title = {Visualizing and Measuring the Geometry of {{BERT}}},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Vi{\'e}gas, Fernanda B. and Coenen, Andy and Pearce, Adam and Kim, Been},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {8592--8600},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ReifYWVCPK19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{reiss.c:2017,
  title = {Substance Free Phonology},
  author = {Reiss, Charles},
  editor = {S. J. Hannahs, A. R. K. Bosch},
  year = {2017},
  journal = {The Routledge handbook of phonological theory},
  pages = {425--452},
  publisher = {Routledge New York},
  date-added = {2019-06-17 08:29:14 -0400},
  date-modified = {2019-06-17 08:38:02 -0400},
  isbn = {9781138025813},
  keywords = {substance free phonology}
}

@inproceedings{renyi.a:1961,
  title = {On Measures of Entropy and Information},
  booktitle = {Proceedings of the Fourth {{Berkeley}} Symposium on Mathematical Statistics and Probability},
  author = {R{\'e}nyi, Alfr{\'e}d},
  editor = {Neyman, Jerzy},
  year = {1961},
  volume = {1},
  pages = {547--561},
  date-added = {2021-10-27 09:20:40 -0400},
  date-modified = {2021-10-27 09:27:42 -0400},
  organization = {University of California Press}
}

@article{rezac.m:2008,
  title = {Phi-Agree and Theta-Related Case},
  author = {Rezac, Milan},
  year = {2008},
  journal = {Phi theory: Phi-features across interfaces and modules},
  pages = {83--129},
  publisher = {Oxford University Press Oxford},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:25:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@unpublished{rezac.m:2016ms,
  type = {Ms. {{UMR}} 5478, {{IKER CNRS}}},
  title = {The Ways of Referential Deficiency: {{Impersonal}} {{{\emph{on}}}} and Its Kin},
  shorttitle = {The Ways of Referential Deficiency},
  author = {Rezac, Milan and Jouitteau, M{\'e}lanie},
  year = {2016},
  month = sep,
  doi = {10.5281/ZENODO.5823635},
  urldate = {2024-05-09},
  abstract = {This work is a study of the French impersonal {$<$}em{$>$}on{$<$}/em{$>$} and a theory of the unique "referential deficiency" of impersonals: a range of uses that spans those covered by indefinites and definites; neutrality about content like number; systematic participation in syntactic and semantic dependencies but with unparalleled restrictions like binding of only local, number-neutral anaphora. Current understanding of the syntax and semantics of DPs and properties of French let us study this behavior in depth and extend previous findings, often in unexpected ways. The study reveals a DP with content unique in French but drawing only on options available in UG. It leads to a theory of impersonal {$<$}em{$>$}on{$<$}/em{$>$} as an indefinite DP whose content interacts with certain theories of phi-features, indefinites and definites, and anaphoric dependencies to give an explanatory account of the nature of impersonals. In turn, impersonal {$<$}em{$>$}on{$<$}/em{$>$} contributes to the theories that enter into its analysis: the relationship between syntactic underspecification and semantic neutrality, the nature of indefinites and their relationship to definites (Heim 1991, 2011; Heim 1982, Elbourne 2013), and minimal pronoun anaphora (Kratzer 2009). The study is extended to the grammaticalisation of a distinct 1PL {$<$}em{$>$}on{$<$}/em{$>$}, whose complex properties reflect the colexicalisation of impersonal {$<$}em{$>$}on{$<$}/em{$>$} and a 1PL element. We end the book on the place of impersonals in the landscape of argument coding and explore the expected parameter space through {$<$}em{$>$}on{$<$}/em{$>$}-like impersonals cross-linguistically. The work builds on the analysis of {$<$}em{$>$}on{$<$}/em{$>$} and its kin in Cinque (1988), Chierchia (1995b), Egerland (2003b), Kayne (2010), and explores them in the Principles-and-Parameters approach to syntax and the "situated descriptions" (Elbourne 2013) extension of the syntax-semantics mapping of Heim and Kratzer (1998).},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@book{riehl.e:2017book,
  title = {Category Theory in Context},
  author = {Riehl, Emily},
  year = {2017},
  publisher = {Courier Dover Publications},
  date-added = {2019-08-24 09:26:31 -0400},
  date-modified = {2019-08-24 09:26:50 -0400},
  keywords = {category theory}
}

@article{rigby.r:2005,
  title = {Generalized Additive Models for Location, Scale and Shape},
  author = {Rigby, R. A. and Stasinopoulos, D. M.},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {54},
  number = {3},
  pages = {507--554},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2005.00510.x},
  urldate = {2023-03-01},
  abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton--Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
  langid = {english},
  keywords = {Beta-binomial distribution,Box-Cox transformation,Centile estimation,Cubic smoothing splines,GAMLSS,Generalized linear mixed model,LMS method,Negative binomial distribution,Non-normality,Nonparametric models,Overdispersion,Penalized likelihood,Random effects,Skewness and kurtosis},
  file = {~/Zotfiles/rigby.r2005GAMLSS Generalized additive models for location.pdf}
}

@article{ritchie.d:1986,
  title = {Shannon and {{Weaver}}: Unravelling the Paradox of Information},
  author = {Ritchie, David},
  year = {1986},
  month = apr,
  journal = {Communication Research},
  volume = {13},
  number = {2},
  pages = {278--298},
  publisher = {SAGE Publications},
  doi = {10.1177/009365086013002007},
  abstract = {A case is presented for separating Shannon's (1949) paper on information theory from Weaver's introduction, which is shown to contain distortions, as well as proofs by coincidence and homonym. Shannon's mathematical tools and methods are distinguished from his theory, which consists of 23 theorems setting forth the conditions for maximum efficiency in electromechanical signal transmission. Attempts to apply Shannon's theory to our field are reviewed, along with previous critiques, and it is recommended that future uses of Shannon's theory adopt a more methodologically rigorous approach. In particular, it is argued that Shannon's assumptions must be shown to hold before his theorems can be successfully applied.},
  bdsk-url-2 = {https://doi.org/10.1177/009365086013002007},
  date-added = {2022-04-07 12:59:21 -0400},
  date-modified = {2022-04-07 13:01:58 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{roark.b:2001,
  title = {Probabilistic Top-down Parsing and Language Modeling},
  author = {Roark, Brian},
  year = {2001},
  journal = {Computational Linguistics},
  volume = {27},
  number = {2},
  pages = {249--276},
  doi = {10.1162/089120101750300526},
  bdsk-url-2 = {https://doi.org/10.1162/089120101750300526},
  file = {~/Zotfiles/roark.b2001 Probabilistic top-down parsing and langu.pdf}
}

@inproceedings{roark.b:2009,
  title = {Deriving Lexical and Syntactic Expectation-Based Measures for Psycholinguistic Modeling via Incremental Top-down Parsing},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
  year = {2009},
  pages = {324--333},
  publisher = {Association for Computational Linguistics},
  address = {Singapore}
}

@techreport{roark.b:2011report,
  type = {Technical Report},
  title = {Expected Surprisal and Entropy},
  author = {Roark, Brian},
  year = {2011},
  number = {CSLU-11-004},
  institution = {{Center for Spoken Language Processing, Oregon Health and Science University}},
  date-added = {2021-06-16 09:42:11 -0400},
  date-modified = {2021-06-16 09:48:54 -0400},
  file = {~/Zotfiles/roark.b2011techreport Expected surprisal and entropy.pdf}
}

@book{robert.c:2004book,
  title = {Monte {{Carlo Statistical Methods}}},
  author = {Robert, Christian P. and Casella, George},
  year = {2004},
  series = {Springer {{Texts}} in {{Statistics}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4757-4145-2},
  urldate = {2025-04-19},
  copyright = {http://www.springer.com/tdm},
  isbn = {978-1-4757-4145-2},
  keywords = {algorithms,computer,Importance Sampling,Markov chain,Markov chain Monte Carlo,Mathematica,mathematical statistics,mathematics,optimization,programming,Random variable,Ringe,simulation,statistical inference,statistics},
  file = {~/Zotfiles/robert.c2004book Monte Carlo Statistical Methods.pdf}
}

@misc{robert.c:2010,
  type = {Blog},
  title = {Effective Sample Size},
  author = {Robert, Christian P.},
  year = {2010},
  month = sep,
  journal = {Xi'an's Og},
  urldate = {2022-12-10},
  langid = {english},
  keywords = {effective sample size,importance sampling}
}

@misc{rogers.a:2024arxiv,
  title = {Position: {{Key Claims}} in {{LLM Research Have}} a {{Long Tail}} of {{Footnotes}}},
  shorttitle = {Position},
  author = {Rogers, Anna and Luccioni, Alexandra Sasha},
  year = {2024},
  month = jun,
  number = {arXiv:2308.07120},
  eprint = {2308.07120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07120},
  urldate = {2024-06-04},
  abstract = {Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/rogers.a2024arxiv Position Key Claims in LLM Research Hav.pdf}
}

@article{rogers.j:2003,
  title = {Syntactic Structures as Multi-Dimensional Trees},
  author = {Rogers, James},
  year = {2003},
  journal = {Research on Language and Computation},
  volume = {1},
  number = {3-4},
  pages = {265--305},
  publisher = {Springer},
  date-added = {2019-06-15 11:50:06 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {automata,control languages}
}

@incollection{rohatgi.v:2015,
  title = {Some Special Distributions},
  booktitle = {An {{Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Rohatgi, Vijay K. and Saleh, A. K. Md. Ehsanes},
  year = {2015},
  pages = {173--244},
  publisher = {John Wiley \& Sons, Ltd},
  urldate = {2022-07-20},
  abstract = {This chapter focuses on some commonly occurring probability distributions and investigates their basic properties. The results of this chapter will be of considerable use in theoretical as well as practical applications. The chapter begins with some univariate and multivariate discrete distributions and proceeds with some continuous models. It then deals with bivariate and multivariate normal distributions and subsequently discusses the exponential family of distributions. Finally, the chapter records briefly some of the several other distributions which are related to these special distributions and their important characteristics.},
  chapter = {5},
  isbn = {978-1-118-79963-5},
  langid = {english},
  keywords = {bivariate normal distribution,continuous distributions,discrete distributions,exponential distribution,multivariate normal distribution},
  file = {~/Zotfiles/rohatgi.v2015 Some special distributions.pdf}
}

@article{ronai.e:2023,
  title = {Memory versus Expectation: Processing Relative Clauses in a Flexible Word Order Language},
  shorttitle = {Memory versus Expectation},
  author = {Ronai, Eszter and Xiang, Ming},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {1},
  pages = {e13227},
  issn = {1551-6709},
  doi = {10.1111/cogs.13227},
  urldate = {2023-03-09},
  abstract = {Memory limitations and probabilistic expectations are two key factors that have been posited to play a role in the incremental processing of natural language. Relative clauses (RCs) have long served as a key proving ground for such theories of language processing. Across three self-paced reading experiments, we test the online comprehension of Hungarian subject- and object-extracted RCs (SRCs and ORCs, respectively). We capitalize on the syntactic properties of Hungarian that allow for a variety of word orders within RCs, which helps us to delineate the processing costs associated with memory demand and violated expectations. Results showed a processing cost at the RC verb for structures that have longer verb-argument distances, despite those structures being more frequent in the corpus. These findings thus support theories that attribute processing difficulty to memory limitations, rather than theories that attribute difficulty to less expected structures.},
  langid = {english},
  keywords = {Language processing,Memory models,object-extracted relative clauses,ORC,Predictive processing,Relative clauses,Syntactic parsing},
  file = {~/Zotfiles/ronai.e2023 Memory versus expectation processing re.pdf}
}

@misc{rosa.r:2019,
  title = {Inducing Syntactic Trees from {{BERT}} Representations},
  author = {Rosa, Rudolf and Mare{\v c}ek, David},
  year = {2019},
  eprint = {1906.11511},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-16 11:51:58 -0400}
}

@incollection{rosenbloom.p:1987,
  title = {Learning by Chunking: A Production System Model of Practice},
  booktitle = {Production {{System Models}} of {{Learning}} and {{Development}}},
  author = {Rosenbloom, Paul and Newell, Allen},
  editor = {Klahr, David and Langley, Patrick W. and Neches, Robert T.},
  year = {1987},
  month = jan,
  pages = {221--286},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/5605.003.0007},
  urldate = {2022-09-25},
  isbn = {978-0-262-31596-8}
}

@inproceedings{rosenkrantz.d:1970,
  title = {Deterministic Left Corner Parsing},
  booktitle = {11th Annual Symposium on Switching and Automata Theory (Swat 1970)},
  author = {Rosenkrantz, D. J. and Lewis, P. M.},
  year = {1970},
  month = oct,
  pages = {139--152},
  publisher = {IEEE},
  address = {Los Angeles, California},
  doi = {10.1109/SWAT.1970.5},
  date-added = {2022-03-11 22:35:17 -0500},
  date-modified = {2022-03-11 22:35:18 -0500}
}

@book{rosenthal.j:2006book2,
  title = {A First Look at Rigorous Probability Theory},
  author = {Rosenthal, Jeffrey S},
  year = {2006},
  month = nov,
  edition = {2},
  publisher = {World Scientific},
  doi = {10.1142/6300},
  bdsk-url-2 = {https://doi.org/10.1142/6300},
  bdsk-url-3 = {http://probability.ca/jeff/grprobbook.html},
  date-added = {2021-10-15 14:24:25 -0400},
  date-modified = {2021-10-15 14:25:40 -0400}
}

@article{rouder.j:2015,
  title = {The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  shorttitle = {The Lognormal Race},
  author = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  year = {2015},
  month = jun,
  journal = {Psychometrika},
  volume = {80},
  number = {2},
  pages = {491--513},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9396-3},
  urldate = {2022-08-13},
  langid = {english}
}

@book{royden.h:2010book4,
  title = {Real Analysis},
  author = {Royden, H. L. and Fitzpatrick, Patrick},
  year = {2010},
  edition = {4},
  publisher = {Prentice Hall},
  address = {Boston},
  abstract = {Real Analysis, Fourth Edition, covers the basic material that every reader should know in the classical theory of functions of a real variable, measure and integration theory, and some of the more important and elementary topics in general topology and normed linear space theory. This text assumes a general background in mathematics and familiarity with the fundamental concepts of analysis. Classical theory of functions, including the classical Banach spaces; General topology and the theory of general Banach spaces; Abstract treatment of measure and integration. For all readers interested in real analysis},
  isbn = {978-0-13-143747-0},
  langid = {english}
}

@article{rubenstein.h:1970,
  title = {Homographic Entries in the Internal Lexicon},
  author = {Rubenstein, Herbert and Garfield, Lonnie and Millikan, Jane A.},
  year = {1970},
  month = oct,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {9},
  number = {5},
  pages = {487--494},
  issn = {0022-5371},
  doi = {10.1016/S0022-5371(70)80091-3},
  urldate = {2025-06-06},
  abstract = {The task was to distinguish between English and nonsense words, which were displayed singly. The display persisted until S pressed the yes-key if he thought the stimulus was English or the no-key if he thought it was nonsense. The response times were faster for English than nonsense, faster for English words of higher frequency than lower frequency, and faster for homographs than nonhomographs. It is hypothesized that word recognition in general requires consulting the internal lexicon. A model of the underlying processes is sketched which proposes that words of higher frequency are recognized sooner because their lexical entries are marked earlier for comparison against the stimulus information. It is also proposed that homographs are recognized sooner than nonhomographs since homographs have more lexical entries available for comparison against the stimulus information.}
}

@article{rumelhart.d:1986,
  title = {Learning Representations by Back-Propagating Errors},
  author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
  year = {1986},
  month = oct,
  journal = {Nature},
  volume = {323},
  number = {6088},
  pages = {533--536},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/323533a0},
  urldate = {2025-04-17},
  abstract = {We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
  copyright = {1986 Springer Nature Limited},
  langid = {english},
  keywords = {error back-propagation,Humanities and Social Sciences,multidisciplinary,Science},
  file = {~/Zotfiles/rumelhart.d1986 Learning representations by back-propaga.pdf}
}

@article{ryskin.r:2018,
  title = {Comprehenders Model the Nature of Noise in the Environment},
  author = {Ryskin, Rachel and Futrell, Richard and Kiran, Swathi and Gibson, Edward},
  year = {2018},
  month = dec,
  journal = {Cognition},
  volume = {181},
  pages = {141--150},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.08.018},
  urldate = {2022-11-28},
  abstract = {In everyday communication, speakers make errors and produce language in a noisy environment. Recent work suggests that comprehenders possess cognitive mechanisms for dealing with noise in the linguistic signal: a noisy-channel model. A key parameter of these models is the noise model: the comprehender's implicit model of how noise affects utterances before they are perceived. Here we examine this noise model in detail, asking whether comprehension behavior reflects a noise model that is adapted to context. We asked readers to correct sentences if they noticed errors, and manipulated context by including exposure sentences containing obvious deletions (A bystander was rescued by the fireman in the nick time.), insertions, exchanges, mixed errors, or no errors. On test sentences (The bat swung the player.), participants' corrections differed depending on the exposure condition. The results demonstrate that participants model specific types of errors and make inferences about the intentions of the speaker accordingly.},
  langid = {english},
  keywords = {Adaptation,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {~/Zotfiles/ryskin.r2018 Comprehenders model the nature of noise.pdf}
}

@article{ryskin.r:2021,
  title = {An {{ERP}} Index of Real-Time Error Correction within a Noisy-Channel Framework of Human Communication},
  author = {Ryskin, Rachel and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Fedorenko, Evelina and Gibson, Edward},
  year = {2021},
  month = jul,
  journal = {Neuropsychologia},
  volume = {158},
  pages = {107855},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107855},
  urldate = {2022-06-24},
  abstract = {Recent evidence suggests that language processing is well-adapted to noise in the input (e.g., spelling or speech errors, misreading or mishearing) and that comprehenders readily correct the input via rational inference over possible intended sentences given probable noise corruptions. In the current study, we probed the processing of noisy linguistic input, asking whether well-studied ERP components may serve as useful indices of this inferential process. In particular, we examined sentences where semantic violations could be attributed to noise---for example, in ``The storyteller could turn any incident into an amusing antidote'', where the implausible word ``antidote'' is orthographically and phonologically close to the intended ``anecdote''. We found that the processing of such sentences---where the probability that the message was corrupted by noise exceeds the probability that it was produced intentionally and perceived accurately---was associated with a reduced (less negative) N400 effect and an increased P600 effect, compared to semantic violations which are unlikely to be attributed to noise (``The storyteller could turn any incident into an amusing hearse''). Further, the magnitudes of these ERP effects were correlated with the probability that the comprehender retrieved a plausible alternative. This work thus adds to the growing body of literature that suggests that many aspects of language processing are optimized for dealing with noise in the input, and opens the door to electrophysiologic investigations of the computations that support the processing of imperfect input.},
  langid = {english},
  keywords = {electroencephalography,event-related potential,N400,noisy-channel,P600},
  file = {~/Zotfiles/ryskin.r2021 An ERP index of real-time error correcti.pdf}
}

@article{ryskin.r:2023,
  title = {Prediction during Language Comprehension: What Is Next?},
  shorttitle = {Prediction during Language Comprehension},
  author = {Ryskin, Rachel and Nieuwland, Mante S.},
  year = {2023},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {27},
  number = {11},
  pages = {1032--1052},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2023.08.003},
  urldate = {2025-03-29},
  langid = {english},
  pmid = {37704456},
  keywords = {cognitive control,individual variability,language,learning,memory,prediction},
  file = {~/Zotfiles/ryskin.r2023 Prediction during language comprehension.pdf}
}

@article{ryskin.r:2025,
  title = {Noisy-Channel Language Comprehension in Aphasia: {{A Bayesian}} Mixture Modeling Approach},
  shorttitle = {Noisy-Channel Language Comprehension in Aphasia},
  author = {Ryskin, Rachel and Gibson, Edward and Kiran, Swathi},
  year = {2025},
  month = jan,
  journal = {Psychonomic Bulletin \& Review},
  issn = {1531-5320},
  doi = {10.3758/s13423-025-02639-z},
  urldate = {2025-02-06},
  abstract = {Individuals with ``agrammatic'' receptive aphasia have long been known to rely on semantic plausibility rather than syntactic cues when interpreting sentences. In contrast to early interpretations of this pattern as indicative of a deficit in syntactic knowledge, a recent proposal views agrammatic comprehension as a case of ``noisy-channel'' language processing with an increased expectation of noise in the input relative to healthy adults. Here, we investigate the nature of the noise model in aphasia and whether it is adapted to the statistics of the environment. We first replicate findings that a) healthy adults (N = 40) make inferences about the intended meaning of a sentence by weighing the prior probability of an intended sentence against the likelihood of a noise corruption and b) their estimate of the probability of noise increases when there are more errors in the input (manipulated via exposure sentences). We then extend prior findings that adults with chronic post-stroke aphasia (N = 28) and healthy age-matched adults (N = 19) similarly engage in noisy-channel inference during comprehension. We use a hierarchical latent mixture modeling approach to account for the fact that rates of guessing are likely to differ between healthy controls and individuals with aphasia and capture individual differences in the tendency to make inferences. We show that individuals with aphasia are more likely than healthy controls to draw noisy-channel inferences when interpreting semantically implausible sentences, even when group differences in the tendency to guess are accounted for. While healthy adults rapidly adapt their inference rates to an increase in noise in their input, whether individuals with aphasia do the same remains equivocal. Further investigation of comprehension through a noisy-channel lens holds promise for a parsimonious understanding of language processing in aphasia and may suggest potential avenues for treatment.},
  langid = {english},
  keywords = {Aphasia,Language comprehension,Noisy-channel,noisy-channel processing},
  file = {~/Zotfiles/ryskin.r2025 Noisy-channel language comprehension in.pdf}
}

@book{sag.i:2003book2,
  title = {Syntactic Theory: {{A}} Formal Introduction},
  author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M.},
  year = {2003},
  edition = {2},
  publisher = {CSLI},
  address = {Stanford, CA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{sag.i:2012,
  title = {Sign-Based Construction Grammar: {{An}} Informal Synopsis},
  booktitle = {Sign--{{Based}} Construction Grammar},
  author = {Sag, Ivan A.},
  editor = {{Boas, Hans abd Sag}, Ivan A.},
  year = {2012},
  pages = {101--107},
  publisher = {CSLI Publications},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:05 -0400}
}

@article{sainburg.t:2019,
  title = {Parallels in the Sequential Organization of Birdsong and Human Speech},
  author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q},
  year = {2019},
  journal = {Nature communications},
  volume = {10},
  publisher = {Nature Publishing Group},
  date-added = {2019-10-01 16:50:44 -0400},
  date-modified = {2019-10-01 16:51:36 -0400},
  keywords = {birdsong,mutual information,structure,syntax}
}

@inproceedings{salimans.t:2015MCVI,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: Bridging the Gap},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  editor = {Bach, Francis and Blei, David},
  year = {2015},
  month = jul,
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {1218--1226},
  publisher = {PMLR},
  address = {Lille, France},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  date-added = {2022-05-05 11:01:25 -0400},
  date-modified = {2022-05-05 11:02:29 -0400},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf},
  keywords = {markov chain monte carlo,markov chain variational inference,variational inference}
}

@misc{salle.a:2019,
  title = {Why so down? {{The}} Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics},
  author = {Salle, Alexandre and Villavicencio, Aline},
  year = {2019},
  eprint = {1908.06941},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-07-13 08:39:45 -0400},
  date-modified = {2020-07-13 08:40:45 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,pmi}
}

@article{salthouse.t:2010,
  title = {Selective Review of Cognitive Aging},
  author = {Salthouse, Timothy A.},
  year = {2010},
  month = sep,
  journal = {Journal of the International Neuropsychological Society},
  volume = {16},
  number = {5},
  pages = {754--760},
  issn = {1469-7661, 1355-6177},
  doi = {10.1017/S1355617710000706},
  urldate = {2024-03-19},
  abstract = {Research concerned with relations between adult age and cognitive functioning is briefly reviewed. The coverage is necessarily selective, and is organized in terms of five major questions. These are what abilities are related to age, how many distinct influences are contributing to the relations between age and cognitive functioning, do the differences between people increase with advancing age, what is responsible for the discrepancies between cross-sectional and longitudinal age comparisons of cognitive functioning, and what methods can be used to identify causes of age-related influences on cognition. Although definitive answers are not yet possible, quite a bit of information relevant to the questions is now available. Moreover, the existing information has implications for the design, analysis, and interpretation of cognitive and neuropsychological research concerned with aging. (JINS, 2010, 16, 754--760.)},
  langid = {english},
  keywords = {cognitive aging,Cognitive aging,Cross-sectional,Longitudinal,Mediators,Moderators,Variability},
  file = {~/Zotfiles/salthouse.t2010 Selective review of cognitive aging.pdf}
}

@article{samson.e:1953,
  title = {Fundamental Natural Concepts of Information Theory},
  author = {Samson, Edward W.},
  year = {1953},
  journal = {ETC: A Review of General Semantics},
  volume = {10},
  number = {4, Summer 1953, special issue on information theory},
  eprint = {42581366},
  eprinttype = {jstor},
  pages = {283--297},
  publisher = {Institute of General Semantics},
  issn = {0014164X, 21689245},
  urldate = {2024-03-25},
  annotation = {original-pages: 25pp},
  file = {~/Zotfiles/samson.e1953 Fundamental natural concepts of informat.pdf}
}

@inproceedings{sanborn.a:2006cogsci,
  title = {A More Rational Model of Categorization},
  booktitle = {Proceedings of the 28th {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Sanborn, Adam and Griffiths, Thomas L. and Navarro, Daniel J.},
  year = {2006},
  eprint = {2440/35610},
  eprinttype = {hdl},
  publisher = {LAWRENCEE},
  address = {Vancouver, British Columbia, Canada},
  urldate = {2022-10-20},
  abstract = {Adam N. Sanborn, Thomas L. Griffiths, Daniel J. Navarro},
  langid = {english},
  keywords = {particle filtering},
  file = {~/Zotfiles/sanborn.a2006cogsci A more rational model of categorization.pdf}
}

@article{sanborn.a:2010,
  title = {Rational Approximations to Rational Models: Alternative Algorithms for Category Learning.},
  shorttitle = {Rational Approximations to Rational Models},
  author = {Sanborn, Adam N. and Griffiths, Thomas L. and Navarro, Daniel J.},
  year = {2010},
  journal = {Psychological Review},
  volume = {117},
  number = {4},
  pages = {1144--1167},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0020511},
  urldate = {2025-02-17},
  langid = {english},
  file = {~/Zotfiles/sanborn.a2010 Rational approximations to rational mode.pdf}
}

@article{sanborn.a:2016,
  title = {Bayesian {{Brains}} without {{Probabilities}}},
  author = {Sanborn, Adam N. and Chater, Nick},
  year = {2016},
  month = dec,
  journal = {Trends in Cognitive Sciences},
  volume = {20},
  number = {12},
  pages = {883--893},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2016.10.003},
  urldate = {2025-02-18},
  langid = {english},
  pmid = {28327290},
  keywords = {Bayesian models of cognition,reasoning biases,sampling,sampling based cognition}
}

@article{sanford.a:2002,
  title = {Depth of Processing in Language Comprehension: Not Noticing the Evidence},
  shorttitle = {Depth of Processing in Language Comprehension},
  author = {Sanford, Anthony J. and Sturt, Patrick},
  year = {2002},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {9},
  pages = {382--386},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01958-7},
  urldate = {2022-06-14},
  abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
  langid = {english},
  keywords = {Language interpretation,Representation,Semantic anomalies,Text-change-blindness,Underspecification}
}

@misc{sanh.v:2019,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019},
  eprint = {1910.01108},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@article{sanz-alonso.d:2018,
  title = {Importance Sampling and Necessary Sample Size: An Information Theory Approach},
  shorttitle = {Importance Sampling and Necessary Sample Size},
  author = {{Sanz-Alonso}, Daniel},
  year = {2018},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {6},
  number = {2},
  pages = {867--879},
  issn = {2166-2525},
  doi = {10.1137/16M1093549},
  urldate = {2022-12-21},
  langid = {english},
  keywords = {importance sampling},
  file = {~/Zotfiles/sanz-alonso.d2018 Importance sampling and necessary sample.pdf}
}

@article{sanz-alonso.d:2021,
  title = {Bayesian Update with Importance Sampling: Required Sample Size},
  shorttitle = {Bayesian Update with Importance Sampling},
  author = {{Sanz-Alonso}, Daniel and Wang, Zijian},
  year = {2021},
  month = jan,
  journal = {Entropy},
  volume = {23},
  number = {1},
  pages = {22},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23010022},
  urldate = {2024-10-29},
  abstract = {Importance sampling is used to approximate Bayes' rule in many computational approaches to Bayesian inverse problems, data assimilation and machine learning. This paper reviews and further investigates the required sample size for importance sampling in terms of the {$\chi$}2-divergence between target and proposal. We illustrate through examples the roles that dimension, noise-level and other model parameters play in approximating the Bayesian update with importance sampling. Our examples also facilitate a new direct comparison of standard and optimal proposals for particle filtering.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian update,importance sampling,particle filtering,sample size,singular limits},
  file = {~/Zotfiles/sanz-alonso.d2021 Bayesian Update with Importance Sampling.pdf}
}

@inproceedings{saphra.n:2019,
  title = {Understanding Learning Dynamics of Language Models with {{SVCCA}}},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Saphra, Naomi and Lopez, Adam},
  year = {2019},
  pages = {3257--3267},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1329},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1329}
}

@article{sartran.l:2022,
  title = {Transformer {{Grammars}}: {{Augmenting Transformer Language Models}} with {{Syntactic Inductive Biases}} at {{Scale}}},
  shorttitle = {Transformer {{Grammars}}},
  author = {Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v s} and Blunsom, Phil and Dyer, Chris},
  year = {2022},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {1423--1439},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00526},
  urldate = {2023-08-18},
  abstract = {We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism---one that is independent of composed syntactic representations---plays an important role in current successful models of long text.},
  file = {~/Zotfiles/sartran.l2022 Transformer Grammars Augmenting Transfo.pdf}
}

@article{sason.i:2016,
  title = {F-Divergence Inequalities},
  author = {Sason, Igal and Verd{\'u}, Sergio},
  year = {2016},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {11},
  pages = {5973--6006},
  issn = {1557-9654},
  doi = {10.1109/TIT.2016.2603151},
  urldate = {2024-07-29},
  abstract = {This paper develops systematic approaches to obtain f -divergence inequalities, dealing with pairs of probability measures defined on arbitrary alphabets. Functional domination is one such approach, where special emphasis is placed on finding the best possible constant upper bounding a ratio of f -divergences. Another approach used for the derivation of bounds among f -divergences relies on moment inequalities and the logarithmic-convexity property, which results in tight bounds on the relative entropy and Bhattacharyya distance in terms of {$\chi$}2 divergences. A rich variety of bounds are shown to hold under boundedness assumptions on the relative information. Special attention is devoted to the total variation distance and its relation to the relative information and relative entropy, including ``reverse Pinsker inequalities,'' as well as on the E{$\gamma$} divergence, which generalizes the total variation distance. Pinsker's inequality is extended for this type of f -divergence, a result which leads to an inequality linking the relative entropy and relative information spectrum. Integral expressions of the R{\'e}nyi divergence in terms of the relative information spectrum are derived, leading to bounds on the R{\'e}nyi divergence in terms of either the variational distance or relative entropy.},
  keywords = {Conferences,Entropy,f-divergence,Information theory,Integral equations,Joining processes,Pinsker's inequality,Q measurement,Relative entropy,relative information,Renyi divergence,Systematics,total variation distance},
  file = {~/Zotfiles/sason.i2016 f -Divergence Inequalities.pdf}
}

@book{savage.l:1972book2,
  title = {The Foundations of Statistics},
  author = {Savage, Leonard J.},
  year = {1972},
  edition = {2},
  publisher = {Dover Publications},
  address = {New York},
  isbn = {978-0-486-62349-8},
  langid = {english},
  keywords = {Statistics}
}

@article{savin.h:1963,
  title = {Word-frequency Effect and Errors in the Perception of Speech},
  author = {Savin, Harris B.},
  year = {1963},
  journal = {The Journal of the Acoustical Society of America},
  volume = {35},
  number = {2},
  pages = {200--206},
  publisher = {Acoustical Society of America},
  isbn = {0001-4966}
}

@inproceedings{savinov.n:2022,
  title = {Step-Unrolled Denoising Autoencoders for Text Generation},
  booktitle = {International Conference on Learning Representations},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and {van den Oord}, Aaron},
  year = {2022},
  date-added = {2022-05-04 10:51:51 -0400},
  date-modified = {2022-05-04 10:52:30 -0400},
  keywords = {autoencoders,denoising,diffusion processes,language modeling,SUNDAE}
}

@incollection{scha.r:1990,
  title = {Taaltheorie En Taaltechnologie; Competence En Performance.},
  booktitle = {Computertoepassingen in de Neerlandistiek},
  author = {Scha, Remko},
  editor = {{de Kort}, R. and Leerdam, G.L.J.},
  year = {1990},
  pages = {7--22},
  publisher = {Landelijke Vereniging van Neerlandici},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  pass = {1},
  readinglist = {Thesis}
}

@phdthesis{schabes.y:1990,
  title = {Mathematical and Computational Aspects of Lexicalized Grammars},
  author = {Schabes, Yves},
  year = {1990},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2022-04-26 21:21:04 -0400},
  project = {syntactic embedding},
  school = {University of Pennsylvania}
}

@misc{schaeffer.r:2025arxiv,
  title = {How {{Do Large Language Monkeys Get Their Power}} ({{Laws}})?},
  author = {Schaeffer, Rylan and Kazdan, Joshua and Hughes, John and Juravsky, Jordan and Price, Sara and Lynch, Aengus and Jones, Erik and Kirk, Robert and Mirhoseini, Azalia and Koyejo, Sanmi},
  year = {2025},
  month = feb,
  number = {arXiv:2502.17578},
  eprint = {2502.17578},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2502.17578},
  urldate = {2025-04-12},
  abstract = {Recent research across mathematical problem solving, proof assistant programming and multimodal jailbreaking documents a striking finding: when (multimodal) language model tackle a suite of tasks with multiple attempts per task -- succeeding if any attempt is correct -- then the negative log of the average success rate scales a power law in the number of attempts. In this work, we identify an apparent puzzle: a simple mathematical calculation predicts that on each problem, the failure rate should fall exponentially with the number of attempts. We confirm this prediction empirically, raising a question: from where does aggregate polynomial scaling emerge? We then answer this question by demonstrating per-problem exponential scaling can be made consistent with aggregate polynomial scaling if the distribution of single-attempt success probabilities is heavy tailed such that a small fraction of tasks with extremely low success probabilities collectively warp the aggregate success trend into a power law - even as each problem scales exponentially on its own. We further demonstrate that this distributional perspective explains previously observed deviations from power law scaling, and provides a simple method for forecasting the power law exponent with an order of magnitude lower relative error, or equivalently, \$\{{\textbackslash}sim\}2-4\$ orders of magnitude less inference compute. Overall, our work contributes to a better understanding of how neural language model performance improves with scaling inference compute and the development of scaling-predictable evaluations of (multimodal) language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {~/Zotfiles/schaeffer.r2025arxiv How Do Large Language Monkeys Get Their.pdf}
}

@misc{schick-poland.k:2021,
  title = {A Partial Information Decomposition for Discrete and Continuous Variables},
  author = {{Schick-Poland}, Kyle and Makkeh, Abdullah and Gutknecht, Aaron J. and Wollstadt, Patricia and Sturm, Anja and Wibral, Michael},
  year = {2021},
  eprint = {2106.12393},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-30 17:30:27 -0400},
  date-modified = {2021-09-30 17:30:29 -0400}
}

@inproceedings{schijndel.m:2013,
  title = {An Analysis of Frequency- and Memory-Based Processing Costs},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2013},
  month = jun,
  pages = {95--105},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, Georgia},
  urldate = {2023-03-01},
  file = {~/Zotfiles/schijndel.m2013NAACL An analysis of frequency- and memory-bas.pdf}
}

@article{schijndel.m:2013topics,
  title = {A Model of Language Processing as Hierarchic Sequential Prediction},
  author = {{van Schijndel}, Marten and Exley, Andy and Schuler, William},
  year = {2013},
  month = jun,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {522--540},
  publisher = {Wiley},
  doi = {10.1111/tops.12034},
  bdsk-url-2 = {https://doi.org/10.1111/tops.12034},
  date-added = {2021-09-13 21:29:14 -0400},
  date-modified = {2021-09-13 21:29:17 -0400}
}

@inproceedings{schijndel.m:2015,
  title = {Hierarchic Syntax Improves Reading Time Prediction},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2015},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/v1/n15-1183},
  bdsk-url-2 = {https://doi.org/10.3115/v1/n15-1183},
  date-added = {2021-09-13 21:36:26 -0400},
  date-modified = {2021-09-13 21:36:30 -0400}
}

@inproceedings{schijndel.m:2017cogsci,
  title = {Approximations of Predictive Entropy Correlate with Reading Times},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Cognitive Science Society}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  editor = {Gunzelmann, Glenn and Andrew Howes, Thora Tenbrink and Davelaar, Eddy},
  year = {2017},
  pages = {1266--1271},
  address = {London, United Kingdom},
  date-added = {2022-04-21 09:42:08 -0400},
  date-modified = {2022-04-21 09:44:53 -0400},
  organization = {Cognitive Science Society},
  keywords = {entropy reduction,predictability,predictive entropy}
}

@inproceedings{schijndel.m:2018,
  title = {A Neural Model of Adaptation in Reading},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2018},
  pages = {4704--4710},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1499},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1499}
}

@inproceedings{schijndel.m:2018cogsci,
  title = {Modeling Garden Path Effects without Explicit Hierarchical Syntax.},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Cognitive Science Society}}},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  editor = {{Rogers} and {Rau} and {Zhu} and {Kalish}},
  year = {2018},
  address = {Madison, Wisconsin},
  date-added = {2021-03-18 10:48:52 -0400},
  date-modified = {2021-03-18 11:56:18 -0400},
  isbn = {978-0-9911967-8-4}
}

@inproceedings{schijndel.m:2019,
  title = {Can Entropy Explain Successor Surprisal Effects in Reading?},
  booktitle = {Proceedings of the Society for Computation in Linguistics ({{SCiL}}), {{NYC}}, January 3--6, 2019},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2019},
  volume = {2},
  pages = {1--7},
  publisher = {University of Massachusetts Amherst},
  doi = {10.7275/QTBB-9D05},
  bdsk-url-2 = {https://doi.org/10.7275/QTBB-9D05},
  date-added = {2022-04-21 09:21:31 -0400},
  date-modified = {2022-04-21 09:39:00 -0400}
}

@misc{schijndel.m:2020,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2020},
  doi = {10.31234/osf.io/sgbqy},
  bdsk-url-1 = {psyarxiv.com/sgbqy},
  date-added = {2021-03-10 11:20:44 -0500},
  date-modified = {2021-03-18 17:20:22 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing}
}

@article{schijndel.m:2021,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12988},
  issn = {1551-6709},
  doi = {10.1111/cogs.12988},
  urldate = {2022-10-11},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  keywords = {Garden paths,Information theory,Neural networks,Self-paced reading,Surprisal},
  file = {~/Zotfiles/schijndel.m2021 Single-stage prediction models do not ex.pdf}
}

@article{schooler.l:1997,
  title = {The Role of Process in the Rational Analysis of Memory},
  author = {Schooler, Lael J. and Anderson, John R.},
  year = {1997},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {32},
  number = {3},
  pages = {219--250},
  issn = {00100285},
  doi = {10.1006/cogp.1997.0652},
  urldate = {2022-09-27},
  langid = {english},
  file = {~/Zotfiles/schooler.l1997 The role of process in the rational anal.pdf}
}

@article{schotter.e:2014,
  title = {Task Effects Reveal Cognitive Flexibility Responding to Frequency and Predictability: {{Evidence}} from Eye Movements in Reading and Proofreading},
  shorttitle = {Task Effects Reveal Cognitive Flexibility Responding to Frequency and Predictability},
  author = {Schotter, Elizabeth R. and Bicknell, Klinton and Howard, Ian and Levy, Roger and Rayner, Keith},
  year = {2014},
  month = apr,
  journal = {Cognition},
  volume = {131},
  number = {1},
  pages = {1--27},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.11.018},
  urldate = {2023-12-01},
  abstract = {It is well-known that word frequency and predictability affect processing time. These effects change magnitude across tasks, but studies testing this use tasks with different response types (e.g., lexical decision, naming, and fixation time during reading; Schilling, Rayner, \& Chumbley, 1998), preventing direct comparison. Recently, Kaakinen and Hy{\"o}n{\"a} (2010) overcame this problem, comparing fixation times in reading for comprehension and proofreading, showing that the frequency effect was larger in proofreading than in reading. This result could be explained by readers exhibiting substantial cognitive flexibility, and qualitatively changing how they process words in the proofreading task in a way that magnifies effects of word frequency. Alternatively, readers may not change word processing so dramatically, and instead may perform more careful identification generally, increasing the magnitude of many word processing effects (e.g., both frequency and predictability). We tested these possibilities with two experiments: subjects read for comprehension and then proofread for spelling errors (letter transpositions) that produce nonwords (e.g., trcak for track as in Kaakinen \& Hy{\"o}n{\"a}) or that produce real but unintended words (e.g., trial for trail) to compare how the task changes these effects. Replicating Kaakinen and Hy{\"o}n{\"a}, frequency effects increased during proofreading. However, predictability effects only increased when integration with the sentence context was necessary to detect errors (i.e., when spelling errors produced words that were inappropriate in the sentence; trial for trail). The results suggest that readers adopt sophisticated word processing strategies to accommodate task demands.},
  keywords = {Eye movements,Frequency,Predictability,Reading,Task effects,typos},
  file = {~/Zotfiles/schotter.e2014 Task effects reveal cognitive flexibilit.pdf}
}

@article{schrimpf.m:2021PNAS,
  title = {The Neural Architecture of Language: {{Integrative}} Modeling Converges on Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {45},
  pages = {e2105646118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2105646118},
  urldate = {2023-05-02},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  file = {~/Zotfiles/schrimpf.m2021 The neural architecture of language Int.pdf}
}

@book{schuchardt.h:1972book,
  title = {Schuchardt, the {{Neogrammarians}}, and the Transformational Theory of Phonological Change: Four Essays},
  shorttitle = {Schuchardt, the Neogrammarians, and the Transformational Theory of Phonological Change},
  author = {Schuchardt, Hugo Ernst Mario},
  translator = {Vennemann, Theo and Wilbur, Terence H.},
  year = {1972},
  series = {Linguistische {{Forschungen}}, {{Bd}}. 26},
  publisher = {Athen{\"a}um Verlag},
  address = {Frankfurt/M.},
  isbn = {978-3-7610-4826-9},
  lccn = {P123 .S36 1972},
  keywords = {Linguistic change}
}

@inproceedings{schuler.w:2008,
  title = {Toward a Psycholinguistically-Motivated Model of Language Processing},
  booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  author = {Schuler, William and AbdelRahman, Samir and Miller, Tim and Schwartz, Lane},
  year = {2008},
  month = aug,
  pages = {785--792},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK},
  date-added = {2022-05-02 11:58:18 -0400},
  date-modified = {2022-05-02 11:58:33 -0400},
  keywords = {incrementality,parsing,parsing algorithm}
}

@inproceedings{schulman.j:2015,
  title = {Trust {{Region Policy Optimization}}},
  booktitle = {Proceedings of the 32nd {{International Conference}} on {{Machine Learning}}},
  author = {Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  year = {2015},
  month = jun,
  pages = {1889--1897},
  publisher = {PMLR},
  issn = {1938-7228},
  urldate = {2025-09-11},
  abstract = {In this article, we describe a method for optimizing control policies, with guaranteed monotonic improvement. By making several approximations to the theoretically-justified scheme, we develop a practical algorithm, called Trust Region Policy Optimization (TRPO). This algorithm is effective for optimizing large nonlinear policies such as neural networks. Our experiments demonstrate its robust performance on a wide variety of tasks: learning simulated robotic swimming, hopping, and walking gaits; and playing Atari games using images of the screen as input. Despite its approximations that deviate from the theory, TRPO tends to give monotonic improvement, with little tuning of hyperparameters.},
  langid = {english},
  file = {/Users/v/Zotfiles/schulman.j2015 Trust Region Policy Optimization.pdf}
}

@misc{schulman.j:2017arxiv,
  title = {Proximal {{Policy Optimization Algorithms}}},
  author = {Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  year = {2017},
  month = aug,
  number = {arXiv:1707.06347},
  eprint = {1707.06347},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1707.06347},
  urldate = {2025-09-11},
  abstract = {We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a "surrogate" objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/v/Zotfiles/schulman.j2017arxiv Proximal Policy Optimization Algorithms.pdf;/Users/v/Zotero/storage/QVADTMYV/1707.html}
}

@article{schutze.c:2003,
  title = {Syncretism and Double Agreement with {{Icelandic}} Nominative Objects},
  author = {Sch{\"u}tze, Carson T},
  year = {2003},
  bdsk-url-1 = {escholarship.org/uc/item/8vn9b04q},
  date-added = {2020-03-05 10:17:01 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {syncretism}
}

@article{schwartz.m:2008,
  title = {The Importance of Stupidity in Scientific Research},
  author = {Schwartz, Martin A.},
  year = {2008},
  month = jun,
  journal = {Journal of Cell Science},
  volume = {121},
  number = {11},
  pages = {1771},
  issn = {0021-9533},
  doi = {10.1242/jcs.033340},
  urldate = {2022-06-16},
  keywords = {scientific method,stupidity},
  file = {~/Zotfiles/schwartz.m2008 The importance of stupidity in scientifi.pdf}
}

@misc{schwartz.r:2019,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  eprint = {1907.10597},
  primaryclass = {cs.CY},
  archiveprefix = {arXiv},
  date-added = {2019-09-30 11:48:17 -0400},
  date-modified = {2019-09-30 11:50:22 -0400},
  keywords = {energy,environmental impact}
}

@inproceedings{scibior.a:2015,
  title = {Practical Probabilistic Programming with Monads},
  booktitle = {Proceedings of the 2015 {{ACM SIGPLAN Symposium}} on {{Haskell}}},
  author = {{\'S}cibior, Adam and Ghahramani, Zoubin and Gordon, Andrew D.},
  year = {2015},
  month = aug,
  series = {Haskell '15},
  pages = {165--176},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2804302.2804317},
  urldate = {2024-12-05},
  abstract = {The machine learning community has recently shown a lot of interest in practical probabilistic programming systems that target the problem of Bayesian inference. Such systems come in different forms, but they all express probabilistic models as computational processes using syntax resembling programming languages. In the functional programming community monads are known to offer a convenient and elegant abstraction for programming with probability distributions, but their use is often limited to very simple inference problems. We show that it is possible to use the monad abstraction to construct probabilistic models for machine learning, while still offering good performance of inference in challenging models. We use a GADT as an underlying representation of a probability distribution and apply Sequential Monte Carlo-based methods to achieve efficient inference. We define a formal semantics via measure theory. We demonstrate a clean and elegant implementation that achieves performance comparable with Anglican, a state-of-the-art probabilistic programming system.},
  isbn = {978-1-4503-3808-0},
  file = {~/Zotfiles/scibior.a2015 Practical probabilistic programming with.pdf}
}

@article{searle.s:1980,
  title = {Population Marginal Means in the Linear Model: An Alternative to Least Squares Means},
  shorttitle = {Population Marginal Means in the Linear Model},
  author = {Searle, S. R. and Speed, F. M. and Milliken, G. A.},
  year = {1980},
  month = nov,
  journal = {The American Statistician},
  volume = {34},
  number = {4},
  pages = {216--221},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.1980.10483031},
  urldate = {2024-05-28},
  abstract = {The parameter concept in the term least squares mean is defined and given the more meaningful name population marginal mean; and its estimation is discussed.},
  keywords = {Covariance,Empty cells,Estimable function,Estimated marginal mean,Least squares mean,Population marginal mean,Unequal subclass numbers},
  file = {~/Zotfiles/searle.s1980 Population marginal means in the linear.pdf}
}

@article{sebastiani.p:2000,
  title = {Maximum Entropy Sampling and Optimal {{Bayesian}} Experimental Design},
  author = {Sebastiani, Paola and Wynn, Henry P.},
  year = {2000},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {62},
  number = {1},
  eprint = {2680683},
  eprinttype = {jstor},
  pages = {145--157},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {13697412, 14679868},
  abstract = {When Shannon entropy is used as a criterion in the optimal design of experiments, advantage can be taken of the classical identity representing the joint entropy of parameters and observations as the sum of the marginal entropy of the observations and the preposterior conditional entropy of the parameters. Following previous work in which this idea was used in spatial sampling, the method is applied to standard parameterized Bayesian optimal experimental design. Under suitable conditions, which include non-linear as well as linear regression models, it is shown in a few steps that maximizing the marginal entropy of the sample is equivalent to minimizing the pre-posterior entropy, the usual Bayesian criterion, thus avoiding the use of conditional distributions. It is shown using this marginal formulation that under normality assumptions every standard model which has a two-point prior distribution on the parameters gives an optimal design supported on a single point. Other results include a new asymptotic formula which applies as the error variance is large and bounds on support size.},
  date-added = {2021-09-15 10:25:32 -0400},
  date-modified = {2021-09-15 10:25:34 -0400}
}

@article{seidenberg.m:1999,
  title = {Do Infants Learn Grammar with Algebra or Statistics?},
  author = {Seidenberg, Mark S. and Elman, Jeff L.},
  year = {1999},
  month = apr,
  journal = {Science},
  volume = {284},
  number = {5413},
  pages = {433--433},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.284.5413.433f},
  urldate = {2024-05-26},
  langid = {english}
}

@inproceedings{sennrich.r:2016,
  title = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = aug,
  pages = {1715--1725},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/P16-1162},
  urldate = {2023-03-03},
  keywords = {BPE,byte-pair encoding,machine translation,neural machine translation,NMT,subword units},
  file = {~/Zotfiles/sennrich.r2016 Neural machine translation of rare words.pdf}
}

@incollection{shachter.r:1990,
  title = {Simulation {{Approaches}} to {{General Probabilistic Inference}} on {{Belief Networks}}},
  booktitle = {Uncertainty in {{Artificial Intelligence}}},
  author = {Shachter, Ross D. and Peot, Mark A.},
  editor = {Henrion, Max and Shachter, Ross D. and Kanal, Laveen N. and Lemmer, John F.},
  year = {1990},
  month = jan,
  series = {Machine {{Intelligence}} and {{Pattern Recognition}}},
  volume = {10},
  pages = {221--231},
  publisher = {North-Holland},
  doi = {10.1016/B978-0-444-88738-2.50024-5},
  urldate = {2025-02-19},
  abstract = {A number of algorithms have been developed to solve probabilistic inference problems on belief networks. These algorithms can be divided into two main groups: exact techniques which exploit the conditional independence revealed when the graph structure is relatively sparse, and probabilistic sampling techniques which exploit the ``conductance'' of an embedded Markov chain when the conditional probabilities have non-extreme values. In this paper, we investigate a family of ``forward'' Monte Carlo sampling techniques similar to Logic Sampling [Henrion, 1988] which appear to perform well even in some multiplyconnected networks with extreme conditional probabilities, and thus would be generally applicable. We consider several enhancements which reduce the posterior variance using this approach and propose a framework and criteria for choosing when to use those enhancements.},
  keywords = {likelihood weighting}
}

@inproceedings{shain.c:2018,
  title = {Deep Syntactic Annotations for Broad-Coverage Psycholinguistic Modeling},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Shain, Cory and {van Schijndel}, Marten},
  editor = {Devereux, Barry and Shutova, Ekaterina and Huang, Chu-Ren},
  year = {2018},
  pages = {33--37},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {979-10-95546-08-5},
  langid = {english}
}

@inproceedings{shain.c:2018a,
  title = {Deconvolutional Time Series Regression: {{A}} Technique for Modeling Temporally Diffuse Effects},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Shain, Cory and Schuler, William},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/d18-1288},
  bdsk-url-2 = {https://doi.org/10.18653/v1/d18-1288},
  date-added = {2021-09-18 22:25:03 -0400},
  date-modified = {2021-09-18 22:25:04 -0400},
  file = {~/Zotfiles/shain.c2018CDR Deconvolutional time series regression.pdf}
}

@inproceedings{shain.c:2019,
  title = {A Large-Scale Study of the Effects of Word Frequency and Predictability in Naturalistic Reading},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Shain, Cory},
  editor = {Burstein, Jill and Doran, Christy and Solorio, Thamar},
  year = {2019},
  month = jun,
  pages = {4086--4094},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1413},
  urldate = {2025-03-25},
  abstract = {A number of psycholinguistic studies have factorially manipulated words' contextual predictabilities and corpus frequencies and shown separable effects of each on measures of human sentence processing, a pattern which has been used to support distinct mechanisms underlying prediction on the one hand and lexical retrieval on the other. This paper examines the generalizability of this finding to more realistic conditions of sentence processing by studying effects of frequency and predictability in three large-scale naturalistic reading corpora. Results show significant effects of word frequency and predictability in isolation but no effect of frequency over and above predictability, and thus do not provide evidence of distinct mechanisms. The non-replication of separable effects in a naturalistic setting raises doubts about the existence of such a distinction in everyday sentence comprehension. Instead, these results are consistent with previous claims that apparent effects of frequency are underlyingly effects of predictability.}
}

@article{shain.c:2020,
  title = {{{fMRI}} Reveals Language-Specific Predictive Coding during Naturalistic Sentence Comprehension},
  author = {Shain, Cory and Blank, Idan Asher and {van Schijndel}, Marten and Schuler, William and Fedorenko, Evelina},
  year = {2020},
  month = feb,
  journal = {Neuropsychologia},
  volume = {138},
  pages = {107307},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2019.107307},
  urldate = {2025-03-03},
  abstract = {Much research in cognitive neuroscience supports prediction as a canonical computation of cognition across domains. Is such predictive coding implemented by feedback from higher-order domain-general circuits, or is it locally implemented in domain-specific circuits? What information sources are used to generate these predictions? This study addresses these two questions in the context of language processing. We present fMRI evidence from a naturalistic comprehension paradigm (1) that predictive coding in the brain's response to language is domain-specific, and (2) that these predictions are sensitive both to local word co-occurrence patterns and to hierarchical structure. Using a recently developed continuous-time deconvolutional regression technique that supports data-driven hemodynamic response function discovery from continuous BOLD signal fluctuations in response to naturalistic stimuli, we found effects of prediction measures in the language network but not in the domain-general multiple-demand network, which supports executive control processes and has been previously implicated in language comprehension. Moreover, within the language network, surface-level and structural prediction effects were separable. The predictability effects in the language network were substantial, with the model capturing over 37\% of explainable variance on held-out data. These findings indicate that human sentence processing mechanisms generate predictions about upcoming words using cognitive processes that are sensitive to hierarchical structure and specialized for language processing, rather than via feedback from high-level executive control mechanisms.},
  keywords = {fMRI,Language,Multiple demand network,Naturalistic,Predictive coding,Sentence processing,Surprisal,Syntactic structure},
  file = {~/Zotfiles/shain.c2020 fMRI reveals language-specific predictiv.pdf}
}

@article{shain.c:2021,
  title = {Continuous-Time Deconvolutional Regression for Psycholinguistic Modeling},
  author = {Shain, Cory and Schuler, William},
  year = {2021},
  month = oct,
  journal = {Cognition},
  volume = {215},
  pages = {104735},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104735},
  urldate = {2022-10-26},
  abstract = {The influence of stimuli in psycholinguistic experiments diffuses across time because the human response to language is not instantaneous. The linear models typically used to analyze psycholinguistic data are unable to account for this phenomenon due to strong temporal independence assumptions, while existing deconvolutional methods for estimating diffuse temporal structure model time discretely and therefore cannot be directly applied to natural language stimuli where events (words) have variable duration. In light of evidence that continuous-time deconvolutional regression (CDR) can address these issues (Shain \& Schuler, 2018), this article motivates the use of CDR for many experimental settings, exposits some of its mathematical properties, and empirically evaluates the influence of various experimental confounds (noise, multicollinearity, and impulse response misspecification), hyperparameter settings, and response types (behavioral and fMRI). Results show that CDR (1) yields highly consistent estimates across a variety of hyperparameter configurations, (2) faithfully recovers the data-generating model on synthetic data, even under adverse training conditions, and (3) outperforms widely-used statistical approaches when applied to naturalistic reading and fMRI data. In addition, procedures for testing scientific hypotheses using CDR are defined and demonstrated, and empirically-motivated best-practices for CDR modeling are proposed. Results support the use of CDR for analyzing psycholinguistic time series, especially in a naturalistic experimental paradigm.},
  langid = {english}
}

@inproceedings{shain.c:2021a,
  title = {{{CDRNN}}: Discovering Complex Dynamics in Human Language Processing},
  shorttitle = {Cdrnn},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Shain, Cory},
  year = {2021},
  month = aug,
  pages = {3718--3734},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.288},
  urldate = {2022-10-26},
  abstract = {The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain \& Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.},
  file = {~/Zotfiles/shain.c2021CDRNN CDRNN discovering complex dynamics in h.pdf}
}

@misc{shain.c:2022arxiv,
  title = {A Deep Learning Approach to Analyzing Continuous-Time Systems},
  author = {Shain, Cory and Schuler, William},
  year = {2022},
  month = sep,
  number = {arXiv:2209.12128},
  eprint = {2209.12128},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.12128},
  urldate = {2022-10-26},
  abstract = {Scientists often use observational time series data to study complex natural processes, from climate change to civil conflict to brain activity. But regression analyses of these data often assume simplistic dynamics. Recent advances in deep learning have yielded startling improvements to the performance of models of complex processes, from speech comprehension to nuclear physics to competitive gaming. But deep learning is generally not used for scientific analysis. Here, we bridge this gap by showing that deep learning can be used, not just to imitate, but to analyze complex processes, providing flexible function approximation while preserving interpretability. Our approach -- the continuous-time deconvolutional regressive neural network (CDRNN) -- relaxes standard simplifying assumptions (e.g., linearity, stationarity, and homoscedasticity) that are implausible for many natural systems and may critically affect the interpretation of data. We evaluate CDRNNs on incremental human language processing, a domain with complex continuous dynamics. We demonstrate dramatic improvements to predictive likelihood in behavioral and neuroimaging data, and we show that CDRNNs enable flexible discovery of novel patterns in exploratory analyses, provide robust control of possible confounds in confirmatory analyses, and open up research questions that are otherwise hard to study using observational data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,Statistics - Methodology}
}

@unpublished{shain.c:2022ms,
  type = {Draft},
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability in Reading},
  author = {Shain, Cory and Meister, Clara and Pimental, Tiago and Levy, Roger P. and Cotterell, Ryan},
  year = {2022},
  copyright = {Confidential},
  file = {~/Zotfiles/shain.c2022draft Large-scale evidence for logarithmic eff.pdf}
}

@misc{shain.c:2022psyarxiv,
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time},
  author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger Philip},
  year = {2022},
  month = nov,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/4hyna},
  urldate = {2023-01-26},
  abstract = {Words which are less predictable in context are harder to process, but theories of language processing diverge in how they explain this fact. Representational theories emphasize the demands of assembling sentence interpretations in memory; these theories predict a linear effect of predictability on processing cost. Inferential theories emphasize the demands of updating a probability distribution over possible sentence interpretations; these theories predict either a logarithmic or a superlogarithmic effect of predictability on processing cost, depending on whether they posit pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here we revisit this question at scale: we analyze six reading datasets, estimate next-word probabilities with diverse language models, and predict reading times with advanced nonlinear regression methods. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.},
  langid = {american},
  keywords = {Cognitive Psychology,Language,Naturalistic,Nonlinear regression,Prediction,Psycholinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {~/Zotfiles/shain.c2022preprint Large-scale evidence for logarithmic eff.pdf}
}

@misc{shain.c:2023psyarxiv,
  title = {Word {{Frequency}} and {{Predictability Dissociate}} in {{Naturalistic Reading}}},
  author = {Shain, Cory},
  year = {2023},
  month = jul,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/9zdfw},
  urldate = {2024-03-01},
  abstract = {Many studies of human language processing have shown that readers slow down at less frequent or less predictable words, but there is debate about whether frequency and predictability effects reflect separable cognitive phenomena: are cognitive operations that retrieve words from the mental lexicon based on sensory cues distinct from those that predict upcoming words based on context? Previous evidence for a frequency-predictability dissociation is mostly based on small samples (both for estimating predictability and frequency and for testing their effects on human behavior), artificial materials (e.g., isolated constructed sentences), and implausible modeling assumptions (discrete-time dynamics, linearity, additivity, constant variance, and invariance over time), which raises the question: do frequency and predictability dissociate in ordinary language comprehension, such as story reading? This study leverages recent progress in open data and computational modeling to address this question at scale. A large collection of naturalistic reading data (six datasets, \&gt;2.2M datapoints) is analyzed using nonlinear continuous-time regression, and frequency and predictability are estimated using statistical language models trained on more data than is currently typical in psycholinguistics. Despite the use of naturalistic data, strong predictability estimates, and flexible regression models, results converge with earlier experimental studies in supporting dissociable and additive frequency and predictability effects.},
  langid = {american},
  annotation = {note: Published in Open Mind 10.1162/opmi\_a\_00119},
  file = {~/Zotfiles/shain.c2023psyarxiv Word Frequency and Predictability Dissoc.pdf}
}

@article{shain.c:2024,
  title = {Word Frequency and Predictability Dissociate in Naturalistic Reading},
  author = {Shain, Cory},
  year = {2024},
  month = mar,
  journal = {Open Mind},
  volume = {8},
  pages = {177--201},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00119},
  urldate = {2024-10-21},
  abstract = {Many studies of human language processing have shown that readers slow down at less frequent or less predictable words, but there is debate about whether frequency and predictability effects reflect separable cognitive phenomena: are cognitive operations that retrieve words from the mental lexicon based on sensory cues distinct from those that predict upcoming words based on context? Previous evidence for a frequency-predictability dissociation is mostly based on small samples (both for estimating predictability and frequency and for testing their effects on human behavior), artificial materials (e.g., isolated constructed sentences), and implausible modeling assumptions (discrete-time dynamics, linearity, additivity, constant variance, and invariance over time), which raises the question: do frequency and predictability dissociate in ordinary language comprehension, such as story reading? This study leverages recent progress in open data and computational modeling to address this question at scale. A large collection of naturalistic reading data (six datasets, \&gt;2.2 M datapoints) is analyzed using nonlinear continuous-time regression, and frequency and predictability are estimated using statistical language models trained on more data than is currently typical in psycholinguistics. Despite the use of naturalistic data, strong predictability estimates, and flexible regression models, results converge with earlier experimental studies in supporting dissociable and additive frequency and predictability effects.},
  file = {~/Zotfiles/shain.c2024 Word Frequency and Predictability Dissoc.pdf}
}

@article{shain.c:2024PNAS,
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time},
  author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger},
  year = {2024},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {10},
  pages = {e2307876121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2307876121},
  urldate = {2024-03-01},
  abstract = {During real-time language comprehension, our minds rapidly decode complex meanings from sequences of words. The difficulty of doing so is known to be related to words' contextual predictability, but what cognitive processes do these predictability effects reflect? In one view, predictability effects reflect facilitation due to anticipatory processing of words that are predictable from context. This view predicts a linear effect of predictability on processing demand. In another view, predictability effects reflect the costs of probabilistic inference over sentence interpretations. This view predicts either a logarithmic or a superlogarithmic effect of predictability on processing demand, depending on whether it assumes pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here, we revisit this question at scale: We analyze six reading datasets, estimate next-word probabilities with diverse statistical language models, and model reading times using recent advances in nonlinear regression. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.},
  file = {/Users/j/MIT Dropbox/Jacob Vigly/Zotfiles/shain.c2024PNAS SI.pdf;~/Zotfiles/shain.c2024PNAS Large-scale evidence for logarithmic eff.pdf}
}

@article{shannon.c:1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  journal = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {Nokia Bell Labs},
  project = {information-entropy},
  keywords = {asymptotic equipartition property,information theory}
}

@article{shannon.c:1948a,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  month = jun,
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  bdsk-url-2 = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  date-added = {2022-04-07 13:03:48 -0400},
  date-modified = {2022-05-04 18:57:35 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{shannon.c:1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, Claude E.},
  year = {1951},
  journal = {Bell System Technical Journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  urldate = {2024-05-26},
  abstract = {A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed.},
  copyright = {{\copyright} 1951 The Bell System Technical Journal},
  langid = {english},
  file = {~/Zotfiles/shannon.c1951 Prediction and entropy of printed Englis.pdf}
}

@misc{shazeer.n:2020arxiv,
  title = {{{GLU}} Variants Improve {{Transformer}}},
  author = {Shazeer, Noam},
  year = {2020},
  month = feb,
  number = {arXiv:2002.05202},
  eprint = {2002.05202},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-27},
  abstract = {Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {~/Zotfiles/shazeer.n2020 GLU variants improve Transformer.pdf}
}

@techreport{sheldon.d:2013,
  type = {Unpublished Report},
  title = {Discrete Adaptive Rejection Sampling},
  author = {Sheldon, Daniel R.},
  year = {2013},
  number = {UM-CS-2013-012},
  institution = {{College of Information and Computer Sciences, University of Massachusetts Amherst}},
  abstract = {Adaptive rejection sampling (ARS) is an algorithm by Gilks and Wild for drawing samples from a continuous log-concave probability distribution with only black-box access to a function that computes the (unnormalized) density function. The ideas extend in a straightforward way to discrete log-concave distributions, but some details of the extension and its implementation can be tricky. This report provides the details of a discrete ARS algorithm. A companion implementation in C, with a MATLAB interface, accompanies the report. 1},
  file = {~/Zotfiles/sheldon.d2013 Discrete adaptive rejection sampling.pdf}
}

@inproceedings{shen.t:2020,
  title = {Blank Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  year = {2020},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2020.emnlp-main.420},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.420},
  date-added = {2022-04-05 19:37:55 -0400},
  date-modified = {2022-04-05 19:38:07 -0400}
}

@inproceedings{shen.y:2018,
  title = {Neural Language Modeling by Jointly Learning Syntax and Lexicon},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron C.},
  year = {2018},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenLHC18.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{shen.y:2019,
  title = {Ordered Neurons: {{Integrating}} Tree Structures into Recurrent Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron C.},
  year = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  file = {~/Zotfiles/shen.y2018 Ordered neurons Integrating tree struct.pdf}
}

@article{shepard.r:1987,
  title = {Toward a Universal Law of Generalization for Psychological Science},
  author = {Shepard, Roger N.},
  year = {1987},
  month = sep,
  journal = {Science},
  volume = {237},
  number = {4820},
  pages = {1317--1323},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.3629243},
  urldate = {2022-10-11},
  file = {~/Zotfiles/shepard.r1987 Toward a universal law of generalization.pdf}
}

@inproceedings{shi.l:2009,
  title = {Neural {{Implementation}} of {{Hierarchical Bayesian Inference}} by {{Importance Sampling}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Shi, Lei and Griffiths, Thomas},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  urldate = {2025-02-20},
  abstract = {The goal of perception is to infer the hidden states in the hierarchical process by which sensory data are generated. Human behavior is consistent with the optimal statistical solution to this problem in many tasks, including cue combination and orientation detection. Understanding the neural mechanisms underlying this behavior is of particular importance, since probabilistic computations are notoriously challenging. Here we propose a simple mechanism for Bayesian inference which involves averaging over a few feature detection neurons which fire at a rate determined by their similarity to a sensory stimulus. This mechanism is based on a Monte Carlo method known as importance sampling, commonly used in computer science and statistics. Moreover, a simple extension to recursive importance sampling can be used to perform hierarchical Bayesian inference. We identify a scheme for implementing importance sampling with spiking neurons, and show that this scheme can account for human behavior in cue combination and oblique effect.},
  keywords = {importance sampling},
  file = {~/Zotfiles/shi.l2009 Neural Implementation of Hierarchical Ba.pdf}
}

@article{shi.l:2010,
  title = {Exemplar Models as a Mechanism for Performing {{Bayesian}} Inference},
  author = {Shi, Lei and Griffiths, Thomas L. and Feldman, Naomi H. and Sanborn, Adam N.},
  year = {2010},
  month = aug,
  journal = {Psychonomic Bulletin \& Review},
  volume = {17},
  number = {4},
  pages = {443--464},
  issn = {1531-5320},
  doi = {10.3758/PBR.17.4.443},
  urldate = {2025-02-17},
  abstract = {Probabilistic models have recently received much attention as accounts of human cognition. However, most research in which probabilistic models have been used has been focused on formulating the abstract problems behind cognitive tasks and their optimal solutions, rather than on mechanisms that could implement these solutions. Exemplar models are a successful class of psychological process models in which an inventory of stored examples is used to solve problems such as identification, categorization, and function learning. We show that exemplar models can be used to perform a sophisticated form of Monte Carlo approximation known as importance sampling and thus provide a way to perform approximate Bayesian inference. Simulations of Bayesian inference in speech perception, generalization along a single dimension, making predictions about everyday events, concept learning, and reconstruction from memory show that exemplar models can often account for human performance with only a few exemplars, for both simple and relatively complex prior distributions. These results suggest that exemplar models provide a possible mechanism for implementing at least some forms of Bayesian inference.},
  langid = {english},
  keywords = {Bayesian Inference,Importance Sampling,Particle Filter,Posterior Distribution,Speech Sound},
  file = {~/Zotfiles/shi.l2010 Exemplar models as a mechanism for perfo.pdf}
}

@book{shields.p:1996book,
  title = {The Ergodic Theory of Discrete Sample Paths},
  author = {Shields, Paul C},
  year = {1996},
  volume = {13},
  publisher = {American Mathematical Soc.},
  date-added = {2020-08-08 19:47:15 -0400},
  date-modified = {2020-08-08 19:49:28 -0400},
  project = {information-compositionality},
  keywords = {entropy,information theory}
}

@article{shmueli.g:2010,
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  urldate = {2022-09-26},
  file = {~/Zotfiles/shmueli.g2010 To explain or to predict.pdf}
}

@misc{shojaee.p:2025,
  title = {The Illusion of Thinking: {{Understanding}} the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity},
  author = {Shojaee, Parshin and Mirzadeh, Iman and Alizadeh, Keivan and Horton, Maxwell and Bengio, Samy and Farajtabar, Mehrdad},
  year = {2025}
}

@misc{shubi.o:2024arxiv,
  title = {Fine-Grained Prediction of Reading Comprehension from Eye Movements},
  author = {Shubi, Omer and Meiri, Yoav and Hadar, Cfir Avraham and Berzak, Yevgeni},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04484},
  eprint = {2410.04484},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04484},
  urldate = {2024-10-22},
  abstract = {Can human reading comprehension be assessed from eye movements in reading? In this work, we address this longstanding question using large-scale eyetracking data over textual materials that are geared towards behavioral analyses of reading comprehension. We focus on a fine-grained and largely unaddressed task of predicting reading comprehension from eye movements at the level of a single question over a passage. We tackle this task using three new multimodal language models, as well as a battery of prior models from the literature. We evaluate the models' ability to generalize to new textual items, new participants, and the combination of both, in two different reading regimes, ordinary reading and information seeking. The evaluations suggest that although the task is highly challenging, eye movements contain useful signals for fine-grained prediction of reading comprehension. Code and data will be made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/shubi.o2024arxiv Fine-Grained Prediction of Reading Compr.pdf}
}

@article{sigurdsson.h:1996,
  title = {Icelandic Finite Verb Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {1996},
  journal = {Working papers in Scandinavian syntax},
  volume = {57},
  pages = {1--46},
  publisher = {Department of Scandinavian Languages},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,syncretism},
  file = {~/Zotfiles/sigurdsson.h1996 Icelandic finite verb agreement.pdf}
}

@incollection{sigurdsson.h:2000,
  title = {The Locus of Case and Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {2000},
  series = {Working Papers in {{Scandinavian}} Syntax},
  volume = {65},
  publisher = {Department of Scandinavian Languages, Lund University},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2021-03-12 11:36:31 -0500},
  howpublished = {Working paper},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@article{sigurdsson.h:2008,
  title = {Icelandic Dative Intervention: {{Person}} and Number Are Separate Probes},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann and Holmberg, Anders},
  year = {2008},
  journal = {Agreement restrictions},
  pages = {251--280},
  publisher = {Mouton de Gruyter Berlin},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:26:40 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,split probe}
}

@article{simon.h:1955,
  title = {A {{Behavioral Model}} of {{Rational Choice}}},
  author = {Simon, Herbert A.},
  year = {1955},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {69},
  number = {1},
  pages = {99--118},
  issn = {0033-5533},
  doi = {10.2307/1884852},
  urldate = {2022-06-13},
  abstract = {Introduction, 99. --- I. Some general features of rational choice, 100.--- II. The essential simplifications, 103. --- III. Existence and uniqueness of solutions, 111. --- IV. Further comments on dynamics, 113. --- V. Conclusion, 114. --- Appendix, 115.},
  file = {~/Zotfiles/simon.h1955 A Behavioral Model of Rational Choice.pdf}
}

@article{simon.h:1956,
  title = {Rational Choice and the Structure of the Environment},
  author = {Simon, H. A.},
  year = {1956},
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {129--138},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/h0042769},
  abstract = {"In this paper I have attempted to identify some of the structural characteristics that are typical of the 'psychological' environments of organisms. We have seen that an organism in an environment with these characteristics requires only very simple perceptual and choice mechanisms to satisfy its several needs and to assure a high probability of its survival over extended periods of time. In particular, no 'utility function' needs to be postulated for the organism, nor does it require any elaborate procedure for calculating marginal rates of substitution among different wants." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Choice Behavior,Decision Making},
  file = {~/Zotfiles/simon.h1956 Rational choice and the structure of the.pdf}
}

@article{simon.h:1990,
  title = {Invariants of Human Behavior},
  author = {Simon, Herbert A.},
  year = {1990},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {41},
  number = {Volume 41, 1990},
  pages = {1--20},
  publisher = {Annual Reviews},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.ps.41.020190.000245},
  urldate = {2024-05-03},
  langid = {english},
  file = {~/Zotfiles/simon.h1990 Invariants of human behavior.pdf}
}

@article{slaats.s:2024,
  title = {What's Surprising about Surprisal},
  author = {Slaats, Sophie and Martin, Andrea E.},
  year = {2024},
  month = feb,
  publisher = {OSF},
  doi = {10.31234/osf.io/7pvau},
  urldate = {2024-02-10},
  abstract = {In the computational and experimental psycholinguistic literature, the mechanisms behind syntactic structure building (e.g., combining words into phrases and sentences) are the subject of considerable debate.  Much experimental work has shown that surprisal is a good predictor of human behavioral and neural data. These findings have led some authors to model language comprehension in a purely probabilistic way. In this paper, we use simulation to exemplify why surprisal works so well to model human data, and to illustrate why exclusive reliance on it can be problematic for the development of a theory of language comprehension in general, and for a theory of composition in particular. Rather than arguing for the importance of structural or distributional information to the exclusion or exhaustion of the other, we argue more emphasis should be placed on how the brain leverages both types of information (viz., statistical and structured). We propose that distributional information is an important cue to the structure in the message, but is not a substitute for the structure itself - neither computationally, formally, nor conceptually. Surprisal and other distributional metrics must play a key role as theoretical objects in any explanatory mechanistic theory of language processing, but that role remains in the service of the brain's goal of constructing structured meaning from sensory input.},
  langid = {american},
  file = {~/Zotfiles/slaats.s2024psyarxiv What's surprising about surprisal.pdf}
}

@misc{smith.d:2023arxiv,
  title = {A Chiral Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = {2023},
  month = may,
  number = {arXiv:2305.17743},
  eprint = {2305.17743},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17743},
  urldate = {2023-06-03},
  abstract = {The recently discovered "hat" aperiodic monotile mixes unreflected and reflected tiles in every tiling it admits, leaving open the question of whether a single shape can tile aperiodically using translations and rotations alone. We show that a close relative of the hat -- the equilateral member of the continuum to which it belongs -- is a weakly chiral aperiodic monotile: it admits only non-periodic tilings if we forbid reflections by fiat. Furthermore, by modifying this polygon's edges we obtain a family of shapes called Spectres that are strictly chiral aperiodic monotiles: they admit only chiral non-periodic tilings based on a hierarchical substitution system.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Discrete Mathematics,F.2.2,G.2.1,Mathematics - Combinatorics,Mathematics - Metric Geometry}
}

@misc{smith.d:2023arxiva,
  title = {An Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = {2023},
  number = {2303.10798 [math.CO]},
  eprint = {2303.10798},
  primaryclass = {math.CO},
  publisher = {arXiv},
  abstract = {A longstanding open problem asks for an aperiodic monotile, also known as an "einstein": a shape that admits tilings of the plane, but never periodic tilings. We answer this problem for topological disk tiles by exhibiting a continuum of combinatorially equivalent aperiodic polygons. We first show that a representative example, the "hat" polykite, can form clusters called "metatiles", for which substitution rules can be defined. Because the metatiles admit tilings of the plane, so too does the hat. We then prove that generic members of our continuum of polygons are aperiodic, through a new kind of geometric incommensurability argument. Separately, we give a combinatorial, computer-assisted proof that the hat must form hierarchical -- and hence aperiodic -- tilings.},
  archiveprefix = {arXiv}
}

@inproceedings{smith.n:2008cogsci,
  title = {Optimal Processing Times in Reading: A Formal Model and Empirical Investigation},
  booktitle = {Proceedings of the 30th Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = jul,
  pages = {570--576},
  address = {Washington, DC, USA},
  date-added = {2021-05-31 12:59:02 -0400},
  date-modified = {2021-12-14 20:34:04 -0500},
  file = {~/Zotfiles/smith.n2008cogsci Optimal processing times in reading a f.pdf}
}

@inproceedings{smith.n:2008csdl,
  title = {Probabilistic Prediction and the Continuity of Language Comprehension},
  booktitle = {9th Conference on Conceptual Structure, Discourse, and Language ({{CSDL9}})},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = oct,
  abstract = {It is well known that humans comprehend language in an incremental fashion, and that while doing so they use diverse contextual clues to make predictions about upcoming words (e.g. Tanenhaus et al., 1995). One way to model such predictions quantitatively is to use conditional probability, with P(continuation{\textbar}context) denoting the fraction of the time that some continuation is expected to occur in a given context (Hale, 2001). For cognitive linguistics, such probabilities have special appeal. Generally, we seek theories that can faithfully describe the messy world of language and thought, and that are precise enough to make sharp, testable predictions; in practice we rarely achieve both goals simultaneously. Conditional probability provides a (partial) framework for representing linguistic knowledge that is subject to precise mathematics, but at the same time - unlike traditional formalisms - is amenable to incremental learning and gradient representations, and generalizes naturally to all levels of linguistic and extra-linguistic structure. Understanding the details of prediction in online language comprehension, therefore, may eventually pay considerable theoretical dividends. To this end, consider one behavioral correlate of predictability: words which are more predictable are processed faster (e.g. Ehrlich and Rayner, 1981). This effect is well known, but there is currently no agreement on why it occurs; and, since previous studies (Rayner and Well, 1996) have used exclusively factorial comparisons, we know the effect's direction but have little insight into its functional form. To address these issues, we first present a novel theory of linguistic processing time that draws inspiration from two sources: the literature on motor control, and on construction grammar (Fillmore, 1988; Kay and Fillmore, 1999). The model consists of an optimal control system (Todorov, 2004) that, rather than controlling muscles, controls the allocation of preparatory resources in the linguistic processing system. Its challenge is to manage the trade-off between conserving resources and processing quickly; for efficiency, it preferentially allocates resources to more probable continuations, which results in a speedup for predictable words. Next, following the principles of construction grammar, we constrain the model by requiring that it have no preferred scale - we assume that processing proceeds continuously and simultaneously at all levels from phoneme to clause, using similar mechanisms. This turns out to make a strong prediction: processing time for an item should be proportional to the logarithm of that item's probability-in-context. Second, we test this prediction by analyzing the relation between probability and fixation time in the Dundee eye-movement corpus (Kennedy et al., 2003), approximating probability using a computational language model. As compared to previous studies, this approach has the advantage of both improved ecological validity and vastly greater statistical power, allowing the examination of curve shape. We find that after controlling for confounds, probability does have a logarithmic effect on standard reading-time measures, and this effect is substantial and systematic over several orders of magnitude. This result invalidates a number of competing theories which predict different curve shapes, confirms the prediction of our model, and thus provides supporting evidence for constructional accounts of language processing.},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2021-03-16 17:32:08 -0400},
  project = {syntactic embedding}
}

@inproceedings{smith.n:2010cogsci,
  title = {Fixation Durations in First-Pass Reading Reflect Uncertainty about Word Identity},
  booktitle = {Proceedings of the 32nd {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Smith, Nathaniel and Levy, Roger},
  year = {2010},
  urldate = {2025-06-25},
  abstract = {Author(s): Smith, Nathaniel; Levy, Roger},
  langid = {english},
  file = {~/Zotfiles/smith.n2010 Fixation durations in first-pass reading.pdf}
}

@inproceedings{smith.n:2011cogsci,
  title = {Cloze but No Cigar: {{The}} Complex Relationship between Cloze, Corpus, and Subjective Probabilities in Language Processing},
  booktitle = {Proceedings of the 33rd Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2011},
  date-added = {2021-03-16 14:39:39 -0400},
  date-modified = {2022-03-16 10:06:04 -0400}
}

@article{smith.n:2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability -- even for differences between highly unpredictable words -- and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2013.02.013},
  date-added = {2021-03-09 22:52:29 -0500},
  date-modified = {2021-11-14 23:57:40 -0500},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading,surprisal},
  file = {~/Zotfiles/smith.n2013 The effect of word predictability on rea.pdf}
}

@article{smith.p:1969,
  title = {Coding Strategies in Language},
  author = {Smith, Philip Twitchell},
  year = {1969},
  journal = {Information and Control},
  volume = {14},
  number = {1},
  pages = {72--97},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(69)90033-3},
  abstract = {The problem of selecting a code to transmit four messages over the binary symmetric channel is studied in relation to two types of channel noise (``substitution error and ``deletion error) and to two types of decoding strategy (maximum hit and minimum error). It is shown that mean Hamming distance is a good general guide to coding efficiency, except in the case of a minimum error strategy with a deletion error channel, where coding efficiency is critically dependent on noise level. An experiment in which subjects selected codes in an artificial language suggests that the process of recall from memory is similar to the process of transmitting over a deletion error channel with a minimum error strategy. A similar interpretation can be placed on the analysis of consonant systems in English, French, German and Welsh, where the sets of consonants of a given class in a given environment are considered as codes whose alphabet is the phonological distinctive feature system of Halle (1958)},
  bdsk-url-2 = {https://doi.org/10.1016/S0019-9958(69)90033-3},
  date-added = {2022-04-27 13:10:31 -0400},
  date-modified = {2022-04-27 13:11:36 -0400},
  keywords = {artificial language,hamming distance,noisy channel coding,phonology}
}

@inproceedings{snyder.b:2009,
  title = {Unsupervised Multilingual Grammar Induction},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {{ACL}} and the 4th International Joint Conference on Natural Language Processing of the {{AFNLP}}: {{Volume}} 1 - Volume 1},
  author = {Snyder, Benjamin and Naseem, Tahira and Barzilay, Regina},
  year = {2009},
  series = {{{ACL}} '09},
  pages = {73--81},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  abstract = {We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting. Using this formalism, our model loosely binds parallel trees while allowing language-specific syntactic structure. We perform inference under this model using Markov Chain Monte Carlo and dynamic programming. Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we find substantial performance gains over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure.},
  date-added = {2022-04-04 12:46:23 -0400},
  date-modified = {2022-04-04 12:47:09 -0400},
  isbn = {978-1-932432-45-9}
}

@inproceedings{socolof.m:2022,
  title = {Measuring Morphological Fusion Using Partial Information Decomposition},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Socolof, Michaela and Hoover, Jacob Louis and Futrell, Richard and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  pages = {44--54},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  abstract = {Morphological systems across languages vary when it comes to the relation between form and meaning. In some languages, a single meaning feature corresponds to a single morpheme, whereas in other languages, multiple meaning features are bundled together into one morpheme. The two types of languages have been called agglutinative and fusional, respectively, but this distinction does not capture the graded nature of the phenomenon. We provide a mathematically precise way of characterizing morphological systems using partial information decomposition, a framework for decomposing mutual information into three components: unique, redundant, and synergistic information. We show that highly fusional languages are characterized by high levels of synergy.},
  copyright = {All rights reserved},
  openaccess = {true},
  file = {~/Zotfiles/socolof.m2022coling Measuring morphological fusion using par.pdf}
}

@inproceedings{socolof.m:2022a,
  title = {Characterizing Idioms: Conventionality and Contingency},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Socolof, Michaela and Cheung, Jackie and Wagner, Michael and O'Donnell, Timothy},
  year = {2022},
  month = may,
  pages = {4024--4037},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  abstract = {Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that introducing special machinery to handle idioms may not be warranted.},
  date-added = {2022-05-17 08:04:25 -0400},
  date-modified = {2022-05-17 08:05:15 -0400}
}

@phdthesis{socolof.m:2024phd,
  title = {Partial Compositionality},
  author = {Socolof, Michaela},
  year = {2024},
  month = jun,
  address = {Montr{\'e}al, Canada},
  langid = {canadian},
  school = {McGill University},
  file = {~/Zotfiles/socolof.m2024phd Partial compositionality.pdf}
}

@misc{sohl-dickstein.j:2015arxiv,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {arXiv},
  urldate = {2022-05-17},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,diffusion processes,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {~/Zotfiles/sohl-dickstein.j2015 Deep Unsupervised Learning using Nonequi.pdf}
}

@book{sonderegger.m:2023book,
  title = {Regression Modeling for Linguistic Data},
  author = {Sonderegger, Morgan},
  year = {2023},
  month = jun,
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-37503-0},
  langid = {english}
}

@article{song.m:2024,
  title = {The Unique Contribution of Uncertainty Reduction during Naturalistic Language Comprehension},
  author = {Song, Ming and Wang, Jing and Cai, Qing},
  year = {2024},
  month = dec,
  journal = {Cortex},
  volume = {181},
  pages = {12--25},
  issn = {0010-9452},
  doi = {10.1016/j.cortex.2024.09.007},
  urldate = {2025-05-20},
  abstract = {Language comprehension is an incremental process with prediction. Delineating various mental states during such a process is critical to understanding the relationship between human cognition and the properties of language. Entropy reduction, which indicates the dynamic decrease of uncertainty as language input unfolds, has been recognized as effective in predicting neural responses during comprehension. According to the entropy reduction hypothesis (Hale, 2006), entropy reduction is related to the processing difficulty of a word, the effect of which may overlap with other well-documented information-theoretical metrics such as surprisal or next-word entropy. However, the processing difficulty was often confused with the information conveyed by a word, especially lacking neural differentiation. We propose that entropy reduction represents the cognitive neural process of information gain that can be dissociated from processing difficulty. This study characterized various information-theoretical metrics using GPT-2 and identified the unique effects of entropy reduction in predicting fMRI time series acquired during language comprehension. In addition to the effects of surprisal and entropy, entropy reduction was associated with activations in the left inferior frontal gyrus, bilateral ventromedial prefrontal cortex, insula, thalamus, basal ganglia, and middle cingulate cortex. The reduction of uncertainty, rather than its fluctuation, proved to be an effective factor in modeling neural responses. The neural substrates underlying the reduction in uncertainty might imply the brain's desire for information regardless of processing difficulty.},
  keywords = {entropy reduction,Entropy reduction,fMRI,Language comprehension,Naturalistic stimuli,Prediction,Surprisal},
  file = {~/Zotfiles/song.m2024 The unique contribution of uncertainty r.pdf}
}

@inproceedings{song.y:2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-07-07},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  file = {~/Zotfiles/song.y2019 Generative modeling by estimating gradie.pdf}
}

@inproceedings{song.y:2022,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2022},
  month = feb,
  urldate = {2022-07-11},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a...},
  langid = {english},
  file = {~/Zotfiles/song.y2022 Score-based generative modeling through.pdf}
}

@article{soskuthy.m:2021,
  title = {Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis},
  author = {S{\'o}skuthy, M{\'a}rton},
  year = {2021},
  month = jan,
  journal = {Journal of Phonetics},
  volume = {84},
  pages = {101017},
  publisher = {Elsevier BV},
  doi = {10.1016/j.wocn.2020.101017},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2020.101017},
  date-added = {2022-03-04 16:06:14 -0500},
  date-modified = {2022-03-04 16:06:18 -0500}
}

@article{spratling.m:2017,
  title = {A Review of Predictive Coding Algorithms},
  author = {Spratling, M. W.},
  year = {2017},
  month = mar,
  journal = {Brain and Cognition},
  series = {Perspectives on {{Human Probabilistic Inferences}} and the '{{Bayesian Brain}}'},
  volume = {112},
  pages = {92--97},
  issn = {0278-2626},
  doi = {10.1016/j.bandc.2015.11.003},
  urldate = {2025-04-03},
  abstract = {Predictive coding is a leading theory of how the brain performs probabilistic inference. However, there are a number of distinct algorithms which are described by the term ``predictive coding''. This article provides a concise review of these different predictive coding algorithms, highlighting their similarities and differences. Five algorithms are covered: linear predictive coding which has a long and influential history in the signal processing literature; the first neuroscience-related application of predictive coding to explaining the function of the retina; and three versions of predictive coding that have been proposed to model cortical function. While all these algorithms aim to fit a generative model to sensory data, they differ in the type of generative model they employ, in the process used to optimise the fit between the model and sensory data, and in the way that they are related to neurobiology.},
  keywords = {Cortex,Free energy,Neural networks,Predictive coding,Retina,Signal processing},
  file = {~/Zotfiles/spratling.m2017 A review of predictive coding algorithms.pdf}
}

@article{st.john.m:1990,
  title = {Learning and Applying Contextual Constraints in Sentence Comprehension},
  author = {St. John, Mark F. and McClelland, James L.},
  year = {1990},
  month = nov,
  journal = {Artificial Intelligence},
  volume = {46},
  number = {1},
  pages = {217--257},
  issn = {0004-3702},
  doi = {10.1016/0004-3702(90)90008-N},
  urldate = {2025-04-15},
  abstract = {A parallel distributed processing model is described that learns to comprehend single clause sentences. Specifically, it assigns thematic roles to sentence constituents, disambiguates ambiguous words, instantiates vague words, and elaborates implied roles. The sentences are pre-segmented into constituent phrases. Each constituent is processed in turn to update an evolving representation of the event described by the sentence. The model uses the information derived from each constituent to revise its ongoing interpretation of the sentence and to anticipate additional constituents. The network learns to perform these tasks through practice on processing example sentence/event pairs. The learning procedure allows the model to take a statistical approach to solving the bootstrapping problem of learning the syntax and semantics of a language from the same data. The model performs very well on the corpus of sentences on which it was trained, and generalizes to sentences on which it was not trained, but learns slowly.},
  file = {~/Zotfiles/st.john.m1990 Learning and applying contextual constra.pdf}
}

@incollection{stabler.e:1997,
  title = {Derivational Minimalism},
  booktitle = {Logical {{Aspects}} of {{Computational Linguistics}}},
  author = {Stabler, Edward},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Retor{\'e}, Christian},
  year = {1997},
  volume = {1328},
  pages = {68--95},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0052152},
  urldate = {2022-09-30},
  isbn = {978-3-540-63700-4 978-3-540-69631-5}
}

@inproceedings{stabler.e:1997a,
  title = {Derivational Minimalism},
  booktitle = {Logical Aspects of Computational Linguistics},
  author = {Stabler, Edward},
  editor = {Retor{\'e}, Christian},
  year = {1997},
  pages = {68--95},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0052152},
  abstract = {A basic idea of the transformational tradition is that constituents move. More recently, there has been a trend towards the view that all features are lexical features. And in recent ``minimalist'' grammars, structure building operations are assumed to be feature driven. A simple grammar formalism with these properties is presented here and briefly explored. Grammars in this formalism can define languages that are not in the ``mildly context sensitive'' class defined by Vijay-Shanker and Weir (1994).},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  isbn = {978-3-540-69631-5},
  project = {syntactic embedding}
}

@article{stabler.e:2013,
  title = {Two Models of Minimalist, Incremental Syntactic Analysis},
  author = {Stabler, Edward P.},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {611--633},
  issn = {17568757},
  doi = {10.1111/tops.12031},
  urldate = {2022-09-30},
  langid = {english}
}

@article{stanley.d:1999,
  title = {A {{Multiplicative Calculus}}},
  author = {Stanley, Dick},
  year = {1999},
  month = jan,
  journal = {PRIMUS},
  volume = {9},
  number = {4},
  pages = {310--326},
  publisher = {Taylor \& Francis},
  issn = {1051-1970},
  doi = {10.1080/10511979908965937},
  urldate = {2023-04-25},
  abstract = {A new type of calculus called ``multiplicative calculus'' is developed and some basic theorems about derivatives, integrals, and infinite products are proved within this calculus. Multiplicative calculus is based on a multiplicative mechanism in the same sense that the usual calculus is based on an additive mechanism. One consequence is that, just as the usual derivative of a linear function is constant, the multiplicative derivative of an exponential function is constant (and just as the usual derivative of a constant function is 0, the multiplicative derivative of a constant function is 1). Similarly, just as two functions that have a constant difference have the same usual derivative, two functions that have a constant ratio have the same multiplicative derivative. Finally, just as many functions have infinite series representations based on the usual derivative, the same functions have infinite product representations based on the multiplicative derivative. These in turn are derived from exponential approximations to functions that are analogous to the linear approximations of the usual calculus. Multiplicative calculus is a useful supplement to the usual calculus in that it is tailored to situations involving exponential functions in the same sense that the usual calculus is tailored to situations involving linear functions.},
  keywords = {additive,Calculus,differential calculus,infinite products,infinite series,integral calculus,multiplicative}
}

@inproceedings{stanojevic.m:2021,
  title = {Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Stanojevi{\'c}, Milo{\v s} and Bhattasali, Shohini and Dunagan, Donald and Campanelli, Luca and Steedman, Mark and Brennan, Jonathan and Hale, John},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.cmcl-1.3},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.3},
  date-added = {2022-04-14 13:33:35 -0400},
  date-modified = {2022-04-14 13:33:47 -0400}
}

@article{staub.a:2010,
  title = {Eye Movements and Processing Difficulty in Object Relative Clauses},
  author = {Staub, Adrian},
  year = {2010},
  month = jul,
  journal = {Cognition},
  volume = {116},
  number = {1},
  pages = {71--86},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2010.04.002},
  abstract = {It is well known that sentences containing object-extracted relative clauses (e.g., The reporter that the senator attacked admitted the error) are more difficult to comprehend than sentences containing subject-extracted relative clauses (e.g., The reporter that attacked the senator admitted the error). Two major accounts of this phenomenon make different predictions about where, in the course of incremental processing of an object relative, difficulty should first appear. An account emphasizing memory processes (Gibson, 1998; Grodner \& Gibson, 2005) predicts difficulty at the relative clause verb, while an account emphasizing experience-based expectations (Hale, 2001; Levy, 2008) predicts earlier difficulty, at the relative clause subject. Two eye movement experiments tested these predictions. Regressive saccades were much more likely from the subject noun phrase of an object relative than from the same noun phrase occurring within a subject relative (Experiment 1) or within a verbal complement clause (Experiment 2). This effect was further amplified when the relative pronoun that was omitted. However, reading time was also inflated on the object relative clause verb in both experiments. These results suggest that the violation of expectations and the difficulty of memory retrieval both contribute to the difficulty of object relative clauses, but that these two sources of difficulty have qualitatively distinct behavioral consequences in normal reading.},
  langid = {english},
  pmid = {20427040},
  keywords = {Eye Movements,Female,Fixation Ocular,Humans,Male,Psycholinguistics,Psychomotor Performance,Reading,Semantics,Young Adult}
}

@article{staub.a:2011,
  title = {The Effect of Lexical Predictability on Distributions of Eye Fixation Durations},
  author = {Staub, Adrian},
  year = {2011},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {18},
  number = {2},
  pages = {371--376},
  issn = {1531-5320},
  doi = {10.3758/s13423-010-0046-9},
  urldate = {2022-10-26},
  abstract = {A word's predictability in context has a well-established effect on fixation durations in reading. To investigate how this effect is manifested in distributional terms, an experiment was carried out in which subjects read each of 50 target words twice, once in a high-predictability context and once in a low-predictability context. The ex-Gaussian distribution was fit to each subject's first-fixation durations and single-fixation durations. For both measures, the {$\mu$} parameter increased when a word was unpredictable, while the {$\tau$} parameter was not significantly affected, indicating that a predictability manipulation shifts the distribution of fixation durations but does not affect the degree of skew. Vincentile plots showed that the mean ex-Gaussian parameters described the typical distribution shapes extremely well. These results suggest that the predictability and frequency effects are functionally distinct, since a frequency manipulation has been shown to influence both {$\mu$} and {$\tau$}. The results may also be seen as consistent with the finding from single-word recognition paradigms that semantic priming affects only {$\mu$}.},
  langid = {english},
  keywords = {Distributional analysis,ex-gaussian,Eye movements in reading,Visual word recognition},
  file = {~/Zotfiles/staub.a2011 The effect of lexical predictability on.pdf}
}

@article{staub.a:2015,
  title = {The Influence of Cloze Probability and Item Constraint on Cloze Task Response Time},
  author = {Staub, Adrian and Grant, Margaret and Astheimer, Lori and Cohen, Andrew},
  year = {2015},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {82},
  pages = {1--17},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.02.004},
  urldate = {2022-09-07},
  abstract = {In research on the role of lexical predictability in language comprehension, predictability is generally defined as the probability that a word is provided as a sentence continuation in the cloze task (Taylor, 1953), in which subjects are asked to guess the next word of a sentence. The present experiments investigate the process by which subjects generate a cloze response, by measuring the latency to initiate a response in a version of the task in which subjects produce a spoken continuation to a visually presented sentence fragment. Higher probability responses were produced faster than lower probability responses. The latency to produce a response was also influenced by item constraint: A response at a given level of probability was issued faster when the context was more constraining, i.e., a single response was elicited with high probability. We show that these patterns are naturally produced by an activation-based race model in which potential responses independently race towards a response threshold. Implications for the interpretation of cloze probability as a measure of lexical predictability are discussed.},
  langid = {english},
  keywords = {Cloze task,Language processing,Prediction,Response time}
}

@article{staub.a:2015a,
  title = {The Effect of Lexical Predictability on Eye Movements in Reading: {{Critical}} Review and Theoretical Interpretation},
  author = {Staub, Adrian},
  year = {2015},
  journal = {Language and Linguistics Compass},
  volume = {9},
  number = {8},
  pages = {311--327},
  publisher = {Wiley},
  doi = {10.1111/lnc3.12151},
  bdsk-url-2 = {https://doi.org/10.1111/lnc3.12151},
  date-added = {2021-05-22 15:55:42 -0400},
  date-modified = {2021-05-22 15:55:54 -0400},
  keywords = {predictability,processing,review}
}

@article{staub.a:2019,
  title = {Failure to Detect Function Word Repetitions and Omissions in Reading: {{Are}} Eye Movements to Blame?},
  shorttitle = {Failure to Detect Function Word Repetitions and Omissions in Reading},
  author = {Staub, Adrian and Dodge, Sophia and Cohen, Andrew L.},
  year = {2019},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {26},
  number = {1},
  pages = {340--346},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1492-z},
  urldate = {2024-12-27},
  abstract = {We tested whether failure to notice repetitions of function words during reading (e.g., Amanda jumped off the the swing and landed on her feet.) is due to the eyes' tendency to skip one of the instances of the word. Eye movements were recorded during reading of sentences with repetitions of the word the or repetitions of a noun, after which readers were asked whether an error was present. A repeated the was detected on 46\% of trials overall. On trials on which both instances of the were fixated, detection was still only 66\%. A repeated noun was detected on 90\% of trials, with no significant effect of eye movement patterns. Detecting an omitted the also proved difficult, with eye movement patterns having only a small effect. Readers frequently overlook function word errors even when their eye movements provide maximal opportunity for noticing such errors, but they notice content word repetitions regardless of eye movement patterns. We propose that readers overlook function word errors because they attribute the apparent error to noise in the eye movement control system.},
  langid = {english},
  keywords = {Eye movements,Language comprehension,Reading},
  file = {~/Zotfiles/staub.a2019 Failure to detect function word repetiti.pdf}
}

@article{staub.a:2024,
  title = {Perceptual Inference Corrects Function Word Errors in Reading: {{Errors}} That Are Not Noticed Do Not Disrupt Eye Movements},
  shorttitle = {Perceptual Inference Corrects Function Word Errors in Reading},
  author = {Staub, Adrian and McMurray, Harper and Wickett, Anthony},
  year = {2024},
  month = nov,
  journal = {Cognitive Psychology},
  volume = {154},
  pages = {101691},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2024.101691},
  urldate = {2024-09-23},
  abstract = {Both everyday experience and laboratory research demonstrate that readers often fail to notice errors such as an omitted or repeated function word. This phenomenon challenges central tenets of reading and sentence processing models, according to which each word is lexically processed and incrementally integrated into a syntactic representation. One solution would propose that apparent failure to notice such errors reflects post-perceptual inference; the reader does initially perceive the error, but then unconsciously 'corrects' the perceived string. Such a post-perceptual account predicts that when readers fail to explicitly notice an error, the error will nevertheless disrupt reading, at least fleetingly. We present a large-scale eyetracking experiment investigating whether disruption is detectable in the eye movement record when readers fail to notice an omitted or repeated two-letter function word in naturalistic sentences. Readers failed to notice both omission and repetition errors over 36\% of the time. In an analysis that included all trials, both omission and repetition resulted in pronounced eye movement disruption, compared to reading of grammatical control sentences. But in an analysis including only trials on which readers failed to notice the errors, neither type of error disrupted eye movements on any measure. Indeed, there was evidence in some measures that reading was relatively fast on the trials on which errors were missed. It does not appear that when an error is not consciously noticed, it is initially perceived, and then later corrected; rather, linguistic knowledge influences what the reader perceives.},
  keywords = {Perceptual inference,Reading,Sentence processing},
  file = {~/Zotfiles/staub.a2024cogpsych Perceptual inference corrects function w.pdf}
}

@article{staub.a:2024a,
  title = {Predictability in Language Comprehension: Prospects and Problems for Surprisal},
  shorttitle = {Predictability in Language Comprehension},
  author = {Staub, Adrian},
  year = {2024},
  month = jul,
  journal = {Annual Review of Linguistics},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011724-121517},
  urldate = {2024-07-25},
  abstract = {Surprisal theory proposes that a word's predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory's detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid = {english},
  file = {~/Zotfiles/staub.a2024 Predictability in Language Comprehension.pdf}
}

@misc{steedman.m:2000,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {MIT Press},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{steedman.m:2000book,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {The MIT press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{steinhardt.j:2014,
  title = {Filtering with Abstract Particles},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  author = {Steinhardt, Jacob and Liang, Percy},
  editor = {Xing, Eric P. and Jebara, Tony},
  year = {2014},
  month = jun,
  series = {Proceedings of Machine Learning Research},
  volume = {32},
  pages = {727--735},
  publisher = {PMLR},
  address = {Bejing, China},
  abstract = {Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue by using ``abstract particles'' that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.},
  date-added = {2022-03-25 12:02:29 -0400},
  date-modified = {2022-03-25 12:02:31 -0400},
  pdf = {http://proceedings.mlr.press/v32/steinhardt14.pdf}
}

@book{stevens.e:2020book,
  title = {Deep Learning with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  year = {2020},
  publisher = {Manning Publications Company},
  date-added = {2021-08-02 19:51:35 -0400},
  date-modified = {2021-08-02 19:52:08 -0400},
  isbn = {978-1-61729-526-3}
}

@inproceedings{stolcke.a:1994,
  title = {Inducing Probabilistic Grammars by {{Bayesian}} Model Merging},
  booktitle = {Grammatical {{Inference}} and {{Applications}}},
  author = {Stolcke, Andreas and Omohundro, Stephen},
  editor = {Carrasco, Rafael C. and Oncina, Jose},
  year = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {106--118},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-58473-0_141},
  abstract = {We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.},
  isbn = {978-3-540-48985-6},
  langid = {english},
  keywords = {Bayesian Posterior Probability,Beam Search,Hide Markov Model,Merging Algorithm,Relative Clause},
  file = {~/Zotfiles/stolcke.a1994 Inducing probabilistic grammars by Bayes.pdf}
}

@article{stolcke.a:1995,
  title = {An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities},
  author = {Stolcke, Andreas},
  year = {1995},
  journal = {Computational Linguistics},
  volume = {21},
  number = {2},
  pages = {165--201},
  file = {~/Zotfiles/stolcke.a1995 An efficient probabilistic context-free.pdf}
}

@article{stone.m:1960,
  title = {Models for Choice-Reaction Time},
  author = {Stone, Mervyn},
  year = {1960},
  month = sep,
  journal = {Psychometrika},
  volume = {25},
  number = {3},
  pages = {251--260},
  issn = {1860-0980},
  doi = {10.1007/BF02289729},
  urldate = {2022-07-04},
  abstract = {In the two-choice situation, the Wald sequential probability ratio decision procedure is applied to relate the mean and variance of the decision times, for each alternative separately, to the error rates and the ratio of the frequencies of presentation of the alternatives. For situations involving more than two choices, a fixed sample decision procedure (selection of the alternative with highest likelihood) is examined, and the relation is found between the decision time (or size of sample), the error rate, and the number of alternatives.},
  langid = {english},
  keywords = {Decision Procedure,Error Rate,High Likelihood,Public Policy,Statistical Theory}
}

@inproceedings{strubell.e:2019,
  title = {Energy and Policy Considerations for Deep Learning in {{NLP}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  pages = {3645--3650},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1355},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1355}
}

@inproceedings{sundermeyer.m:2012,
  title = {{{LSTM}} Neural Networks for Language Modeling},
  booktitle = {Interspeech 2012},
  author = {Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2012},
  month = sep,
  pages = {194--197},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2012-65},
  urldate = {2024-05-18},
  langid = {english},
  file = {~/Zotfiles/sundermeyer.m2012LSTMsForLM LSTM neural networks for language modeli.pdf}
}

@article{svenonius.p:2002,
  title = {Icelandic Case and the Structure of Events},
  author = {Svenonius, Peter},
  year = {2002},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {5},
  number = {1-3},
  pages = {197--225},
  publisher = {Springer},
  date-added = {2020-03-06 14:58:07 -0800},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@article{swets.b:2008,
  title = {Underspecification of Syntactic Ambiguities: {{Evidence}} from Self-Paced Reading},
  shorttitle = {Underspecification of Syntactic Ambiguities},
  author = {Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
  year = {2008},
  month = jan,
  journal = {Memory \& Cognition},
  volume = {36},
  number = {1},
  pages = {201--216},
  issn = {1532-5946},
  doi = {10.3758/MC.36.1.201},
  urldate = {2023-08-01},
  abstract = {Syntactically ambiguous sentences are sometimes read faster than disambiguated strings. Models of parsing have explained this tendency by appealing either to a race in the construction of alternative structures or to reanalysis. However, it is also possible that readers of ambiguous sentences save time by strategically underspecifying interpretations of ambiguous attachments. In a self-paced reading study, participants viewed sentences with relative clauses that could attach to one of two sites. Type of question was also manipulated between participants in order to test whether goals can influence reading/parsing strategies. The experiment revealed an ambiguity advantage in reading times, but only when participants expected superficial comprehension questions. When participants expected queries about relative clause interpretation, disambiguating regions were inspected with more care, and the ambiguity advantage was attenuated. However, even when participants expected relative clause queries, question-answering times suggested underspecified representations of ambiguous relative clause attachments. The results support the construal and ``good-enough'' models of parsing.},
  langid = {english},
  keywords = {Question Type,Race Model,Reading Time,Relative Clause,Sentence Type,underspecification},
  file = {~/Zotfiles/swets.bdesmet.t2008 Underspecification of syntactic ambiguit.pdf}
}

@article{szewczyk.j:2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  year = {2022},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2021.104311},
  urldate = {2024-03-01},
  abstract = {Stimuli are easier to process when context makes them predictable, but does context-based facilitation arise from preactivation of a limited set of relatively probable upcoming stimuli (with facilitation then linearly related to probability) or, instead, because the system maintains and updates a probability distribution across all items (with facilitation logarithmically related to probability)? We measured the N400, an index of semantic access, to words of varying probability, including unpredictable words. Word predictability was measured using both cloze probabilities and a state-of-the-art machine learning language model (GPT-2). We reanalyzed five datasets (n~=~138) to demonstrate and then replicate that context-based facilitation on the N400 is graded, even among unpredictable words. Furthermore, we established that the relationship between word predictability and context-based facilitation combines linear and logarithmic functions. We argue that this composite function reveals properties of the mapping between words and semantic features and how feature- and word-related information is activated on-line.},
  keywords = {Context-based facilitation,GPT-2,N400,Semantic access},
  file = {~/Zotfiles/szewczyk.j2022 Context-based facilitation of semantic a.pdf}
}

@article{tabor.w:2004,
  title = {Evidence for Self-Organized Sentence Processing: {{Digging-in}} Effects.},
  author = {Tabor, Whitney and Hutchins, Sean},
  year = {2004},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {30},
  number = {2},
  pages = {431--450},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0278-7393.30.2.431},
  bdsk-url-2 = {https://doi.org/10.1037/0278-7393.30.2.431},
  date-added = {2021-04-11 18:58:19 -0400},
  date-modified = {2021-04-11 18:58:36 -0400},
  keywords = {diggin in effect,reading time}
}

@inproceedings{takabatake.k:2004,
  title = {Information Geometry of {{Gibbs}} Sampler},
  booktitle = {Proc. of {{WSEAS}} Int. {{Conf}}. on Neural Networks and Applications ({{NNA}})},
  author = {Takabatake, Kazuya},
  year = {2004},
  date-added = {2021-03-11 16:56:36 -0500},
  date-modified = {2021-03-11 17:08:06 -0500},
  keywords = {information theory,sampling},
  file = {~/Zotfiles/takabatake.k2004 Information geometry of Gibbs sampler.pdf}
}

@inproceedings{tan.s:2023,
  title = {Sparse Universal Transformer},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Tan, Shawn and Shen, Yikang and Chen, Zhenfang and Courville, Aaron and Gan, Chuang},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {169--179},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.12},
  urldate = {2025-04-17},
  abstract = {The Universal Transformer (UT) is a variant of the Transformer that shares parameters across its layers and is Turing-complete under certain assumptions. Empirical evidence also shows that UTs have better compositional generalization than Vanilla Transformers (VTs) in formal language tasks. The parameter-sharing also affords it better parameter efficiency than VTs. Despite its many advantages, most state-of-the-art NLP systems use VTs as their backbone model instead of UTs. This is mainly because scaling UT parameters is more compute and memory intensive than scaling up a VT. This paper proposes the Sparse Universal Transformer (SUT), which leverages Sparse Mixture of Experts (SMoE) to reduce UT`s computation complexity while retaining its parameter efficiency and generalization ability. Experiments show that SUT combines the best of both worlds, achieving strong generalization results on formal language tasks (Logical inference and CFQ) and impressive parameter and computation efficiency on standard natural language benchmarks like WMT`14.},
  file = {~/Zotfiles/tan.s2023 Sparse Universal Transformer.pdf}
}

@article{tanenhaus.m:1995,
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Tanenhaus, Michael K. and {Spivey-Knowlton}, Michael J. and Eberhard, Kathleen M. and Sedivy, Julie C.},
  year = {1995},
  month = jun,
  journal = {Science (New York, N.Y.)},
  volume = {268},
  number = {5217},
  pages = {1632--1634},
  publisher = {American Association for the Advancement of Science (AAAS)},
  doi = {10.1126/science.7777863},
  abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
  bdsk-url-2 = {https://doi.org/10.1126/science.7777863},
  date-added = {2022-04-20 13:14:59 -0400},
  date-modified = {2022-05-02 14:45:52 -0400},
  keywords = {incrementality}
}

@incollection{taraldsen.k:1995,
  title = {On Agreement and Nominative Objects in {{Icelandic}}},
  booktitle = {Studies in Comparative {{Germanic}} Syntax},
  author = {Taraldsen, Knut Tarald},
  year = {1995},
  pages = {307--327},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:17:31 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,split probe}
}

@article{tax.t:2017,
  title = {The Partial Information Decomposition of Generative Neural Network Models},
  author = {Tax, Tycho M.S. and Mediano, Pedro A.M. and Shanahan, Murray},
  year = {2017},
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {19},
  number = {9},
  issn = {1099-4300},
  doi = {10.3390/e19090474},
  abstract = {In this work we study the distributed representations learnt by generative neural network models. In particular, we investigate the properties of redundant and synergistic information that groups of hidden neurons contain about the target variable. To this end, we use an emerging branch of information theory called partial information decomposition (PID) and track the informational properties of the neurons through training. We find two differentiated phases during the training process: a first short phase in which the neurons learn redundant information about the target, and a second phase in which neurons start specialising and each of them learns unique information about the target. We also find that in smaller networks individual neurons learn more specific information about certain features of the input, suggesting that learning pressure can encourage disentangled representations.},
  article-number = {474},
  bdsk-url-2 = {https://doi.org/10.3390/e19090474},
  date-added = {2022-05-14 10:28:04 -0400},
  date-modified = {2022-05-14 10:28:21 -0400},
  keywords = {neural networks,partial information decomposition}
}

@misc{tay.y:2022,
  title = {Transformer Memory as a Differentiable Search Index},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.06991},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2202.06991},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 11:01:29 -0400},
  date-modified = {2022-03-31 11:02:20 -0400},
  keywords = {transformer},
  file = {~/Zotfiles/tay.y2022 Transformer memory as a differentiable s.pdf}
}

@misc{tay.y:2022arxiv,
  title = {Transcending Scaling Laws with 0.1\% Extra Compute},
  author = {Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q. and So, David R. and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and Zhou, Denny and Metzler, Donald and Petrov, Slav and Houlsby, Neil and Le, Quoc V. and Dehghani, Mostafa},
  year = {2022},
  month = nov,
  number = {arXiv:2210.11399},
  eprint = {2210.11399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-31},
  abstract = {Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \${\textbackslash}sim\$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/tay.y2022UL2R Transcending scaling laws with 0.1% extr.pdf}
}

@misc{tay.y:2023UL2,
  title = {{{UL2}}: {{Unifying}} Language Learning Paradigms},
  shorttitle = {Ul2},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  year = {2023},
  month = feb,
  number = {arXiv:2205.05131},
  eprint = {2205.05131},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized \& unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 \& GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B \& Flan-UL2 20B.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM},
  file = {~/Zotfiles/tay.y2023UL2 UL2 Unifying language learning paradigm.pdf}
}

@article{taylor.w:1953,
  title = {`{{Cloze}} Procedure': {{A}} New Tool for Measuring Readability},
  author = {Taylor, Wilson L.},
  year = {1953},
  journal = {Journalism Quarterly},
  volume = {30},
  number = {4},
  pages = {415--433},
  publisher = {SAGE Publications},
  doi = {10.1177/107769905303000401},
  bdsk-url-2 = {https://doi.org/10.1177/107769905303000401},
  date-added = {2021-03-18 10:41:54 -0400},
  date-modified = {2021-03-18 11:25:18 -0400},
  keywords = {cloze,processing},
  file = {~/Zotfiles/taylor.w1953 Cloze procedure A new tool for measur.pdf}
}

@inproceedings{teh.y:2006,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman-Yor}} Processes},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Computational Linguistics}} and the 44th Annual Meeting of the {{ACL}} - {{ACL}} 06},
  author = {Teh, Yee Whye},
  year = {2006},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1220175.1220299},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220299},
  date-added = {2022-04-25 21:37:17 -0400},
  date-modified = {2022-04-25 21:38:48 -0400},
  keywords = {bayesian,HMM,Natural language processing,Pitman-Yor processes}
}

@techreport{teh.y:2006report,
  type = {Technical Report},
  title = {A {{Bayesian}} Interpretation of Interpolated {{Kneser-Ney}}},
  author = {Teh, Yee Whye},
  year = {2006},
  number = {TRA2/06},
  institution = {School of Computing, National University of Singapore},
  abstract = {Interpolated Kneser-Ney is one of the best smoothing methods for n-gram language models. Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method. We propose a novel interpretation of interpolated Kneser-Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman-Yor processes. As opposed to past explanations, our interpretation can recover exactly the formulation of interpolated Kneser-Ney, and performs better than interpolated Kneser-Ney when a better inference procedure is used.},
  date-added = {2022-04-26 10:12:38 -0400},
  date-modified = {2022-04-26 10:16:03 -0400},
  keywords = {bayesian,Dirichlet processes,hierarchical clustering,language modeling,Pitman-Yor processes},
  file = {~/Zotfiles/teh.y2006techreport A Bayesian interpretation of interpolate.pdf}
}

@inproceedings{tenenbaum.j:1998,
  title = {Bayesian Modeling of Human Concept Learning},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Tenenbaum, Joshua},
  year = {1998},
  volume = {11},
  publisher = {MIT Press},
  urldate = {2025-01-13},
  abstract = {I consider the problem of learning concepts from small numbers of pos(cid:173) itive examples,  a feat  which humans perform routinely but which com(cid:173) puters  are  rarely  capable  of.  Bridging machine  learning  and  cognitive  science perspectives, I present both theoretical analysis and an empirical  study with human subjects for the simple task oflearning concepts corre(cid:173) sponding to axis-aligned rectangles in a multidimensional feature space.  Existing learning models, when applied to this task, cannot explain how  subjects generalize from only a few  examples of the concept.  I propose  a principled Bayesian model based on the assumption that the examples  are  a random sample from  the concept to be  learned.  The  model  gives  precise fits to human behavior on this simple task and provides qualitati ve  insights into more complex, realistic cases of concept learning.},
  keywords = {bayesian size principle},
  file = {~/Zotfiles/tenenbaum.j1998 Bayesian Modeling of Human Concept Learn.pdf}
}

@phdthesis{tenenbaum.j:1999phd,
  title = {A {{Bayesian}} Framework for Concept Learning},
  author = {Tenenbaum, Joshua B},
  year = {1999},
  urldate = {2025-01-13},
  school = {Massachusetts Institute of Technology},
  keywords = {bayesian size principle},
  file = {~/Zotfiles/tenenbaum.j1999phdthesis A Bayesian framework for concept learnin.pdf}
}

@article{tenenbaum.j:2001,
  title = {Generalization, Similarity, and {{Bayesian}} Inference},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2001},
  month = aug,
  journal = {Behavioral and Brain Sciences},
  volume = {24},
  number = {4},
  pages = {629--640},
  publisher = {Cambridge University Press},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X01000061},
  urldate = {2022-10-11},
  abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
  langid = {english},
  keywords = {additive clustering,Bayesian inference,categorization,concept learning,contrast model,features,generalization,psychological space,similarity},
  file = {~/Zotfiles/tenenbaum.j2001 Generalization, similarity, and Bayesian.pdf}
}

@article{tenenbaum.j:2011,
  title = {How to Grow a Mind: Statistics, Structure, and Abstraction},
  shorttitle = {How to Grow a Mind},
  author = {Tenenbaum, Joshua B. and Kemp, Charles and Griffiths, Thomas L. and Goodman, Noah D.},
  year = {2011},
  month = mar,
  journal = {Science},
  volume = {331},
  number = {6022},
  pages = {1279--1285},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1192788},
  urldate = {2025-02-17},
  abstract = {In coming to understand the world---in learning concepts, acquiring language, and grasping causal relations---our minds make inferences that appear to go far beyond the data available. How do we do it? This review describes recent approaches to reverse-engineering human learning and cognitive development and, in parallel, engineering more humanlike machine learning systems. Computational models that perform probabilistic inference over hierarchies of flexibly structured representations can address some of the deepest questions about the nature and origins of human thought: How does abstract knowledge guide learning and reasoning from sparse data? What forms does our knowledge take, across different domains and tasks? And how is that abstract knowledge itself acquired?},
  langid = {english}
}

@inproceedings{tenney.i:2019,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{tenney.i:2019a,
  title = {{{BERT}} Rediscovers the Classical {{NLP}} Pipeline},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  pages = {4593--4601},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1452},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1452}
}

@inproceedings{terra.e:2003,
  title = {Frequency Estimates for Statistical Word Similarity Measures},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Terra, Egidio L. and Clarke, Charles L. A.},
  year = {2003},
  pages = {244--251}
}

@book{tesniere.l:1959book,
  title = {{\'E}lements de Syntaxe Structurale. {{Pr{\'e}f}}. de Jean Fourquet},
  author = {Tesni{\`e}re, Lucien},
  year = {1959},
  publisher = {C. Klincksieck},
  date-added = {2021-07-16 19:40:41 -0400},
  date-modified = {2021-07-16 19:40:43 -0400}
}

@book{tesniere.l:2015book,
  title = {Elements of Structural Syntax},
  author = {Tesni{\`e}re, Lucien},
  translator = {Osborne, Timothy and Kahane, Sylvain},
  year = {2015},
  publisher = {John Benjamins Publishing Company},
  doi = {10.1075/z.185},
  bdsk-url-2 = {https://doi.org/10.1075/z.185},
  date-added = {2021-06-24 10:12:36 -0400},
  date-modified = {2021-06-24 10:13:38 -0400},
  isbn = {978-90-272-6999-7},
  langid = {english}
}

@inproceedings{thai.b:2020,
  title = {Fully {{Convolutional ASR}} for {{Less-Resourced Endangered Languages}}},
  booktitle = {Proceedings of the 1st {{Joint Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages ({{SLTU}}) and {{Collaboration}} and {{Computing}} for {{Under-Resourced Languages}} ({{CCURL}})},
  author = {Thai, Bao and Jimerson, Robert and Ptucha, Raymond and Prud'hommeaux, Emily},
  year = {2020},
  month = may,
  pages = {126--130},
  publisher = {European Language Resources association},
  address = {Marseille, France},
  urldate = {2022-06-06},
  abstract = {The application of deep learning to automatic speech recognition (ASR) has yielded dramatic accuracy increases for languages with abundant training data, but languages with limited training resources have yet to see accuracy improvements on this scale. In this paper, we compare a fully convolutional approach for acoustic modelling in ASR with a variety of established acoustic modeling approaches. We evaluate our method on Seneca, a low-resource endangered language spoken in North America. Our method yields word error rates up to 40\% lower than those reported using both standard GMM-HMM approaches and established deep neural methods, with a substantial reduction in training time. These results show particular promise for languages like Seneca that are both endangered and lack extensive documentation.},
  isbn = {979-10-95546-35-1},
  langid = {english},
  keywords = {automatic speech recognition,computational revitalization,iroquoian},
  file = {~/Zotfiles/thai.b2020 Fully Convolutional ASR for Less-Resourc.pdf}
}

@book{thrun.s:2005book,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2005},
  publisher = {MIT Press},
  date-added = {2022-05-05 10:19:08 -0400},
  date-modified = {2022-05-05 10:21:11 -0400},
  isbn = {978-0-262-36380-8}
}

@misc{timkey.w:2023arxiv,
  title = {A {{Language Model}} with {{Limited Memory Capacity Captures Interference}} in {{Human Sentence Processing}}},
  author = {Timkey, William and Linzen, Tal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16142},
  eprint = {2310.16142},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's single attention head captures semantic and syntactic interference effects observed in human experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,memory effects,parsing,processing},
  file = {~/Zotfiles/timkey.w2023arxiv A Language Model with Limited Memory Cap.pdf}
}

@misc{tishby.n:2000,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {2000},
  eprint = {physics/0004057},
  archiveprefix = {arXiv},
  date-added = {2020-07-21 08:44:18 -0400},
  date-modified = {2020-07-21 08:47:35 -0400},
  project = {syntactic embedding},
  keywords = {information bottleneck,information theory,variational inference}
}

@inproceedings{titov.i:2007,
  title = {A Latent Variable Model for Generative Dependency Parsing},
  booktitle = {Proceedings of the Tenth International Conference on Parsing Technologies},
  author = {Titov, Ivan and Henderson, James},
  year = {2007},
  month = jun,
  pages = {144--155},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic},
  date-added = {2022-04-26 17:35:02 -0400},
  date-modified = {2022-04-26 17:35:31 -0400},
  keywords = {Dependency Grammar,dependency parsing,generative grammar}
}

@article{tokdar.s:2010,
  title = {Importance Sampling: A Review},
  shorttitle = {Importance Sampling},
  author = {Tokdar, Surya T. and Kass, Robert E.},
  year = {2010},
  journal = {WIREs Computational Statistics},
  volume = {2},
  number = {1},
  pages = {54--60},
  issn = {1939-0068},
  doi = {10.1002/wics.56},
  urldate = {2025-02-20},
  abstract = {We provide a short overview of importance sampling---a popular sampling tool used for Monte Carlo computing. We discuss its mathematical foundation and properties that determine its accuracy in Monte Carlo approximations. We review the fundamental developments in designing efficient importance sampling (IS) for practical use. This includes parametric approximation with optimization-based adaptation, sequential sampling with dynamic adaptation through resampling and population-based approaches that make use of Markov chain sampling. Copyright {\copyright} 2009 John Wiley \& Sons, Inc. This article is categorized under: Statistical and Graphical Methods of Data Analysis {$>$} Sampling},
  langid = {english},
  keywords = {importance sampling,Markov chain sampling,Monte Carlo approximation,resampling,sequential sampling},
  file = {~/Zotfiles/tokdar.s2010 Importance sampling a review.pdf}
}

@misc{touvron.h:2023arxiv,
  title = {{{LLaMA}}: {{Open}} and Efficient Foundation Language Models},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-01},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{touvron.h:2023arxiva,
  title = {Llama 2: {{Open}} Foundation and Fine-Tuned Chat Models},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-01},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@incollection{townsend.j:1974,
  title = {Issues and Models Concerning the Processing of a Finite Number of Inputs},
  booktitle = {Human Information Processing},
  author = {Townsend, James T.},
  year = {1974},
  publisher = {Routledge},
  abstract = {The broadest and most critical arena of investigation and contention with regard to these two matters has always been consciousness itself. Broadbent unveiled a theoretical structure that allowed collation of a substantial body of experimental literature and that was seminal in its influence on later developments. Hybrid models have been of limited current theoretical interest, probably due in part to the difficulty in testing them experimentally. As remarked earlier, the quite special case of seriality versus parallelism is difficult enough to discriminate experimentally. Among such hybrid models are those that represent processing as being serial part of the time and partially parallel within trials. There are many occasions in perceptual and memorial experiments where the information sufficient to make a correct response is embedded in only part of the total stimulus pattern presented to the subject. A finding of independence of total completion times, for instance, although perhaps more intuitively associated with parallel models, can be predicted by serial models.},
  isbn = {978-1-003-17668-8},
  file = {~/Zotfiles/townsend.j1974 Issues and models concerning the process.pdf}
}

@article{townsend.j:1990,
  title = {Serial vs. Parallel Processing: Sometimes They Look like Tweedledum and Tweedledee but They Can (and Should) Be Distinguished},
  shorttitle = {Serial vs. Parallel Processing},
  author = {Townsend, James T.},
  year = {1990},
  month = jan,
  journal = {Psychological science},
  volume = {1},
  number = {1},
  pages = {46--54},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1990.tb00067.x},
  urldate = {2022-06-24},
  abstract = {A number of important models of information processing depend on whether processing is serial or parallel. However, many of the studies purporting to settle the case use weak experimental paradigms or results to draw conclusions. A brief history of the issue is given along with examples from the literature. Then a number of promising methods are presented from a variety of sources with some discussion of their potential. A brief discussion of the topic with regard to overall issues of model testing and applications concludes the paper.},
  langid = {english},
  file = {~/Zotfiles/townsend.j1990 Serial vs. parallel processing sometime.pdf}
}

@article{townsend.j:2004,
  title = {Parallel versus Serial Processing and Individual Differences in High-Speed Search in Human Memory},
  author = {Townsend, James T. and Fifi{\'c}, Mario},
  year = {2004},
  month = aug,
  journal = {Perception \& Psychophysics},
  volume = {66},
  number = {6},
  pages = {953--962},
  issn = {1532-5962},
  doi = {10.3758/BF03194987},
  urldate = {2022-06-24},
  abstract = {Many mental tasks that involve operations on a number of items take place within a few hundred milliseconds. In such tasks, whether the items are processed simultaneously (in parallel) or sequentially (serially) has long been of interest to psychologists. Although certain types of parallel and serial models have been ruled out, it has proven extremely difficult to entirely separate reasonable serial and limitedcapacity parallel models on the basis of typical data. Recent advances in theory-driven methodology now permit strong tests of serial versus parallel processing in such tasks, in ways that bypass the capacity issue and that are distribution and parameter free. We employ new methodologies to assess serial versus parallel processing and find strong evidence for pure serial or pure parallel processing, with some striking apparent differences across individuals and interstimulus conditions.},
  langid = {english},
  keywords = {Memory Search,Parallel Model,Parallel Processing,Serial Processing,Visual Search},
  file = {~/Zotfiles/townsend.j2004 Parallel versus serial processing and in.pdf}
}

@article{traxler.m:1998,
  title = {Adjunct {{Attachment Is Not}} a {{Form}} of {{Lexical Ambiguity Resolution}}},
  author = {Traxler, Matthew J. and Pickering, Martin J. and Clifton, Charles},
  year = {1998},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {39},
  number = {4},
  pages = {558--592},
  issn = {0749-596X},
  doi = {10.1006/jmla.1998.2600},
  urldate = {2025-06-06},
  abstract = {Three eye-tracking experiments investigated ambiguity resolution in sentences containing adjunct modifiers. The experiments tested readers' response to sentences that began with a noun phrase complex containing two nouns and a preposition (oforwith). A prepositional phrase or relative clause modified one of the noun phrases. The sentences were either temporarily or fully ambiguous as to which noun phrase was modified. The first and third experiments used semantic plausibility to disambiguate attachment (when disambiguation was possible). The second experiment used gender agreement to disambiguate attachment. The type of modifier, prepositional phrase versus relative clause, affected processing of the modifier as did the type of preposition in the noun phrase complex, theta-assigning versus non-theta-assigning. The data challenge the idea that syntactic ambiguity resolution is a form of lexical ambiguity resolution achieved via competition (MacDonald, 1994; MacDonald, Pearlmutter, \& Seidenberg, 1994; Spivey-Knowlton \& Sedivy, 1995).},
  keywords = {ambiguity advantage effect}
}

@article{traxler.m:2002,
  title = {Processing Subject and Object Relative Clauses: {{Evidence}} from Eye Movements},
  shorttitle = {Processing Subject and Object Relative Clauses},
  author = {Traxler, Matthew J and Morris, Robin K and Seely, Rachel E},
  year = {2002},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {47},
  number = {1},
  pages = {69--90},
  issn = {0749-596X},
  doi = {10.1006/jmla.2001.2836},
  urldate = {2023-03-09},
  abstract = {Three eye-movement-monitoring experiments investigated processing of sentences containing subject-relative and object-relative clauses. The first experiment showed that sentences containing object-relative clauses were more difficult to process than sentences containing subject-relative clauses during the relative clause and the matrix verb. The second experiment manipulated the plausibility of the sentential subject and the noun within the relative clause as the agent of the action represented by the verb in the relative clause. Readers experienced greater difficulty during processing of sentences containing object-relative clauses than subject-relative clauses. The third experiment manipulated the animacy of the sentential subject and the noun within the relative clause. This experiment demonstrated that the difficulty associated with object-relative clauses was greatly reduced when the sentential subject was inanimate. We interpret the results with respect to theories of syntactic parsing.},
  langid = {english},
  keywords = {parsing,relative clauses,sentence processing,syntax,working memory.},
  file = {~/Zotfiles/traxler.m2002 Processing subject and object relative c.pdf}
}

@inproceedings{trevisan.l:2009,
  title = {Regularity, {{Boosting}}, and {{Efficiently Simulating Every High-Entropy Distribution}}},
  booktitle = {2009 24th {{Annual IEEE Conference}} on {{Computational Complexity}}},
  author = {Trevisan, Luca and Tulsiani, Madhur and Vadhan, Salil},
  year = {2009},
  month = jul,
  pages = {126--136},
  issn = {1093-0159},
  doi = {10.1109/CCC.2009.41},
  abstract = {We show that every bounded function g: 0,1n rarr [0,1] admits an efficiently computable "simulator" function h: 0,1n rarr [0,1] such that every fixed polynomial size circuit has approximately the same correlation with g as with h. If g describes (up to scaling) a high min-entropy distribution D, then h can be used to efficiently sample a distribution D' of the same min-entropy that is indistinguishable from D by circuits of fixed polynomial size. We state and prove our result in a more abstract setting, in which we allow arbitrary finite domains instead of 0,1n, and arbitrary families of distinguishers, instead of fixed polynomial size circuits. Our result implies (a) the weak Szemeredi regularity Lemma of Frieze and Kannan (b) a constructive version of the dense model theorem of Green, Tao and Ziegler with better quantitative parameters (polynomial rather than exponential in the distinguishing probability), and (c) the Impagliazzo hardcore set Lemma. It appears to be the general result underlying the known connections between "regularity" results in graph theory, "decomposition" results in additive combinatorics, and the hardcore Lemma in complexity theory. We present two proofs of our result, one in the spirit of Nisan's proof of the hardcore Lemma via duality of linear programming, and one similar to Impagliazzo's "boosting" proof. A third proof by iterative partitioning, which gives the complexity of the sampler to be exponential in the distinguishing probability, is also implicit in the Green-Tao-Ziegler proofs of the dense model theorem.},
  keywords = {additive combinatorics,average-case complexity,boosting,Boosting,Circuit simulation,Combinatorial mathematics,Complexity theory,Computational complexity,Computational modeling,Computer science,Computer simulation,Graph theory,Polynomials,pseudorandomness},
  file = {~/Zotfiles/trevisan.l2009 Regularity, Boosting, and Efficiently Si.pdf}
}

@article{tribus.m:1961,
  title = {Information Theory as the Basis for Thermostatics and Thermodynamics},
  author = {Tribus, Myron},
  year = {1961},
  month = mar,
  journal = {Journal of Applied Mechanics},
  volume = {28},
  number = {1},
  pages = {1--8},
  issn = {0021-8936, 1528-9036},
  doi = {10.1115/1.3640461},
  abstract = {Information theory described in Part 1 provides a meaning for the concept of entropy independent of the field of thermodynamics. Using this meaning (uncertainty) it is possible to derive all of statistical and classical thermodynamics in a direct and simple way. Many of the concepts and definitions of classical thermodynamics are given a new interpretation. Using information theory as a basis for a statistical description of an open system, in Part 2, the laws and theorems of thermodynamics are seen to follow in a simple way. The coupling of irreversible flows (Onsager's relation) is seen as a natural connection between thermostatics and thermodynamics if the functions introduced by Massieu are used instead of those introduced by Gibbs.},
  langid = {english},
  file = {~/Zotfiles/tribus.m1961 Information Theory as the Basis for Ther.pdf}
}

@book{triggs.b:2019book,
  title = {Accordion Revolution: A People's History of the Accordion in {{North America}} from the Industrial Revolution to Rock 'n' Roll},
  shorttitle = {Accordion Revolution},
  author = {Triggs, Bruce},
  year = {2019},
  publisher = {Demian \& Sons Publications},
  address = {Canada},
  abstract = {"Before the dawn of rock 'n' roll, the accordion ranked among North America's most popular instruments. Arriving in the arms of immigrants, nearly every ethnicity on the continent played the squeezebox: Irish, Scottish, French, German, Eastern European, Latino, Jewish. The instrument packed barn dances, jazz clubs, and recital halls, and was heard in norte{\~n}o groups on the Mexican frontier; Creole string bands in New Orleans, and Inuit square dances above the Arctic Circle. Portable, cheap, and loud, accordions became the soundtrack for modernity as the music industry exploited them on records, radio, film, and television. Millions of people played accordions until a disastrous combination of economics, demographics, and electronic instruments nearly erased them from mainstream culture. Emerging from exile with a new generation of followers, this book invites beginner or seasoned accordionists and music fans in general to rediscover a forgotten legion of little-known artists. With an eye for colorful characters and a sharp sense of humor, accordion historian Bruce Triggs uncovers the hidden back-story of the squeezebox in everyone's closet"--Back cover},
  isbn = {978-1-9990677-0-0},
  langid = {english},
  annotation = {OCLC: 1117470211},
  file = {~/Zotfiles/triggs.b2019book Accordion revolution a people's history.pdf}
}

@book{trueswell.j:2005book,
  title = {Approaches to Studying World-Situated Language Use: Bridging the Language-as-Product and Language-as-Action Traditions},
  shorttitle = {Approaches to Studying World-Situated Language Use},
  author = {Trueswell, John C. and Tanenhaus, Michael K.},
  year = {2005},
  publisher = {MIT Press},
  abstract = {Recent approaches to language processing have focused either on individual cognitive processes in producing and understanding language or on social cognitive factors in interactive conversation. Although the cognitive and social approaches to language processing would seem to have little theoretical or methodological common ground, the goal of this book is to encourage the merging of these two traditions. The contributors to this volume hope to demonstrate that attention to both cognitive and social approaches is important for understanding how language is processed in natural settings.The book opens with four review/position papers; these are followed by shorter reports of experimental findings--"a snapshot of current work that begins to bridge the product and action traditions." These treat linguistic processing issues in conversational settings, the interactions of language and nonlinguistic information from visual scenes, product approaches to issues traditionally discussed in the action tradition, and Gricean phenomena.},
  isbn = {978-0-262-70104-4},
  langid = {english},
  keywords = {Language Arts & Disciplines / Linguistics / General,Language Arts & Disciplines / Linguistics / Psycholinguistics / General,Psychology / Cognitive Psychology & Cognition,Psychology / General}
}

@inproceedings{tsipidi.e:2024,
  title = {Surprise! {{Uniform Information Density Isn}}'t the {{Whole Story}}: {{Predicting Surprisal Contours}} in {{Long-form Discourse}}},
  shorttitle = {Surprise! {{Uniform Information Density Isn}}'t the {{Whole Story}}},
  booktitle = {Proceedings of the 2024 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Tsipidi, Eleftheria and Nowak, Franz and Cotterell, Ryan and Wilcox, Ethan and Giulianelli, Mario and Warstadt, Alex},
  editor = {{Al-Onaizan}, Yaser and Bansal, Mohit and Chen, Yun-Nung},
  year = {2024},
  month = nov,
  pages = {18820--18836},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA},
  doi = {10.18653/v1/2024.emnlp-main.1047},
  urldate = {2025-06-11},
  abstract = {The Uniform Information Density (UID) hypothesis posits that speakers tend to distribute information evenly across linguistic units to achieve efficient communication. Of course, information rate in texts and discourses is not perfectly uniform. While these fluctuations can be viewed as theoretically uninteresting noise on top of a uniform target, another explanation is that UID is not the only functional pressure regulating information content in a language. Speakers may also seek to maintain interest, adhere to writing conventions, and build compelling arguments. In this paper, we propose one such functional pressure; namely that speakers modulate information rate based on location within a hierarchically-structured model of discourse. We term this the Structured Context Hypothesis and test it by predicting the surprisal contours of naturally occurring discourses extracted from large language models using predictors derived from discourse structure. We find that hierarchical predictors are significant predictors of a discourse's information contour and that deeply nested hierarchical predictors are more predictive than shallow ones. This work takes an initial step beyond UID to propose testable hypotheses for why the information rate fluctuates in predictable ways.},
  file = {~/Zotfiles/tsipidi.e2024 Surprise! Uniform Information Density Is.pdf}
}

@misc{tucker.g:2018arxiv,
  title = {Doubly {{Reparameterized Gradient Estimators}} for {{Monte Carlo Objectives}}},
  author = {Tucker, George and Lawson, Dieterich and Gu, Shixiang and Maddison, Chris J.},
  year = {2018},
  month = nov,
  number = {arXiv:1810.04152},
  eprint = {1810.04152},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1810.04152},
  urldate = {2025-08-12},
  abstract = {Deep latent variable models have become a popular model choice due to the scalable learning algorithms introduced by (Kingma \& Welling, 2013; Rezende et al., 2014). These approaches maximize a variational lower bound on the intractable log likelihood of the observed data. Burda et al. (2015) introduced a multi-sample variational bound, IWAE, that is at least as tight as the standard variational lower bound and becomes increasingly tight as the number of samples increases. Counterintuitively, the typical inference network gradient estimator for the IWAE bound performs poorly as the number of samples increases (Rainforth et al., 2018; Le et al., 2018). Roeder et al. (2017) propose an improved gradient estimator, however, are unable to show it is unbiased. We show that it is in fact biased and that the bias can be estimated efficiently with a second application of the reparameterization trick. The doubly reparameterized gradient (DReG) estimator does not suffer as the number of samples increases, resolving the previously raised issues. The same idea can be used to improve many recently introduced training techniques for latent variable models. In particular, we show that this estimator reduces the variance of the IWAE gradient, the reweighted wake-sleep update (RWS) (Bornschein \& Bengio, 2014), and the jackknife variational inference (JVI) gradient (Nowozin, 2018). Finally, we show that this computationally efficient, unbiased drop-in gradient estimator translates to improved performance for all three objectives on several modeling tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {~/Zotfiles/tucker.g2018arxiv Doubly Reparameterized Gradient Estimato.pdf}
}

@article{turing.a:1937,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}},
  author = {Turing, A. M.},
  year = {1937},
  journal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  issn = {1460-244X},
  doi = {10.1112/plms/s2-42.1.230},
  urldate = {2023-05-20},
  langid = {english},
  file = {~/Zotfiles/turing.a1937TuringMachine On computable numbers, with an applicati.pdf}
}

@article{turing.a:1938,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}. {{A}} Correction},
  author = {Turing, A. M.},
  year = {1938},
  journal = {Proceedings of the London Mathematical Society},
  volume = {s2-43},
  number = {1},
  pages = {544--546},
  issn = {1460-244X},
  doi = {10.1112/plms/s2-43.6.544},
  urldate = {2023-05-20},
  langid = {english},
  file = {~/Zotfiles/turing.a1938TuringMachineCorrection On computable numbers, with an applicati.pdf}
}

@article{turing.a:1950,
  title = {Computing Machinery and Intelligence},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2023-05-20},
  file = {~/Zotfiles/turing.a1950TuringTest Computing machinery and intelligence.pdf}
}

@article{tversky.a:1971,
  title = {Belief in the Law of Small Numbers.},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/h0031322},
  bdsk-url-2 = {https://doi.org/10.1037/h0031322},
  date-added = {2021-08-08 20:24:01 -0400},
  date-modified = {2021-08-08 20:24:02 -0400}
}

@article{upper.d:1974,
  title = {The {{Unsuccessful Self-Treatment}} of a {{Case}} of ``{{Writer}}'s {{Block}}''1},
  author = {Upper, Dennis},
  year = {1974},
  journal = {Journal of Applied Behavior Analysis},
  volume = {7},
  number = {3},
  pages = {497--497},
  issn = {1938-3703},
  doi = {10.1901/jaba.1974.7-497a},
  urldate = {2022-06-16},
  langid = {english},
  keywords = {humor,writer's block},
  file = {~/Zotfiles/upper.d1974 The Unsuccessful Self-Treatment of a Cas.pdf}
}

@article{ussery.c:2017,
  title = {Dimensions of Variation},
  author = {Ussery, Cherlon},
  year = {2017},
  journal = {Syntactic variation in insular Scandinavian},
  volume = {1},
  pages = {165},
  publisher = {John Benjamins Publishing Company},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:38:48 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,syntactic variation}
}

@article{vandemeerendonk.n:2011,
  title = {Monitoring in Language Perception: {{Electrophysiological}} and Hemodynamic Responses to Spelling Violations},
  shorttitle = {Monitoring in Language Perception},
  author = {{van de Meerendonk}, Nan and Indefrey, Peter and Chwilla, Dorothee J. and Kolk, Herman H. J.},
  year = {2011},
  month = feb,
  journal = {NeuroImage},
  volume = {54},
  number = {3},
  pages = {2350--2363},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.022},
  urldate = {2022-06-24},
  abstract = {The monitoring theory of language perception proposes that competing representations that are caused by strong expectancy violations can trigger a conflict which elicits reprocessing of the input to check for possible processing errors. This monitoring process is thought to be reflected by the P600 component in the EEG. The present study further investigated this monitoring process by comparing syntactic and spelling violations in an EEG and an fMRI experiment. To assess the effect of conflict strength, misspellings were embedded in sentences that were weakly or strongly predictive of a critical word. In support of the monitoring theory, syntactic and spelling violations elicited similarly distributed P600 effects. Furthermore, the P600 effect was larger to misspellings in the strongly compared to the weakly predictive sentences. The fMRI results showed that both syntactic and spelling violations increased activation in the left inferior frontal gyrus (lIFG), while only the misspellings activated additional areas. Conflict strength did not affect the hemodynamic response to spelling violations. These results extend the idea that the lIFG is involved in implementing cognitive control in the presence of representational conflicts in general to the processing of errors in language perception.},
  langid = {english},
  keywords = {Cognitive control,Conflict,Left inferior frontal gyrus,P600,Reprocessing}
}

@misc{vandenoord.a:2018,
  title = {Representation Learning with Contrastive Predictive Coding},
  author = {{van den Oord}, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  eprint = {1807.03748},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:07:24 -0400},
  date-modified = {2019-07-05 11:08:29 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,representation learning}
}

@article{vandermude.a:1978,
  title = {On the Inference of Stochastic Regular Grammars},
  author = {{Van der Mude}, Antony and Walker, Adrian},
  year = {1978},
  journal = {Information and Control},
  volume = {38},
  number = {3},
  pages = {310--329},
  publisher = {Elsevier},
  doi = {10.1016/S0019-9958(78)90106-7},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:33:31 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information}
}

@article{vandyke.j:2006,
  title = {Retrieval Interference in Sentence Comprehension},
  author = {Van Dyke, Julie A. and McElree, Brian},
  year = {2006},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {55},
  number = {2},
  pages = {157--166},
  issn = {0749596X},
  doi = {10.1016/j.jml.2006.03.007},
  urldate = {2022-08-13},
  langid = {english},
  file = {~/Zotfiles/vandyke.j2006 Retrieval interference in sentence compr.pdf}
}

@article{vanerven.t:2014,
  title = {R{\'e}nyi Ivergence and {{Kullback-Leibler}} Divergence},
  author = {{van Erven}, Tim and Harremos, Peter},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {60},
  number = {7},
  pages = {3797--3820},
  issn = {1557-9654},
  doi = {10.1109/TIT.2014.2320500},
  abstract = {R{\'e}nyi divergence is related to R{\'e}nyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by R{\'e}nyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the R{\'e}nyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of R{\'e}nyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of {\textbackslash}({\textbackslash}sigma {\textbackslash}) -algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results.},
  keywords = {alpha-divergence,Bhattacharyya distance,Convergence,Data processing,Entropy,information divergence,Kullback-Leibler divergence,Markov processes,Pythagorean inequality,Q measurement,Renyi divergence},
  file = {~/Zotfiles/vanerven.t2014 Rnyi ivergence and Kullback-Leibler div.pdf}
}

@article{vani.p:2021cogsci,
  title = {Using the Interpolated Maze Task to Assess Incremental Processing in {{English}} Relative Clauses},
  author = {Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger},
  year = {2021},
  journal = {Proceedings of the 43rd Annual Meeting of the Cognitive Science Society},
  urldate = {2023-03-09},
  abstract = {In English, Subject Relative Clauses are processed more quickly than Object Relative Clauses, but open questions remain about where in the clause slowdown occurs. The surprisal theory of incremental processing, under which processing difficulty corresponds to probabilistic expectations about upcoming material, predicts that slowdown should occur immediately on material that disambiguates the subject from object relative clause. However, evidence from eye tracking and self-paced reading studies suggests that slowdown occurs downstream of RC-disambiguating material, on the relative clause verb. These methods, however, suffer from well-known spillover effects which makes their results difficult to interpret. To address these issues, we introduce and deploy a novel variant of the Maze task for reading times (Forster, Guerrera, \&amp; Elliot, 2009), called the Interpolated Maze in two English web-based experiments. In Experiment 1, we find that the locus of reading-time differences between SRCs and ORCs falls on immediate disambiguating definite determiner. Experiment 2 provides a control, showing that ORCs are read more slowly than lexically-matching, non-anomalous material. These results provide new evidence for the locus of processing difficulty in relative clauses and support the surprisal theory of incremental processing.},
  langid = {english},
  file = {~/Zotfiles/vani.p2021 Using the interpolated maze task to asse.pdf}
}

@article{vanoostendorp.h:1990,
  title = {Moses Beats {{Adam}}: {{A}} Semantic Relatedness Effect on a Semantic Illusion},
  shorttitle = {Moses Beats {{Adam}}},
  author = {Van Oostendorp, Herre and De Mul, Sjaak},
  year = {1990},
  month = jun,
  journal = {Acta Psychologica},
  volume = {74},
  number = {1},
  pages = {35--46},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(90)90033-C},
  urldate = {2024-05-28},
  abstract = {The purpose of the present study is to investigate the role of semantic relatedness in the occurence of semantic illusions like the Moses illusion (first described by Erickson and Mattson 1981). This illusion is investigated by using statements with inaccurate proper names varying in degree of overlap in attributes with target names and by registering judgment times. The results show a clear effect of semantic relatedness. Inaccuracies more often appeared to be left unnoticed in high-related statements than in low-related statements. Furthermore, the results of the corresponding judgment times indicate that judging statements with high-related inaccurate names need the same amount of time as judging statements with low-related names. However, more errors are made in this same amount of time, which indicates that high-related statements are processed less extensively. Finally, to achieve a precise judgment of high-related statements takes more time compared with low-related statements. In other words, more time is needed to unmask a high-related inaccuracy.},
  file = {~/Zotfiles/vanoostendorp.h1990a Moses beats Adam A semantic relatedness.pdf}
}

@article{vanoostendorp.h:1990a,
  title = {Failing to Notice Errors in Sentences},
  author = {{van Oostendorp}, Herre and Kok, Ineke},
  year = {1990},
  month = apr,
  journal = {Language and Cognitive Processes},
  volume = {5},
  number = {2},
  pages = {105--113},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/01690969008402100},
  urldate = {2024-05-28},
  abstract = {The purpose of the present study was to investigate the role of conceptual relatedness of proper names in failure to notice errors in sentences. We hypothesised that sentences with highly related, but inaccurate proper names may lead to less complete processing than sentences with less related proper names, and hence to failure to notice factual inaccuracies. This hypothesis was investigated in two ways: first, by using inaccurate proper names varying in degree of overlap in attributes with target names; and, secondly, by using a paired-associate learning task in order to strengthen relations. The results indicate that greater overlap between proper names and stronger relations lead to more frequent failures to notice errors in sentences.},
  file = {~/Zotfiles/vanoostendorp.h1990 Failing to notice errors in sentences.pdf}
}

@inproceedings{varda.a:2023,
  title = {Scaling in {{Cognitive Modelling}}: A {{Multilingual Approach}} to {{Human Reading Times}}},
  shorttitle = {Scaling in {{Cognitive Modelling}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {{de Varda}, Andrea and Marelli, Marco},
  year = {2023},
  month = jul,
  pages = {139--149},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-11},
  abstract = {Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.},
  keywords = {surprisal theory},
  file = {~/Zotfiles/devarda.a2023 Scaling in Cognitive Modelling a Multil.pdf}
}

@article{varda.a:2024aligned,
  title = {Cloze Probability, Predictability Ratings, and Computational Estimates for 205 {{English}} Sentences, Aligned with Existing {{EEG}} and Reading Time Data},
  author = {{de Varda}, Andrea Gregor and Marelli, Marco and Amenta, Simona},
  year = {2024},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {5},
  pages = {5190--5213},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02261-8},
  urldate = {2024-10-17},
  abstract = {We release a database of cloze probability values, predictability ratings, and computational estimates for a sample of 205 English sentences (1726 words), aligned with previously released word-by-word reading time data (both self-paced reading and eye-movement records; Frank et al., Behavior Research Methods, 45(4), 1182--1190. 2013) and EEG responses (Frank et al., Brain and Language, 140, 1--11. 2015). Our analyses show that predictability ratings are the best predictors of the EEG signal (N400, P600, LAN) self-paced reading times, and eye movement patterns, when spillover effects are taken into account. The computational estimates are particularly effective at explaining variance in the eye-tracking data without spillover. Cloze probability estimates have decent overall psychometric accuracy and are the best predictors of early fixation patterns (first fixation duration). Our results indicate that the choice of the best measurement of word predictability in context critically depends on the processing index being considered.},
  langid = {english},
  keywords = {Cloze probability,Predictability ratings,Prediction,Surprisal estimates},
  file = {~/Zotfiles/varda.a2024aligned Cloze probability, predictability rating.pdf}
}

@inproceedings{vasishth.s:2006,
  title = {On the Proper Treatment of Spillover in Real-Time Reading Studies: {{Consequences}} for Psycholinguistic Theories},
  booktitle = {Proceedings of the International Conference on Linguistic Evidence},
  author = {Vasishth, Shravan},
  year = {2006},
  pages = {96--100}
}

@article{vasishth.s:2006a,
  title = {Argument-Head Distance and Processing Complexity: Explaining Both Locality and Antilocality Effects},
  author = {Vasishth, Shravan and Lewis, Richard L.},
  year = {2006},
  journal = {Language},
  volume = {82},
  number = {4},
  eprint = {4490268},
  eprinttype = {jstor},
  pages = {767--794},
  publisher = {Linguistic Society of America},
  issn = {00978507, 15350665},
  abstract = {Although proximity between arguments and verbs (locality) is a relatively robust determinant of sentence-processing difficulty (Hawkins 1998, 2001, Gibson 2000), increasing argument-verb distance can also facilitate processing (Konieczny 2000). We present two self-paced reading (SPR) experiments involving Hindi that provide further evidence of antilocality, and a third SPR experiment which suggests that similarity-based interference can attenuate this distance-based facilitation. A unified explanation of interference, locality, and antilocality effects is proposed via an independently motivated theory of activation decay and retrieval interference (Anderson et al. 2004).},
  date-added = {2022-03-31 11:51:04 -0400},
  date-modified = {2022-03-31 11:52:05 -0400},
  keywords = {antilocality effects,Dependency locality theory,locality effects,processing,processing complexity,self-paced reading}
}

@article{vasishth.s:2010,
  title = {Short-Term Forgetting in Sentence Comprehension: Crosslinguistic Evidence from Verb-Final Structures},
  author = {Vasishth, Shravan and Suckow, Katja and Lewis, Richard L. and Kern, Sabine},
  year = {2010},
  month = may,
  journal = {Language and Cognitive Processes},
  volume = {25},
  number = {4},
  pages = {533--567},
  publisher = {Informa UK Limited},
  doi = {10.1080/01690960903310587},
  bdsk-url-2 = {https://doi.org/10.1080/01690960903310587},
  date-added = {2022-04-19 22:48:38 -0400},
  date-modified = {2022-04-19 22:48:39 -0400}
}

@article{vasishth.s:2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
  year = {2018},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  urldate = {2022-07-01},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  langid = {english},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error},
  file = {~/Zotfiles/vasishth.s2018 The statistical significance filter lead.pdf}
}

@article{vasishth.s:2019,
  title = {Computational Models of Retrieval Processes in Sentence Processing},
  author = {Vasishth, Shravan and Nicenboim, Bruno and Engelmann, Felix and Burchert, Frank},
  year = {2019},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {968--982},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.09.003},
  urldate = {2022-08-13},
  langid = {english},
  file = {~/Zotfiles/vasishth.s2019 Computational models of retrieval proces.pdf}
}

@incollection{vasishth.s:2021,
  title = {The Core {{ACT-R-based}} Model of Retrieval Processes},
  booktitle = {Sentence {{Comprehension}} as a {{Cognitive Process}}: {{A Computational Approach}}},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  pages = {49--70},
  publisher = {Cambridge University Press},
  address = {Cambridge, England},
  doi = {10.1017/9781316459560.008},
  urldate = {2022-10-12},
  abstract = {The core model of sentence processing used in the book is introduced and its empirical coverage relative to the existing reading time data is considered. Here, we also discuss the Approximate Bayesian Computation method for parameter estimation for model evaluation.},
  isbn = {978-1-107-13311-2},
  keywords = {ACT-R,cue-based retrieval,parsing,psycholinguistics,sentence comprehension},
  file = {~/Zotfiles/vasishth.s2021bookch3 The core ACT-R-based model of retrieval.pdf}
}

@book{vasishth.s:2021book,
  title = {Sentence Comprehension as a Cognitive Process: A Computational Approach},
  shorttitle = {Sentence Comprehension as a Cognitive Process},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  month = oct,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781316459560},
  urldate = {2022-10-12},
  abstract = {Sentence comprehension - the way we process and understand spoken and written language - is a central and important area of research within psycholinguistics. This book explores the contribution of computational linguistics to the field, showing how computational models of sentence processing can help scientists in their investigation of human cognitive processes. It presents the leading computational model of retrieval processes in sentence processing, the Lewis and Vasishth cue-based retrieval mode, and develops a principled methodology for parameter estimation and model comparison/evaluation using benchmark data, to enable researchers to test their own models of retrieval against the present model. It also provides readers with an overview of the last 20 years of research on the topic of retrieval processes in sentence comprehension, along with source code that allows researchers to extend the model and carry out new research. Comprehensive in its scope, this book is essential reading for researchers in cognitive science.},
  isbn = {978-1-316-45956-0}
}

@inproceedings{vaswani.a:2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, {{December}} 4-9, 2017, {{Long Beach}}, {{CA}}, {{USA}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {5998--6008},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{verdu.s:1998,
  title = {Fifty Years of {{Shannon}} Theory},
  author = {Verdu, S.},
  year = {1998},
  journal = {IEEE Transactions on information theory},
  volume = {44},
  number = {6},
  pages = {2057--2078},
  issn = {1557-9654},
  doi = {10.1109/18.720531},
  abstract = {A brief chronicle is given of the historical development of the central problems in the theory of fundamental limits of data compression and reliable communication.},
  date-added = {2020-08-17 11:37:37 -0400},
  date-modified = {2020-08-17 11:39:25 -0400},
  project = {information-entropy},
  keywords = {channel capacity,data compression,information theory,rate distortion theory,source coding}
}

@misc{vieira.t:2014blog,
  type = {Blog},
  title = {Gumbel-Max Trick and Weighted Reservoir Sampling},
  author = {Vieira, Tim},
  year = {2014},
  month = aug,
  journal = {Graduate Descent},
  urldate = {2022-11-06},
  howpublished = {https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/}
}

@misc{vieira.t:2017,
  type = {Blog},
  title = {Estimating Means in a Finite Universe},
  author = {Vieira, Tim},
  year = {2017},
  month = jul,
  journal = {Graduate Descent},
  urldate = {2025-02-25},
  langid = {english},
  keywords = {gumbel-max trick reservoir sampling sampling}
}

@article{vieira.t:2017a,
  title = {Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing},
  author = {Vieira, Tim and Eisner, Jason},
  year = {2017},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {263--278},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00060},
  abstract = {Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the lols algorithm. lols training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier---i.e., parsers which are more accurate for a given runtime.},
  keywords = {chart parsing,pruning},
  file = {~/Zotfiles/vieira.t2017 Learning to prune exploring the frontie.pdf}
}

@misc{vieira.t:2024arxiv,
  title = {From Language Models over Tokens to Language Models over Characters},
  author = {Vieira, Tim and LeBrun, Ben and Giulianelli, Mario and Gastaldi, Juan Luis and DuSell, Brian and Terilla, John and O'Donnell, Timothy J. and Cotterell, Ryan},
  year = {2024},
  month = dec,
  number = {arXiv:2412.03719},
  eprint = {2412.03719},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2412.03719},
  urldate = {2024-12-06},
  abstract = {Modern language models are internally -- and mathematically -- distributions over token strings rather than {\textbackslash}emph\{character\} strings, posing numerous challenges for programmers building user applications on top of them. For example, if a prompt is specified as a character string, it must be tokenized before passing it to the token-level language model. Thus, the tokenizer and consequent analyses are very sensitive to the specification of the prompt (e.g., if the prompt ends with a space or not). This paper presents algorithms for converting token-level language models to character-level ones. We present both exact and approximate algorithms. In the empirical portion of the paper, we benchmark the practical runtime and approximation quality. We find that -- even with a small computation budget -- our method is able to accurately approximate the character-level distribution (less than 0.00021 excess bits / character) at reasonably fast speeds (46.3 characters / second) on the Llama 3.1 8B language model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {comment: ICML 2025},
  file = {~/Zotfiles/vieira.t2024arxiv From Language Models over Tokens to Lang.pdf}
}

@inproceedings{vieira.t:2025,
  title = {Language {{Models}} over {{Canonical Byte-Pair Encodings}}},
  booktitle = {Forty-Second {{International Conference}} on {{Machine Learning}}},
  author = {Vieira, Tim and Liu, Tianyu and Pasti, Clemente and Emara, Yahya and DuSell, Brian and LeBrun, Benjamin and Giulianelli, Mario and Gastaldi, Juan Luis and O'Donnell, Timothy J. and Cotterell, Ryan},
  year = {2025},
  month = jun,
  urldate = {2025-09-09},
  abstract = {Modern language models represent probability distributions over character strings as distributions over (shorter) token strings derived via a deterministic tokenizer, such as byte-pair encoding. While this approach is highly effective at scaling up language models to large corpora, its current incarnations have a concerning property: the model assigns nonzero probability mass to an exponential number of *noncanonical* token encodings of each character string---these are token strings that decode to valid character strings but are impossible under the deterministic tokenizer (i.e., they will never be seen in any training corpus, no matter how large). This misallocation is both erroneous, as noncanonical strings never appear in training data, and wasteful, diverting probability mass away from plausible outputs. These are avoidable mistakes! In this work, we propose methods to enforce canonicality in token-level language models, ensuring that only canonical token strings are assigned positive probability. We present two approaches: (1) canonicality by conditioning, leveraging test-time inference strategies without additional training, and (2) canonicality by construction, a model parameterization that guarantees canonical outputs but requires training. We demonstrate that fixing canonicality mistakes improves the likelihood of held-out data for several models and corpora.},
  langid = {english},
  file = {~/Zotfiles/vieira.t2025 Language Models over Canonical Byte-Pair.pdf}
}

@inproceedings{vieira.t:2025a,
  title = {From Language Models over Tokens to Language Models over Characters},
  booktitle = {Proceedings of the 42nd International Conference on Machine Learning},
  author = {Vieira, Tim and LeBrun, Benjamin and Giulianelli, Mario and Gastaldi, Juan Luis and DuSell, Brian and Terilla, John and O'Donnell, Timothy J. and Cotterell, Ryan},
  year = {2025},
  series = {Proceedings of Machine Learning Research},
  publisher = {PMLR}
}

@misc{vigly.j:2025cogsci,
  type = {Poster},
  title = {When Unpredictable Does Not Mean Difficult to Process},
  author = {Vigly, Jacob Hoover and Qian, Peng and Sonderegger, Morgan and O'Donnell, Timothy J.},
  year = {2025},
  month = jul,
  address = {San Francisco, CA}
}

@misc{vigly.j:2025HSP,
  type = {Poster},
  title = {Implicit Gender Bias in Linguistic Descriptions for Expected Events: {{The}} Case of the 2024 {{United States}} Presidential Election},
  author = {Vigly, Jacob Hoover and Boyce, Veronica and Socolof, Michaela and Michaelov, James and {von der Malsburg}, Titus and Levy, Roger},
  year = {2025},
  month = mar,
  address = {University of Maryland, College Park},
  keywords = {comprehension,gender bias,gender stereotypes,implicit bias,poster,production,pronouns}
}

@misc{vigly.j:2025psyarxiv,
  title = {Comprehension Effort as the Cost of Inference},
  author = {Vigly, Jacob Hoover and Qian, Peng and Sonderegger, Morgan and O'Donnell, Timothy J.},
  year = {2025},
  month = jun,
  number = {2498w},
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/2498w_v1},
  urldate = {2025-06-18},
  abstract = {As you read this text, word by word, you build an understanding of its meaning. What cognitive mechanisms underlie this ability?  An influential approach to answering this question comes from viewing comprehension as probabilistic inference over potential interpretations given linguistic input. Motivated within this perspective, a wealth of previous literature in psycholinguistics has focused on an important empirical relationship made precise by surprisal theory (Hale, 2001; Levy, 2008a), the hypothesis that the effort required to process a word scales in its negative log probability, in context. However, the standard derivation of surprisal within the inference framework relies on a crucial assumption: that there is a deterministic relationship between the latent interpretations targeted by inference and the observable input. In this work we propose relaxing this assumption and formalize inference cost directly as the amount of change in probabilistic beliefs. This proposal forms a nontrivial generalization of standard surprisal theory, which provides a more direct connection to algorithmic theories, and naturally explains phenomena where unpredictable input requires little processing effort. To test this framework against surprisal theory, we conduct a self-paced reading time study targeting words with orthographic errors, a specific setting where our approach predicts substantially different patterns. We find that processing effort follows the predictions of belief-update rather than surprisal, in a noisy-channel model of comprehension as inference about intended words. These results demonstrate a clear case where surface surprisal cannot explain human processing cost, and provide further support for models of language comprehension as rational inference.},
  langid = {american},
  file = {~/Zotfiles/vigly.j2025psyarxiv Comprehension effort as the cost of infe.pdf}
}

@inproceedings{vijayakumar.a:2018,
  title = {Diverse Beam Search for Improved Description of Complex Scenes},
  booktitle = {Proceedings of the Thirty-Second {{AAAI}} Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth {{AAAI}} Symposium on Educational Advances in Artificial Intelligence},
  author = {Vijayakumar, Ashwin K. and Cogswell, Michael and Selvaraju, Ramprasaath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  year = {2018},
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  publisher = {AAAI Press},
  address = {New Orleans, Louisiana, USA},
  abstract = {A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse -- with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks -- image captioning and visual question generation -- particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.},
  articleno = {903},
  date-added = {2022-03-25 22:32:37 -0400},
  date-modified = {2022-03-25 22:34:11 -0400},
  isbn = {978-1-57735-800-8}
}

@misc{vilnis.l:2022arxiv,
  title = {Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models},
  shorttitle = {Arithmetic Sampling},
  author = {Vilnis, Luke and Zemlyanskiy, Yury and Murray, Patrick and Passos, Alexandre and Sanghai, Sumit},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15458},
  eprint = {2210.15458},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.15458},
  urldate = {2022-11-07},
  abstract = {Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, showing substantially reduced variance when estimating expected BLEU score and up to 1 point increased BLEU in oracle experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {~/Zotfiles/vilnis.l2022 Arithmetic sampling parallel diverse de.pdf}
}

@article{vincent.p:2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  month = dec,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  pages = {3371--3408},
  publisher = {JMLR.org},
  issn = {1532-4435},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  issue_date = {3/1/2010}
}

@inproceedings{vinyals.o:2015,
  title = {Grammar as a Foreign Language},
  booktitle = {Advances in Neural Information Processing Systems 28: {{Annual}} Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey E.},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  year = {2015},
  pages = {2773--2781},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VinyalsKKPSH15.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{voita.e:2020,
  title = {Information-Theoretic Probing with Minimum Description Length},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  pages = {183--196},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.14},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.14}
}

@article{vondermalsburg.t:2011,
  title = {What Is the Scanpath Signature of Syntactic Reanalysis?},
  author = {{von der Malsburg}, Titus and Vasishth, Shravan},
  year = {2011},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {65},
  number = {2},
  pages = {109--127},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2011.02.004},
  urldate = {2024-10-10},
  abstract = {Which repair strategy does the language system deploy when it gets garden-pathed, and what can regressive eye movements in reading tell us about reanalysis strategies? Several influential eye-tracking studies on syntactic reanalysis (Frazier and Rayner, 1982, Meseguer et al., 2002, Mitchell et al., 2008) have addressed this question by examining scanpaths, i.e., sequential patterns of eye fixations. However, in the absence of a suitable method for analyzing scanpaths, these studies relied on simplified dependent measures that are arguably ambiguous and hard to interpret. We address the theoretical question of repair strategy by developing a new method that quantifies scanpath similarity. Our method reveals several distinct fixation strategies associated with reanalysis that went undetected in a previously published data set (Meseguer et al., 2002). One prevalent pattern suggests re-parsing of the sentence, a strategy that has been discussed in the literature (Frazier \& Rayner, 1982); however, readers differed tremendously in how they orchestrated the various fixation strategies. Our results suggest that the human parsing system non-deterministically adopts different strategies when confronted with the disambiguating material in garden-path sentences.},
  keywords = {Eye movements,Individual differences,Parsing,Reading,Scanpaths,Syntactic reanalysis}
}

@article{vondermalsburg.t:2020,
  title = {Implicit Gender Bias in Linguistic Descriptions for Expected Events: The Cases of the 2016 {{United States}} and 2017 {{United Kingdom}} Elections},
  shorttitle = {Implicit Gender Bias in Linguistic Descriptions for Expected Events},
  author = {{von der Malsburg}, Titus and Poppels, Till and Levy, Roger P.},
  year = {2020},
  month = feb,
  journal = {Psychological Science},
  volume = {31},
  number = {2},
  pages = {115--128},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797619890619},
  urldate = {2024-10-22},
  abstract = {Gender stereotypes influence subjective beliefs about the world, and this is reflected in our use of language. But do gender biases in language transparently reflect subjective beliefs? Or is the process of translating thought to language itself biased? During the 2016 United States (N = 24,863) and 2017 United Kingdom (N = 2,609) electoral campaigns, we compared participants' beliefs about the gender of the next head of government with their use and interpretation of pronouns referring to the next head of government. In the United States, even when the female candidate was expected to win, she pronouns were rarely produced and induced substantial comprehension disruption. In the United Kingdom, where the incumbent female candidate was heavily favored, she pronouns were preferred in production but yielded no comprehension advantage. These and other findings suggest that the language system itself is a source of implicit biases above and beyond previously known biases, such as those measured by the Implicit Association Test.},
  langid = {english},
  file = {~/Zotfiles/malsburg.t2020 Implicit Gender Bias in Linguistic Descr 2.pdf;~/Zotfiles/malsburg.t2020 Implicit Gender Bias in Linguistic Descr.pdf}
}

@phdthesis{vul.e:2010phd,
  title = {Sampling in Human Cognition},
  author = {Vul, Edward},
  year = {2010},
  urldate = {2025-01-10},
  abstract = {Bayesian Decision Theory describes optimal methods for combining sparse, noisy data with prior knowledge to build models of an uncertain world and to use those models to plan actions and make novel decisions. Bayesian computational models correctly predict aspects of human behavior in cognitive domains ranging from perception to motor control and language. However the predictive success of Bayesian models of cognition has highlighted long-standing challenges in bridging the computational and process levels of cognition. First, the computations required for exact Bayesian inference are incommensurate with the limited resources available to cognition (e.g., computational speed; and memory). Second, Bayesian models describe computations but not the processes that carry out these computations and fail to accurately predict human behavior under conditions of cognitive load or deficits. I suggest a resolution to both challenges: The mind approximates Bayesian inference by sampling. Experiments across a wide range of cognition demonstrate Monte-Carlo-like behavior by human observers; moreover, models of cognition based on specific Monte Carlo algorithms can describe previously elusive cognitive phenomena such as perceptual bistability and probability matching. When sampling algorithms are treated as process models of human cognition, the computational and process levels can be modeled jointly to shed light on new and old cognitive phenomena..},
  copyright = {M.I.T. theses are protected by  copyright. They may be viewed from this source for any purpose, but  reproduction or distribution in any format is prohibited without written  permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  annotation = {Accepted: 2011-04-04T17:40:06Z},
  file = {~/Zotfiles/vul.e2010phd Sampling in human cognition_1.pdf;~/Zotfiles/vul.e2010phd Sampling in human cognition.pdf}
}

@article{vul.e:2014,
  title = {One and Done? {{Optimal}} Decisions from Very Few Samples},
  author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  year = {2014},
  journal = {Cognitive Science},
  volume = {38},
  number = {4},
  pages = {599--637},
  publisher = {Wiley},
  doi = {10.1111/cogs.12101},
  date-added = {2021-03-16 23:51:30 -0400},
  date-modified = {2021-03-16 23:51:30 -0400},
  keywords = {bayesian,bounded rationality,inference algorithms,sampling}
}

@article{vulkan.n:2000,
  title = {An Economist's Perspective on Probability Matching},
  author = {Vulkan, Nir},
  year = {2000},
  journal = {Journal of Economic Surveys},
  volume = {14},
  number = {1},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-6419.00106},
  pages = {101--118},
  doi = {10.1111/1467-6419.00106},
  abstract = {The experimental phenomenon known as `probability matching' is often offered as evidence in support of adaptive learning models and against the idea that people maximise their expected utility. Recent interest in dynamic-based equilibrium theories means the term re-appears in Economics. However, there seems to be conflicting views on what is actually meant by the term and about the validity of the data. The purpose of this paper is therefore threefold: First, to introduce today's readers to what is meant by probability matching, and in particular to clarify which aspects of this phenomenon challenge the utility-maximisation hypothesis. Second, to familiarise the reader with the different theoretical approaches to behaviour in such circumstances, and to focus on the differences in predictions between these theories in light of recent advances. Third, to provide a comprehensive survey of repeated, binary choice experiments.},
  bdsk-url-2 = {https://doi.org/10.1111/1467-6419.00106},
  date-added = {2021-05-31 13:57:04 -0400},
  date-modified = {2021-05-31 13:57:05 -0400},
  keywords = {Optimisation,Probability matching,Stochastic learning}
}

@article{wagers.m:2009,
  title = {Agreement Attraction in Comprehension: {{Representations}} and Processes},
  shorttitle = {Agreement Attraction in Comprehension},
  author = {Wagers, Matthew W. and Lau, Ellen F. and Phillips, Colin},
  year = {2009},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {61},
  number = {2},
  pages = {206--237},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2009.04.002},
  urldate = {2023-04-03},
  abstract = {Much work has demonstrated so-called attraction errors in the production of subject--verb agreement (e.g., `The key to the cabinets are on the table', [Bock, J. K., \& Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45--93]), in which a verb erroneously agrees with an intervening noun. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject--verb agreement violations when these `attractor' nouns intervene. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories. First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., `The drivers who the runner wave to each morning'). Second, we observe a `grammatical asymmetry': attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb.},
  langid = {english},
  keywords = {Agreement,agreement attraction,Comprehension,Prediction,Retrieval,Syntax},
  file = {~/Zotfiles/wagers.m2009 Agreement attraction in comprehension R.pdf}
}

@article{wainwright.m:2007,
  title = {Graphical Models, Exponential Families, and Variational Inference},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  journal = {Foundations and Trends in Machine Learning},
  volume = {1},
  number = {1--2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  urldate = {2024-05-24},
  langid = {english},
  file = {~/Zotfiles/wainwright.m2007 Graphical models, exponential families,.pdf}
}

@article{wald.a:1939,
  title = {Contributions to the {{Theory}} of {{Statistical Estimation}} and {{Testing Hypotheses}}},
  author = {Wald, Abraham},
  year = {1939},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {10},
  number = {4},
  pages = {299--326},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732144},
  urldate = {2022-05-21},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {decision theory}
}

@article{wald.a:1945,
  title = {Sequential {{Tests}} of {{Statistical Hypotheses}}},
  author = {Wald, A.},
  year = {1945},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177731118},
  urldate = {2023-11-04},
  langid = {english},
  file = {~/Zotfiles/wald.a1945 Sequential Tests of Statistical Hypothes.pdf}
}

@article{wald.a:1947,
  title = {Foundations of a General Theory of Sequential Decision Functions},
  author = {Wald, Abraham},
  year = {1947},
  journal = {Econometrica},
  volume = {15},
  number = {4},
  eprint = {1905331},
  eprinttype = {jstor},
  pages = {279--313},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1905331},
  urldate = {2022-07-04}
}

@book{walters.p:1982book,
  title = {An Introduction to Ergodic Theory},
  author = {Walters, Peter},
  year = {1982},
  series = {Graduate Texts in Mathematics},
  number = {79},
  publisher = {Springer},
  address = {New York},
  abstract = {This text provides an introduction to ergodic theory suitable for readers knowing basic measure theory. The mathematical prerequisites are summarized in Chapter 0. It is hoped the reader will be ready to tackle research papers after reading the book. The first part of the text is concerned with measure-preserving transformations of probability spaces; recurrence properties, mixing properties, the Birkhoff ergodic theorem, isomorphism and spectral isomorphism, and entropy theory are discussed. Some examples are described and are studied in detail when new properties are presented. The second part of the text focuses on the ergodic theory of continuous transformations of compact metrizable spaces. The family of invariant probability measures for such a transformation is studied and related to properties of the transformation such as topological traitivity, minimality, the size of the non-wandering set, and existence of periodic points. Topological entropy is introduced and related to measure-theoretic entropy. Topological pressure and equilibrium states are discussed, and a proof is given of the variational principle that relates pressure to measure-theoretic entropies. Several examples are studied in detail. The final chapter outlines significant results and some applications of ergodic theory to other branches of mathematics.},
  isbn = {978-0-387-95152-2},
  langid = {english},
  file = {~/Zotfiles/walters.p1982 An introduction to ergodic theory.djvu}
}

@inproceedings{wang.a:2019,
  title = {{{SuperGLUE}}: {{A}} Stickier Benchmark for General-Purpose Language Understanding Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3261--3275},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@misc{wang.b:2021,
  title = {{{GPT-J-6B}}: {{A}} 6 Billion Parameter Autoregressive Language Model},
  author = {Wang, Ben and Komatsuzaki, Aran},
  year = {2021},
  month = may,
  date-added = {2021-11-30 10:11:28 -0500},
  date-modified = {2021-12-13 19:43:33 -0500},
  howpublished = {Software}
}

@article{wang.l:2009,
  title = {Semantic Illusion Depends on Information Structure: {{ERP}} Evidence},
  shorttitle = {Semantic Illusion Depends on Information Structure},
  author = {Wang, Lin and Hagoort, Peter and Yang, Yufang},
  year = {2009},
  month = jul,
  journal = {Brain Research},
  volume = {1282},
  pages = {50--56},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2009.05.069},
  urldate = {2024-05-28},
  abstract = {Next to propositional content, speakers distribute information in their utterances in such a way that listeners can make a distinction between new (focused) and given (non-focused) information. This is referred to as information structure. We measured event-related potentials (ERPs) to explore the role of information structure in semantic processing. Following different questions in wh-question--answer pairs (e.g. What kind of vegetable did Ming buy for cooking today?/Who bought the vegetables for cooking today?), the answer sentences (e.g., Ming bought eggplant/beef to cook today.) contained a critical word, which was either semantically appropriate (eggplant) or inappropriate (beef), and either focus or non-focus. The results showed a full N400 effect only when the critical words were in focus position. In non-focus position a strongly reduced N400 effect was observed, in line with the well-known semantic illusion effect. The results suggest that information structure facilitates semantic processing by devoting more resources to focused information.},
  keywords = {Information structure,N400 effect,Semantic illusion,Semantic integration,Wh-question-answer pair},
  file = {~/Zotfiles/wang.l2009 Semantic illusion depends on information.pdf}
}

@article{wang.l:2025,
  title = {An Implemented Predictive Coding Model of Lexico-Semantic Processing Explains the Dynamics of Univariate and Multivariate Activity within the Left Ventromedial Temporal Lobe during Reading Comprehension},
  author = {Wang, Lin and Nour Eddine, Samer and Brothers, Trevor and Jensen, Ole and Kuperberg, Gina R.},
  year = {2025},
  month = mar,
  journal = {NeuroImage},
  volume = {308},
  pages = {120977},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2024.120977},
  urldate = {2025-04-17},
  abstract = {During language comprehension, the larger neural response to unexpected versus expected inputs is often taken as evidence for predictive coding---a specific computational architecture and optimization algorithm proposed to approximate probabilistic inference in the brain. However, other predictive processing frameworks can also account for this effect, leaving the unique claims of predictive coding untested. In this study, we used MEG to examine both univariate and multivariate neural activity in response to expected and unexpected inputs during word-by-word reading comprehension. We further simulated this activity using an implemented predictive coding model that infers the meaning of words from their orthographic form. Consistent with previous findings, the univariate analysis showed that, between 300 and 500 ms, unexpected words produced a larger evoked response than expected words within a left ventromedial temporal region that supports the mapping of orthographic word-forms onto lexical and conceptual representations. Our model explained this larger evoked response as the enhanced lexico-semantic prediction error produced when prior top-down predictions failed to suppress activity within lexical and semantic ``error units''. Critically, our simulations showed that despite producing minimal prediction error, expected inputs nonetheless reinstated top-down predictions within the model's lexical and semantic ``state'' units. Two types of multivariate analyses provided evidence for this functional distinction between state and error units within the ventromedial temporal region. First, within each trial, the same individual voxels that produced a larger response to unexpected inputs between 300 and 500 ms produced unique temporal patterns to expected inputs that resembled the patterns produced within a pre-activation time window. Second, across trials, and again within the same 300--500 ms time window and left ventromedial temporal region, pairs of expected words produced spatial patterns that were more similar to one another than the spatial patterns produced by pairs of expected and unexpected words, regardless of specific item. Together, these findings provide compelling evidence that the left ventromedial temporal lobe employs predictive coding to infer the meaning of incoming words from their orthographic form during reading comprehension.},
  keywords = {Language comprehension,MEG,Modeling,N400,Predictive coding,RSA}
}

@misc{wang.t:2022arxiv,
  title = {What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?},
  author = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05832},
  eprint = {2204.05832},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-25},
  abstract = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {~/Zotfiles/wang.t2022 What language model architecture and pre.pdf}
}

@article{wardjr..j:1963,
  title = {Hierarchical Grouping to Optimize an Objective Function},
  author = {Ward, Jr., Joe H.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {Taylor \& Francis Group},
  bdsk-url-2 = {https://pdfs.semanticscholar.org/0430/b241bdd0b67d37e1143370f8d24fc46d83e9.pdf},
  bdsk-url-3 = {https://doi.org/10.1080/01621459.1963.10500845},
  date-added = {2019-06-15 15:57:19 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {0},
  keywords = {hierarchical clustering}
}

@article{warstadt.a:2019,
  title = {Neural Network Acceptability Judgments},
  author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
  year = {2019},
  month = nov,
  volume = {7},
  pages = {625--641},
  publisher = {MIT Press - Journals},
  doi = {10.1162/tacl_a_00290},
  date-added = {2021-10-19 00:10:14 -0400},
  date-modified = {2021-10-19 00:10:15 -0400},
  file = {~/Zotfiles/warstadt.a2019 Neural network acceptability judgments.pdf}
}

@article{warstadt.a:2020,
  title = {{{BLiMP}}: {{The Benchmark}} of {{Linguistic Minimal Pairs}} for {{English}}},
  shorttitle = {{{BLiMP}}},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  year = {2020},
  month = jul,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {377--392},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00321},
  urldate = {2023-04-30},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs---that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
  file = {~/Zotfiles/warstadt.a2020 BLiMP The Benchmark of Linguistic Minim.pdf}
}

@article{wason.p:1979,
  title = {A {{Verbal Illusion}}},
  author = {Wason, Peter C. and Reich, Shuli S.},
  year = {1979},
  month = nov,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {31},
  number = {4},
  pages = {591--597},
  publisher = {SAGE Publications},
  issn = {0033-555X},
  doi = {10.1080/14640747908400750},
  urldate = {2023-07-27},
  abstract = {The sentence, No head injury is too trivial to be ignored, tends to be systematically misconstrued to mean that one should not ignore head injuries. It is argued that the sentence is anomalous in two ways. It is semantically anomalous because the relation between the adjective and the verb is the same as in the sentence, No sinner is too wicked to be condemned. It is pragmatically anomalous because the relation between the noun and the verb expresses an injunction which is inconsistent with commonly held beliefs. An experiment was conducted to investigate the effect of this pragmatic anomaly on comprehension. Four pragmatic and four non-pragmatic sentences were paraphrased. The prediction was confirmed that the pragmatic sentences were paraphrased more accurately than the non-pragmatic sentences. Some alternative explanations are considered.},
  langid = {english},
  keywords = {depth-charge illusion,grammatical illusions},
  file = {~/Zotfiles/wason.p1979 A Verbal Illusion.pdf}
}

@book{watrous.j:2018book,
  title = {The Theory of Quantum Information},
  author = {Watrous, John},
  year = {2018},
  publisher = {Cambridge University Press},
  date-added = {2020-02-15 11:52:26 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {information-entropy},
  keywords = {entropy,information theory,quantum information theory}
}

@article{weaver.w:1948,
  title = {Probability, Rarity, Interest, and Surprise},
  author = {Weaver, Warren},
  year = {1948},
  journal = {The Scientific Monthly},
  volume = {67},
  number = {6},
  eprint = {22339},
  eprinttype = {jstor},
  pages = {390--392},
  publisher = {American Association for the Advancement of Science},
  issn = {0096-3771},
  urldate = {2025-02-13},
  file = {~/Zotfiles/weaver.w1948 Probability, Rarity, Interest, and Surpr.pdf}
}

@misc{weissbart.h:2024biorxiv,
  title = {The Structure and Statistics of Language Jointly Shape Cross-Frequency Neural Dynamics during Spoken Language Comprehension},
  author = {Weissbart, Hugo and Martin, Andrea E.},
  year = {2024},
  month = may,
  publisher = {bioRxiv},
  doi = {10.1101/2023.10.06.561087},
  urldate = {2024-05-25},
  abstract = {Humans excel at extracting structurally-determined meaning from speech despite inherent physical variability. This study explores the brain's ability to predict and understand spoken language robustly. It investigates the relationship between structural and statistical language knowledge in brain dynamics, focusing on phase and amplitude modulation. Using syntactic features from constituent hierarchies and surface statistics from a transformer model as predictors of forward encoding models, we reconstructed cross-frequency neural dynamics from MEG data during audiobook listening. Our findings challenge a strict separation of linguistic structure and statistics in the brain, with both aiding neural signal reconstruction. Syntactic features had a more temporally spread impact, and both word entropy and the number of closing syntactic constituents were linked to the phase-amplitude coupling of neural dynamics, implying a role in temporal prediction and cortical oscillation alignment during speech processing. Our results indicate that structured and statistical information jointly shape neural dynamics during spoken language comprehension and suggest an integration process via a cross-frequency coupling mechanism.},
  archiveprefix = {bioRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{wellwood.a:2018,
  title = {The {{Anatomy}} of a {{Comparative Illusion}}},
  author = {Wellwood, Alexis and Pancheva, Roumyana and Hacquard, Valentine and Phillips, Colin},
  year = {2018},
  month = aug,
  journal = {Journal of Semantics},
  volume = {35},
  number = {3},
  pages = {543--583},
  issn = {0167-5133},
  doi = {10.1093/jos/ffy014},
  urldate = {2023-08-01},
  abstract = {Comparative constructions like More people have been to Russia than I have are reported to be acceptable and meaningful by native speakers of English; yet, upon closer reflection, they are judged to be incoherent. This mismatch between initial perception and more considered judgment challenges the idea that we perceive sentences veridically, and interpret them fully; it is thus potentially revealing about the relationship between grammar and language processing. This paper presents the results of the first detailed investigation of these so-called `comparative illusions'. We test four hypotheses about their source: a shallow syntactic parser, some type of repair by ellipsis, an incorrectly-resolved lexical ambiguity, or a persistent event comparison interpretation. Two formal acceptability studies show that speakers are most prone to the illusion when the matrix clause supports an event comparison reading. A verbatim recall task tests and finds evidence for such construals in speakers' recollections of the sentences. We suggest that this reflects speakers' entertaining an interpretation that is initially consistent with the sentence, but failing to notice when this interpretation becomes unavailable at the than-clause. In particular, semantic knowledge blinds people to an illicit operator-variable configuration in the syntax. Rather than illustrating processing in the absence of grammatical analysis, comparative illusions thus underscore the importance of syntactic and semantic rules in sentence processing.},
  keywords = {comparative illusions}
}

@article{white.s:2008,
  title = {Eye Movements When Reading Transposed Text: {{The}} Importance of Word-Beginning Letters.},
  shorttitle = {Eye Movements When Reading Transposed Text},
  author = {White, Sarah J. and Johnson, Rebecca L. and Liversedge, Simon P. and Rayner, Keith},
  year = {2008},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {34},
  number = {5},
  pages = {1261--1276},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.34.5.1261},
  urldate = {2024-02-24},
  langid = {english},
  file = {~/Zotfiles/white.s2008 Eye movements when reading transposed te.pdf}
}

@article{wieling.m:2016,
  title = {Investigating Dialectal Differences Using Articulography},
  author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Br{\"o}ker, Franziska and Thiele, Samuel and Wood, Simon N. and Baayen, R. Harald},
  year = {2016},
  month = nov,
  journal = {Journal of Phonetics},
  volume = {59},
  pages = {122--143},
  publisher = {Elsevier BV},
  doi = {10.1016/j.wocn.2016.09.004},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2016.09.004},
  date-added = {2021-12-03 00:24:24 -0500},
  date-modified = {2021-12-03 00:24:40 -0500}
}

@book{wiener.n:2019book2,
  title = {Cybernetics or Control and Communication in the Animal and the Machine},
  author = {Wiener, Norbert},
  year = {2019},
  month = oct,
  edition = {2},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/11810.001.0001},
  urldate = {2025-02-13},
  abstract = {A classic and influential work that laid the theoretical foundations for information theory and a timely text for contemporary informations theorists and p},
  isbn = {978-0-262-35590-2},
  langid = {english}
}

@inproceedings{wilcox.e:2020cogsci,
  title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Cognitive Science Society}}},
  author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {1707--1713},
  publisher = {Cognitive Science Society},
  address = {Virtual},
  keywords = {sentence processing},
  file = {~/Zotfiles/wilcox.e2020 On the predictive power of neural langua.pdf}
}

@inproceedings{wilcox.e:2021,
  title = {A Targeted Assessment of Incremental Processing in Neural Language Models and Humans},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.acl-long.76},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.76},
  date-added = {2022-04-15 16:04:48 -0400},
  date-modified = {2022-04-15 16:04:50 -0400}
}

@phdthesis{wilcox.e:2022phd,
  title = {Informative {{Presupposition}} \& {{Accommodation}}},
  author = {Wilcox, Ethan Gotlieb},
  year = {2022},
  month = may,
  urldate = {2023-09-30},
  abstract = {Presuppositions are the parts of meanings of utterances which are backgrounded and strongly committed to by the speaker. They are carried by a diverse range of lexical items called presupposition triggers, which include determiners, particles, open class verbs, and syntactic constructions. For example, the sentence "Lee read War and Peace again" asserts that Lee read War and Peace and presupposes that she has done so previously via the trigger 'again.' Most triggers occur in contexts where their presuppositions are supported (i.e. already entailed) by a local context; however some can also occur in contexts that lack local support, in which case their presuppositions are informative. Informative use of presupposition is typically modeled via an accommodation mechanism (Lewis, 1979) that pre-updates a context prior to utterance interpretation to go along with the presuppositions of a sentence. Understanding when triggers can communicate novel information using accommodation---which I refer to as the "Novelty Problem" for presupposition triggers---is the main goal of this dissertation. The dissertation is arranged into five chapters. Chapter 1 provides a background on presupposition and accommodation, and introduces the notion of Contextual Felicity Constraints, or CFCs (Tonhauser et al., 2013). If a trigger is infelicitous in cases where its presuppositions are not entailed, it is said to be subject to a strong CFC. Chapter 2 measures the CFCs for thirteen English presupposition triggers in two online comprehension studies, making it the largest cross-trigger comparison of CFCs reported in the literature to-date. A ranking of triggers is proposed, from weak-CFC triggers to strong-CFC triggers. Chapter 3 presents a theoretical solution to the novelty problem, which views CFCs as the result of an information-structural discourse clash. The proposal, which I refer to as the Maximalty/Accommodation Clash (or MAC), treats CFCs as arising not from accommodation failure, but from downstream semantic contradictions that result from successful accommodation. Chapter 4 develops this proposal within alternative pragmatic frameworks. Finally, Chapter 5 presents two studies that test the MAC experimentally. Taken together, the results lend support for the perspective that presupposition triggers impose constraints on the context in which they are uttered and that their contextual felicity is modulated by local information structure---the two key ingredients of the MAC approach},
  school = {Harvard University},
  file = {~/Zotfiles/wilcox.e2022phd Informative Presupposition & Accommodati.pdf}
}

@inproceedings{wilcox.e:2023,
  title = {Language {{Model Quality Correlates}} with {{Psychometric Predictive Power}} in {{Multiple Languages}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Wilcox, Ethan and Meister, Clara and Cotterell, Ryan and Pimentel, Tiago},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {7503--7511},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.466},
  urldate = {2025-04-17},
  abstract = {Surprisal theory (Hale, 2001; Levy, 2008) posits that a word`s reading time is proportional to its surprisal (i.e., to its negative log probability given the proceeding context). Since we are unable to access a word`s ground-truth probability, surprisal theory has been empirically tested using surprisal estimates from language models (LMs). Under the premise that surprisal theory holds, we would expect that higher quality language models provide more powerful predictors of human reading behavior---a conjecture we dub the quality--power (QP) hypothesis. Unfortunately, empirical support for the QP hypothesis is mixed. Some studies in English have found correlations between LM quality and predictive power, but other studies using Japanese data, as well as using larger English LMs, find no such correlations. In this work, we conduct a systematic crosslinguistic assessment of the QP hypothesis. We train LMs from scratch on small- and medium-sized datasets from 13 languages (across five language families) and assess their ability to predict eye tracking data. We find correlations between LM quality and power in eleven of these thirteen languages, suggesting that, within the range of model classes and sizes tested, better language models are indeed better predictors of human language processing behaviors.},
  file = {~/Zotfiles/wilcox.e2023 Language Model Quality Correlates with P.pdf}
}

@article{wilcox.e:2023a,
  title = {Testing the {{Predictions}} of {{Surprisal Theory}} in 11 {{Languages}}},
  author = {Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
  year = {2023},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1451--1470},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00612},
  urldate = {2024-03-27},
  abstract = {Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.},
  file = {~/Zotfiles/wilcox.e2023tacl Testing the Predictions of Surprisal The.pdf}
}

@misc{wilcox.e:2023arxiv,
  title = {Testing the Predictions of Surprisal Theory in 11 Languages},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03667},
  eprint = {2307.03667},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.03667},
  urldate = {2023-07-11},
  abstract = {A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, is predictive of reading times; (iii) and whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to-date between information theory and incremental language processing across languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,surprisal theory}
}

@article{wilcox.e:2024,
  title = {Using Computational Models to Test Syntactic Learnability},
  author = {Wilcox, Ethan Gotlieb and Futrell, Richard and Levy, Roger},
  year = {2024},
  month = oct,
  journal = {Linguistic Inquiry},
  volume = {55},
  number = {4},
  pages = {805--848},
  issn = {0024-3892},
  doi = {10.1162/ling_a_00491},
  urldate = {2025-02-05},
  abstract = {We studied the learnability of English filler-gap dependencies and the ``island'' constraints on them by assessing the generalizations made by autoregressive (incremental) language models that use deep learning to predict the next word given preceding context. Using factorial tests inspired by experimental psycholinguistics, we found that models acquire not only the basic contingency between fillers and gaps, but also the unboundedness and hierarchical constraints implicated in the dependency. We evaluated a model's acquisition of island constraints by demonstrating that its expectation for a filler-gap contingency is attenuated within an island environment. Our results provide empirical evidence against the argument from the poverty of the stimulus for this particular structure.},
  file = {~/Zotfiles/wilcox.e2024LI Using Computational Models to Test Synta.pdf}
}

@article{wilcox.e:2024a,
  title = {Mouse {{Tracking}} for {{Reading}} ({{MoTR}}): {{A}} New Naturalistic Incremental Processing Measurement Tool},
  shorttitle = {Mouse {{Tracking}} for {{Reading}} ({{MoTR}})},
  author = {Wilcox, Ethan Gotlieb and Ding, Cui and Sachan, Mrinmaya and J{\"a}ger, Lena Ann},
  year = {2024},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {138},
  pages = {104534},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104534},
  urldate = {2024-06-04},
  abstract = {We introduce Mouse Tracking for Reading (MoTR) a new incremental processing measurement tool that can be used to collect word-by-word reading times. In a MoTR trial, participants are presented with text, which is blurred, except for a small region around the tip of the mouse. Participants must move the mouse to reveal and read the text. Mouse movement is recorded, and, using a postprocessing pipeline we present, can be analyzed to produce scanpaths as well as word-by-word reading times. We validate MoTR in two suites of experiments. In the first experiment, we collect data for the English-language Provo Corpus (Luke and Christianson, 2018). We analyze scanpaths and show that participants interpolate between two types of strategies for reading during a MoTR trial -- sometimes they fixate on individual words, somewhat akin to eye-tracking, while other times they produce a more constant pass over the text, slowing down in response to processing difficulties. Taking these strategies into account, we show that the word-by-word reading times produced by our data analysis pipeline correlate well with previously collected eye-tracking data for this corpus, and that these correlations are higher than those produced by SPR data, which we also collect for the corpus. Furthermore, we demonstrate that there is a linear relationship between by-word MoTR values and word-level surprisal values, as has been previously shown for eye-tracking data (Smith and Levy, 2013). In the second experiment, we assess whether MoTR can be used to study sentence processing phenomena in targeted psycholinguistics experiments. Using materials from Witzel et al. (2012), we show that MoTR can reveal English speakers' preferences for low attachment during online sentence comprehension. We argue that MoTR presents a compelling tradeoff between multiple experimental considerations: It is cheap to run and can be presented in a browser enabling the collection of data over the internet. It is more naturalistic than some alternative processing measures, allowing participants to skip words and regress to previous sentence regions. Finally, it has good sensitivity, detecting signatures of psycholinguistic processing behaviors from a relatively small number of participants.},
  keywords = {Experimental methods,Eye-tracking,Maze task,Mouse Tracking,Reading times,Self-paced reading},
  file = {~/Zotfiles/wilcox.e2024MoTR Mouse Tracking for Reading (MoTR) A new.pdf}
}

@article{wilcox.e:2024b,
  title = {An Information-Theoretic Analysis of Targeted Regressions during Reading},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},
  year = {2024},
  month = aug,
  journal = {Cognition},
  volume = {249},
  pages = {105765},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2024.105765},
  urldate = {2024-05-26},
  abstract = {Regressions, or backward saccades, are common during reading, accounting for between 5\% and 20\% of all saccades. And yet, relatively little is known about what causes them. We provide an information-theoretic operationalization for two previous qualitative hypotheses about regressions, which we dub reactivation and reanalysis. We argue that these hypotheses make different predictions about the pointwise mutual information or pmi between a regression's source and target. Intuitively, the pmi between two words measures how much more (or less) likely one word is to be present given the other. On one hand, the reactivation hypothesis predicts that regressions occur between words that are associated, implying high positive values of pmi. On the other hand, the reanalysis hypothesis predicts that regressions should occur between words that are not associated with each other, implying negative, low values of pmi. As a second theoretical contribution, we expand on previous theories by considering not only pmi but also expected values of pmi, E[pmi], where the expectation is taken over all possible realizations of the regression's target. The rationale for this is that language processing involves making inferences under uncertainty, and readers may be uncertain about what they have read, especially if a previous word was skipped. To test both theories, we use contemporary language models to estimate pmi-based statistics over word pairs in three corpora of eye tracking data in English, as well as in six languages across three language families (Indo-European, Uralic, and Turkic). Our results are consistent across languages and models tested: Positive values of pmi and E[pmi] consistently help to predict the patterns of regressions during reading, whereas negative values of pmi and E[pmi] do not. Our information-theoretic interpretation increases the predictive scope of both theories and our studies present the first systematic crosslinguistic analysis of regressions in the literature. Our results support the reactivation hypothesis and, more broadly, they expand the number of language processing behaviors that can be linked to information-theoretic principles.},
  keywords = {Eye tracking,Information theory,Language processing,Mutual information,Reading,Regressions},
  file = {~/Zotfiles/wilcox.e2024cognition An information-theoretic analysis of tar.pdf}
}

@misc{wilcox.e:2024psyarxiv,
  title = {An {{Information-Theoretic Analysis}} of {{Targeted Regressions}} during {{Reading}}},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},
  year = {2024},
  month = mar,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/3qf9a},
  urldate = {2024-04-11},
  abstract = {Regressions, or backward saccades, are common during reading, accounting for between 5\% and 20\% of all saccades. And yet, relatively little is known about what causes them. We provide an information-theoretic operationalization for two previous qualitative hypotheses about regressions, which we dub reactivation and reanalysis. We argue that these hypotheses make different predictions about the pointwise mutual information or pmi between a regression's source and target. Intuitively, the pmi between two words measures how much more (or less) likely one word is to be present given the other. On one hand, the reactivation hypothesis predicts that regressions occur between words that are associated, implying high positive values of pmi. On the other hand, the reanalysis hypothesis predicts that regressions should occur between words that are disassociated with each other, implying negative, low values of pmi. As a second theoretical contribution, we expand on previous theories by considering not only pmi but also expected values of pmi, E[pmi], where the expectation is taken over all possible realizations of the regression's target. The rationale for this is that language processing involves making inferences under uncertainty, and readers may be uncertain about what they have read, especially if a previous word was skipped. To test both theories, we use contemporary language models to estimate pmi-based statistics over word pairs in three corpora of eye tracking data in English, as well as in six languages across three language families (Indo-European, Uralic, and Turkic). Our results are consistent across languages and models tested: Positive values of pmi and E[pmi] consistently help to predict the patterns of regressions during reading, whereas negative values of pmi and E[pmi] do not. Our information-theoretic interpretation increases the predictive scope of both theories and our studies present the first systematic crosslinguistic analysis of regressions in the literature. Our results support the reactivation hypothesis and, more broadly, they expand the number of language processing behaviors that can be linked to information-theoretic principles.},
  langid = {american},
  keywords = {Eye-tracking,Information Theory,Language Processing,Mutual Information,Reading,Regressions},
  annotation = {Published as: 10.1016/j.cognition.2024.105765}
}

@article{wilcox.e:2025,
  title = {Bigger Is Not Always Better: {{The}} Importance of Human-Scale Language Modeling for Psycholinguistics},
  shorttitle = {Bigger Is Not Always Better},
  author = {Wilcox, Ethan Gotlieb and Hu, Michael Y. and Mueller, Aaron and Warstadt, Alex and Choshen, Leshem and Zhuang, Chengxu and Williams, Adina and Cotterell, Ryan and Linzen, Tal},
  year = {2025},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {144},
  pages = {104650},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2025.104650},
  urldate = {2025-06-27},
  abstract = {When trained to place high probability on a training corpus, neural network language models can learn a surprising amount about language. Recent work has demonstrated that large performance improvements can arise from simply increasing, i.e., scaling, the size of the corpora they are trained on and the number of parameters in those models. Accordingly, many contemporary systems are trained on trillions of words. While largely beneficial to performance on language applications, scaling has several downsides for both computational psycholinguistics and natural language processing research. We discuss the scientific challenges presented by the scaling paradigm, as well as the benefits that would result from language models that can learn from human-scale data. In the second half of this paper, we report on findings from a recent effort to bring about human-scale language model pretraining: the first iteration of the BabyLM Challenge, a shared task organized by the authors that invited participants to train a language model on 100 million words or less. The challenge produced several concrete best practices for practitioners interested in small-scale language modeling. For cognitive scientists, the challenge demonstrated that robust linguistic generalizations can be learned by models trained on a human-scale dataset, though this is not yet achieved through cognitively plausible mechanisms. Furthermore, it established a population of ``BabyLMs'' that are all effective at data-efficient language learning. Studying such models can help us identify hypotheses for the computational mechanisms that underlie human language acquisition.},
  keywords = {Cognitive modeling,Connectionist networks,Language acquisition,Language modeling,Psycholinguistics,Scaling},
  file = {~/Zotfiles/wilcox.e2025 Bigger is not always better The importa.pdf}
}

@misc{willard.b:2023arxiv,
  title = {Efficient Guided Generation for Large Language Models},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  number = {arXiv:2307.09702},
  eprint = {2307.09702},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09702},
  urldate = {2024-10-10},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/willard.b2023outlines Efficient Guided Generation for Large La.pdf}
}

@misc{williams.p:2010,
  title = {Nonnegative Decomposition of Multivariate Information},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2010},
  eprint = {1004.2515},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:31:54 -0400},
  date-modified = {2021-09-29 21:31:56 -0400}
}

@misc{williams.p:2011,
  title = {Generalized Measures of Information Transfer},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2011},
  eprint = {1102.1507},
  primaryclass = {physics.data-an},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:29:14 -0400},
  date-modified = {2021-09-29 21:29:15 -0400}
}

@article{williams.r:1992REINFORCE,
  title = {Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning},
  author = {Williams, Ronald J.},
  year = {1992},
  month = may,
  journal = {Machine Learning},
  volume = {8},
  number = {3},
  pages = {229--256},
  issn = {1573-0565},
  doi = {10.1007/BF00992696},
  urldate = {2025-09-11},
  abstract = {This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms.},
  langid = {english},
  keywords = {connectionist networks,gradient descent,mathematical analysis,Reinforcement learning},
  file = {~/Zotfiles/williams.r1992REINFORCE Simple statistical gradient-following al.pdf}
}

@incollection{wilson.k:1954,
  title = {Applications of Entropy Measures to Problems of Sequential Structure},
  booktitle = {Psycholinguistics},
  author = {Wilson, Kellogg and Carroll, John B},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {103--110},
  publisher = {Indiana University Press},
  doi = {10.1037/h0063655},
  chapter = {5.3},
  date-added = {2021-05-20 11:54:33 -0400},
  date-modified = {2022-04-14 23:51:44 -0400},
  keywords = {entropy reduction}
}

@book{winter.b:2019book,
  title = {Statistics for Linguists: An Introduction Using {{R}}},
  shorttitle = {Statistics for Linguists},
  author = {Winter, Bodo},
  year = {2019},
  month = nov,
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781315165547},
  abstract = {Statistics for Linguists: An Introduction Using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced undergraduate students of Linguistics statistics courses as well as those in other fields, including Psychology, Cognitive Science, and Data Science.},
  isbn = {978-1-315-16554-7}
}

@techreport{wit.e:1999,
  type = {Technical Report},
  title = {What Is Linguistic Redundancy?},
  author = {Wit, Ernst-Jan C and Gillette, Marie},
  year = {1999},
  month = mar,
  institution = {University of Chicago},
  abstract = {Linguistic redundancy is a multifaceted phenomenon within language that illustrates that successful communication is not merely a superficial quality of language, but a constraint at the heart of its origin and the dynamics of its development. Redundancy is deeply rooted in language and one can find many redundancy features within grammar, syntax and other aspects of language. We propose a new classification of linguistic redundancy into what we call `contextual redundancy' and `grammatical redundancy.' This new classification is powerful enough to incorporate and enlighten all forms of linguistic redundancy known to us.},
  langid = {english},
  file = {~/Zotfiles/wit.e1999 What is Linguistic Redundancy.pdf}
}

@article{witzel.n:2012,
  title = {Comparisons of {{Online Reading Paradigms}}: {{Eye Tracking}}, {{Moving-Window}}, and {{Maze}}},
  shorttitle = {Comparisons of {{Online Reading Paradigms}}},
  author = {Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth},
  year = {2012},
  month = apr,
  journal = {Journal of Psycholinguistic Research},
  volume = {41},
  number = {2},
  pages = {105--128},
  issn = {1573-6555},
  doi = {10.1007/s10936-011-9179-x},
  urldate = {2024-02-22},
  abstract = {This study compares four methodologies used to examine online sentence processing during reading. Specifically, self-paced, non-cumulative, moving-window reading (Just et al. in J Exp Psychol Gen 111:228--238, 1982), eye tracking (see e.g., Rayner in Q J Exp Psychol 62:1457--1506, 2009), and two versions of the maze task (Forster et al. in Behav Res Methods 41:163--171, 2009)---the lexicality maze and the grammaticality maze---were used to investigate the processing of sentences containing temporary structural ambiguities. Of particular interest were (i) whether each task was capable of revealing processing differences on these sentences and (ii) whether these effects were indicated precisely at the predicted word/region. Although there was considerable overlap in the general pattern of results from the four tasks, there were also clear differences among them in terms of the strength and timing of the observed effects. In particular, excepting sentences that tap into clause-closure commitments, both maze task versions provided robust, ``localized'' indications of incremental sentence processing difficulty relative to self-paced reading and eye tracking.},
  langid = {english},
  keywords = {Eye tracking,Maze task,Moving-window reading,Sentence processing},
  file = {~/Zotfiles/witzel.n2012 Comparisons of Online Reading Paradigms.pdf}
}

@inproceedings{wolf.t:2020,
  title = {Transformers: {{State-of-the-art}} Natural Language Processing},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: {{System}} Demonstrations},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year = {2020},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-demos.6}
}

@article{wong.c:1980,
  title = {An Efficient Method for Weighted Sampling without Replacement},
  author = {Wong, C. K. and Easton, M. C.},
  year = {1980},
  month = feb,
  journal = {SIAM Journal on Computing},
  volume = {9},
  number = {1},
  pages = {111--113},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0209009},
  urldate = {2022-07-10},
  langid = {english},
  file = {~/Zotfiles/wong.c1980 An efficient method for weighted samplin.pdf}
}

@misc{wong.l:2023arxiv,
  title = {From {{Word Models}} to {{World Models}}: {{Translating}} from {{Natural Language}} to the {{Probabilistic Language}} of {{Thought}}},
  shorttitle = {From {{Word Models}} to {{World Models}}},
  author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12672},
  eprint = {2306.12672},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  file = {~/Zotfiles/wong.l2023arxiv From Word Models to World Models Transl.pdf}
}

@misc{wong.l:2025arxiv,
  title = {Modeling {{Open-World Cognition}} as {{On-Demand Synthesis}} of {{Probabilistic Models}}},
  author = {Wong, Lionel and Collins, Katherine M. and Ying, Lance and Zhang, Cedegao E. and Weller, Adrian and Gerstenberg, Tobias and O'Donnell, Timothy and Lew, Alexander K. and Andreas, Jacob D. and Tenenbaum, Joshua B. and {Brooke-Wilson}, Tyler},
  year = {2025},
  month = jul,
  number = {arXiv:2507.12547},
  eprint = {2507.12547},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2507.12547},
  urldate = {2025-08-01},
  abstract = {When faced with novel situations, people are able to marshal relevant considerations from a wide range of background knowledge and put these to use in inferences and predictions. What permits us to draw in globally relevant information and reason over it coherently? Here, we explore the hypothesis that people use a combination of distributed and symbolic representations to construct bespoke mental models tailored to novel situations. We propose a computational implementation of this idea -- a ``Model Synthesis Architecture'' (MSA) -- using language models to implement global relevance-based retrieval and model synthesis and probabilistic programs to implement bespoke, coherent world models. We evaluate our MSA as a model of human judgments on a novel reasoning dataset. The dataset -- built around a `Model Olympics` domain of sports vignettes -- tests models' capacity for human-like, open-ended reasoning by requiring (i) judgments about novel causal structures described in language; (ii) drawing on large bodies of background knowledge; and (iii) doing both in light of observations that introduce arbitrary novel variables. Our MSA approach captures human judgments better than language model-only baselines, under both direct and chain-of-thought generations from the LM that supports model synthesis. These results suggest that MSAs can be implemented in a way that mirrors people's ability to deliver locally coherent reasoning over globally relevant variables, offering a path to understanding and replicating human reasoning in open-ended domains.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Programming Languages},
  file = {~/Zotfiles/wong.l2025arxiv Modeling Open-World Cognition as On-Dema.pdf}
}

@article{wood.s:2003,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  year = {2003},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  publisher = {Wiley},
  doi = {10.1111/1467-9868.00374},
  bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00374},
  date-added = {2021-12-02 20:55:01 -0500},
  date-modified = {2021-12-02 21:03:21 -0500}
}

@article{wood.s:2004,
  title = {Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models},
  author = {Wood, Simon N},
  year = {2004},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673--686},
  publisher = {Informa UK Limited},
  doi = {10.1198/016214504000000980},
  bdsk-url-2 = {https://doi.org/10.1198/016214504000000980},
  date-added = {2021-12-02 20:52:03 -0500},
  date-modified = {2021-12-02 20:52:18 -0500}
}

@article{wood.s:2011,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N.},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {3--36},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2010.00749.x},
  date-added = {2021-12-02 20:46:39 -0500},
  date-modified = {2021-12-02 20:49:53 -0500}
}

@article{wood.s:2014,
  title = {Generalized Additive Models for Large Data Sets},
  author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
  year = {2014},
  month = may,
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {64},
  number = {1},
  pages = {139--155},
  publisher = {Wiley},
  doi = {10.1111/rssc.12068},
  bdsk-url-2 = {https://doi.org/10.1111/rssc.12068},
  date-added = {2021-12-14 20:13:41 -0500},
  date-modified = {2021-12-14 20:21:11 -0500},
  file = {~/Zotfiles/wood.s2014bam Generalized additive models for large da.pdf}
}

@article{wood.s:2016,
  title = {Smoothing Parameter and Model Selection for General Smooth Models},
  author = {Wood, Simon N. and Pya, Natalya and S{\"a}fken, Benjamin},
  year = {2016},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1548--1563},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1180986},
  urldate = {2022-09-27},
  abstract = {This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online.},
  keywords = {Additive model,AIC,Distributional regression,GAM,gaulss,location scale additive models,Location scale and shape model,Ordered categorical regression,Penalized regression spline,REML,Smooth Cox model,Smoothing parameter uncertainty,Statistical algorithm,Tweedie distribution.}
}

@book{wood.s:2017book,
  title = {Generalized Additive Models},
  author = {Wood, Simon N.},
  year = {2017},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315370279}
}

@article{wu.b:1999,
  title = {Approximation and Exact Algorithms for Constructing Minimum Ultrametric Trees from Distance Matrices},
  author = {Wu, Bang Ye and Chao, Kun-Mao and Tang, Chuan Yi},
  year = {1999},
  journal = {Journal of Combinatorial Optimization},
  volume = {3},
  number = {2},
  pages = {199--211},
  issn = {1573-2886},
  abstract = {An edge-weighted tree is called ultrametric if the distances from the root to all the leaves in the tree are equal. For an n by n distance matrix M, the minimum ultrametric tree for M is an ultrametric tree T = (V, E, w) with leaf set \{1,..., n\} such that dT(i, j) {$\geq$} M[i, j] for all i, j and \$\${\textbackslash}sum \{\_\{e {\textbackslash}in E\} w(e)\}\$\$is minimum, where dT(i, j) is the distance between i and j on T. Constructing minimum ultrametric trees from distance matrices is an important problem in computational biology. In this paper, we examine its computational complexity and approximability. When the distances satisfy the triangle inequality, we show that the minimum ultrametric tree problem can be approximated in polynomial time with error ratio 1.5(1 + {$\lceil$}log n{$\rceil$}), where n is the number of species. We also develop an efficient branch-and-bound algorithm for constructing the minimum ultrametric tree for both metric and non-metric inputs. The experimental results show that it can find an optimal solution for 25 species within reasonable time, while, to the best of our knowledge, there is no report of algorithms solving the problem even for 12 species.},
  date-added = {2019-07-17 13:39:35 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@misc{wu.m:2022arxiv,
  title = {Foundation Posteriors for Approximate Probabilistic Inference},
  author = {Wu, Mike and Goodman, Noah},
  year = {2022},
  month = may,
  number = {arXiv:2205.09735},
  eprint = {2205.09735},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09735},
  urldate = {2022-08-18},
  abstract = {Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a ``foundation'' posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,inference,probabilistic programming,Statistics - Machine Learning},
  file = {~/Zotfiles/wu.m2022 Foundation posteriors for approximate pr.pdf}
}

@inproceedings{wu.s:2010,
  title = {Complexity {{Metrics}} in an {{Incremental Right-Corner Parser}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Stephen and Bachrach, Asaf and Cardenas, Carlos and Schuler, William},
  year = {2010},
  month = jul,
  pages = {1189--1198},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  urldate = {2022-05-31},
  file = {~/Zotfiles/wu.s2010 Complexity Metrics in an Incremental Rig.pdf}
}

@inproceedings{wu.z:2020,
  title = {Perturbed Masking: {{Parameter-free}} Probing for Analyzing and Interpreting {{BERT}}},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  year = {2020},
  pages = {4166--4176},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.383},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.383},
  file = {~/Zotfiles/wu.z2021 Perturbed masking Parameter-free probin.pdf}
}

@misc{xefteri.v:2025arxiv,
  title = {Syntactic {{Control}} of {{Language Models}} by {{Posterior Inference}}},
  author = {Xefteri, Vicky and Vieira, Tim and Cotterell, Ryan and Amini, Afra},
  year = {2025},
  month = jun,
  number = {arXiv:2506.07154},
  eprint = {2506.07154},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2506.07154},
  urldate = {2025-06-26},
  abstract = {Controlling the syntactic structure of text generated by language models is valuable for applications requiring clarity, stylistic consistency, or interpretability, yet it remains a challenging task. In this paper, we argue that sampling algorithms based on the posterior inference can effectively enforce a target constituency structure during generation. Our approach combines sequential Monte Carlo, which estimates the posterior distribution by sampling from a proposal distribution, with a syntactic tagger that ensures that each generated token aligns with the desired syntactic structure. Our experiments with GPT2 and Llama3-8B models show that with an appropriate proposal distribution, we can improve syntactic accuracy, increasing the F1 score from \$12.31\$ (GPT2-large) and \$35.33\$ (Llama3-8B) to about \$93\$ in both cases without compromising the language model's fluency. These results underscore both the complexity of syntactic control and the effectiveness of sampling algorithms, offering a promising approach for applications where precise control over syntax is essential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/xefteri.v2025arxiv Syntactic Control of Language Models by.pdf}
}

@article{xiang.m:2009,
  title = {Illusory Licensing Effects across Dependency Types: {{ERP}} Evidence},
  shorttitle = {Illusory Licensing Effects across Dependency Types},
  author = {Xiang, Ming and Dillon, Brian and Phillips, Colin},
  year = {2009},
  month = jan,
  journal = {Brain and Language},
  volume = {108},
  number = {1},
  pages = {40--55},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2008.10.002},
  urldate = {2024-05-27},
  abstract = {A number of recent studies have argued that grammatical illusions can arise in the process of completing linguistic dependencies, such that unlicensed material is temporarily treated as licensed due to the presence of a potential licensor that is semantically appropriate but in a syntactically inappropriate position. A frequently studied case involves illusory licensing of negative polarity items (NPIs) like ever and any, which must appear in the scope (i.e., c-command domain) of a negative element. Speakers often show intrusive licensing effects in sentences where an NPI is preceded but not c-commanded by a negative element, as in {$\ast$}The restaurants that no newspapers have recommended in their reviews have ever gone out of business. Existing accounts of intrusive licensing have focused on the role of general memory retrieval processes. In contrast, we propose that intrusive licensing of NPIs reflects semantic/pragmatic processes that are more specific to NPI licensing. As a test of this claim, we present results from an ERP study that presents a structurally matched comparison of intrusive licensing in two types of linguistic dependencies, namely NPI licensing and the binding of reflexive anaphors like himself, and herself. In the absence of a potential licensor, both NPIs and reflexives elicit a P600 response, but whereas there is an immediate ERP analog of the intrusion effect for NPI licensing, no such effect is found for reflexive binding. This suggests that the NPI intrusion effect does not reflect general-purpose retrieval mechanisms.},
  keywords = {Anaphora,Binding,ERP,Negative polarity items (NPI),Semantic/pragmatic processing,Sentence processing},
  file = {~/Zotfiles/xiang.m2009 Illusory licensing effects across depend.pdf}
}

@article{xu.f:2007,
  title = {Word Learning as {{Bayesian}} Inference},
  author = {Xu, Fei and Tenenbaum, Joshua B.},
  year = {2007},
  journal = {Psychological Review},
  volume = {114},
  number = {2},
  pages = {245--272},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.114.2.245},
  abstract = {The authors present a Bayesian framework for understanding how adults and children learn the meanings of words. The theory explains how learners can generalize meaningfully from just one or a few positive examples of a novel word's referents, by making rational inductive inferences that integrate prior knowledge about plausible word meanings with the statistical structure of the observed examples. The theory addresses shortcomings of the two best known approaches to modeling word learning, based on deductive hypothesis elimination and associative learning. Three experiments with adults and children test the Bayesian account's predictions in the context of learning words for object categories at multiple levels of a taxonomic hierarchy. Results provide strong support for the Bayesian account over competing accounts, in terms of both quantitative model fits and the ability to explain important qualitative phenomena. Several extensions of the basic theory are discussed, illustrating the broader potential for Bayesian models of word learning. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {bayesian size principle,Concepts,Inference,Learning,Words (Phonetic Units)},
  file = {~/Zotfiles/xu.f2007 Word learning as Bayesian inference.pdf}
}

@inproceedings{xu.w:2023,
  title = {The Linearity of the Effect of Surprisal on Reading Times across Languages},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Xu, Weijie and Chon, Jason and Liu, Tianran and Futrell, Richard},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {15711--15721},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.1052},
  urldate = {2024-01-14},
  abstract = {In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into the linearity of surprisal on reading times. Here, we extend the investigation by examining eyetracking corpora of seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian. We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.},
  keywords = {superlinearity},
  file = {~/Zotfiles/xu.w2023 The linearity of the effect of surprisal.pdf}
}

@article{xu.w:2025,
  title = {Informativity Enhances Memory Robustness against Interference in Sentence Comprehension},
  author = {Xu, Weijie and Futrell, Richard},
  year = {2025},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {142},
  pages = {104603},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104603},
  urldate = {2025-03-05},
  abstract = {Language comprehension has been argued to be expectation-based, with more predictable linguistic units being easier to process. However, as a communicative tool, language is often used to deliver messages that are novel and informative, suggesting the necessity of some cognitive mechanisms handling less predictable but more informative content. This paper proposes strategic memory allocation as one such mechanism. Although less predictable linguistic units require greater processing effort for memory encoding, recognizing the inconsistency between top-down predictions and bottom-up perceptual input may signal the working memory system to prioritize these units, enhancing the robustness of their representation against interference. We examine this hypothesis through the lens of the agreement attraction effect in two self-paced reading experiments. In Experiment 1, we find that less predictable but more informative target nouns exhibit weaker agreement attraction in online reading times, especially with more fine-grained measures of predictability such as the surprisal from large language models. This weaker agreement attraction effect for less predictable target nouns confirms our hypothesis that informative linguistic units are prioritized and receive more robust memory representation. In Experiment 2, however, no modulation of agreement attraction emerges when we manipulate the predictability of distractor nouns, suggesting the need for a more nuanced characterization of how information is structured and operated in memory. Our findings highlight an interplay of memory, predictive processing, and implicit learning. We also discuss the implications of our result for memory efficiency and memory compression. More broadly, by demonstrating that the limited memory resources are dynamically optimized for the relevant processing task, the current study highlights a connection to the resource-rational analysis of human cognition in general.},
  keywords = {Agreement attraction,Informativity,Memory interference,Prediction,Resource-rational,Sentence processing,Strategic memory allocation}
}

@misc{xu.w:2025arxiv,
  title = {Strategic Resource Allocation in Memory Encoding: {{An}} Efficiency Principle Shaping Language Processing},
  shorttitle = {Strategic Resource Allocation in Memory Encoding},
  author = {Xu, Weijie and Futrell, Richard},
  year = {2025},
  month = mar,
  number = {arXiv:2503.14728},
  eprint = {2503.14728},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2503.14728},
  urldate = {2025-03-25},
  abstract = {How is the limited capacity of working memory efficiently used to support human linguistic behaviors? In this paper, we investigate strategic resource allocation as an efficiency principle for memory encoding in sentence processing. The idea is that working memory resources are dynamically and strategically allocated to prioritize novel and unexpected information, enhancing their representations to make them less susceptible to memory decay and interference. Theoretically, from a resource-rational perspective, we argue that this efficiency principle naturally arises from two functional assumptions about working memory, namely, its limited capacity and its noisy representation. Empirically, through naturalistic corpus data, we find converging evidence for strategic resource allocation in the context of dependency locality from both the production and the comprehension side, where non-local dependencies with less predictable antecedents are associated with reduced locality effect. However, our results also reveal considerable cross-linguistic variability, highlighting the need for a closer examination of how strategic resource allocation, as a universal efficiency principle, interacts with language-specific phrase structures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {~/Zotfiles/xu.w2025arxiv Strategic resource allocation in memory.pdf}
}

@article{yacovone.a:2021,
  title = {Unexpected Words or Unexpected Languages? {{Two ERP}} Effects of Code-Switching in Naturalistic Discourse},
  shorttitle = {Unexpected Words or Unexpected Languages?},
  author = {Yacovone, Anthony and Moya, Emily and Snedeker, Jesse},
  year = {2021},
  month = oct,
  journal = {Cognition},
  volume = {215},
  pages = {104814},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104814},
  urldate = {2025-03-12},
  abstract = {Bilingual speakers often switch between languages in conversation without any advance notice. Psycholinguistic research has found that these language shifts (or code-switches) can be costly for comprehenders in certain situations. The present study explores the nature of these costs by comparing code-switches to other types of unexpected linguistic material. To do this, we used a novel EEG paradigm, the Storytime task, in which we record readings of natural texts, and then experimentally manipulate their properties by splicing in words. In this study, we manipulated the language of our target words (English, Spanish) and their fit with the preceding context (strong-fit, weak-fit). If code-switching incurs a unique cost beyond that incurred by an unexpected word, then we should see an additive pattern in our ERP indices. If an effect is driven by lexical expectation alone, then there should be a non-additive interaction such that all unexpected forms incur a similar cost. We found three effects: a general prediction effect (a non-additive N400), a post-lexical recognition of the switch in languages (an LPC for code-switched words), and a prolonged integration difficulty associated with weak-fitting words regardless of language (a sustained negativity). We interpret these findings as suggesting that the processing difficulties experienced by bilinguals can largely be understood within more general frameworks for understanding language comprehension. Our findings are consistent with the broader literature demonstrating that bilinguals do not have two wholly separate language systems but rather a single language system capable of using two coding systems.},
  keywords = {Bilingualism,Discourse,EEG/ERP,Form-based prediction,LPC,N400},
  file = {~/Zotfiles/yacovone.a2021 Unexpected words or unexpected languages.pdf}
}

@article{yacovone.a:2025,
  title = {Let Them Eat Ceke: {{An}} Electrophysiological Study of Form-Based Prediction in Rich Naturalistic Contexts},
  shorttitle = {Let Them Eat Ceke},
  author = {Yacovone, Anthony and Waite, Briony and Levari, Tatyana and Snedeker, Jesse},
  year = {2025},
  journal = {Journal of Experimental Psychology: General},
  volume = {154},
  number = {3},
  pages = {711--738},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222},
  doi = {10.1037/xge0001677},
  abstract = {It is well-established that people make predictions during language comprehension----the nature and specificity of these predictions, however, remain unclear. For example, do comprehenders routinely make predictions about which words (and phonological forms) might come next in a conversation, or do they simply make broad predictions about the gist of the unfolding context? Prior EEG studies using tightly controlled experimental designs have shown that form-based prediction can occur during comprehension, as N400s to unexpected words are reduced when they resemble the form of a predicted word (e.g., ceke when expecting cake). One limitation, however, is that these studies often create environments that are optimal for eliciting form-based prediction (e.g., highly constraining sentences, slower-than-natural rates of presentation). Thus, questions remain about whether form-based prediction can occur in settings that more closely resemble everyday comprehension. To address this, the present study explores form-based prediction during naturalistic spoken language comprehension. English-speaking adults listened to a story in which some of the words had been altered. Specifically, we experimentally manipulated whether participants heard the original word from the story (cake), a form-similar nonword (ceke), or a less-similar nonword (vake). Half of the target words were predictable given their context, and the other half were unpredictable. Consistent with the prior work, we found reduced N400s for form-similar nonwords (ceke) relative to less-similar nonwords (vake)---but only in predictable contexts. This study demonstrates that form-based prediction can emerge in naturalistic contexts, and therefore, it is likely to be a common aspect of language comprehension in the wild. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Comprehension,Electrophysiology,Evoked Potentials,Form and Shape Perception,Prediction,Words (Phonetic Units)},
  file = {~/Zotfiles/yacovone.a2025 Let them eat ceke An electrophysiologic.pdf}
}

@inproceedings{yamakoshi.t:2022,
  title = {Probing {{BERT}}'s Priors with Serial Reproduction Chains},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Yamakoshi, Takateru and Griffiths, Thomas and Hawkins, Robert},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {3977--3992},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.314},
  urldate = {2023-12-27},
  abstract = {Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of serial reproduction chains to sample from BERT's priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.},
  file = {~/Zotfiles/yamakoshi.t2022 Probing BERT's priors with serial reprod.pdf}
}

@inproceedings{yang.k:2020,
  title = {Strongly Incremental Constituency Parsing with Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Kaiyu and Deng, Jia},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {21687--21698},
  publisher = {Curran Associates, Inc.},
  file = {~/Zotfiles/yang.k2020 Strongly incremental constituency parsin.pdf}
}

@inproceedings{yang.s:2020,
  title = {Second-Order Unsupervised Neural Dependency Parsing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  author = {Yang, Songlin and Jiang, Yong and Han, Wenjuan and Tu, Kewei},
  year = {2020},
  pages = {3911--3924},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.347},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.347},
  file = {~/Zotfiles/yang.s2020 Second-order unsupervised neural depende.pdf}
}

@inproceedings{yang.z:2018,
  title = {Breaking the Softmax Bottleneck: {{A}} High-Rank {{RNN}} Language Model},
  booktitle = {International Conference on Learning Representations},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
  year = {2018}
}

@inproceedings{yang.z:2019,
  title = {{{XLNet}}: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  month = dec,
  volume = {32},
  pages = {5754--5764},
  publisher = {Curran Associates, Inc.},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
  date-modified = {2021-09-09 23:04:25 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {~/Zotfiles/yang.z2019 XLNet Generalized autoregressive pretra.pdf}
}

@article{yarkoni.t:2008,
  title = {Moving beyond {{Coltheart}}'s {{N}}: {{A}} New Measure of Orthographic Similarity},
  shorttitle = {Moving beyond {{Coltheart}}'s {{N}}},
  author = {Yarkoni, Tal and Balota, David and Yap, Melvin},
  year = {2008},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {15},
  number = {5},
  pages = {971--979},
  issn = {1531-5320},
  doi = {10.3758/PBR.15.5.971},
  urldate = {2025-06-20},
  abstract = {Visual word recognition studies commonly measure the orthographic similarity of words using Coltheart's orthographic neighborhood size metric (ON). Although ON reliably predicts behavioral variability in many lexical tasks, its utility is inherently limited by its relatively restrictive definition. In the present article, we introduce a new measure of orthographic similarity generated using a standard computer science metric of string similarity (Levenshtein distance). Unlike ON, the new measure---named orthographic Levenshtein distance 20 (OLD20)---incorporates comparisons between all pairs of words in the lexicon, including words of different lengths. We demonstrate that OLD20 provides significant advantages over ON in predicting both lexical decision and pronunciation performance in three large data sets. Moreover, OLD20 interacts more strongly with word frequency and shows stronger effects of neighborhood frequency than does ON. The discussion section focuses on the implications of these results for models of visual word recognition.},
  langid = {english},
  keywords = {Graphemics,Lexical Access,Lexical Decision,Literary Diction,OLD20,Orthographic Neighborhood,Orthography,Psychometrics,Similar Word,Structural Variation,Stylistics,Visual Word Recognition},
  file = {~/Zotfiles/yarkoni.t2008 Moving beyond Colthearts N A new measu.pdf}
}

@article{ye.l:2020,
  title = {Monte {{Carlo}} Co-Ordinate Ascent Variational Inference},
  author = {Ye, Lifeng and Beskos, Alexandros and De Iorio, Maria and Hao, Jie},
  year = {2020},
  month = jul,
  journal = {Statistics and Computing},
  volume = {30},
  number = {4},
  pages = {887--905},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09924-y},
  urldate = {2022-06-27},
  abstract = {In variational inference (VI), coordinate-ascent and gradient-based approaches are two major types of algorithms for approximating difficult-to-compute probability densities. In real-world implementations of complex models, Monte Carlo methods are widely used to estimate expectations in coordinate-ascent approaches and gradients in derivative-driven ones. We discuss a Monte Carlo co-ordinate ascent VI (MC-CAVI) algorithm that makes use of Markov chain Monte Carlo (MCMC) methods in the calculation of expectations required within co-ordinate ascent VI (CAVI). We show that, under regularity conditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of the evidence lower bound with any given high probability. In numerical examples, the performance of MC-CAVI algorithm is compared with that of MCMC and---as a representative of derivative-based VI methods---of Black Box VI (BBVI). We discuss and demonstrate MC-CAVI's suitability for models with hard constraints in simulated and real examples. We compare MC-CAVI's performance with that of MCMC in an important complex model used in nuclear magnetic resonance spectroscopy data analysis---BBVI is nearly impossible to be employed in this setting due to the hard constraints involved in the model.},
  langid = {english},
  keywords = {Bayesian inference,Coordinate-ascent,Gradient-based optimisation,Markov chain Monte Carlo,Nuclear magnetic resonance,Variational inference},
  file = {~/Zotfiles/ye.l2020 Monte Carlo co-ordinate ascent variation.pdf}
}

@incollection{yeung.r:,
  title = {The {{I-Measure}}},
  booktitle = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  series = {Information Technology Transmission Processing and Storage},
  pages = {51--80},
  publisher = {Springer US},
  doi = {10.1007/978-0-387-79234-7_3},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7{$_{3}$}},
  chapter = {3},
  date-added = {2021-09-21 18:19:43 -0400},
  date-modified = {2021-09-21 18:22:32 -0400}
}

@article{yeung.r:1991,
  title = {A New Outlook on {{Shannon}}'s Information Measures},
  author = {Yeung, Raymond W.},
  year = {1991},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {3},
  pages = {466--474},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.79902},
  bdsk-url-2 = {https://doi.org/10.1109/18.79902},
  date-added = {2021-09-20 18:00:57 -0400},
  date-modified = {2021-09-21 18:18:41 -0400}
}

@book{yeung.r:2008book,
  title = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  year = {2008},
  series = {Information Technology Transmission Processing and Storage},
  publisher = {Springer US},
  doi = {10.1007/978-0-387-79234-7},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7},
  date-added = {2021-09-21 18:16:02 -0400},
  date-modified = {2021-09-21 18:18:28 -0400}
}

@article{yngve.v:1960,
  title = {A Model and an Hypothesis for Language Structure},
  author = {Yngve, Victor H.},
  year = {1960},
  journal = {Proceedings of the American Philosophical Society},
  volume = {104},
  number = {5},
  eprint = {985230},
  eprinttype = {jstor},
  pages = {444--466},
  publisher = {American Philosophical Society},
  issn = {0003-049X},
  urldate = {2023-03-10},
  file = {~/Zotfiles/yngve.v1960 A model and an hypothesis for language s.pdf}
}

@inproceedings{yoshida.r:2021,
  title = {Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yoshida, Ryo and Noji, Hiroshi and Oseki, Yohei},
  year = {2021},
  pages = {2964--2973},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.235},
  urldate = {2022-08-13},
  langid = {english},
  file = {~/Zotfiles/yoshida.r2021 Modeling human sentence processing with.pdf}
}

@inproceedings{yoshida.r:2022,
  title = {Composition, {{Attention}}, or {{Both}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Yoshida, Ryo and Oseki, Yohei},
  year = {2022},
  month = dec,
  pages = {5822--5834},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.findings-emnlp.428},
  urldate = {2023-08-18},
  abstract = {In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components---the composition function and the self-attention mechanism---can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations.},
  file = {~/Zotfiles/yoshida.r2022 Composition, Attention, or Both.pdf}
}

@inproceedings{yu.l:2022,
  title = {The Neural Noisy Channel},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yu, Lei and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Kocisky, Tomas},
  year = {2022},
  month = jul,
  urldate = {2022-10-25},
  abstract = {We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.},
  langid = {english},
  file = {~/Zotfiles/yu.l2022 The neural noisy channel.pdf}
}

@article{yun.j:2014,
  title = {Uncertainty in Processing Relative Clauses across {{East Asian}} Languages},
  author = {Yun, Jiwon and Chen, Zhong and Hunter, Tim and Whitman, John and Hale, John},
  year = {2014},
  journal = {Journal of East Asian Linguistics},
  volume = {24},
  number = {2},
  pages = {113--148},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10831-014-9126-6},
  bdsk-url-2 = {https://doi.org/10.1007/s10831-014-9126-6},
  date-added = {2021-03-18 10:38:55 -0400},
  date-modified = {2021-03-18 10:39:18 -0400},
  keywords = {entropy reduction,processing},
  file = {~/Zotfiles/yun.j2014 Uncertainty in processing relative claus.pdf}
}

@phdthesis{yuret.d:1998phd,
  title = {Discovery of Linguistic Relations Using Lexical Attraction},
  author = {Yuret, Deniz},
  year = {1998},
  eprint = {cmp-lg/9805009},
  archiveprefix = {arXiv},
  date-added = {2020-04-23 12:44:41 -0400},
  date-modified = {2020-04-24 12:35:24 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science},
  keywords = {dependency parsing,dependency structures,mutual information}
}

@article{yuret.d:2006,
  title = {Lexical Attraction Models of Language},
  author = {Yuret, Deniz},
  year = {2006},
  journal = {Ms., Ko{\c c} University, Istanbul, Turkey,},
  date-added = {2019-09-12 19:59:44 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lexical attraction,mutual information}
}

@incollection{zaenen.a:1985,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1985},
  pages = {93--136},
  publisher = {Brill},
  date-added = {2020-02-15 18:32:26 -0500},
  date-modified = {2020-02-15 18:38:25 -0500},
  project = {Icelandic gluttony},
  keywords = {dative subjecthood}
}

@incollection{zaenen.a:1990,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1990},
  pages = {93--136},
  publisher = {Brill},
  date-added = {2020-02-03 16:19:23 -0500},
  date-modified = {2020-02-03 16:19:39 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{zarcone.a:2016,
  title = {Salience and Attention in Surprisal-Based Accounts of Language Processing},
  author = {Zarcone, Alessandra and {van Schijndel}, Marten and Vogels, Jorrig and Demberg, Vera},
  year = {2016},
  month = jun,
  journal = {Frontiers in Psychology},
  volume = {7},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00844},
  urldate = {2024-07-22},
  abstract = {{$<$}p{$>$}The notion of {$<$}italic{$>$}salience{$<$}/italic{$>$} has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g., visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g., prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as {$<$}italic{$>$}surprisal{$<$}/italic{$>$}. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience-based accounts and surprisal-based accounts. We capitalize upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.{$<$}/p{$>$}},
  langid = {english},
  keywords = {accessibility,Attention,goals,Language,predictability,predictive coding,relevance,salience,surprisal},
  file = {~/Zotfiles/zarcone.a2016 Salience and Attention in Surprisal-Base.pdf}
}

@article{zaslavsky.n:2018PNAS,
  title = {Efficient Compression in Color Naming and Its Evolution},
  author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {31},
  eprint = {https://www.pnas.org/content/115/31/7937.full.pdf},
  pages = {7937--7942},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1800521115},
  date-added = {2019-05-15 00:03:28 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory}
}

@misc{zehr.j:2018,
  title = {{{PennController}} for {{Internet Based Experiments}} ({{IBEX}})},
  author = {Zehr, J{\'e}r{\'e}my and Schwarz, Florian},
  year = {2018},
  month = mar,
  doi = {10.17605/OSF.IO/MD832},
  urldate = {2024-02-22},
  abstract = {PennController is a library extension for IBEX. It introduces a flexible and user-friendly syntax to design dynamic (e.g., scripted/timed events) and interactive (e.g., pictures, audio, videos, ...) trials.},
  collaborator = {{Open Science Framework}},
  copyright = {BSD 3-Clause "New"/"Revised" License},
  howpublished = {Open Science Framework}
}

@misc{zeng.a:2022arxiv,
  title = {{{GLM-130B}}: {{An}} Open Bilingual Pre-Trained Model},
  shorttitle = {Glm-130b},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02414},
  eprint = {2210.02414},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-31},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4\${\textbackslash}times\$RTX 3090 (24G) or 8\${\textbackslash}times\$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/zeng.a2022GLM130B GLM-130B An open bilingual pre-trained.pdf}
}

@article{zenon.a:2019,
  title = {An Information-Theoretic Perspective on the Costs of Cognition},
  author = {Z{\'e}non, Alexandre and Solopchuk, Oleg and Pezzulo, Giovanni},
  year = {2019},
  month = feb,
  journal = {Neuropsychologia},
  series = {Cognitive {{Effort}}},
  volume = {123},
  pages = {5--18},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2018.09.013},
  urldate = {2025-05-13},
  abstract = {In statistics and machine learning, model accuracy is traded off with complexity, which can be viewed as the amount of information extracted from the data. Here, we discuss how cognitive costs can be expressed in terms of similar information costs, i.e. as a function of the amount of information required to update a person's prior knowledge (or internal model) to effectively solve a task. We then examine the theoretical consequences that ensue from this assumption. This framework naturally explains why some tasks -- for example, unfamiliar or dual tasks -- are costly and permits to quantify these costs using information-theoretic measures. Finally, we discuss brain implementation of this principle and show that subjective cognitive costs can originate either from local or global capacity limitations on information processing or from increased rate of metabolic alterations. These views shed light on the potential adaptive value of cost-avoidance mechanisms.},
  keywords = {Active inference,Cognitive effort,Computational neuroscience,Efficient coding,Information theory,Predictive coding},
  file = {~/Zotfiles/zenon.a2019 An information-theoretic perspective on.pdf}
}

@inproceedings{zhan.m:2018,
  title = {Comparing Theories of Speaker Choice Using a Model of Classifier Production in {{Mandarin Chinese}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Zhan, Meilin and Levy, Roger},
  year = {2018},
  month = jun,
  pages = {1997--2005},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1181},
  urldate = {2022-10-29},
  abstract = {Speakers often have more than one way to express the same meaning. What general principles govern speaker choice in the face of optionality when near semantically invariant alternation exists? Studies have shown that optional reduction in language is sensitive to contextual predictability, such that more predictable a linguistic unit is, the more likely it is to get reduced. Yet it is unclear whether these cases of speaker choice are driven by audience design versus toward facilitating production. Here we argue that for a different optionality phenomenon, namely classifier choice in Mandarin Chinese, Uniform Information Density and at least one plausible variant of availability-based production make opposite predictions regarding the relationship between the predictability of the upcoming material and speaker choices. In a corpus analysis of Mandarin Chinese, we show that the distribution of speaker choices supports the availability-based production account and not the Uniform Information Density.},
  keywords = {uniform information density}
}

@misc{zhang.k:2018arxiv,
  title = {Language Modeling Teaches You More Syntax than Translation Does: {{Lessons}} Learned through Auxiliary Task Analysis},
  author = {Zhang, Kelly W. and Bowman, Samuel R.},
  year = {2018},
  number = {arXiv:1809.10040},
  eprint = {1809.10040},
  primaryclass = {cs},
  publisher = {arXiv},
  archiveprefix = {arXiv},
  keywords = {CoVe,ELMo,LSTM,syntactic information},
  file = {~/Zotfiles/zhang.k2018 Language modeling teaches you more synta.pdf}
}

@inproceedings{zhang.m:2020,
  title = {Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced {{Monte-Carlo}} Approach},
  shorttitle = {Language Generation via Combinatorial Constraint Satisfaction},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Zhang, Maosen and Jiang, Nan and Li, Lei and Xue, Yexiang},
  year = {2020},
  month = nov,
  pages = {1286--1298},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.115},
  urldate = {2023-06-03},
  abstract = {Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMC, an efficient method to generate high likelihood sentences with respect to a pre-trained language model while satisfying the constraints. Our approach is highly flexible, requires no task-specific train- ing, and leverages efficient constraint satisfaction solving techniques. To better handle the combinatorial constraints, a tree search algorithm is embedded into the proposal process of the Markov Chain Monte Carlo (MCMC) to explore candidates that satisfy more constraints. Compared to existing MCMC approaches, our sampling approach has a better mixing performance. Experiments show that TSMC achieves consistent and significant improvement on multiple language generation tasks.},
  file = {~/Zotfiles/zhang.m2020 Language generation via combinatorial co.pdf}
}

@misc{zhang.s:2022OPT,
  title = {{{OPT}}: {{Open Pre-trained Transformer}} Language Models},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01068},
  urldate = {2023-05-04},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {~/Zotfiles/zhang.s2022OPT OPT Open Pre-trained Transformer langua.pdf}
}

@article{zhang.t:2021,
  title = {On the Inductive Bias of Masked Language Modeling: {{From}} Statistical to Syntactic Dependencies},
  author = {Zhang, Tianyi and Hashimoto, Tatsunori},
  year = {2021},
  journal = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.naacl-main.404},
  date-added = {2021-09-08 11:13:42 -0400},
  date-modified = {2021-09-08 11:13:47 -0400},
  file = {~/Zotfiles/zhang.t2021 On the inductive bias of masked language.pdf}
}

@inproceedings{zhang.y:2008,
  title = {A Tale of Two Parsers: Investigating and Combining Graph-Based and Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Yue and Clark, Stephen},
  year = {2008},
  month = oct,
  pages = {562--571},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  date-added = {2022-03-25 22:14:22 -0400},
  date-modified = {2022-03-25 22:14:24 -0400},
  file = {~/Zotfiles/zhang.y2008 A tale of two parsers investigating and.pdf}
}

@article{zhang.y:2023,
  title = {A Noisy-Channel Approach to Depth-Charge Illusions},
  author = {Zhang, Yuhan and Ryskin, Rachel and Gibson, Edward},
  year = {2023},
  month = mar,
  journal = {Cognition},
  volume = {232},
  pages = {105346},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105346},
  urldate = {2023-07-26},
  abstract = {The ``depth-charge'' sentence, No head injury is too trivial to be ignored, is often interpreted as ``no matter how trivial head injuries are, we should not ignore them'' while the literal meaning is the opposite -- ``we should ignore them''. Four decades of research have failed to resolve the source of this entrenched semantic illusion. Here we adopt the noisy-channel framework for language comprehension to provide a potential explanation. We hypothesize that depth-charge sentences result from inferences whereby comprehenders derive the interpretation by weighing the plausibility of possible readings of the depth-charge sentences against the likelihood of plausible sentences being produced with errors. In four experiments, we find that (1) the more plausible the intended meaning of the depth-charge sentence is, the more likely the sentence is to be misinterpreted; and (2) the higher the likelihood of our hypothesized noise operations, the more likely depth-charge sentences are to be misinterpreted. These results suggest that misinterpretation is affected by both world knowledge and the distance between the depth-charge sentence and a plausible alternative, which is consistent with the noisy-channel framework.},
  langid = {english},
  keywords = {Depth-charge sentence,Language comprehension,Noisy-channel framework,noisy-channel processing,Semantic illusion},
  file = {~/Zotfiles/zhang.y2023 A noisy-channel approach to depth-charge.pdf}
}

@misc{zhang.y:2023amlap,
  type = {Talk},
  title = {A Noisy-Channel Explanation of the Comparative Illusion},
  author = {Zhang, Yuhan and Kauf, Carina and Gibson, Edward},
  year = {2023},
  month = sep,
  address = {Donostia-San Sebastian, Spain},
  abstract = {The comparative illusion sentence, "More students have been to Russia than I have", is acceptable on first reading, but upon closer examination, its meaning becomes hard to discern. Previous research has identified several factors that affect the illusion degree -- the repeatability of the action ("go to Russia" vs. "escape from Russia"), the than-clause subject form (pronoun "I" vs. noun phrase "the teacher"), and the subject number (singular vs. plural) (O'Connor, 2015; Wellwood et al. 2018), but there has not been a thorough explanation of how the illusion arises. We propose a noisy-channel explanation (Gibson et al. 2013; Levy, 2008) and argue that readers could arrive at any of three possible interpretations (event comparison, event negation, individual comparison) depending on (1) the structural distance between the perceived anomalous sentence and the alternative which carries the interpretation and (2) the plausibility of the associated interpretation. Through a series of experiments, we show that, given equal plausibility of the possible interpretations, participants are more likely to choose the interpretation associated with the structure that necessitates the fewest and more probable edits. This study provides another linguistic phenomenon supporting the rational aspect of human language processing.},
  file = {~/Zotfiles/zhang.y2023amlap A noisy-channel explanation of the compa.pdf}
}

@inproceedings{zhang.y:2023conll,
  title = {Can Language Models Be Tricked by Language Illusions? {{Easier}} with Syntax, Harder with Semantics},
  shorttitle = {Can Language Models Be Tricked by Language Illusions?},
  booktitle = {Proceedings of the 27th {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Zhang, Yuhan and Gibson, Edward and Davis, Forrest},
  editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
  year = {2023},
  month = dec,
  pages = {1--14},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.conll-1.1},
  urldate = {2024-05-27},
  abstract = {Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with ``language illusions'' -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. ``More people have been to Russia than I have''), the depth-charge illusion (e.g. ``No head injury is too trivial to be ignored''), and the negative polarity item (NPI) illusion (e.g. ``The hunter who no villager believed to be trustworthy will ever shoot a bear''). We found that probabilities represented by LMs were more likely to align with human judgments of being ``tricked'' by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.},
  file = {~/Zotfiles/zhang.y2023conll Can language models be tricked by langua.pdf}
}

@misc{zhang.y:2024HSP,
  type = {Talk},
  title = {A Memory-Based Account of Robust Negative Polarity Illusion Effects},
  author = {Zhang, Yuhan and Gibson, Edward},
  year = {2024},
  month = may,
  address = {University of Michigan, Ann Arbor},
  file = {~/Zotfiles/zhang.y2024HSP A memory-based account of robust negativ.pdf}
}

@phdthesis{zhang.y:2024phd,
  title = {The Rational Processing of Language Illusions},
  author = {Zhang, Yuhan},
  year = {2024},
  month = jun,
  address = {Cambridge, Massachusetts},
  langid = {english},
  school = {Harvard University},
  file = {~/Zotfiles/zhang.y2024phd The rational processing of language illu.pdf}
}

@misc{zhang.y:2024psyarxiv,
  title = {Comparative Illusions Are Evidence of Rational Inference in Language Comprehension},
  author = {Zhang, Yuhan and Kauf, Carina and Levy, Roger Philip and Gibson, Edward},
  year = {2024},
  month = may,
  number = {efr3q},
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/efr3q},
  urldate = {2024-05-27},
  abstract = {During language comprehension, people sometimes accept sentences that are ungrammatical or semantically implausible and the cognitive mechanism underlying this language illusion is understudied. In this paper, we study the ``comparative illusion'' (CI) phenomenon where people believe the sentence "More people have been to Russia than I have" to be acceptable while in fact it is semantically ill-formed. We provide a potential explanation for the reasons behind the language illusion from the noisy-channel framework. We hypothesize that comprehenders make rational inference over the perceived sentence by entertaining alternative plausible interpretations and weighing the likelihood of the plausible interpretation being produced with errors. In four experiments, we identified the construction that elicited the most salient illusion effect, established the range of the plausible interpretations, and found that the probability for comprehenders to assign a certain plausible interpretation to the CI sentence is proportional to how likely they think that that interpretation is encapsulated into the CI sentence during speech production. This finding is consistent with the noisy-channel predictions and provides another example supporting the rational behavior underlying language comprehension.},
  langid = {american},
  keywords = {information theory,language illusion,language processing,psycholinguistics,rational inference}
}

@inproceedings{zhao.s:2024,
  title = {Probabilistic Inference in Language Models via Twisted Sequential Monte Carlo},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger Baker},
  year = {2024},
  month = jul,
  pages = {60704--60748},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-30},
  abstract = {Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.},
  langid = {english},
  file = {~/Zotfiles/zhao.s2024twistedSMC Probabilistic Inference in Language Mode.pdf}
}

@misc{zhao.w:2023arxiv,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = nov,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2024-05-23},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/zhao.w2023arxiv A survey of large language models.pdf}
}

@misc{zhao.w:2023arxiva,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = may,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2023-05-31},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/zhao.w2023 A survey of large language models.pdf}
}

@inproceedings{zhou.j:2019,
  title = {Head-{{Driven Phrase Structure Grammar}} Parsing on {{Penn Treebank}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Zhou, Junru and Zhao, Hai},
  year = {2019},
  pages = {2396--2408},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1230},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1230},
  file = {~/Zotfiles/zhou.j2019 Head-Driven Phrase Structure Grammar par.pdf}
}

@article{zhu.j:2020,
  title = {The {{Bayesian}} Sampler: {{Generic Bayesian}} Inference Causes Incoherence in Human Probability Judgments},
  shorttitle = {The {{Bayesian}} Sampler},
  author = {Zhu, Jian-Qiao and Sanborn, Adam N. and Chater, Nick},
  year = {2020},
  journal = {Psychological Review},
  volume = {127},
  number = {5},
  pages = {719--748},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/rev0000190},
  abstract = {Human probability judgments are systematically biased, in apparent tension with Bayesian models of cognition. But perhaps the brain does not represent probabilities explicitly, but approximates probabilistic calculations through a process of sampling, as used in computational probabilistic models in statistics. Na{\"i}ve probability estimates can be obtained by calculating the relative frequency of an event within a sample, but these estimates tend to be extreme when the sample size is small. We propose instead that people use a generic prior to improve the accuracy of their probability estimates based on samples, and we call this model the Bayesian sampler. The Bayesian sampler trades off the coherence of probabilistic judgments for improved accuracy, and provides a single framework for explaining phenomena associated with diverse biases and heuristics such as conservatism and the conjunction fallacy. The approach turns out to provide a rational reinterpretation of ``noise'' in an important recent model of probability judgment, the probability theory plus noise model (Costello \& Watts, 2014, 2016a, 2017; Costello \& Watts, 2019; Costello, Watts, \& Fisher, 2018), making equivalent average predictions for simple events, conjunctions, and disjunctions. The Bayesian sampler does, however, make distinct predictions for conditional probabilities and distributions of probability estimates. We show in 2 new experiments that this model better captures these mean judgments both qualitatively and quantitatively; which model best fits individual distributions of responses depends on the assumed size of the cognitive sample. (PsycInfo Database Record (c) 2020 APA, all rights reserved)},
  keywords = {Cognitive Bias,Judgment,Models,Prediction,Probability,Probability Judgment,Sampling (Experimental),Statistical Probability,Statistics},
  file = {~/Zotfiles/zhu.j2020 The Bayesian sampler Generic Bayesian i.pdf}
}

@article{zhu.j:2023cogsci,
  title = {Computation-{{Limited Bayesian Updating}}},
  author = {Zhu, Jian-Qiao and Sanborn, Adam and Chater, Nick and Griffiths, Tom},
  year = {2023},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {45},
  number = {45},
  urldate = {2023-12-07},
  abstract = {Effectively updating one's beliefs requires sufficient empirical evidence (i.e., data) and the computational capacity to process it. Yet both data and computational resources are limited for human minds. Here, we study the problem of belief updating under limited data and limited computation. Using information theory to characterize constraints on computation, we find that the solution to the resulting optimization problem links the data and computational limitations together: when computational resources are tight, agents may not be able to integrate new empirical evidence. The resource-rational belief updating rule we identify offers a novel interpretation of conservative Bayesian updating.},
  langid = {english},
  keywords = {KL divergence,rational analysis,resource rationality},
  file = {~/Zotfiles/zhu.j2023 Computation-Limited Bayesian Updating.pdf}
}

@misc{zhuang.y:2025arxiv,
  title = {Text {{Generation Beyond Discrete Token Sampling}}},
  author = {Zhuang, Yufan and Liu, Liyuan and Singh, Chandan and Shang, Jingbo and Gao, Jianfeng},
  year = {2025},
  month = may,
  number = {arXiv:2505.14827},
  eprint = {2505.14827},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2505.14827},
  urldate = {2025-05-23},
  abstract = {In standard autoregressive generation, an LLM predicts the next-token distribution, samples a discrete token, and then discards the distribution, passing only the sampled token as new input. To preserve this distribution's rich information, we propose Mixture of Inputs (MoI), a training-free method for autoregressive generation. After generating a token following the standard paradigm, we construct a new input that blends the generated discrete token with the previously discarded token distribution. Specifically, we employ a Bayesian estimation method that treats the token distribution as the prior, the sampled token as the observation, and replaces the conventional one-hot vector with the continuous posterior expectation as the new model input. MoI allows the model to maintain a richer internal representation throughout the generation process, resulting in improved text quality and reasoning capabilities. On mathematical reasoning, code generation, and PhD-level QA tasks, MoI consistently improves performance across multiple models including QwQ-32B, Nemotron-Super-49B, Gemma-3-27B, and DAPO-Qwen-32B, with no additional training and negligible computational overhead.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {~/Zotfiles/zhuang.y2025arxiv Text Generation Beyond Discrete Token Sa.pdf}
}

@article{zipf.g:1929,
  title = {Relative Frequency as a Determinant of Phonetic Change},
  author = {Zipf, George Kingsley},
  year = {1929},
  journal = {Harvard Studies in Classical Philology},
  volume = {40},
  eprint = {310585},
  eprinttype = {jstor},
  pages = {1--95},
  publisher = {Department of the Classics, Harvard University},
  issn = {0073-0688},
  doi = {10.2307/310585},
  urldate = {2024-05-05},
  file = {~/Zotfiles/zipf.g1929 Relative frequency as a determinant of p.pdf}
}

@book{zipf.g:2012book,
  title = {Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology},
  shorttitle = {Human Behavior and the Principle of Least Effort},
  author = {Zipf, George Kingsley},
  year = {2012},
  publisher = {Martino Publishing},
  address = {Mansfield Centre, CT},
  isbn = {978-1-61427-312-7},
  langid = {english}
}

@book{zipf.g:2013book,
  title = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {2013},
  month = nov,
  publisher = {Routledge},
  address = {London, United Kingdom},
  doi = {10.4324/9781315009421},
  urldate = {2022-09-28},
  isbn = {978-1-136-31046-1},
  langid = {english}
}

@inproceedings{zouhar.v:2023,
  title = {Tokenization and the Noiseless Channel},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan and Du, Li and Sachan, Mrinmaya and Cotterell, Ryan},
  year = {2023},
  month = jul,
  pages = {5184--5207},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-24},
  abstract = {Subword tokenization is a key part of most NLP pipelines.However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of R{\'e}nyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance.In machine translation, we find that across multiple tokenizers, the R{\'e}nyi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length.},
  file = {~/Zotfiles/zouhar.v2023 Tokenization and the noiseless channel.pdf}
}

@inproceedings{zwarts.s:2011,
  title = {The Impact of Language Models and Loss Functions on Repair Disfluency Detection},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zwarts, Simon and Johnson, Mark},
  year = {2011},
  month = jun,
  pages = {703--711},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  urldate = {2022-10-24},
  keywords = {noisy channel}
}
