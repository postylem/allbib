@article{aaronson.s:2014,
  title = {The Equivalence of Sampling and Searching},
  author = {Aaronson, Scott},
  year = {2014},
  month = aug,
  journal = {Theory of Computing Systems},
  volume = {55},
  number = {2},
  pages = {281--298},
  issn = {1433-0490},
  doi = {10.1007/s00224-013-9527-3},
  urldate = {2022-10-27},
  abstract = {In a sampling problem, we are given an input x{$\in$}\{0,1\}n, and asked to sample approximately from a probability distribution \${\textbackslash}mathcal\{D\}\_\{x\}\$over \${\textbackslash}operatorname\{poly\} ( n ) \$-bit strings. In a search problem, we are given an input x{$\in$}\{0,1\}n, and asked to find a member of a nonempty set Axwith high probability. (An example is finding a Nash equilibrium.) In this paper, we use tools from Kolmogorov complexity to show that sampling and search problems are ``essentially equivalent.'' More precisely, for any sampling problem S, there exists a search problem RSsuch that, if \${\textbackslash}mathcal\{C\}\$is any ``reasonable'' complexity class, then RSis in the search version of \${\textbackslash}mathcal\{C\}\$if and only if S is in the sampling version. What makes this nontrivial is that the same RSworks for every~\${\textbackslash}mathcal\{C\}\$.},
  langid = {english},
  keywords = {Algorithmic information theory,Extended Church-Turing Thesis,FBQP,Function problems,Kolmogorov complexity,Quantum computing,Relational problems,Sampling problems,Search problems},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/aaronson.s2014 The equivalence of sampling and searchin.pdf}
}

@article{abney.s:1991,
  title = {Memory Requirements and Local Ambiguities of Parsing Strategies},
  author = {Abney, Steven P. and Johnson, Mark},
  year = {1991},
  month = may,
  journal = {Journal of Psycholinguistic Research},
  volume = {20},
  number = {3},
  pages = {233--250},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/bf01067217},
  abstract = {We present a method for calculating lower bounds on the space required and local ambiguities entailed by parsing strategies. A fast, compact natural language parser must implement a strategy with low space requirements and few local ambiguities. It is also widely assumed in the psycholinguistics literature that extremely limited short-term space is available to the human parser, and that sentences containing center-embedded constructions are incomprehensible because processing them requires more space than is available. However, we show that the parsing strategies most psycholinguists assume require less space for processing center-embedded constructions than for processing other perfectly comprehensible constructions. We present alternative strategies for which center-embedded constructions do require more space than other constructions.},
  bdsk-url-2 = {https://doi.org/10.1007/bf01067217},
  date-added = {2022-03-31 09:33:23 -0400},
  date-modified = {2022-03-31 09:38:15 -0400},
  keywords = {memory,parsing,space-complexity}
}

@incollection{abney.s:1991chunks,
  title = {Parsing by Chunks},
  booktitle = {Studies in Linguistics and Philosophy},
  author = {Abney, Steven P.},
  year = {1991},
  pages = {257--278},
  publisher = {Springer Netherlands},
  doi = {10.1007/978-94-011-3474-3_10},
  bdsk-url-2 = {https://doi.org/10.1007/978-94-011-3474-3{$_1$}0},
  date-added = {2022-03-31 09:42:19 -0400},
  date-modified = {2022-03-31 09:45:05 -0400},
  keywords = {context free grammar,parsing}
}

@inproceedings{abney.s:1999,
  title = {Relating Probabilistic Grammars and Automata},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Association}} for {{Computational Linguistics}} on {{Computational Linguistics}} -},
  author = {Abney, Steven and McAllester, David and Pereira, Fernando},
  year = {1999},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1034678.1034759},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034759},
  date-added = {2022-03-31 09:45:57 -0400},
  date-modified = {2022-03-31 09:47:10 -0400},
  keywords = {automata,context free grammar,parsing,probabilistic context free grammar,push-down automata}
}

@article{adamek.j:2004,
  title = {Abstract and Concrete Categories. {{The}} Joy of Cats},
  author = {Ad{\'a}mek, Ji{\v r}{\'i} and Herrlich, Horst and Strecker, George E},
  year = {2004},
  publisher = {Citeseer},
  date-added = {2019-08-24 09:17:33 -0400},
  date-modified = {2019-08-24 09:18:04 -0400},
  keywords = {category theory}
}

@misc{adams.r:2013blog,
  type = {Blog},
  title = {The {{Gumbel-max}} Trick for Discrete Distributions},
  author = {Adams, Ryan},
  year = {2013},
  month = apr,
  journal = {Laboratory for Intelligent Probabilistic Systems Blog},
  urldate = {2022-11-06},
  howpublished = {https://lips.cs.princeton.edu/the-gumbel-max-trick-for-discrete-distributions/},
  keywords = {gumbel-max trick}
}

@article{adani.f:2010,
  title = {Grammatical Feature Dissimilarities Make Relative Clauses Easier: {{A}} Comprehension Study with {{Italian}} Children},
  shorttitle = {Grammatical Feature Dissimilarities Make Relative Clauses Easier},
  author = {Adani, Flavia and Van Der Lely, Heather K.J. and Forgiarini, Matteo and Guasti, Maria Teresa},
  year = {2010},
  month = sep,
  journal = {Lingua},
  volume = {120},
  number = {9},
  pages = {2148--2166},
  issn = {00243841},
  doi = {10.1016/j.lingua.2010.03.018},
  urldate = {2023-10-29},
  langid = {english},
  keywords = {agreement attraction,italian}
}

@article{adelman.j:2008,
  title = {Modeling Lexical Decision: {{The}} Form of Frequency and Diversity Effects},
  shorttitle = {Modeling Lexical Decision},
  author = {Adelman, James S. and Brown, Gordon D. A.},
  year = {2008},
  journal = {Psychological Review},
  volume = {115},
  number = {1},
  pages = {214--227},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.115.1.214},
  abstract = {What is the root cause of word frequency effects on lexical decision times? W. S. Murray and K. I. Forster (2004) argued that such effects are linear in rank frequency, consistent with a serial search model of lexical access. In this article, the authors (a) describe a method of testing models of such effects that takes into account the possibility of parametric overfitting; (b) illustrate the effect of corpus choice on estimates of rank frequency; (c) give derivations of nine functional forms as predictions of models of lexical decision; (d) detail the assessment of these models and the rank model against existing data regarding the functional form of frequency effects; and (e) report further assessments using contextual diversity, a factor confounded with word frequency. The relationship between the occurrence distribution of words and lexical decision latencies to those words does not appear compatible with the rank hypothesis, undermining the case for serial search models of lexical access. Three transformations of contextual diversity based on extensions of instance models do, however, remain as plausible explanations of the effect. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {drift diffusion models,Lexical Access,Lexical Decision,Mathematical Modeling,random walk,Word Frequency},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/adelman.j2008 Modeling lexical decision The form of f 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/adelman.j2008 Modeling lexical decision The form of f.pdf}
}

@article{adger.d:2009,
  title = {Features in Minimalist Syntax},
  author = {Adger, David and Svenonius, Peter},
  year = {2009},
  journal = {The Oxford Handbook of Minimalist Syntax},
  date-added = {2020-02-20 12:37:20 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,minimalist syntax,phi features}
}

@article{agapiou.s:2017,
  title = {Importance Sampling: Intrinsic Dimension and Computational Cost},
  shorttitle = {Importance Sampling},
  author = {Agapiou, S. and Papaspiliopoulos, O. and {Sanz-Alonso}, D. and Stuart, A. M.},
  year = {2017},
  month = aug,
  journal = {Statistical Science},
  volume = {32},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/17-STS611},
  urldate = {2022-12-21},
  keywords = {importance sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension 3.pdf;/Users/j/Dropbox (MIT)/Zotfiles/agapiou.s2017 Importance sampling intrinsic dimension.pdf}
}

@inproceedings{ait-mokhtar.s:1997,
  title = {Incremental Finite-State Parsing},
  booktitle = {Fifth Conference on Applied Natural Language Processing},
  author = {{Ait-Mokhtar}, Salah and Chanod, Jean-Pierre},
  year = {1997},
  pages = {72--79},
  publisher = {Association for Computational Linguistics},
  address = {Washington, DC, USA},
  doi = {10.3115/974557.974569},
  bdsk-url-2 = {https://doi.org/10.3115/974557.974569}
}

@misc{alain.g:2015arxiv,
  title = {{{GSNs}} : Generative Stochastic Networks},
  shorttitle = {{{GSNs}}},
  author = {Alain, Guillaume and Bengio, Yoshua and Yao, Li and Yosinski, Jason and {Thibodeau-Laufer}, Eric and Zhang, Saizheng and Vincent, Pascal},
  year = {2015},
  month = mar,
  number = {arXiv:1503.05571},
  eprint = {1503.05571},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.1503.05571},
  urldate = {2023-12-27},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution. Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by back-propagation. The theorems provided here generalize recent work on the probabilistic interpretation of denoising auto-encoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). We study how GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest. Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/alain.g2015arxiv GSNs  generative stochastic networks.pdf}
}

@inproceedings{alemi.a:2016,
  title = {Deep Variational Information Bottleneck},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Alemi, Alexander A. and Fischer, Ian and Dillon, Joshua V. and Murphy, Kevin},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/AlemiFD017.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@article{alexiadou.a:2014,
  title = {Opaque and Transparent Datives, and How They Behave in Passives},
  author = {Alexiadou, Artemis and Anagnostopoulou, Elena and Sevdali, Christina},
  year = {2014},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {17},
  number = {1},
  pages = {1--34},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{allen.c:2019,
  title = {Analogies Explained: {{Towards}} Understanding Word Embeddings},
  booktitle = {Proceedings of the 36th International Conference on Machine Learning},
  author = {Allen, Carl and Hospedales, Timothy M.},
  editor = {Chaudhuri, Kamalika and Salakhutdinov, Ruslan},
  year = {2019},
  series = {Proceedings of Machine Learning Research},
  volume = {97},
  pages = {223--231},
  publisher = {PMLR},
  address = {Long Beach, California, USA}
}

@article{allopenna.p:1998,
  title = {Tracking the {{Time Course}} of {{Spoken Word Recognition Using Eye Movements}}: {{Evidence}} for {{Continuous Mapping Models}}},
  shorttitle = {Tracking the {{Time Course}} of {{Spoken Word Recognition Using Eye Movements}}},
  author = {Allopenna, Paul D. and Magnuson, James S. and Tanenhaus, Michael K.},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {419--439},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2558},
  urldate = {2023-09-27},
  abstract = {Eye movements to pictures of four objects on a screen were monitored as participants followed a spoken instruction to move one of the objects, e.g., ``Pick up the beaker; now put it below the diamond'' (Experiment 1) or heard progressively larger gates and tried to identify the referent (Experiment 2). The distractor objects included a cohort competitor with a name that began with the same onset and vowel as the name of the target object (e.g.,beetle), a rhyme competitor (e.g.speaker), and an unrelated competitor (e.g.,carriage). In Experiment 1, there was clear evidence for both cohort and rhyme activation as predicted by continuous mapping models such as TRACE (McClelland and Elman, 1986) and Shortlist (Norris, 1994). Additionally, the time course and probabilities of eye movements closely corresponded to response probabilities derived from TRACE simulations using the Luce choice rule (Luce, 1959). In the gating task, which emphasizes word-initial information, there was clear evidence for multiple activation of cohort members, as measured by judgments and eye movements, but no suggestion of rhyme effects. Given that the same sets of pictures were present during the gating task as in Experiment 1, we conclude that the rhyme effects in Experiment 1 were not an artifact of using a small set of visible alternatives.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/allopenna.p1998 Tracking the Time Course of Spoken Word.pdf}
}

@article{altmann.g:1988,
  title = {Interaction with Context during Human Sentence Processing},
  author = {Altmann, Gerry and Steedman, Mark},
  year = {1988},
  month = dec,
  journal = {Cognition},
  volume = {30},
  number = {3},
  pages = {191--238},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0277(88)90020-0},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0277(88)90020-0},
  date-added = {2022-04-14 13:35:33 -0400},
  date-modified = {2022-04-14 13:35:37 -0400}
}

@article{altmann.g:1999,
  title = {Incremental Interpretation at Verbs: Restricting the Domain of Subsequent Reference},
  shorttitle = {Incremental Interpretation at Verbs},
  author = {Altmann, Gerry T. M and Kamide, Yuki},
  year = {1999},
  month = dec,
  journal = {Cognition},
  volume = {73},
  number = {3},
  pages = {247--264},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(99)00059-1},
  urldate = {2023-10-25},
  abstract = {Participants' eye movements were recorded as they inspected a semi-realistic visual scene showing a boy, a cake, and various distractor objects. Whilst viewing this scene, they heard sentences such as `the boy will move the cake' or `the boy will eat the cake'. The cake was the only edible object portrayed in the scene. In each of two experiments, the onset of saccadic eye movements to the target object (the cake) was significantly later in the move condition than in the eat condition; saccades to the target were launched after the onset of the spoken word cake in the move condition, but before its onset in the eat condition. The results suggest that information at the verb can be used to restrict the domain within the context to which subsequent reference will be made by the (as yet unencountered) post-verbal grammatical object. The data support a hypothesis in which sentence processing is driven by the predictive relationships between verbs, their syntactic arguments, and the real-world contexts in which they occur.},
  keywords = {Eye movements,incremental processing,Parsing,Thematic roles},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/altmann.g1999 Incremental interpretation at verbs res.pdf}
}

@article{amari.s:1992,
  title = {Information Geometry of {{Boltzmann}} Machines},
  author = {Amari, S. and Kurata, K. and Nagaoka, H.},
  year = {1992},
  month = mar,
  journal = {IEEE Transactions on Neural Networks},
  volume = {3},
  number = {2},
  pages = {260--271},
  issn = {1941-0093},
  doi = {10.1109/72.125867},
  abstract = {A Boltzmann machine is a network of stochastic neurons. The set of all the Boltzmann machines with a fixed topology forms a geometric manifold of high dimension, where modifiable synaptic weights of connections play the role of a coordinate system to specify networks. A learning trajectory, for example, is a curve in this manifold. It is important to study the geometry of the neural manifold, rather than the behavior of a single network, in order to know the capabilities and limitations of neural networks of a fixed topology. Using the new theory of information geometry, a natural invariant Riemannian metric and a dual pair of affine connections on the Boltzmann neural network manifold are established. The meaning of geometrical structures is elucidated from the stochastic and the statistical point of view. This leads to a natural modification of the Boltzmann machine learning rule.{$<>$}},
  keywords = {Computer architecture,information geometry,Information geometry,Information processing,Machine learning,Manifolds,Network topology,Neural networks,Neurons,Probability distribution,Stochastic processes},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/amari.s1992 Information geometry of Boltzmann machin.pdf}
}

@book{anagnostopoulou.e:2003,
  title = {The Syntax of Ditransitives: {{Evidence}} from Clitics},
  author = {Anagnostopoulou, Elena},
  year = {2003},
  volume = {54},
  publisher = {Walter de Gruyter},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:07 -0400},
  project = {Icelandic gluttony},
  keywords = {clitics,hierarchy effects}
}

@article{anagnostopoulou.e:2017,
  title = {The {{Person Case Constraint}}},
  author = {Anagnostopoulou, Elena},
  year = {2017},
  journal = {The Wiley Blackwell Companion to Syntax, Second Edition},
  pages = {1--47},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@book{anderson.j:1990,
  title = {The Adaptive Character of Thought},
  author = {Anderson, John R.},
  year = {1990},
  month = jan,
  publisher = {Psychology Press},
  bdsk-url-2 = {https://doi.org/10.4324/9780203771730},
  date-added = {2022-04-04 11:54:23 -0400},
  date-modified = {2022-04-04 12:18:17 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j1990 The adaptive character of thought.pdf}
}

@article{anderson.j:1991,
  title = {Is Human Cognition Adaptive?},
  author = {Anderson, John R.},
  year = {1991},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {14},
  number = {3},
  pages = {471--485},
  publisher = {Cambridge University Press},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00070801},
  urldate = {2022-06-12},
  abstract = {Can the output of human cognition be predicted from the assumption that it is an optimal response to the information-processing demands of the environment? A methodology called rational analysis is described for deriving predictions about cognitive phenomena using optimization assumptions. The predictions flow from the statistical structure of the environment and not the assumed structure of the mind. Bayesian inference is used, assuming that people start with a weak prior model of the world which they integrate with experience to develop stronger models of specific aspects of the world. Cognitive performance maximizes the difference between the expected gain and cost of mental effort. (1) Memory performance can be predicted on the assumption that retrieval seeks a maximal trade-off between the probability of finding the relevant memories and the effort required to do so; in (2) categorization performance there is a similar trade-off between accuracy in predicting object features and the cost of hypothesis formation; in (3) casual inference the trade-off is between accuracy in predicting future events and the cost of hypothesis formation; and in (4) problem solving it is between the probability of achieving goals and the cost of both external and mental problem-solving search. The implemention of these rational prescriptions in neurally plausible architecture is also discussed.},
  langid = {english},
  keywords = {Bayes,categorization,causal inference,computation,memory,optimality,problem solving,rational analysis,rationality},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j1991 Is human cognition adaptive.pdf}
}

@incollection{anderson.j:1991architectures,
  title = {The Place of Cognitive Architectures in a Rational Analysis},
  booktitle = {Architectures for Intelligence},
  author = {Anderson, John R.},
  year = {1991},
  pages = {1--24},
  publisher = {Psychology Press},
  abstract = {The basic goal of a theorist in specifying a cognitive architecture is to specify the mind's principles of operation and organization much like you would specify those of a computer. Any cognitive phenomena should be derivative from these principles. As this conference gives witness, there are many cognitive architectures. This chapter will try to make some claims about the role of architectures generally in psychological theory, but it will do this by taking as examples three of the architectures which figure prominently at Carnegie Mellon University. There is the Soar architecture of Laird, Newell, and Rosenbloom (1987) my own ACT* architecture (Anderson, 1983), and the PDP architecture of McClelland and Rumelhart (McClelland \& Rumelhart, 1986, Rumelhart \& McClelland, 1986).},
  isbn = {978-1-315-80784-3},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j1991architectures The place of cognitive architectures in.pdf}
}

@article{anderson.j:1991reflections,
  title = {Reflections of the Environment in Memory},
  author = {Anderson, John R. and Schooler, Lael J.},
  year = {1991},
  month = nov,
  journal = {Psychological Science},
  volume = {2},
  number = {6},
  pages = {396--408},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1991.tb00174.x},
  urldate = {2022-09-08},
  abstract = {Availability of human memories for specific items shows reliable relationships to frequency, recency, and pattern of prior exposures to the item. These relationships have defied a systematic theoretical treatment. A number of environmental sources (New York Times, parental speech, electronic mail) are examined to show that the probability that a memory will be needed also shows reliable relationships to frequency, recency, and pattern of prior exposures. Moreover, the environmental relationships are the same as the memory relationships. It is argued that human memory has the form it does because it is adapted to these environmental relationships. Models for both the environment and human memory are described. Among the memory phenomena addressed are the practice function, the retention function, the effect of spacing of practice, and the relationship between degree of practice and retention.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j1991reflections Reflections of the environment in memory.pdf}
}

@article{anderson.j:1998,
  title = {An Integrated Theory of List Memory},
  author = {Anderson, John R. and Bothell, Dan and Lebiere, Christian and Matessa, Michael},
  year = {1998},
  month = may,
  journal = {Journal of Memory and Language},
  volume = {38},
  number = {4},
  pages = {341--380},
  issn = {0749-596X},
  doi = {10.1006/jmla.1997.2553},
  urldate = {2022-09-08},
  abstract = {The ACT-R theory (Anderson, 1993; Anderson \& Lebiere, 1998) is applied to the list memory paradigms of serial recall, recognition memory, free recall, and implicit memory. List memory performance in ACT-R is determined by the level of activation of declarative chunks which encode that items occur in the list. This level of activation is in turn determined by amount of rehearsal, delay, and associative fan from a list node. This theory accounts for accuracy and latency profiles in backward and forward serial recall, set size effects in the Sternberg paradigm, length--strength effects in recognition memory, the Tulving--Wiseman function, serial position, length and practice effects in free recall, and lexical priming in implicit memory paradigms. This wide variety of effects is predicted with minimal parameter variation. It is argued that the strength of the ACT-R theory is that it offers a completely specified processing architecture that serves to integrate many existing models in the literature.},
  langid = {english}
}

@book{anderson.j:1998atomic,
  title = {The Atomic Components of Thought},
  author = {Anderson, John R. and Lebiere, Christian},
  year = {1998},
  publisher = {Psychology Press},
  address = {New York},
  doi = {10.4324/9781315805696},
  isbn = {978-1-315-80569-6},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j1998atomic The atomic components of thought.pdf}
}

@article{anderson.j:2004,
  title = {An Integrated Theory of the Mind},
  author = {Anderson, John R. and Bothell, Daniel and Byrne, Michael D. and Douglass, Scott and Lebiere, Christian and Qin, Yulin},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {4},
  pages = {1036--1060},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.111.4.1036},
  urldate = {2022-09-17},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/anderson.j2004 An integrated theory of the mind.pdf}
}

@inproceedings{andreas.j:2022,
  title = {Language {{Models}} as {{Agent Models}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Andreas, Jacob},
  year = {2022},
  month = dec,
  pages = {5769--5779},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-07-29},
  abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them---a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that---even in today's non-robust and error-prone models---LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/andreas.j2022 Language Models as Agent Models.pdf}
}

@article{andrews.s:1996,
  title = {Lexical {{Retrieval}} and {{Selection Processes}}: {{Effects}} of {{Transposed-Letter Confusability}}},
  shorttitle = {Lexical {{Retrieval}} and {{Selection Processes}}},
  author = {Andrews, Sally},
  year = {1996},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {35},
  number = {6},
  pages = {775--800},
  issn = {0749-596X},
  doi = {10.1006/jmla.1996.0040},
  urldate = {2023-10-29},
  abstract = {Three experiments investigated performance for words which differ from another word only by the transposition of two letters (e.g.,salt, slat). In Experiment 1, high frequency words from transposed-letter (TL) confusable pairs were responded to more slowly than carefully matched control words in both the lexical decision and word naming task. Low frequency TL words were responded to less accurately than control words in the naming but not the lexical decision task. Experiment 2 replicated the naming data of Experiment 1 and also revealed that naming accuracy for TL word targets was reduced when they were preceded by a brief masked presentation of their confusable mate. Experiment 3 provided a third replication of the impaired naming performance for TL target words and demonstrated that the effect was insensitive to concurrent dual task demands. These TL confusability effects provide strong constraints that can contribute to evaluation and specification of current models of visual word recognition.}
}

@article{andrieu.c:2010PMCMC,
  title = {Particle {{Markov}} Chain {{Monte Carlo}} Methods},
  author = {Andrieu, Christophe and Doucet, Arnaud and Holenstein, Roman},
  year = {2010},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {72},
  number = {3},
  pages = {269--342},
  doi = {10.1111/j.1467-9868.2009.00736.x},
  abstract = {Summary. Markov chain Monte Carlo and sequential Monte Carlo methods have emerged as the two main tools to sample from high dimensional probability distributions. Although asymptotic convergence of Markov chain Monte Carlo algorithms is ensured under weak assumptions, the performance of these algorithms is unreliable when the proposal distributions that are used to explore the space are poorly chosen and/or if highly correlated variables are updated independently. We show here how it is possible to build efficient high dimensional proposal distributions by using sequential Monte Carlo methods. This allows us not only to improve over standard Markov chain Monte Carlo schemes but also to make Bayesian inference feasible for a large class of statistical models where this was not previously so. We demonstrate these algorithms on a non-linear state space model and a L{\'e}vy-driven stochastic volatility model.},
  keywords = {bayesian inference,conditional sequential monte carlo,markov chain monte Carlo,sequential monte carlo,state space models},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/andrieu.c2010PMCMC Particle Markov chain Monte Carlo method.pdf}
}

@article{angele.b:2015,
  title = {Do Successor Effects in Reading Reflect Lexical Parafoveal Processing? {{Evidence}} from Corpus-Based and Experimental Eye Movement Data},
  author = {Angele, Bernhard and Schotter, Elizabeth R. and Slattery, Timothy J. and Tenenbaum, Tara L. and Bicknell, Klinton and Rayner, Keith},
  year = {2015},
  month = feb,
  journal = {Journal of Memory and Language},
  volume = {79--80},
  pages = {76--96},
  publisher = {Elsevier BV},
  doi = {10.1016/j.jml.2014.11.003},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2014.11.003},
  date-added = {2022-04-21 09:33:17 -0400},
  date-modified = {2022-04-21 09:33:18 -0400}
}

@inproceedings{arehalli.s:2022CoNLL,
  title = {Syntactic Surprisal from Neural Models Predicts, but Underestimates, Human Processing Difficulty from Syntactic Ambiguities},
  booktitle = {Proceedings of the 26th Conference on Computational Natural Language Learning ({{CoNLL}})},
  author = {Arehalli, Suhas and Dillon, Brian and Linzen, Tal},
  year = {2022},
  month = dec,
  pages = {301--313},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates (Hybrid)},
  abstract = {Humans exhibit garden path effects: When reading sentences that are temporarily structurally ambiguous, they slow down when the structure is disambiguated in favor of the less preferred alternative. Surprisal theory (Hale, 2001; Levy, 2008), a prominent explanation of this finding, proposes that these slowdowns are due to the unpredictability of each of the words that occur in these sentences. Challenging this hypothesis, van Schijndel and Linzen (2021) find that estimates of the cost of word predictability derived from language models severely underestimate the magnitude of human garden path effects. In this work, we consider whether this underestimation is due to the fact that humans weight syntactic factors in their predictions more highly than language models do. We propose a method for estimating syntactic predictability from a language model, allowing us to weigh the cost of lexical and syntactic predictability independently. We find that treating syntactic predictability independently from lexical predictability indeed results in larger estimates of garden path. At the same time, even when syntactic predictability is independently weighted, surprisal still greatly underestimate the magnitude of human garden path effects. Our results support the hypothesis that predictability is not the only factor responsible for the processing cost associated with garden path sentences.}
}

@misc{arehalli.s:2022long,
  title = {Neural Networks as Cognitive Models of the Processing of Syntactic Constraints},
  author = {Arehalli, Suhas and Linzen, Tal},
  year = {2022},
  month = oct,
  abstract = {Languages are governed by syntactic constraints --- structural rules that determine which sentences are grammatical in the language. In English, one such constraint is subject-verb agreement, which dictates that the number of a verb must match the number of its corresponding subject: ``the dogs run'', but ``the dog runs''. While this constraint appears to be simple, in practice speakers make a substantial number of agreement errors, especially if a noun phrase near the verb differs in number from the subject (for example, a speaker might produce the ungrammatical sentence ``the key to the cabinets are rusty''). This phenomenon, referred to as agreement attraction, is sensitive to a wide range of properties of the sentence; no single existing model is able to generate predictions for the wide variety of materials studied in the human experimental literature. We explore the viability of neural network language models---broad-coverage systems trained to predict the next word in a corpus---as a framework for addressing this limitation. We analyze the agreement errors made by Long Short-Term Memory (LSTM) networks and compare them to those of humans. The models successfully simulate certain results, such as the so-called number asymmetry and the difference between attraction strength in grammatical and ungrammatical sentences, but failed to simulate others, such as the effect of syntactic distance or notional (conceptual) number. We further evaluate networks trained with explicit syntactic supervision, and find that this form of supervision does not always lead to more human-like syntactic behavior. Finally, we show that the corpus used to train a network significantly affects the pattern of agreement errors produced by the network, and discuss the strengths and limitations of neural networks as a tool for understanding human syntactic processing.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/arehalli.s2022long Neural networks as cognitive models of t.pdf}
}

@misc{arel-bundock.v:2024,
  title = {{{{\textbf{marginaleffects}}}}: Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis Tests},
  author = {{Arel-Bundock}, Vincent},
  year = {2024}
}

@misc{armengol-estape.j:2021,
  title = {On the Multilingual Capabilities of Very Large-Scale {{English}} Language Models},
  author = {{Armengol-Estap{\'e}}, Jordi and {de Gibert Bonet}, Ona and Melero, Maite},
  year = {2021},
  eprint = {2108.13349},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-12-13 19:48:21 -0500},
  date-modified = {2021-12-13 19:48:22 -0500}
}

@misc{arora.k:2023arxiv,
  title = {The Stable Entropy Hypothesis and Entropy-Aware Decoding: An Analysis and Algorithm for Robust Natural Language Generation},
  shorttitle = {The Stable Entropy Hypothesis and Entropy-Aware Decoding},
  author = {Arora, Kushal and O'Donnell, Timothy J. and Precup, Doina and Weston, Jason and Cheung, Jackie C. K.},
  year = {2023},
  month = feb,
  number = {arXiv:2302.06784},
  eprint = {2302.06784},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-05},
  abstract = {State-of-the-art language generation models can degenerate when applied to open-ended generation problems such as text completion, story generation, or dialog modeling. This degeneration usually shows up in the form of incoherence, lack of vocabulary diversity, and self-repetition or copying from the context. In this paper, we postulate that ``human-like'' generations usually lie in a narrow and nearly flat entropy band, and violation of these entropy bounds correlates with degenerate behavior. Our experiments show that this stable narrow entropy zone exists across models, tasks, and domains and confirm the hypothesis that violations of this zone correlate with degeneration. We then use this insight to propose an entropy-aware decoding algorithm that respects these entropy bounds resulting in less degenerate, more contextual, and "human-like" language generation in open-ended text generation settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/arora.k2023 The stable entropy hypothesis and entrop.pdf}
}

@misc{arroyo-fernandez.i:2019,
  title = {On the Possibility of Rewarding Structure Learning Agents: {{Mutual}} Information on Linguistic Random Sets},
  author = {{Arroyo-Fern{\'a}ndez}, Ignacio and {Carrasco-Ru{\'i}z}, Mauricio and {Arias-Aguilar}, J. Anibal},
  year = {2019},
  eprint = {1910.04023},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2020-01-27 11:44:31 -0500},
  date-modified = {2020-01-27 11:47:03 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised grammar induction}
}

@article{atlamaz.u:2018,
  title = {On Partial Agreement and Oblique Case},
  author = {Atlamaz, {\"U}mit and Baker, Mark},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {3},
  pages = {195--237},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{attias.h:1999,
  title = {A Variational Baysian Framework for Graphical Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Attias, Hagai},
  editor = {Solla, S. and Leen, T. and M{\"u}ller, K.},
  year = {1999},
  volume = {12},
  publisher = {MIT Press},
  urldate = {2022-06-27},
  abstract = {This paper presents a novel practical framework for Bayesian model averaging and model selection in probabilistic graphical models. Our approach approximates full posterior distributions over model parameters and structures, as well as latent variables, in an analytical manner. These posteriors fall out of a free-form optimization procedure, which naturally incorporates conjugate priors. Unlike in large sample approximations, the posteriors are generally non-Gaussian and no Hessian needs to be computed. Predictive quantities are obtained analytically. The resulting algorithm generalizes the standard Expectation Maximization algorithm, and its convergence is guaranteed. We demonstrate that this approach can be applied to a large class of models in several domains, including mixture models and source separation.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/attias.h1999 A variational baysian framework for grap.pdf}
}

@book{attneave.f:1959,
  title = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  shorttitle = {Applications of Information Theory to Psychology},
  author = {Attneave, Fred},
  year = {1959},
  series = {Applications of Information Theory to Psychology: {{A}} Summary of Basic Concepts, Methods, and Results},
  publisher = {Henry Holt},
  address = {Oxford, England},
  abstract = {Summarizes existing informational methods used in psychological research, and illustrates the methods of calculating some of the measures. Chapter 1 develops quantitative expressions of uncertainty and redundancy from qualitative examples. Chapter 2 describes informational methods for analyzing sequences of events. Chapter 3 gives methods of describing rates of transmission of information and reviews pertinent research. Chapter 4 concerns possible applications of information measures, particularly to the study of perceptual problems of patterning. Appendices illustrate the calculation of information measures from variance statistics and provide convenient tables and a nomograph used in calculating information measures. 87 refs. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {information theory,surprisal}
}

@inproceedings{aurnhammer.c:2019cogsci,
  title = {Comparing Gated and Simple Recurrent Neural Network Architectures as Models of Human Sentence Processing},
  booktitle = {Proceedings of the 41st {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  pages = {112--118},
  date-added = {2021-11-29 11:40:22 -0500},
  date-modified = {2021-11-29 12:44:15 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/aurnhammer.c2019cogsci Comparing gated and simple recurrent neu.pdf}
}

@article{aurnhammer.c:2019LIG,
  title = {Evaluating Information-Theoretic Measures of Word Prediction in Naturalistic Sentence Reading},
  author = {Aurnhammer, Christoph and Frank, Stefan L.},
  year = {2019},
  month = nov,
  journal = {Neuropsychologia},
  volume = {134},
  number = {107198},
  publisher = {Elsevier BV},
  doi = {10.1016/j.neuropsychologia.2019.107198},
  abstract = {We review information-theoretic measures of cognitive load during sentence processing that have been used to quantify word prediction effort. Two such measures, surprisal and next-word entropy, suffer from shortcomings when employed for a predictive processing view. We propose a novel metric, lookahead information gain, that can overcome these short-comings. We estimate the different measures using probabilistic language models. Subsequently, we put them to the test by analysing how well the estimated measures predict human processing effort in three data sets of naturalistic sentence reading. Our results replicate the well known effect of surprisal on word reading effort, but do not indicate a role of next-word entropy or lookahead information gain. Our computational results suggest that, in a predictive processing system, the costs of predicting may outweigh the gains. This idea poses a potential limit to the value of a predictive mechanism for the processing of language. The result illustrates the unresolved problem of finding estimations of word-by-word prediction that, first, are truly independent of perceptual processing of the to-be-predicted words, second, are statistically reliable predictors of experimental data, and third, can be derived from more general assumptions about the cognitive processes involved.},
  bdsk-url-2 = {https://doi.org/10.1016/j.neuropsychologia.2019.107198},
  date-added = {2021-11-29 11:28:26 -0500},
  date-modified = {2022-04-21 09:12:00 -0400}
}

@book{awodey.s:2010,
  title = {Category Theory},
  author = {Awodey, Steve},
  year = {2010},
  publisher = {Oxford University Press},
  date-added = {2019-08-24 09:18:40 -0400},
  date-modified = {2019-08-24 09:19:06 -0400},
  keywords = {category theory}
}

@book{axler.s:2020,
  title = {Measure, {{Integration}} \& {{Real Analysis}}},
  author = {Axler, Sheldon},
  year = {2020},
  series = {Graduate {{Texts}} in {{Mathematics}}},
  volume = {282},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-030-33143-6},
  urldate = {2022-06-23},
  isbn = {978-3-030-33142-9},
  langid = {english}
}

@article{aylett.m:2004,
  title = {The Smooth Signal Redundancy Hypothesis: A Functional Explanation for Relationships between Redundancy, Prosodic Prominence, and Duration in Spontaneous Speech},
  author = {Aylett, Matthew and Turk, Alice},
  year = {2004},
  journal = {Language and Speech},
  volume = {47},
  number = {1},
  eprint = {https://doi.org/10.1177/00238309040470010201},
  pages = {31--56},
  doi = {10.1177/00238309040470010201},
  abstract = {This paper explores two related factors which influence variation in duration, prosodic structure and redundancy in spontaneous speech. We argue that the constraint of producing robust communication while efficiently expending articulatory effort leads to an inverse relationship between language redundancy and duration. The inverse relationship improves communication robustness by spreading information more evenly across the speech signal, yielding a smoother signal redundancy profile.We argue that prosodic prominence is a linguistic means of achieving smooth signal redundancy. Prosodic prominence increases syllable duration and coincides to a large extent with unpredictable sections of speech, and thus leads to a smoother signal redundancy.The results of linear regressions carried out between measures of redundancy, syllable duration and prosodic structure in a large corpus of spontaneous speech confirm: (1) an inverse relationship between language redundancy and duration, and (2) a strong relationship between prosodic prominence and duration.The fact that a large proportion of the variance predicted by language redundancy and prosodic prominence is nonunique suggests that, in English, prosodic prominence structure is the means with which constraints caused by a robust signal requirement are expressed in spontaneous speech.},
  date-added = {2022-04-27 12:19:58 -0400},
  date-modified = {2022-04-27 12:20:19 -0400},
  pmid = {15298329},
  keywords = {noisy channel coding}
}

@book{baayen.r:2001book,
  title = {Word Frequency Distributions},
  author = {Baayen, R. Harald},
  editor = {Ide, Nancy and V{\'e}ronis, Jean},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  volume = {18},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-0844-0},
  urldate = {2022-10-02},
  isbn = {978-1-4020-0927-3 978-94-010-0844-0},
  keywords = {best fit,corpus,Estimator},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/baayen.r2001book Word frequency distributions.pdf}
}

@incollection{baayen.r:2001bookch1,
  title = {Word Frequencies},
  booktitle = {Word {{Frequency Distributions}}},
  author = {Baayen, R. Harald},
  editor = {Baayen, R. Harald},
  year = {2001},
  series = {Text, {{Speech}} and {{Language Technology}}},
  pages = {1--38},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-0844-0_1},
  urldate = {2022-10-02},
  abstract = {This chapter introduces two fundamental issues in lexical statistics. The first issue concerns the role of the sample size, the number of words in a text or corpus. The sample size crucially determines a great many measures that have been proposed as characteristic text constants. However, the values of these measures change systematically as a function of the sample size. Similarly, the parameters of many models for word frequency distribution are highly dependent on the sample size. This property sets lexical statistics apart from most other areas in statistics, where an increase in the sample size leads to enhanced accuracy and not to systematic changes in basic measures and parameters.},
  isbn = {978-94-010-0844-0},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/baayen.r2001bookch1 Word frequencies.pdf}
}

@phdthesis{bachrach.a:2008phd,
  title = {Imaging Neural Correlates of Syntactic Complexity in a Naturalistic Context},
  author = {Bachrach, Asaf},
  year = {2008},
  date-added = {2021-06-09 09:00:47 -0400},
  date-modified = {2022-04-20 10:20:04 -0400},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Zotfiles/bachrach.a2008phd Imaging neural correlates of syntactic c.pdf}
}

@unpublished{bachrach.a:2009,
  type = {Unpublished Manuscript},
  title = {Incremental Prediction in Naturalistic Language Processing: {{An fMRI}} Study},
  author = {Bachrach, Asaf and Roark, Brian and Marantz, Alex and {Whitfield-Gabrieli}, Susan and Cardenas, Carlos and Gabrieli, John},
  year = {2009}
}

@incollection{bader.m:1994,
  title = {German Verb-Final Clauses and Sentence Processing: {{Evidence}} for Immediate Attachment},
  shorttitle = {German Verb-Final Clauses and Sentence Processing},
  booktitle = {Perspectives on Sentence Processing},
  author = {Bader, Markus and Lasser, Ingeborg},
  editor = {Clifton, Jr., Charles and Frazier, Lyn and Rayner, Keith},
  year = {1994},
  pages = {225--242},
  publisher = {Lawrence Erlbaum Associates, Inc},
  address = {Hillsdale, NJ, US},
  abstract = {one central question in sentence processing concerns the relationship between knowledge of language and the way this knowledge is put to use / this relationship . . . has received a great deal of attention in the discussion of models of the human parsing mechanism [responsible for computing syntactic structures] / despite this attention, the issue of how linguistic knowledge is used during sentence comprehension is far from settled / goal [is] to narrow down the number of possible parsing models by introducing some on-line data from German  discuss a particular class of parsers that assumes both principles and parameters theory and a transparent grammar--parser relationship / call these parsers head-driven licensing parsers / on the basis of experimental evidence from German verb-final structures, we reject the particular interpretation of grammar-parser transparency / introduce certain properties of current syntactic theory and then show how these properties have found their way into head-driven licensing parsers / report an experiment that aims to test the prediction of head-driven licensing parsers for verb-final clauses; namely, that, in these clauses, all attachments have to be delayed until the end of the clause / [Ss were 24 native German speaking university students] (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  isbn = {978-0-8058-1581-8 978-0-8058-1582-5},
  keywords = {Grammar,Psycholinguistics,Sentence Comprehension,Sentence Structure,Syntax,Verbs}
}

@misc{bahdanau.d:2016,
  title = {Neural {{Machine Translation}} by {{Jointly Learning}} to {{Align}} and {{Translate}}},
  author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  year = {2016},
  month = may,
  number = {arXiv:1409.0473},
  eprint = {1409.0473},
  primaryclass = {cs, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1409.0473},
  urldate = {2022-05-19},
  abstract = {Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bahdanau.d2016 Neural Machine Translation by Jointly Le.pdf}
}

@inproceedings{bailly.r:2020,
  title = {Emergence of Syntax Needs Minimal Supervision},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Bailly, Rapha{\"e}l and G{\'a}bor, Kata},
  year = {2020},
  pages = {477--487},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.46},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.46}
}

@article{baker.j:1979,
  title = {Trainable Grammars for Speech Recognition},
  author = {Baker, J. K.},
  year = {1979},
  month = jun,
  journal = {The Journal of the Acoustical Society of America},
  volume = {65},
  number = {S1},
  pages = {S132-S132},
  publisher = {Acoustical Society of America},
  issn = {0001-4966},
  doi = {10.1121/1.2017061},
  urldate = {2022-07-04}
}

@incollection{baldi.p:2002,
  title = {A Computational Theory of Surprise},
  booktitle = {Information, {{Coding}} and {{Mathematics}}: {{Proceedings}} of {{Workshop}} Honoring {{Prof}}. {{Bob McEliece}} on His 60th Birthday},
  author = {Baldi, Pierre},
  editor = {Blaum, Mario and Farrell, Patrick G. and {van Tilborg}, Henk C. A.},
  year = {2002},
  series = {The {{Springer International Series}} in {{Engineering}} and {{Computer Science}}},
  pages = {1--25},
  publisher = {Springer US},
  address = {Boston, MA},
  doi = {10.1007/978-1-4757-3585-7_1},
  urldate = {2024-02-11},
  abstract = {While eminently successful for the transmission of data, Shannon's theory of information does not address semantic and subjective dimensions of data, such as relevance and surprise. We propose an observer-dependent computational theory of surprise where surprise is defined by the relative entropy between the prior and the posterior distributions of an observer. Surprise requires integration over the space of models in contrast with Shannon's entropy, which requires integration over the space of data. We show how surprise can be computed exactly in a number of discrete and continuous cases using distributions from the exponential family with conjugate priors. We show that during sequential Bayesian learning, surprise decreases like 1/N and study how surprise differs and complements Shannon's definition of information.},
  isbn = {978-1-4757-3585-7},
  langid = {english},
  keywords = {Bayesian Probabilities,Entropy,Information,kl theory,Relative Entropy.,Relevance,Surprise},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/baldi.p2002 A computational theory of surprise.pdf}
}

@article{baldi.p:2010,
  title = {Of Bits and Wows: {{A Bayesian}} Theory of Surprise with Applications to Attention},
  shorttitle = {Of Bits and Wows},
  author = {Baldi, Pierre and Itti, Laurent},
  year = {2010},
  month = jun,
  journal = {Neural Networks},
  volume = {23},
  number = {5},
  pages = {649--666},
  issn = {0893-6080},
  doi = {10.1016/j.neunet.2009.12.007},
  urldate = {2024-02-11},
  abstract = {The amount of information contained in a piece of data can be measured by the effect this data has on its observer. Fundamentally, this effect is to transform the observer's prior beliefs into posterior beliefs, according to Bayes theorem. Thus the amount of information can be measured in a natural way by the distance (relative entropy) between the prior and posterior distributions of the observer over the available space of hypotheses. This facet of information, termed ``surprise'', is important in dynamic situations where beliefs change, in particular during learning and adaptation. Surprise can often be computed analytically, for instance in the case of distributions from the exponential family, or it can be numerically approximated. During sequential Bayesian learning, surprise decreases as the inverse of the number of training examples. Theoretical properties of surprise are discussed, in particular how it differs and complements Shannon's definition of information. A computer vision neural network architecture is then presented capable of computing surprise over images and video stimuli. Hypothesizing that surprising data ought to attract natural or artificial attention systems, the output of this architecture is used in a psychophysical experiment to analyze human eye movements in the presence of natural video stimuli. Surprise is found to yield robust performance at predicting human gaze (ROC-like ordinal dominance score {$\sim$}0.7 compared to {$\sim$}0.8 for human inter-observer repeatability, {$\sim$}0.6 for simpler intensity contrast-based predictor, and 0.5 for chance). The resulting theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  keywords = {Attention,Eye movements,Information,Relative entropy,Surprise},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/baldi.p2010 Of bits and wows A Bayesian theory of s.pdf}
}

@article{balota.d:1985,
  title = {The Interaction of Contextual Constraints and Parafoveal Visual Information in Reading},
  author = {Balota, David A and Pollatsek, Alexander and Rayner, Keith},
  year = {1985},
  journal = {Cognitive Psychology},
  volume = {17},
  number = {3},
  pages = {364--390},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0285(85)90013-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(85)90013-1},
  date-added = {2021-05-22 15:35:25 -0400},
  date-modified = {2021-05-22 15:35:39 -0400},
  keywords = {predictability,processing}
}

@misc{bao.h:2020arxiv,
  title = {{{UniLMv2}}: {{Pseudo-masked}} Language Models for Unified Language Model Pre-Training},
  shorttitle = {Unilmv2},
  author = {Bao, Hangbo and Dong, Li and Wei, Furu and Wang, Wenhui and Yang, Nan and Liu, Xiaodong and Wang, Yu and Piao, Songhao and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2020},
  month = feb,
  number = {arXiv:2002.12804},
  eprint = {2002.12804},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {We propose to pre-train a unified language model for both autoencoding and partially autoregressive language modeling tasks using a novel training procedure, referred to as a pseudo-masked language model (PMLM). Given an input text with masked tokens, we rely on conventional masks to learn inter-relations between corrupted tokens and context via autoencoding, and pseudo masks to learn intra-relations between masked spans via partially autoregressive modeling. With well-designed position embeddings and self-attention masks, the context encodings are reused to avoid redundant computation. Moreover, conventional masks used for autoencoding provide global masking information, so that all the position embeddings are accessible in partially autoregressive language modeling. In addition, the two tasks pre-train a unified language model as a bidirectional encoder and a sequence-to-sequence decoder, respectively. Our experiments show that the unified language models pre-trained using PMLM achieve new state-of-the-art results on a wide range of natural language understanding and generation tasks across several widely used benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bao.h2020 UniLMv2 Pseudo-masked language models f.pdf}
}

@article{bar-hillel.y:1953,
  title = {A Quasi-Arithmetical Notation for Syntactic Description},
  author = {{Bar-Hillel}, Yehoshua},
  year = {1953},
  journal = {Language},
  volume = {29},
  number = {1},
  pages = {47},
  publisher = {JSTOR},
  doi = {10.2307/410452},
  bdsk-url-2 = {https://doi.org/10.2307/410452},
  date-added = {2021-06-25 00:50:06 -0400},
  date-modified = {2021-06-25 00:50:07 -0400}
}

@incollection{barber.d:2003,
  title = {The {{IM}} Algorithm: A Variational Approach to Information Maximization},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Barber, David and Agakov, Felix},
  year = {2003},
  pages = {201--208},
  bdsk-url-2 = {https://papers.nips.cc/paper/2003/file/a6ea8471c120fe8cc35a2954c9b9c595-Paper.pdf},
  date-added = {2019-10-08 23:33:35 -0400},
  date-modified = {2021-03-07 16:09:06 -0500},
  project = {syntactic embedding},
  keywords = {mutual information,variational inference}
}

@article{barnard.g:1946,
  title = {Sequential Tests in Industrial Statistics},
  author = {Barnard, G. A.},
  year = {1946},
  journal = {Supplement to the Journal of the Royal Statistical Society},
  volume = {8},
  number = {1},
  pages = {1--21},
  issn = {2517-617X},
  doi = {10.2307/2983610},
  urldate = {2022-07-04},
  abstract = {After an introductory and an historical note, an elementary problem of simple qualitative inspection of a box of components is treated by using a ``lattice diagram representation.'' This leads to the consideration of sequential tests for such cases. Procedures for determining ``Target-Handicap'' forms of inspection, and their operating and sample size properties are given. This leads to a consideration of general linear sequential tests, which are those test procedures which can be formulated in terms of a ``score.'' Such procedures are shown to be similar to classical games of chance, and to physical diffusion processes. The diffusion analogy leads to a differential equation which gives the approximate characteristics of any such linear test. In many cases, Wald's ``Probability Ratio Sequential Test'' takes the form of a linear test. The conditions for this are determined. The P.R.S. test is seen to be ``best possible linear test,'' in the sense of minimizing average sample size. The effects of deviations from normality, and general distributions are considered. Reference is made to Wald's work on tests which involve parameters other than those being estimated, and then consideration is restricted to tests for the mean of normal populations where the variance is unknown. Methods of reducing such tests to simple binomial tests are indicated. A number of procedures for use with 2 {\texttimes} 2 comparative trials, and double dichotomies, are given, and their properties discussed. Returning to general inspection problems, the paper indicates that these are not always to be identified with problems involving merely tests of statistical hypotheses. The notions of Consumer's Lot, Producer's Batch, the Lot Quality Curve, the Process Curve, are explained, and their importance indicated. A distinction is made between Acceptance Inspection schemes and Rectifying Inspection schemes, and the notions of Operating Characteristic Curve, Operating Characteristic Matrix, and the Sample Size distribution function are explained. The lattice diagram is used to bring out relationships between notions involved in general inspection, and some other uses are also indicated. Finally, some reflections on the relevance of the matters discussed to matters of current debate among statisticians are given.},
  langid = {english}
}

@inproceedings{barrett.m:2015,
  title = {The {{Dundee}} Treebank},
  booktitle = {The 14th International Workshop on Treebanks and Linguistic Theories ({{TLT}} 14)},
  author = {Barrett, Maria and Agic, {\v Z}eljko and S{\o}gaard, Anders},
  year = {2015},
  pages = {242--248},
  date-added = {2021-09-16 13:19:25 -0400},
  date-modified = {2021-09-16 13:20:18 -0400}
}

@article{barton.s:1993,
  title = {A Case Study of Anomaly Detection: {{Shallow}} Semantic Processing and Cohesion Establishment},
  shorttitle = {A Case Study of Anomaly Detection},
  author = {Barton, Stephen B. and Sanford, Anthony J.},
  year = {1993},
  month = jul,
  journal = {Memory \& Cognition},
  volume = {21},
  number = {4},
  pages = {477--487},
  issn = {1532-5946},
  doi = {10.3758/BF03197179},
  urldate = {2024-05-28},
  abstract = {Although the establishment of a coherent mental representation depends on semantic analysis, such analysis is not necessarily complete. This is illustrated by failures to notice the anomaly in questions such as, ``When an airplane crashes, where should the survivors be buried?'' Four experiments were carried out to extend knowledge of what determines the incidental detection of the critical item. Detection is a function of the goodness of global fit of the item (Experiments 1 and 2) and the extent to which the scenario predicts the item (Experiment 3). Global good fit appears to result in shallow processing of details. In Experiment 4, it is shown that if satisfactory coherence can be established without detailed semantic analysis, through the recruitment of suitable information from a sentence, then processing is indeed shallow. The studies also show that a text is not understood by first producing a local semantic representation and then incorporating this into a global model, and that semantic processing is not strictly incremental.},
  langid = {english},
  keywords = {Anomaly Detection,Bicycle Accident,Critical Item,Detection Rate,Noun Phrase},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/barton.s1993 A case study of anomaly detection Shall.pdf}
}

@article{bashirov.a:2008,
  title = {Multiplicative Calculus and Its Applications},
  author = {Bashirov, Agamirza E. and Kurp{\i}nar, Emine M{\i}s{\i}rl{\i} and {\"O}zyap{\i}c{\i}, Ali},
  year = {2008},
  month = jan,
  journal = {Journal of Mathematical Analysis and Applications},
  volume = {337},
  number = {1},
  pages = {36--48},
  issn = {0022-247X},
  doi = {10.1016/j.jmaa.2007.03.081},
  urldate = {2023-04-25},
  abstract = {Two operations, differentiation and integration, are basic in calculus and analysis. In fact, they are the infinitesimal versions of the subtraction and addition operations on numbers, respectively. In the period from 1967 till 1970 Michael Grossman and Robert Katz gave definitions of a new kind of derivative and integral, moving the roles of subtraction and addition to division and multiplication, and thus established a new calculus, called multiplicative calculus. In the present paper our aim is to bring up this calculus to the attention of researchers and demonstrate its usefulness.},
  langid = {english},
  keywords = {Calculus,Calculus of variations,Derivative,Differential equation,haar measure,Integral,Limit,multiplicative integral,product integral,Semigroup}
}

@article{bates.c:2020,
  title = {Efficient Data Compression in Perception and Perceptual Memory.},
  author = {Bates, Christopher J. and Jacobs, Robert A.},
  year = {2020},
  month = oct,
  journal = {Psychological Review},
  volume = {127},
  number = {5},
  pages = {891--917},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000197},
  urldate = {2022-11-28},
  abstract = {Efficient data compression is essential for capacity-limited systems, such as biological perception and perceptual memory. We hypothesize that the need for efficient compression shapes biological systems in many of the same ways that it shapes engineered systems. If true, then the tools that engineers use to analyze and design systems, namely rate-distortion theory (RDT), can profitably be used to understand human perception and memory. The first portion of this article discusses how three general principles for efficient data compression provide accounts for many important behavioral phenomena and experimental results. We also discuss how these principles are embodied in RDT. The second portion notes that exact RDT methods are computationally feasible only in low-dimensional stimulus spaces. To date, researchers have used deep neural networks to approximately implement RDT in high-dimensional spaces, but these implementations have been limited to tasks in which the sole goal is compression with respect to reconstruction error. Here, we introduce a new deep neural network architecture that approximately implements RDT. An important property of our architecture is that it is trained ``end-to-end,'' operating on raw perceptual input (e.g., pixel values) rather than intermediate levels of abstraction, as is the case with most psychological models. The article's final portion conjectures on how efficient compression can occur in memory over time, thereby providing motivations for multiple memory systems operating at different time scales, and on how efficient compression may explain some attentional phenomena such as RTs in visual search.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bates.c2020 Efficient data compression in perception.pdf}
}

@article{bates.d:2015lme4,
  title = {Fitting {{Linear Mixed-Effects Models Using}} {{{\textbf{lme4}}}}},
  author = {Bates, Douglas and M{\"a}chler, Martin and Bolker, Ben and Walker, Steve},
  year = {2015},
  journal = {Journal of Statistical Software},
  volume = {67},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v067.i01},
  urldate = {2024-02-25},
  langid = {english}
}

@article{baum.c:1994,
  title = {A Sequential Procedure for Multihypothesis Testing},
  author = {Baum, C.W. and Veeravalli, V.V.},
  year = {1994},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {40},
  number = {6},
  pages = {1994--2007},
  issn = {1557-9654},
  doi = {10.1109/18.340472},
  abstract = {The sequential testing of more than two hypotheses has important applications in direct-sequence spread spectrum signal acquisition, multiple-resolution-element radar, and other areas. A useful sequential test which we term the MSPRT is studied in this paper. The test is shown to be a generalization of the sequential probability ratio test. Under Bayesian assumptions, it is argued that the MSPRT approximates the much more complicated optimal test when error probabilities are small and expected stopping times are large. Bounds on error probabilities are derived, and asymptotic expressions for the stopping time and error probabilities are given. A design procedure is presented for determining the parameters of the MSPRT. Two examples involving Gaussian densities are included, and comparisons are made between simulation results and asymptotic expressions. Comparisons with Bayesian fixed sample size tests are also made, and it is found that the MSPRT requires two to three times fewer samples on average.{$<>$}},
  keywords = {Bayesian methods,Clinical trials,Error probability,Fault detection,Medical tests,Radar applications,Sequential analysis,Spread spectrum radar,Testing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/baum.c1994 A sequential procedure for multihypothes.pdf}
}

@article{baumann.s:2018,
  title = {What Makes a Word Prominent? {{Predicting}} Untrained {{German}} Listeners' Perceptual Judgments},
  author = {Baumann, Stefan and Winter, Bodo},
  year = {2018},
  journal = {Journal of Phonetics},
  volume = {70},
  pages = {20--38},
  publisher = {Elsevier},
  bdsk-url-2 = {https://www.sciencedirect.com/science/article/pii/S0095447017301298},
  date-added = {2020-02-27 22:24:39 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {intonation,phonetics,prominence,prosody,random forests}
}

@article{bayes.t:1763,
  title = {{An essay towards solving a problem in the doctrine of chances}},
  author = {Bayes, Thomas and Price, Richard},
  year = {1763},
  month = dec,
  journal = {Philosophical Transactions of the Royal Society of London},
  volume = {53},
  pages = {370--418},
  issn = {0261-0523, 2053-9223},
  doi = {10.1098/rstl.1763.0053},
  urldate = {2024-05-14},
  abstract = {Dear Sir, I Now send you an essay which I have found among the papers of our deceased friend Mr. Bayes, and which, in my opinion, has great merit, and well deserves to be preserved.},
  copyright = {https://royalsociety.org/journals/ethics-policies/data-sharing-mining/},
  langid = {latin},
  annotation = {note: By the late Rev. Mr. Bayes, F. R. S. communicated by Mr. Price, in a letter to John Canton, A. M. F. R. S.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bayes.t1763 An essay towards solving a problem in th.pdf}
}

@misc{beauchene.c:2022,
  title = {Dynamic Cognitive States Predict Individual Variability in Behavior and Modulate with {{EEG}} Functional Connectivity during Working Memory},
  author = {Beauchene, Christine and Hinault, Thomas and Sarma, Sridevi V. and Courtney, Susan},
  year = {2022},
  month = jan,
  pages = {2021.08.02.454757},
  institution = {bioRxiv},
  doi = {10.1101/2021.08.02.454757},
  urldate = {2022-06-13},
  abstract = {Fluctuations in strategy, attention, or motivation can cause large variability in performance across task trials. Typically, this variability is treated as noise, and assumed to cancel out, leaving supposedly stable relationships among behavior, neural activity, and experimental task conditions. Those relationships, however, could change with a participant's internal cognitive states, and variability in performance may carry important information regarding those states, which cannot be directly measured. Therefore, we used a mathematical, state-space modeling framework to estimate internal states from measured behavioral data, quantifying each participant's sensitivity to factors such as past errors or distractions, to predict their reaction time fluctuations. We show how modeling these states greatly improves trial-by-trial prediction of behavior. Further, we identify EEG functional connectivity features that modulate with each state. These results illustrate the potential of this approach and how it could enable quantification of intra- and inter-individual differences and provide insight into their neural bases. Statement of Relevance Cognitive behavioral performance and its neural bases vary both across individuals and within individuals over time. Understanding this variability may be key to the success of clinical or educational interventions. Internal cognitive states reflecting differences in strategy, attention, and motivation may drive much of these inter- and intra-individual differences, but often cannot be reliably controlled or measured in cognitive neuroscience research. The mathematical modeling framework developed here uses measured data to estimate a participant's dynamic, internal cognitive states, with each state derived from specific factors hypothesized to affect attention, motivation or strategy. The results highlight potential sources of behavioral variability and reveal EEG features that modulate with each state. Our method quantifies and characterizes individual behavioral differences and highlights their underlying neural mechanisms, which could be used for future targeted training or neuromodulation therapies to improve cognitive performance.},
  chapter = {New Results},
  copyright = {{\copyright} 2022, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/beauchene.c2022 Dynamic cognitive states predict individ.pdf}
}

@phdthesis{behrenfeldt.j:2009,
  title = {A Linguist's Survey of Pumping Lemmata},
  author = {Behrenfeldt, Johan},
  year = {2009},
  date-added = {2020-02-12 12:04:22 -0500},
  date-modified = {2021-03-12 11:46:23 -0500},
  school = {University of Gothenburg},
  keywords = {formal languages,pumping lemmata}
}

@incollection{bejar.s:2003,
  title = {Person Licensing and the Derivation of {{PCC}} Effects},
  booktitle = {Romance Linguistics: {{Theory}} and Acquisition},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  editor = {{Perez-Leroux}, Ana Teresa and Roberg, Yves},
  year = {2003},
  pages = {49--62},
  publisher = {J. Benjamins},
  address = {Amsterdam},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:23:56 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{bejar.s:2009,
  title = {Cyclic Agree},
  author = {B{\'e}jar, Susana and Rezac, Milan},
  year = {2009},
  journal = {Linguistic Inquiry},
  volume = {40},
  number = {1},
  pages = {35--73},
  publisher = {MIT Press},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:27:46 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects}
}

@inproceedings{belinkov.y:2017,
  title = {What Do Neural Machine Translation Models Learn about Morphology?},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Belinkov, Yonatan and Durrani, Nadir and Dalvi, Fahim and Sajjad, Hassan and Glass, James},
  year = {2017},
  pages = {861--872},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, Canada},
  doi = {10.18653/v1/P17-1080},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P17-1080}
}

@misc{bell-souder.d:2021cuny,
  title = {Language Modeling Using a Neural Network Shows Effects on {{N400}} beyond Just Surprisal},
  author = {{Bell-Souder}, Don and McKnight, Shannon and Zhdanov, Vladimir and Mullen, Sean and Miyake, Akira and Gilley, Phillip and Kim, Albert},
  year = {2021},
  month = mar,
  address = {University of Pennsylvania, Philadelphia, PA},
  urldate = {2023-05-23},
  langid = {american},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bell-souder.d2021cuny Language modeling using a neural network.pdf}
}

@inproceedings{bell.a:2003,
  title = {The Co-Information Lattice},
  booktitle = {Proceedings of the Fifth International Workshop on Independent Component Analysis and Blind Signal Separation: {{ICA}}},
  author = {Bell, Anthony J},
  year = {2003},
  volume = {2003},
  date-added = {2019-05-15 00:08:25 -0400},
  date-modified = {2021-07-19 22:04:55 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,synergy}
}

@article{belletti.a:2012,
  title = {Does Gender Make a Difference? {{Comparing}} the Effect of Gender on Children's Comprehension of Relative Clauses in {{Hebrew}} and {{Italian}}},
  shorttitle = {Does Gender Make a Difference?},
  author = {Belletti, Adriana and Friedmann, Naama and Brunato, Dominique and Rizzi, Luigi},
  year = {2012},
  month = aug,
  journal = {Lingua},
  volume = {122},
  number = {10},
  pages = {1053--1069},
  issn = {00243841},
  doi = {10.1016/j.lingua.2012.02.007},
  urldate = {2023-10-29},
  langid = {english},
  keywords = {agreement attraction,hebrew}
}

@misc{belrose.n:2023,
  title = {Eliciting Latent Predictions from Transformers with the Tuned Lens},
  author = {Belrose, Nora and Furman, Zach and Smith, Logan and Halawi, Danny and Ostrovsky, Igor and McKinney, Lev and Biderman, Stella and Steinhardt, Jacob},
  year = {2023},
  month = mar,
  number = {arXiv:2303.08112},
  eprint = {2303.08112},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.08112},
  urldate = {2023-03-16},
  abstract = {We analyze transformers from the perspective of iterative inference, seeking to understand how model predictions are refined layer by layer. To do so, we train an affine probe for each block in a frozen pretrained model, making it possible to decode every hidden state into a distribution over the vocabulary. Our method, the {\textbackslash}emph\{tuned lens\}, is a refinement of the earlier ``logit lens'' technique, which yielded useful insights but is often brittle. We test our method on various autoregressive language models with up to 20B parameters, showing it to be more predictive, reliable and unbiased than the logit lens. With causal experiments, we show the tuned lens uses similar features to the model itself. We also find the trajectory of latent predictions can be used to detect malicious inputs with high accuracy. All code needed to reproduce our results can be found at https://github.com/AlignmentResearch/tuned-lens.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning}
}

@inproceedings{bender.e:2021,
  title = {On the Dangers of Stochastic Parrots: {{Can}} Language Models Be Too Big? 🦜},
  shorttitle = {On the Dangers of Stochastic Parrots},
  booktitle = {Proceedings of the 2021 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Bender, Emily M. and Gebru, Timnit and {McMillan-Major}, Angelina and Shmitchell, Shmargaret},
  year = {2021},
  month = mar,
  series = {{{FAccT}} '21},
  pages = {610--623},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3442188.3445922},
  urldate = {2022-11-16},
  abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
  isbn = {978-1-4503-8309-7},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bender.e2021 On the dangers of stochastic parrots Ca.pdf}
}

@inproceedings{bengio.y:2000,
  title = {A Neural Probabilistic Language Model},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal},
  year = {2000},
  volume = {13},
  publisher = {MIT Press},
  urldate = {2024-05-15},
  abstract = {A goal  of statistical language modeling is  to  learn  the joint probability  function of sequences of words.  This is intrinsically difficult because of  the curse of dimensionality:  we propose to fight it with its own weapons.  In the proposed approach one learns simultaneously (1) a distributed rep(cid:173) resentation for each word (i.e.  a similarity between words) along with (2)  the probability function for word sequences, expressed with these repre(cid:173) sentations.  Generalization is  obtained because a sequence of words that  has  never been seen before gets  high probability if it is  made of words  that are similar to words forming an already seen sentence.  We report on  experiments using neural networks for the probability function, showing  on  two  text  corpora that  the  proposed approach  very  significantly  im(cid:173) proves on a state-of-the-art trigram model.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bengio.y2000 A neural probabilistic language model.pdf}
}

@article{bengio.y:2003,
  title = {A Neural Probabilistic Language Model},
  author = {Bengio, Yoshua and Ducharme, R{\'e}jean and Vincent, Pascal and Jauvin, Christian},
  year = {2003},
  journal = {Journal of Machine Learning Research},
  volume = {3},
  number = {Feb},
  pages = {1137--1155},
  issn = {ISSN 1533-7928},
  urldate = {2024-05-15},
  abstract = {A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bengio.y2003 A neural probabilistic language model.pdf}
}

@inproceedings{bengio.y:2014,
  title = {Deep Generative Stochastic Networks Trainable by Backprop},
  booktitle = {Proceedings of the 31st {{International Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Laufer, Eric and Alain, Guillaume and Yosinski, Jason},
  editor = {Xing, Eric P. and Jebara, Tony},
  year = {2014},
  month = jun,
  volume = {32},
  pages = {226--234},
  publisher = {PMLR},
  address = {Beijing, China},
  issn = {1938-7228},
  urldate = {2023-12-27},
  abstract = {We introduce a novel training principle for probabilistic models that is an alternative to maximum likelihood. The proposed Generative Stochastic Networks (GSN) framework is based on learning the transition operator of a Markov chain whose stationary distribution estimates the data distribution.  Because the transition distribution is a conditional distribution generally involving a small move, it has fewer dominant modes, being unimodal in the limit of small moves. Thus, it is easier to learn, more like learning to perform supervised function approximation, with gradients that can be obtained by backprop. The theorems provided here generalize recent work on the probabilistic interpretation of denoising autoencoders and provide an interesting justification for dependency networks and generalized pseudolikelihood (along with defining an appropriate joint distribution and sampling mechanism, even when the conditionals are not consistent). GSNs can be used with missing inputs and can be used to sample subsets of variables given the rest.  Successful experiments are conducted, validating these theoretical results, on two image datasets and with a particular architecture that mimics the Deep Boltzmann Machine Gibbs sampler but allows training to proceed with backprop, without the need for layerwise pretraining.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bengio.y2014 Deep generative stochastic networks trai 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/bengio.y2014 Deep generative stochastic networks trai.pdf}
}

@article{bennett.r:2009,
  title = {English Resumptive Pronouns and the Highest-Subject Restriction: {{A}} Corpus Study},
  author = {Bennett, Ryan},
  year = {2009},
  journal = {Trilateral (TREND) Linguistics Weekend, UC Santa Cruz},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bennett.r2009 English resumptive pronouns and the high 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/bennett.r2009 English resumptive pronouns and the high.pdf}
}

@techreport{bennett.v:2010,
  title = {Wasatch {{Solar Project}} Final Report},
  author = {Bennett, Vicki and Bowman, Kate and Wright, Sarah},
  year = {2018},
  number = {DOE-SLC-6903-1},
  address = {Salt Lake City, UT},
  institution = {Salt Lake City Corporation},
  doi = {10.2172/1474305},
  date-added = {2021-03-12 11:39:14 -0500},
  date-modified = {2021-03-12 11:43:47 -0500}
}

@book{bernardo.j:1994bt1,
  title = {Bayesian Theory},
  author = {Bernardo, Jos{\'e} M. and Smith, Adrian F. M.},
  year = {1994},
  month = may,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9780470316870},
  urldate = {2024-05-21},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-0-471-49464-5 978-0-470-31687-0},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bernardo.j1994bt1 Bayesian theory.pdf}
}

@article{berwick.r:1982,
  title = {Parsing Efficiency, Computational Complexity, and the Evaluation of Grammatical Theories},
  author = {Berwick, Robert C. and Weinberg, Amy S.},
  year = {1982},
  journal = {Linguistic Inquiry},
  volume = {13},
  number = {2},
  eprint = {4178272},
  eprinttype = {jstor},
  pages = {165--191},
  publisher = {The MIT Press},
  issn = {00243892, 15309150},
  date-added = {2022-03-29 20:33:12 -0400},
  date-modified = {2022-03-29 20:33:17 -0400}
}

@inproceedings{bicknell.k:2009,
  title = {A Model of Local Coherence Effects in Human Sentence Processing as Consequences of Updates from Bottom-up Prior to Posterior Beliefs},
  booktitle = {Proceedings of Human Language Technologies: The 2009 Annual Conference of the {{North American}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2009},
  month = jun,
  pages = {665--673},
  publisher = {Association for Computational Linguistics},
  address = {Boulder, Colorado},
  date-added = {2022-04-21 10:50:28 -0400},
  date-modified = {2022-04-21 10:50:29 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bicknell.k2009 A model of local coherence effects in hu.pdf}
}

@inproceedings{bicknell.k:2010,
  title = {A Rational Model of Eye Movement Control in Reading},
  booktitle = {Proceedings of the 48th Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2010},
  pages = {1168--1178},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bicknell.k2010 A rational model of eye movement control.pdf}
}

@phdthesis{bicknell.k:2011,
  title = {Eye Movements in Reading as Rational Behavior},
  author = {Bicknell, Klinton},
  year = {2011},
  school = {University of California, San Diego},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bicknell.k2011 Eye movements in reading as rational beh.pdf}
}

@inproceedings{bicknell.k:2012cogsci,
  title = {Word Predictability and Frequency Effects in a Rational Model of Reading},
  booktitle = {Proceedings of the 34th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Bicknell, Klinton and Levy, Roger},
  year = {2012},
  volume = {34},
  pages = {126--131},
  publisher = {Cognitive Science Society},
  address = {Sapporo, Japan},
  abstract = {This paper presents results from the first rational model of eye movement control in reading to make predictions for the full range of the eye movement record. The model identifies the text through Bayesian inference and makes eye movement de-cisions to maximize the efficiency of text identification, go-ing beyond leading approaches which select model parame-ters to maximize the fit to human data. Two simulations with the model demonstrate that it can produce effects of word pre-dictability and frequency on eye movements in reading similar to those produced by humans, providing evidence that many properties of human reading behavior may be understood as following from the nature of efficient text identification.},
  keywords = {surprisal theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bicknell.k2012cogsci Word predictability and frequency effect.pdf}
}

@book{billingsley.p:1995,
  title = {Probability and Measure},
  author = {Billingsley, Patrick},
  year = {1995},
  edition = {Third edition},
  publisher = {Wiley},
  bdsk-url-2 = {https://www.colorado.edu/amath/sites/default/files/attached-files/billingsley.pdf},
  date-added = {2021-03-28 11:47:36 -0400},
  date-modified = {2021-08-21 16:16:00 -0400},
  keywords = {measure theory,probability theory}
}

@book{bishop.c:2006,
  title = {Pattern {{Recognition}} and {{Machine Learning}}},
  author = {Bishop, Christopher M.},
  year = {2006},
  series = {Information {{Science}} and {{Statistics}}},
  edition = {1},
  publisher = {Springer},
  address = {New York, USA},
  urldate = {2022-06-10},
  isbn = {978-0-387-31073-2},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bishop.c2006 Pattern Recognition and Machine Learning.pdf}
}

@article{bitzer.s:2014,
  title = {Perceptual Decision Making: Drift-Diffusion Model Is Equivalent to a {{Bayesian}} Model},
  shorttitle = {Perceptual Decision Making},
  author = {Bitzer, Sebastian and Park, Hame and Blankenburg, Felix and Kiebel, Stefan},
  year = {2014},
  journal = {Frontiers in Human Neuroscience},
  volume = {8},
  issn = {1662-5161},
  urldate = {2022-07-04},
  abstract = {Behavioral data obtained with perceptual decision making experiments are typically analyzed with the drift-diffusion model. This parsimonious model accumulates noisy pieces of evidence toward a decision bound to explain the accuracy and reaction times of subjects. Recently, Bayesian models have been proposed to explain how the brain extracts information from noisy input as typically presented in perceptual decision making tasks. It has long been known that the drift-diffusion model is tightly linked with such functional Bayesian models but the precise relationship of the two mechanisms was never made explicit. Using a Bayesian model, we derived the equations which relate parameter values between these models. In practice we show that this equivalence is useful when fitting multi-subject data. We further show that the Bayesian model suggests different decision variables which all predict equal responses and discuss how these may be discriminated based on neural correlates of accumulated evidence. In addition, we discuss extensions to the Bayesian model which would be difficult to derive for the drift-diffusion model. We suggest that these and other extensions may be highly useful for deriving new experiments which test novel hypotheses.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bitzer.s2014 Perceptual decision making drift-diffus.pdf}
}

@article{blachman.n:1968,
  title = {The Amount of Information That y Gives about {{X}}},
  author = {Blachman, N.},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {1},
  pages = {27--31},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tit.1968.1054094},
  keywords = {entropy,entropy reduction,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/blachman.n1968 The amount of information that y gives a.pdf}
}

@misc{black.s:2021GPT-Neo,
  title = {{{GPT-Neo}}: {{Large}} Scale Autoregressive Language Modeling with Meshtensorflow},
  shorttitle = {{{GPT-Neo}}},
  author = {Black, Sid and Gao, Leo and Wang, Phil and Leahy, Connor and Biderman, Stella},
  year = {2021},
  month = oct,
  doi = {10.5281/ZENODO.5551208},
  urldate = {2024-02-01},
  abstract = {An implementation of model parallel GPT-2 and GPT-3-style models using the mesh-tensorflow library.},
  copyright = {Open Access},
  howpublished = {Zenodo}
}

@inproceedings{black.s:2022GPT-NeoX,
  title = {{{GPT-NeoX-20B}}: {{An}} Open-Source Autoregressive Language Model},
  shorttitle = {{{GPT-NeoX-20B}}},
  booktitle = {Proceedings of {{BigScience Episode}} \#5 -- {{Workshop}} on {{Challenges}} \& {{Perspectives}} in {{Creating Large Language Models}}},
  author = {Black, Sidney and Biderman, Stella and Hallahan, Eric and Anthony, Quentin and Gao, Leo and Golding, Laurence and He, Horace and Leahy, Connor and McDonell, Kyle and Phang, Jason and Pieler, Michael and Prashanth, Usvsn Sai and Purohit, Shivanshu and Reynolds, Laria and Tow, Jonathan and Wang, Ben and Weinbach, Samuel},
  editor = {Fan, Angela and Ilic, Suzana and Wolf, Thomas and Gall{\'e}, Matthias},
  year = {2022},
  month = may,
  pages = {95--136},
  publisher = {Association for Computational Linguistics},
  address = {virtual+Dublin},
  doi = {10.18653/v1/2022.bigscience-1.9},
  urldate = {2024-02-01},
  abstract = {We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe GPT-NeoX-20B's architecture and training, and evaluate its performance. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.}
}

@article{blei.d:2017,
  title = {Variational Inference: A Review for Statisticians},
  shorttitle = {Variational Inference},
  author = {Blei, David M. and Kucukelbir, Alp and McAuliffe, Jon D.},
  year = {2017},
  month = apr,
  journal = {Journal of the American Statistical Association},
  volume = {112},
  number = {518},
  pages = {859--877},
  issn = {0162-1459, 1537-274X},
  doi = {10.1080/01621459.2017.1285773},
  urldate = {2022-06-29},
  abstract = {One of the core problems of modern statistics is to approximate difficult-to-compute probability densities. This problem is especially important in Bayesian statistics, which frames all inference about unknown quantities as a calculation involving the posterior density. In this paper, we review variational inference (VI), a method from machine learning that approximates probability densities through optimization. VI has been used in many applications and tends to be faster than classical methods, such as Markov chain Monte Carlo sampling. The idea behind VI is to first posit a family of densities and then to find the member of that family which is close to the target. Closeness is measured by Kullback-Leibler divergence. We review the ideas behind mean-field variational inference, discuss the special case of VI applied to exponential family models, present a full example with a Bayesian mixture of Gaussians, and derive a variant that uses stochastic optimization to scale up to massive data. We discuss modern research in VI and highlight important open problems. VI is powerful, but it is not yet well understood. Our hope in writing this paper is to catalyze statistical research on this class of algorithms.},
  keywords = {Computer Science - Machine Learning,Statistics - Computation,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/blei.d2017 Variational inference a review for stat.pdf}
}

@inproceedings{blevins.t:2018,
  title = {Deep {{RNNs}} Encode Soft Hierarchical Syntax},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Blevins, Terra and Levy, Omer and Zettlemoyer, Luke},
  year = {2018},
  pages = {14--19},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-2003},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-2003}
}

@misc{bncconsortium:2007,
  type = {Corpus},
  title = {British {{National Corpus}}, {{XML}} Edition},
  author = {{BNC Consortium}},
  year = {2007},
  month = mar,
  publisher = {Oxford Text Archive},
  address = {ota:2554},
  urldate = {2023-12-18},
  abstract = {British National Corpus is a snapshot of British English in the early 1990s. The British National Corpus is (i) a sample corpus: composed of text samples generally no longer than 45,000 words; (ii) a synchronic corpus: the corpus includes imaginative texts from 1960-1990, informative texts from 1975-1990; (iii) a general corpus: not specifically restricted to any particular subject field, register or genre; (iv) a monolingual British English corpus: it comprises text samples which are substantially the product of speakers of British English; (v) a mixed corpus: it contains examples of both spoken and written language. The corpus is described in full in the Users Reference Guide at http://www.natcorp.ox.ac.uk/docs/URG/. Some XSL files are available for reformatting the XML texts in various ways, also from the BNC web site.},
  copyright = {Distributed by the University of Oxford under the BNC User Licence. Clicking to download implies acceptance of the licence conditions.},
  langid = {english},
  file = {/Users/j/DATA/BNC_XML/download/welcome.htm}
}

@book{boas.r:1997,
  title = {A Primer of Real Functions},
  author = {Boas, Ralph P. and Boas, Harold P.},
  year = {1997},
  month = jan,
  series = {The {{Carus}} Mathematical Monographs},
  edition = {4th ed},
  number = {no. 13},
  publisher = {Mathematical Association of America},
  address = {Washington, D.C.},
  isbn = {978-0-88385-029-9},
  lccn = {QA331.5 .B57 1996},
  keywords = {Functions of real variables}
}

@article{bobaljik.j:1996,
  title = {Subject Positions and the Roles of {{TP}}},
  author = {Bobaljik, Jonathan David and Jonas, Dianne},
  year = {1996},
  journal = {Linguistic Inquiry},
  volume = {27},
  number = {2},
  eprint = {4178934},
  eprinttype = {jstor},
  pages = {195--236},
  publisher = {The MIT Press},
  issn = {00243892, 15309150},
  abstract = {We propose that the specifier of a VP-external functional projection-Tense Phrase-may host subject NPs under certain conditions. We present empirical evidence that nonspecific subject NPs that have elsewhere been analyzed as remaining VP-internal occupy this position. We also offer theoretical arguments that transitive subjects may never remain internal to the VP at S-Structure in languages for which the Extended Projection Principle holds. Extending work by Bures (1992, 1993), we argue further that [Spec, TP] is implicated as a subject position in NP object shift constructions. Parametric availability of this one position accounts for a cluster of properties within the Germanic languages.},
  date-added = {2019-06-14 09:34:00 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {expletives,subject positions}
}

@article{bobaljik.j:2006,
  title = {Where's Phi},
  author = {Bobaljik, Jonathan David},
  year = {2006},
  journal = {Agreement as a postsyntactic operation. Ms. University of Connecticut},
  publisher = {Citeseer},
  date-added = {2020-02-02 22:23:49 -0500},
  date-modified = {2020-02-02 22:24:22 -0500},
  project = {Icelandic gluttony},
  keywords = {dative intervention,phi features}
}

@article{bock.k:1991,
  title = {Broken Agreement},
  author = {Bock, Kathryn and Miller, Carol A},
  year = {1991},
  month = jan,
  journal = {Cognitive Psychology},
  volume = {23},
  number = {1},
  pages = {45--93},
  issn = {0010-0285},
  doi = {10.1016/0010-0285(91)90003-7},
  urldate = {2023-04-03},
  abstract = {The subjects and verbs of English sentences agree in number. This superficially simple syntactic operation is regularly implemented by speakers, but occasionally derails in sentences such as The cost of the improvements have not yet been estimated. We examined whether the incidence of such errors was related to the presence of subject-like semantic features in the immediate preverbal nouns, in light of current questions about the semantic versus syntactic nature of sentence subjects and the interactivity of language processing. In three experiments, speakers completed sentence fragments designed to elicit erroneous agreement. We varied the number and animacy of the head noun and the immediate preverbal (local) noun, as well as the amount of material separating the head noun from the verb. The plurality of the local noun phrase had a large and reliable effect on the incidence of agreement errors, but neither its animacy nor its length affected their occurrence. The latter findings suggest, respectively, that the semantic features of sentence subjects are of minimal relevance to the syntactic and morphological processes that implement agreement, and that agreement features are specified at a point in processing where the eventual length of sentential constituents has little effect on syntactic planning. Both results follow naturally from explanations of language production that emphasize the segregation of sentence formulation processes into relatively autonomous components.},
  langid = {english},
  keywords = {agreement attraction}
}

@incollection{bod.r:2003,
  title = {Data-Oriented Parsing},
  booktitle = {Data-Oriented Parsing},
  editor = {Bod, Rens and Scha, Remko and Sima'an, Khalil},
  year = {2003},
  publisher = {CSLI},
  address = {Stanford, CA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{bod.r:2006,
  title = {An All-Subtrees Approach to Unsupervised Parsing},
  booktitle = {Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics},
  author = {Bod, Rens},
  year = {2006},
  pages = {865--872},
  publisher = {Association for Computational Linguistics},
  address = {Sydney, Australia},
  doi = {10.3115/1220175.1220284},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220284}
}

@article{boeckx.c:2000,
  title = {Quirky Agreement},
  author = {Boeckx, Cedric},
  year = {2000},
  journal = {Studia linguistica},
  volume = {54},
  number = {3},
  pages = {354--380},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{bogacz.r:2017,
  title = {A Tutorial on the Free-Energy Framework for Modelling Perception and Learning},
  author = {Bogacz, Rafal},
  year = {2017},
  month = feb,
  journal = {Journal of Mathematical Psychology},
  series = {Model-Based {{Cognitive Neuroscience}}},
  volume = {76},
  pages = {198--211},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2015.11.003},
  urldate = {2022-08-07},
  abstract = {This paper provides an easy to follow tutorial on the free-energy framework for modelling perception developed by Friston, which extends the predictive coding model of Rao and Ballard. These models assume that the sensory cortex infers the most likely values of attributes or features of sensory stimuli from the noisy inputs encoding the stimuli. Remarkably, these models describe how this inference could be implemented in a network of very simple computational elements, suggesting that this inference could be performed by biological networks of neurons. Furthermore, learning about the parameters describing the features and their uncertainty is implemented in these models by simple rules of synaptic plasticity based on Hebbian learning. This tutorial introduces the free-energy framework using very simple examples, and provides step-by-step derivations of the model. It also discusses in more detail how the model could be implemented in biological neural circuits. In particular, it presents an extended version of the model in which the neurons only sum their inputs, and synaptic plasticity only depends on activity of pre-synaptic and post-synaptic neurons.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bogacz.r2017 A tutorial on the free-energy framework.pdf}
}

@misc{bommasani.r:2022arxiv,
  title = {On the Opportunities and Risks of Foundation Models},
  author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and {von Arx}, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and {Fei-Fei}, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and R{\'e}, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tram{\`e}r, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
  year = {2022},
  month = jul,
  number = {arXiv:2108.07258},
  eprint = {2108.07258},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2108.07258},
  urldate = {2024-05-23},
  abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {Note: Authored by the Center for Research on Foundation Models (CRFM) at the Stanford Institute for Human-Centered Artificial Intelligence (HAI)},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bommasani.r2022arxiv On the opportunities and risks of founda.pdf}
}

@phdthesis{bonet.m:1991,
  title = {Morphology after Syntax: {{Pronominal}} Clitics in {{Romance}}},
  author = {Bonet, M. Eul{\'a}lia},
  year = {1991},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:14:38 -0400},
  project = {Icelandic gluttony},
  school = {MIT},
  keywords = {clitics,hierarchy effects}
}

@article{boston.m:2008,
  title = {Parsing Costs as Predictors of Reading Difficulty: {{An}} Evaluation Using the {{Potsdam Sentence Corpus}}},
  shorttitle = {Parsing Costs as Predictors of Reading Difficulty},
  author = {Boston, Marisa Ferrara and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
  year = {2008},
  month = sep,
  journal = {Journal of Eye Movement Research},
  volume = {2},
  number = {1},
  issn = {1995-8692},
  doi = {10.16910/jemr.2.1.1},
  urldate = {2022-10-13},
  abstract = {The surprisal of a word on a probabilistic grammar constitutes a promising complexity metric for human sentence comprehension difficulty. Using two different grammar types, surprisal is shown to have an effect on fixation durations and regression probabilities in a sample of German readers' eye movements, the Potsdam Sentence Corpus. A linear mixed-effects model was used to quantify the effect of surprisal while taking into account unigram frequency and bigram frequency (transitional probability), word length, and empirically-derived word predictability; the socalled ``early'' and ``late'' measures of processing difficulty both showed an effect of surprisal. Surprisal is also shown to have a small but statistically non-significant effect on empirically-derived predictability itself. This work thus demonstrates the importance of including parsing costs as a predictor of comprehension difficulty in models of reading, and suggests that a simple identification of syntactic parsing costs with early measures and late measures with durations of post-syntactic events may be difficult to uphold.},
  copyright = {Copyright (c)},
  langid = {english},
  keywords = {parsing costs,parsing difficulty,potsdam sentence corpus,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/boston.m2008 Parsing costs as predictors of reading d.pdf}
}

@incollection{boston.m:2009,
  title = {Dependency Structures Derived from Minimalist Grammars},
  booktitle = {The Mathematics of Language},
  author = {Boston, Marisa Ferrara and Hale, John T. and Kuhlmann, Marco},
  year = {2009},
  pages = {1--12},
  publisher = {Springer},
  date-added = {2019-06-15 11:31:22 -0400},
  date-modified = {2022-04-20 13:49:51 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,minimalist grammars}
}

@inproceedings{bouchard-cote.a:2009,
  title = {Randomized Pruning: Efficiently Calculating Expectations in Large Dynamic Programs},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {{Bouchard-C{\^o}t{\'e}}, Alexandre and Petrov, Slav and Klein, Dan},
  editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. and Williams, C. and Culotta, A.},
  year = {2009},
  volume = {22},
  publisher = {Curran Associates, Inc.},
  date-added = {2022-03-31 10:05:14 -0400},
  date-modified = {2022-03-31 22:30:35 -0400},
  keywords = {markov chain monte carlo,pruning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bouchard-cote.a2009 Randomized pruning efficiently calculat.pdf}
}

@inproceedings{bouma.g:2009npmi,
  title = {Normalized (Pointwise) Mutual Information in Collocation Extraction},
  booktitle = {Von Der {{Form}} Zur {{Bedeutung}}: {{Texte}} Automatisch Verarbeiten / {{From Form}} to {{Meaning}}: {{Processing Texts Automatically}}},
  author = {Bouma, Gerlof},
  editor = {Chiarcos, Christian and {de Castilho}, Richard Eckart and Stede, Manfred},
  year = {2009},
  month = sep,
  pages = {43--53},
  publisher = {Narr Francke Attempto Verlag GmbH + Co. KG},
  address = {T{\"u}bingen},
  abstract = {In this paper, we discuss the related information theoretical association measures of mutual information and pointwise mutual information, in the context of collocation extraction. We introduce normalized variants of these measures in order to make them more easily interpretable and at the same time less sensitive to occurrence frequency. We also provide a small empirical study to give more insight into the behaviour of these new measures in a collocation extraction setup.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bouma.g2009npmi Normalized (pointwise) mutual informatio.pdf}
}

@misc{boyce.v:2020amlap,
  type = {Talk},
  title = {A-Maze of {{Natural Stories}}: Texts Are Comprehensible Using the {{Maze}} Task},
  author = {Boyce, Veronica and Levy, Roger},
  year = {2020},
  month = sep,
  address = {Potsdam, Germany},
  collaborator = {{von der Malsburg}, Titus and Vasishth, Shravan and Wartenburger, Isabell}
}

@misc{boyce.v:2020amlap-github,
  title = {Amaze-Natural-Stories},
  author = {Boyce, Veronica},
  year = {2022},
  month = mar,
  urldate = {2022-09-24},
  abstract = {Materials, data, code for A-maze of Natural Stories talk},
  keywords = {amlap}
}

@article{boyce.v:2020jml,
  title = {Maze Made Easy: Better and Easier Measurement of Incremental Processing Difficulty},
  author = {Boyce, Veronica and Futrell, Richard and Levy, Roger},
  year = {2020},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {111},
  pages = {104082},
  publisher = {Elsevier BV},
  doi = {10.1016/j.jml.2019.104082},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2019.104082},
  date-added = {2021-09-30 07:52:16 -0400},
  date-modified = {2022-04-20 13:48:24 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/boyce.v2020jml Maze made easy better and easier measur.pdf}
}

@misc{boyce.v:2022psyarxiv,
  title = {A-Maze of {{Natural Stories}}: Comprehension and Surprisal in the Maze Task},
  shorttitle = {A-Maze of Natural Stories},
  author = {Boyce, Veronica and Levy, Roger Philip},
  year = {2022},
  month = aug,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/8xkrf},
  urldate = {2023-03-01},
  abstract = {Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task. Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, with little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task.},
  langid = {american},
  keywords = {A-Maze,incremental processing,Linguistics,naturalistic text,Psycholinguistics and Neurolinguistics,self-paced reading,Social and Behavioral Sciences,surprisal}
}

@article{boyce.v:2023,
  title = {A-Maze of {{Natural Stories}}: Comprehension and Surprisal in the {{Maze}} Task},
  shorttitle = {A-Maze of Natural Stories},
  author = {Boyce, Veronica and Levy, Roger},
  year = {2023},
  month = apr,
  journal = {Glossa Psycholinguistics},
  volume = {2},
  number = {1},
  issn = {2767-0279},
  doi = {10.5070/G6011190},
  urldate = {2023-08-02},
  abstract = {Behavioral measures of word-by-word reading time provide experimental evidence to test theories of language processing. A-maze is a recent method for measuring incremental sentence processing that can localize slowdowns related to syntactic ambiguities in individual sentences. We adapted A-maze for use on longer passages and tested it on the Natural Stories corpus. Participants were able to comprehend these longer text passages that they read via the Maze task. Moreover, the Maze task yielded useable reaction time data with word predictability effects that were linearly related to surprisal, the same pattern found with other incremental methods. Crucially, Maze reaction times show a tight relationship with properties of the current word, with little spillover of effects from previous words. This superior localization is an advantage of Maze compared with other methods. Overall, we expanded the scope of experimental materials, and thus theoretical questions, that can be studied with the Maze task.}
}

@book{brasoveanu.a:2020,
  title = {Computational Cognitive Modeling and Linguistic Theory},
  author = {Brasoveanu, Adrian and Dotla{\v c}il, Jakub},
  year = {2020},
  publisher = {Springer Nature},
  doi = {10.1007/978-3-030-31846-8},
  urldate = {2022-09-08},
  abstract = {This open access book introduces a general framework that allows natural language researchers to enhance existing competence theories with fully specified performance and processing components. Gradually developing increasingly complex and cognitively realistic competence-performance models, it provides running code for these models and shows how to fit them to real-time experimental data. This computational cognitive modeling approach opens up exciting new directions for research in formal semantics, and linguistics more generally, and offers new ways of (re)connecting semantics and the broader field of cognitive science. The approach of this book is novel in more ways than one. Assuming the mental architecture and procedural modalities of Anderson's ACT-R framework, it presents fine-grained computational models of human language processing tasks which make detailed quantitative predictions that can be checked against the results of self-paced reading and other psycho-linguistic experiments. All models are presented as computer programs that readers can run on their own computer and on inputs of their choice, thereby learning to design, program and run their own models. But even for readers who won't do all that, the book will show how such detailed, quantitatively predicting modeling of linguistic processes is possible. A methodological breakthrough and a must for anyone concerned about the future of linguistics! (Hans Kamp) This book constitutes a major step forward in linguistics and psycholinguistics. It constitutes a unique synthesis of several different research traditions: computational models of psycholinguistic processes, and formal models of semantics and discourse processing. The work also introduces a sophisticated python-based software environment for modeling linguistic processes. This book has the potential to revolutionize not only formal models of linguistics, but also models of language processing more generally. (Shravan Vasishth)},
  isbn = {978-3-030-31846-8},
  langid = {english},
  keywords = {ACT-R Based Left-corner Parser,bic Book Industry Communication::C Language::CF linguistics::CFA Philosophy of language,bic Book Industry Communication::C Language::CF linguistics::CFD Psycholinguistics,bic Book Industry Communication::C Language::CF linguistics::CFG Semantics,Cataphoric Presupposition Resolution,Cognitive Aspects of Processing Semantic Representations,discourse analysis,Enriched Semantics,etc,Incremental Dynamic Predicate Logic,Language Interpretation Processes,Linguistics,Meaning Representations in Formal Semantics,Natural Language Processing,Open Access,Philosophy of language,Philosophy of Language,Processing Enriched Logical Forms,Processing of Lexical Semantic and Syntactic Representations,Psycholinguistics,Psycholinguistics and Cognitive Lingusitics,Psycholinguistics on Incremental Interpretation,Real-time Construction of Syntactic Representations,Real-time Semantic Interpretation,Semantics,Semantics and Processing,stylistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/brasoveanu.a2020 Computational cognitive modeling and lin.pdf}
}

@article{braun.d:2014,
  title = {Information-Theoretic Bounded Rationality and {$\varepsilon$}-Optimality},
  author = {Braun, Daniel A. and Ortega, Pedro A.},
  year = {2014},
  month = aug,
  journal = {Entropy},
  volume = {16},
  number = {8},
  pages = {4662--4676},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e16084662},
  urldate = {2022-06-08},
  abstract = {Bounded rationality concerns the study of decision makers with limited information processing resources. Previously, the free energy difference functional has been suggested to model bounded rational decision making, as it provides a natural trade-off between an energy or utility function that is to be optimized and information processing costs that are measured by entropic search costs. The main question of this article is how the information-theoretic free energy model relates to simple {$\varepsilon$}-optimality models of bounded rational decision making, where the decision maker is satisfied with any action in an {$\varepsilon$}-neighborhood of the optimal utility. We find that the stochastic policies that optimize the free energy trade-off comply with the notion of {$\varepsilon$}-optimality. Moreover, this optimality criterion even holds when the environment is adversarial. We conclude that the study of bounded rationality based on {$\varepsilon$}-optimality criteria that abstract away from the particulars of the information processing constraints is compatible with the information-theoretic free energy model of bounded rationality.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {-optimality,ambiguity,bounded rationality,decision theory,information theory,probabilistic choice},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/braun.d2014 Information-theoretic bounded rationalit.pdf}
}

@inproceedings{brekelmans.r:2022,
  title = {Improving Mutual Information Estimation with Annealed and Energy-Based Bounds},
  booktitle = {International Conference on Learning Representations},
  author = {Brekelmans, Rob and Huang, Sicong and Ghassemi, Marzyeh and Steeg, Greg Ver and Grosse, Roger Baker and Makhzani, Alireza},
  year = {2022},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/brekelmans.r2022 Improving mutual information estimation.pdf}
}

@book{bresnan.j:2001,
  title = {Lexical-Functional Syntax},
  author = {Bresnan, Joan},
  year = {2001},
  publisher = {Wiley-Blackwell},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{brody.s:2010,
  title = {It Depends on the Translation: {{Unsupervised}} Dependency Parsing via Word Alignment},
  booktitle = {Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing},
  author = {Brody, Samuel},
  year = {2010},
  pages = {1214--1222},
  publisher = {Association for Computational Linguistics},
  address = {Cambridge, MA}
}

@article{brothers.t:2021,
  title = {Word Predictability Effects Are Linear, Not Logarithmic: Implications for Probabilistic Models of Sentence Comprehension},
  author = {Brothers, Trevor and Kuperberg, Gina R.},
  year = {2021},
  journal = {Journal of Memory and Language},
  volume = {116},
  pages = {104174},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2020.104174},
  abstract = {During language comprehension, we routinely use information from the prior context to help identify the meaning of individual words. While measures of online processing difficulty, such as reading times, are strongly influenced by contextual predictability, there is disagreement about the mechanisms underlying this lexical predictability effect, with different models predicting different linking functions -- linear (Reichle, Rayner, \& Pollatsek, 2003) or logarithmic (Levy, 2008). To help resolve this debate, we conducted two highly-powered experiments (self-paced reading, N = 216; cross-modal picture naming, N = 36), and a meta-analysis of prior eye-tracking while reading studies (total N = 218). We observed a robust linear relationship between lexical predictability and word processing times across all three studies. Beyond their methodological implications, these findings also place important constraints on predictive processing models of language comprehension. In particular, these results directly contradict the empirical predictions of surprisal theory, while supporting a proportional pre-activation account of lexical prediction effects in comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.jml.2020.104174},
  date-added = {2021-03-09 22:52:14 -0500},
  date-modified = {2021-11-14 23:57:23 -0500},
  keywords = {Information theory,Language comprehension,Prediction,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/brothers.t2021 Word predictability effects are linear,.pdf}
}

@article{brown.p:1993,
  title = {The Mathematics of Statistical Machine Translation: {{Parameter}} Estimation},
  author = {Brown, Peter F. and Della Pietra, Stephen A. and Della Pietra, Vincent J. and Mercer, Robert L.},
  year = {1993},
  journal = {Computational Linguistics},
  volume = {19},
  number = {2},
  pages = {263--311}
}

@inproceedings{brown.t:2020GPT3,
  title = {Language Models Are Few-Shot Learners},
  booktitle = {Advances in Neural Information Processing Systems 33: {{Annual}} Conference on Neural Information Processing Systems 2020, {{NeurIPS}} 2020, {{December}} 6-12, 2020, Virtual},
  author = {Brown, Tom B. and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and {Herbert-Voss}, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel M. and Wu, Jeffrey and Winter, Clemens and Hesse, Christopher and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
  editor = {Larochelle, Hugo and Ranzato, Marc'Aurelio and Hadsell, Raia and Balcan, Maria-Florina and Lin, Hsuan-Tien},
  year = {2020},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/BrownMRSKDNSSAA20.bib},
  date-modified = {2022-04-30 13:50:00 -0400},
  keywords = {GPT,GPT3},
  timestamp = {Tue, 19 Jan 2021 00:00:00 +0100}
}

@article{bruening.b:2012,
  title = {{\emph{By}} Phrases in Passives and Nominals},
  author = {Bruening, Benjamin},
  year = {2012},
  journal = {Syntax (Oxford, England)},
  volume = {16},
  number = {1},
  pages = {1--41},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9612.2012.00171.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2012.00171.x},
  date-added = {2021-03-22 13:11:47 -0400},
  date-modified = {2021-03-24 11:10:53 -0400}
}

@unpublished{bruening.b:2015,
  title = {Idioms: {{Movement}} and Non-Movement Dependencies},
  author = {Bruening, Benjamin},
  year = {2015},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {Idioms}
}

@misc{bubeck.s:2023arxiv,
  title = {Sparks of Artificial General Intelligence: Early Experiments with {{GPT-4}}},
  shorttitle = {Sparks of Artificial General Intelligence},
  author = {Bubeck, S{\'e}bastien and Chandrasekaran, Varun and Eldan, Ronen and Gehrke, Johannes and Horvitz, Eric and Kamar, Ece and Lee, Peter and Lee, Yin Tat and Li, Yuanzhi and Lundberg, Scott and Nori, Harsha and Palangi, Hamid and Ribeiro, Marco Tulio and Zhang, Yi},
  year = {2023},
  month = apr,
  number = {arXiv:2303.12712},
  eprint = {2303.12712},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.12712},
  urldate = {2024-05-23},
  abstract = {Artificial intelligence (AI) researchers have been developing and refining large language models (LLMs) that exhibit remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. The latest model developed by OpenAI, GPT-4, was trained using an unprecedented scale of compute and data. In this paper, we report on our investigation of an early version of GPT-4, when it was still in active development by OpenAI. We contend that (this early version of) GPT-4 is part of a new cohort of LLMs (along with ChatGPT and Google's PaLM for example) that exhibit more general intelligence than previous AI models. We discuss the rising capabilities and implications of these models. We demonstrate that, beyond its mastery of language, GPT-4 can solve novel and difficult tasks that span mathematics, coding, vision, medicine, law, psychology and more, without needing any special prompting. Moreover, in all of these tasks, GPT-4's performance is strikingly close to human-level performance, and often vastly surpasses prior models such as ChatGPT. Given the breadth and depth of GPT-4's capabilities, we believe that it could reasonably be viewed as an early (yet still incomplete) version of an artificial general intelligence (AGI) system. In our exploration of GPT-4, we put special emphasis on discovering its limitations, and we discuss the challenges ahead for advancing towards deeper and more comprehensive versions of AGI, including the possible need for pursuing a new paradigm that moves beyond next-word prediction. We conclude with reflections on societal influences of the recent technological leap and future research directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/bubeck.s2023arxiv Sparks of artificial general intelligenc.pdf}
}

@article{buckley.c:2017,
  title = {The Free Energy Principle for Action and Perception: {{A}} Mathematical Review},
  shorttitle = {The Free Energy Principle for Action and Perception},
  author = {Buckley, Christopher L. and Kim, Chang Sub and McGregor, Simon and Seth, Anil K.},
  year = {2017},
  month = dec,
  journal = {Journal of Mathematical Psychology},
  volume = {81},
  pages = {55--79},
  issn = {0022-2496},
  doi = {10.1016/j.jmp.2017.09.004},
  urldate = {2022-07-27},
  abstract = {The `free energy principle' (FEP) has been suggested to provide a unified theory of the brain, integrating data and theory relating to action, perception, and learning. The theory and implementation of the FEP combines insights from Helmholtzian `perception as inference', machine learning theory, and statistical thermodynamics. Here, we provide a detailed mathematical evaluation of a suggested biologically plausible implementation of the FEP that has been widely used to develop the theory. Our objectives are (i) to describe within a single article the mathematical structure of this implementation of the FEP; (ii) provide a simple but complete agent-based model utilising the FEP and (iii) to disclose the assumption structure of this implementation of the FEP to help elucidate its significance for the brain sciences.},
  langid = {english},
  keywords = {Action,Agent-based model,Bayesian brain,free energy principle,Free energy principle,Inference,Perception},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/buckley.c2017 The free energy principle for action and.pdf}
}

@article{burchill.z:2024,
  title = {How Reliable Are Standard Reading Time Analyses? {{Hierarchical}} Bootstrap Reveals Substantial Power over-Optimism and Scale-Dependent {{Type I}} Error Inflation},
  shorttitle = {How Reliable Are Standard Reading Time Analyses?},
  author = {Burchill, Zachary J. and Jaeger, T. Florian},
  year = {2024},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {136},
  pages = {104494},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2023.104494},
  urldate = {2024-02-04},
  abstract = {We investigate the statistical power and Type I error rate of the two most common approaches to reading time (RT) analyses: assuming normality of residuals and homogeneity of variance in raw or log-transformed RTs. We first show that the assumptions of such analyses---such as t-tests, ANOVAs, and linear mixed-effects models---are neither consistently met by raw RTs, nor by log-transformed RTs (or any other common power transforms, incl. inverse-transformed RTs). Only a non-power transform (log-shift) provides a decent fit for all data sets and data preparation steps we consider. We then compare the statistical power and Type I error rate for linear mixed-effects models over raw or log-transformed RTs. Previous studies on this matter relied on parametrically generated data. We show why this is problematic, and introduce as an alternative a hierarchical bootstrap approach over naturally distributed reading times. This approach yields substantially different---and arguably more informative---results than the parametric simulation approaches we compare it to. Our results suggests that it is time to heed the advice others have provided for reading research: for any but the simplest designs, we find both the rate of spurious significances and the rate of undetected true effects can strongly depend on the scale (e.g., raw or log-RTs) in which effects are assumed to be linear. Researchers should thus clearly motivate the choice of analysis based on theoretical grounds, assess the robustness of findings under different analysis approaches, and discuss potential mismatches between analyses. The R scripts and libraries shared in the accompanying OSF repo allow researchers to assess the reliability of their analyses via hierarchical bootstrap over their own data.},
  keywords = {Data analysis,Hierarchical bootstrap,Power,Reading times,Type I error},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/burchill.z2024 How reliable are standard reading time a.pdf}
}

@misc{burda.y:2015IWAE,
  title = {Importance Weighted Autoencoders},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2015},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1509.00519},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1509.00519},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:17:42 -0400},
  date-modified = {2022-05-05 09:19:51 -0400},
  keywords = {importance weighted autoencoders}
}

@misc{burda.y:2016,
  title = {Importance {{Weighted Autoencoders}}},
  author = {Burda, Yuri and Grosse, Roger and Salakhutdinov, Ruslan},
  year = {2016},
  month = nov,
  number = {arXiv:1509.00519},
  eprint = {1509.00519},
  primaryclass = {cs, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1509.00519},
  urldate = {2022-05-18},
  abstract = {The variational autoencoder (VAE; Kingma, Welling (2014)) is a recently proposed generative model pairing a top-down generative network with a bottom-up recognition network which approximates posterior inference. It typically makes strong assumptions about posterior inference, for instance that the posterior distribution is approximately factorial, and that its parameters can be approximated with nonlinear regression from the observations. As we show empirically, the VAE objective can lead to overly simplified representations which fail to use the network's entire modeling capacity. We present the importance weighted autoencoder (IWAE), a generative model with the same architecture as the VAE, but which uses a strictly tighter log-likelihood lower bound derived from importance weighting. In the IWAE, the recognition network uses multiple samples to approximate the posterior, giving it increased flexibility to model complex posteriors which do not fit the VAE modeling assumptions. We show empirically that IWAEs learn richer latent space representations than VAEs, leading to improved test log-likelihood on density estimation benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/burda.y2016 Importance Weighted Autoencoders.pdf}
}

@article{burkner.p:2017brms,
  title = {{{{\textbf{brms}}}}: An {{R}} Package for {{Bayesian}} Multilevel Models Using {{Stan}}},
  author = {B{\"u}rkner, Paul-Christian},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {80},
  number = {1},
  pages = {1--28},
  doi = {10.18637/jss.v080.i01},
  encoding = {UTF-8}
}

@book{burnham.k:2004book,
  title = {Model {{Selection}} and {{Multimodel Inference}}},
  editor = {Burnham, Kenneth P. and Anderson, David R.},
  year = {2004},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/b97636},
  urldate = {2024-03-14},
  isbn = {978-0-387-95364-9},
  langid = {english},
  keywords = {data analysis,Estimator,Inference,information theory,Likelihood,Model Selection},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/burnham.k2004book Model Selection and Multimodel Inference.pdf}
}

@inproceedings{buys.j:2015,
  title = {A {{Bayesian}} Model for Generative Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the Third International Conference on Dependency Linguistics (Depling 2015)},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = aug,
  pages = {58--67},
  publisher = {Uppsala University, Uppsala, Sweden},
  address = {Uppsala, Sweden},
  date-added = {2022-04-25 20:09:07 -0400},
  date-modified = {2022-04-25 20:09:41 -0400},
  keywords = {bayesian,dependency parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/buys.j2015 A Bayesian model for generative transiti.pdf}
}

@inproceedings{buys.j:2015short,
  title = {Generative {{Incremental Dependency Parsing}} with {{Neural Networks}}},
  booktitle = {Proceedings of the 53rd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 7th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 2: {{Short Papers}})},
  author = {Buys, Jan and Blunsom, Phil},
  year = {2015},
  month = jul,
  pages = {863--869},
  publisher = {Association for Computational Linguistics},
  address = {Beijing, China},
  doi = {10.3115/v1/P15-2142},
  urldate = {2022-06-14},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/buys.j2015short Generative Incremental Dependency Parsin.pdf}
}

@phdthesis{buys.j:2018phd,
  title = {Incremental Generative Models for Syntactic and Semantic Natural Language Processing},
  author = {Buys, Jan},
  year = {2018},
  urldate = {2022-06-14},
  abstract = {{$<$}p{$>$}This thesis investigates the role of linguistically-motivated generative models of syntax and semantic structure in natural language processing (NLP). Syntactic well-formedness is crucial in language generation, but most statistical models do not account for the hierarchical structure of sentences. Many applications exhibiting natural language understanding rely on structured semantic representations to enable querying, inference and reasoning. Yet most semantic parsers produce domain-specific or inadequately expressive representations.{$<$}/p{$>$} {$<$}p{$>$}We propose a series of generative transition-based models for dependency syntax which can be applied as both parsers and language models while being amenable to supervised or unsupervised learning. Two models are based on Markov assumptions commonly made in NLP: The first is a Bayesian model with hierarchical smoothing, the second is parameterised by feed-forward neural networks. The Bayesian model enables careful analysis of the structure of the conditioning contexts required for generative parsers, but the neural network is more accurate. As a language model the syntactic neural model outperforms both the Bayesian model and n-gram neural networks, pointing to the complementary nature of distributed and structured representations for syntactic prediction. We propose approximate inference methods based on particle filtering. The third model is parameterised by recurrent neural networks (RNNs), dropping the Markov assumptions. Exact inference with dynamic programming is made tractable here by simplifying the structure of the conditioning contexts.{$<$}/p{$>$} {$<$}p{$>$}We then shift the focus to semantics and propose models for parsing sentences to labelled semantic graphs. We introduce a transition-based parser which incrementally predicts graph nodes (predicates) and edges (arguments). This approach is contrasted against predicting top-down graph traversals. RNNs and pointer networks are key components in approaching graph parsing as an incremental prediction problem. The RNN architecture is augmented to condition the model explicitly on the transition system configuration. We develop a robust parser for Minimal Recursion Semantics, a linguistically-expressive framework for compositional semantics which has previously been parsed only with grammar-based approaches. Our parser is much faster than the grammar-based model, while the same approach improves the accuracy of neural Abstract Meaning Representation parsing.{$<$}/p{$>$}},
  langid = {english},
  school = {University of Oxford},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/buys.j2018phd Incremental generative models for syntac.pdf}
}

@inproceedings{cai.w:2014,
  title = {Making Comparisons Fair: How {{LS-means}} Unify the Analysis of Linear Models},
  booktitle = {Proceedings of the {{SAS Global Forum}}},
  author = {Cai, Weijie},
  year = {2014},
  volume = {Paper SAS060-2014},
  pages = {1--22},
  publisher = {SAS Institute Inc.},
  address = {Washington, D.C.}
}

@inproceedings{cao.k:2021,
  title = {You Should Evaluate Your Language Model on Marginal Likelihood over Tokenisations},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Cao, Kris and Rimell, Laura},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {2104--2114},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.161},
  urldate = {2024-02-25},
  abstract = {Neural language models typically tokenise input text into sub-word units to achieve an open vocabulary. The standard approach is to use a single canonical tokenisation at both train and test time. We suggest that this approach is unsatisfactory and may bottleneck our evaluation of language model performance. Using only the one-best tokenisation ignores tokeniser uncertainty over alternative tokenisations, which may hurt model out-of-domain performance. In this paper, we argue that instead, language models should be evaluated on their marginal likelihood over tokenisations. We compare different estimators for the marginal likelihood based on sampling, and show that it is feasible to estimate the marginal likelihood with a manageable number of samples. We then evaluate a pretrained language model on both the one-best-tokenisation and marginal perplexities, and show that the marginal perplexity can be significantly better than the one best, especially on out-of-domain data. We link this difference in perplexity to the tokeniser uncertainty as measured by tokeniser entropy. We discuss some implications of our results for language model training and evaluation, particularly with regard to tokenisation robustness.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cao.k2021 You should evaluate your language model.pdf}
}

@misc{cao.q:2023arxiv,
  title = {Unnatural {{Error Correction}}: {{GPT-4 Can Almost Perfectly Handle Unnatural Scrambled Text}}},
  shorttitle = {Unnatural {{Error Correction}}},
  author = {Cao, Qi and Kojima, Takeshi and Matsuo, Yutaka and Iwasawa, Yusuke},
  year = {2023},
  month = nov,
  number = {arXiv:2311.18805},
  eprint = {2311.18805},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-07},
  abstract = {While Large Language Models (LLMs) have achieved remarkable performance in many tasks, much about their inner workings remains unclear. In this study, we present novel experimental insights into the resilience of LLMs, particularly GPT-4, when subjected to extensive character-level permutations. To investigate this, we first propose the Scrambled Bench, a suite designed to measure the capacity of LLMs to handle scrambled input, in terms of both recovering scrambled sentences and answering questions given scrambled context. The experimental results indicate that most powerful LLMs demonstrate the capability akin to typoglycemia, a phenomenon where humans can understand the meaning of words even when the letters within those words are scrambled, as long as the first and last letters remain in place. More surprisingly, we found that only GPT-4 nearly flawlessly processes inputs with unnatural errors, even under the extreme condition, a task that poses significant challenges for other LLMs and often even for humans. Specifically, GPT-4 can almost perfectly reconstruct the original sentences from scrambled ones, decreasing the edit distance by 95\%, even when all letters within each word are entirely scrambled. It is counter-intuitive that LLMs can exhibit such resilience despite severe disruption to input tokenization caused by scrambled text.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cao.q2023arxiv Unnatural Error Correction GPT-4 Can Al.pdf}
}

@article{cappe.o:2007,
  title = {An Overview of Existing Methods and Recent Advances in Sequential {{Monte Carlo}}},
  author = {Cappe, Olivier and Godsill, Simon J. and Moulines, Eric},
  year = {2007},
  month = may,
  journal = {Proceedings of the IEEE},
  volume = {95},
  number = {5},
  pages = {899--924},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/jproc.2007.893250},
  bdsk-url-2 = {https://doi.org/10.1109/jproc.2007.893250},
  date-added = {2022-03-25 11:26:52 -0400},
  date-modified = {2022-03-25 11:26:52 -0400}
}

@article{carpenter.b:2017Stan,
  title = {{\textbf{Stan}}: A Probabilistic Programming Language},
  shorttitle = {{\emph{Stan}}},
  author = {Carpenter, Bob and Gelman, Andrew and Hoffman, Matthew D. and Lee, Daniel and Goodrich, Ben and Betancourt, Michael and Brubaker, Marcus and Guo, Jiqiang and Li, Peter and Riddell, Allen},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {76},
  number = {1},
  issn = {1548-7660},
  doi = {10.18637/jss.v076.i01},
  urldate = {2024-05-21},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/carpenter.b2017Stan Stan a probabilistic programming.pdf}
}

@article{carpenter.r:1995,
  title = {Neural Computation of Log Likelihood in Control of Saccadic Eye Movements},
  author = {Carpenter, R. H. S. and Williams, M. L. L.},
  year = {1995},
  month = sep,
  journal = {Nature},
  volume = {377},
  number = {6544},
  pages = {59--62},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/377059a0},
  urldate = {2022-06-24},
  abstract = {THE latency between the appearance of a visual target and the start of the saccadic eye movement made to look at it varies from trial to trial to an extent that is inexplicable in terms of ordinary 'physiological' processes such as synaptic delays and conduction velocities. An alternative interpretation is that it represents the time needed to decide whether a target is in fact present: decision processes are necessarily stochastic, because they depend on extracting information from noisy sensory signals1. In one such model2, the presence of a target causes a signal in a decision unit to rise linearly at a rate r from its initial value s0 until it reaches a fixed threshold 0, when a saccade is initiated. One can regard this decision signal as a neural estimate of the log likelihood of the hypothesis that the target is present, the threshold being the significance criterion or likelihood level at which the target is presumed to be present. Experiments manipulating the prior probability of the target's appearing confirm this notion: the latency distribution then changes in the way expected if s0 simply reflects the prior log likelihood of the stimulus.},
  copyright = {1995 Nature Publishing Group},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/carpenter.r1995 Neural computation of log likelihood in.pdf}
}

@article{carreiras.m:1997,
  title = {Effects of the Orthographic Neighborhood in Visual Word Recognition: {{Cross-task}} Comparisons},
  shorttitle = {Effects of the Orthographic Neighborhood in Visual Word Recognition},
  author = {Carreiras, Manuel and Perea, Manuel and Grainger, Jonathan},
  year = {1997},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {23},
  number = {4},
  pages = {857--871},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285},
  doi = {10.1037/0278-7393.23.4.857},
  abstract = {Effects of orthographic neighborhood in visual word recognition in Spanish were examined in 5 paradigms: progressive demasking, standard lexical decision, lexical decision with blocking of neighborhood density, naming, and semantic categorization. The results showed inhibitory effects of neighborhood frequency in the progressive-demasking task, in both lexical-decision tasks, as well as for low-density words in the naming task, and for high-density words in the semantic-categorization task. Higher levels of neighborhood density produced an inhibitory trend in the progressive-demasking task, facilitation in lexical decision (significant only when neighborhood density was blocked), and a robust facilitation effect in naming (only for words with higher frequency neighbors). A global analysis across tasks and one simulation study helped outline some of the underlying task-specific and task-independent mechanisms. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Classification (Cognitive Process),Lexical Decision,Naming,Orthography,Semantics,Visual Masking,Word Recognition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/carreiras.m1997 Effects of the orthographic neighborhood.pdf}
}

@techreport{carroll.g:1992,
  type = {{{AAAI}} Technical Report},
  title = {Two Experiments on Learning Probabilistic Dependency Grammars from Corpora},
  author = {Carroll, Glenn and Charniak, Eugene},
  year = {1992},
  journal = {Working notes of the AAAI workshop statistically-based NLP techniques},
  number = {WS-92-01},
  pages = {1--13},
  institution = {AAAI},
  abstract = {We present a scheme for learning prohabilistic dependency grammars from positive training examples plus constraints on rules. In particular we present the results of two experiments. The first, in which the constraints were minimal, was unsuccessful. The second, with significant constraints, was successful within the bounds of the task we had set.},
  date-added = {2021-04-18 10:28:37 -0400},
  date-modified = {2021-04-18 10:33:11 -0400},
  project = {syntactic embedding},
  keywords = {Dependency Grammar,dependency parsing,dependency structures,mutual information,pmi,unsupervised parsing}
}

@article{casadio.c:2002,
  title = {A Tale of Four Grammars},
  author = {Casadio, Claudia and Lambek, Joachim},
  year = {2002},
  journal = {Studia Logica. An International Journal for Symbolic Logic},
  volume = {71},
  number = {3},
  pages = {315--329},
  publisher = {Springer},
  date-added = {2019-10-25 23:52:18 -0400},
  date-modified = {2019-10-25 23:54:15 -0400},
  keywords = {bilinear logic,category theory,pregroup grammar}
}

@article{chalmers.d:1994,
  title = {On Implementing a Computation},
  author = {Chalmers, David J.},
  year = {1994},
  month = nov,
  journal = {Minds and Machines},
  volume = {4},
  number = {4},
  pages = {391--402},
  issn = {0924-6495, 1572-8641},
  doi = {10.1007/BF00974166},
  urldate = {2023-05-26},
  langid = {english}
}

@article{chaloner.k:1995,
  title = {Bayesian Experimental Design: A Review},
  author = {Chaloner, Kathryn and Verdinelli, Isabella},
  year = {1995},
  journal = {Statistical Science},
  volume = {10},
  number = {3},
  eprint = {2246015},
  eprinttype = {jstor},
  pages = {273--304},
  publisher = {Institute of Mathematical Statistics},
  issn = {08834237},
  abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision-theoretic approach. This framework casts criteria from the Bayesian literature of design as part of a single coherent approach. The decision-theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility function are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion.},
  date-added = {2021-09-15 10:23:51 -0400},
  date-modified = {2021-09-15 10:23:53 -0400}
}

@inproceedings{chang.h:2022,
  title = {Softmax {{Bottleneck Makes Language Models Unable}} to {{Represent Multi-mode Word Distributions}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chang, Haw-Shiuan and McCallum, Andrew},
  year = {2022},
  month = may,
  pages = {8048--8073},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.554},
  urldate = {2022-06-23},
  abstract = {Neural language models (LMs) such as GPT-2 estimate the probability distribution over the next word by a softmax over the vocabulary. The softmax layer produces the distribution based on the dot products of a single hidden state and the embeddings of words in the vocabulary. However, we discover that this single hidden state cannot produce all probability distributions regardless of the LM size or training data size because the single hidden state embedding cannot be close to the embeddings of all the possible next words simultaneously when there are other interfering word embeddings between them. In this work, we demonstrate the importance of this limitation both theoretically and practically. Our work not only deepens our understanding of softmax bottleneck and mixture of softmax (MoS) but also inspires us to propose multi-facet softmax (MFS) to address the limitations of MoS. Extensive empirical analyses confirm our findings and show that against MoS, the proposed MFS achieves two-fold improvements in the perplexity of GPT-2 and BERT.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chang.h2022 Softmax Bottleneck Makes Language Models.pdf}
}

@article{chang.j:1997,
  title = {Conditioning as Disintegration},
  author = {Chang, Joseph T and Pollard, David},
  year = {1997},
  journal = {Statistica Neerlandica},
  volume = {51},
  number = {3},
  pages = {287--317},
  publisher = {Wiley Online Library},
  doi = {10.1111/1467-9574.00056},
  date-added = {2021-02-05 12:20:15 -0500},
  date-modified = {2021-02-05 12:22:28 -0500},
  keywords = {probability theory}
}

@article{chang.y:2024,
  title = {A Survey on Evaluation of Large Language Models},
  author = {Chang, Yupeng and Wang, Xu and Wang, Jindong and Wu, Yuan and Yang, Linyi and Zhu, Kaijie and Chen, Hao and Yi, Xiaoyuan and Wang, Cunxiang and Wang, Yidong and Ye, Wei and Zhang, Yue and Chang, Yi and Yu, Philip S. and Yang, Qiang and Xie, Xing},
  year = {2024},
  month = jun,
  journal = {ACM Transactions on Intelligent Systems and Technology},
  volume = {15},
  number = {3},
  pages = {1--45},
  issn = {2157-6904, 2157-6912},
  doi = {10.1145/3641289},
  urldate = {2024-05-23},
  abstract = {Large language models (LLMs) are gaining increasing popularity in both academia and industry, owing to their unprecedented performance in various applications. As LLMs continue to play a vital role in both research and daily use, their evaluation becomes increasingly critical, not only at the task level, but also at the society level for better understanding of their potential risks. Over the past years, significant efforts have been made to examine LLMs from various perspectives. This paper presents a comprehensive review of these evaluation methods for LLMs, focusing on three key dimensions:               what to evaluate               ,               where to evaluate               , and               how to evaluate               . Firstly, we provide an overview from the perspective of evaluation tasks, encompassing general natural language processing tasks, reasoning, medical usage, ethics, education, natural and social sciences, agent applications, and other areas. Secondly, we answer the `where' and `how' questions by diving into the evaluation methods and benchmarks, which serve as crucial components in assessing the performance of LLMs. Then, we summarize the success and failure cases of LLMs in different tasks. Finally, we shed light on several future challenges that lie ahead in LLMs evaluation. Our aim is to offer invaluable insights to researchers in the realm of LLMs evaluation, thereby aiding the development of more proficient LLMs. Our key point is that evaluation should be treated as an essential discipline to better assist the development of LLMs. We consistently maintain the related open-source materials at:               https://github.com/MLGroupJLU/LLM-eval-survey},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chang.y2024 A survey on evaluation of large language.pdf}
}

@incollection{chater.n:1998,
  title = {The Rational Analysis of Inquiry: The Case of Parsing},
  shorttitle = {The Rational Analysis of Inquiry},
  booktitle = {Rational Models of Cognition},
  author = {Chater, Nick and Crocker, Matthew J. and Pickering, Martin J.},
  editor = {Oaksford, Mike and Chater, Nick},
  year = {1998},
  month = nov,
  pages = {441--468},
  publisher = {Oxford University Press},
  address = {Oxford, England},
  doi = {10.1093/oso/9780198524151.003.0020},
  urldate = {2024-05-14},
  abstract = {The cognitive system is not merely a passive receiver of information. It has some measure of control of what information it receives; and how that information is processed. Control over the information received may be exercised in a wide variety of ways, from adjustments to the sense organs (e.g. by moving the eyes), to decisions concerning which newspaper to read. Control over how information is processed is equally ubiquitous, ranging from attentional mechanisms (presuming that such mechanisms at least to some degree bias the resources applied to processing different aspects of the sensory input) to how much effort to spend thinking about a possible move in a chess game, or on a decision in everyday life.},
  isbn = {978-0-19-852415-1},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chater.n1998 The rational analysis of inquiry the ca.pdf}
}

@article{chater.n:1999,
  title = {Ten Years of the Rational Analysis of Cognition},
  author = {Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, N. and Oaksford, M. and Chater, Nick and Oaksford, Mike},
  year = {1999},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {3},
  number = {2},
  pages = {57--65},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(98)01273-X},
  urldate = {2022-07-07},
  langid = {english},
  pmid = {10234228},
  keywords = {Memory,Neuroscience,Rational analysis,Rationality,Reasoning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chater.n1999 Ten years of the rational analysis of co.pdf}
}

@article{chater.n:2006,
  title = {Probabilistic Models of Language Processing and Acquisition},
  author = {Chater, Nick and Manning, Christopher D.},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  series = {Special Issue: {{Probabilistic}} Models of Cognition},
  volume = {10},
  number = {7},
  pages = {335--344},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.05.006},
  urldate = {2022-10-22},
  abstract = {Probabilistic methods are providing new explanatory approaches to fundamental cognitive science questions of how humans structure, process and acquire language. This review examines probabilistic models defined over traditional symbolic structures. Language comprehension and production involve probabilistic inference in such models; and acquisition involves choosing the best model, given innate constraints and linguistic and other input. Probabilistic models can account for the learning and processing of language, while maintaining the sophistication of symbolic models. A recent burgeoning of theoretical developments and online corpus creation has enabled large models to be tested, revealing probabilistic constraints in processing, undermining acquisition arguments based on a perceived poverty of the stimulus, and suggesting fruitful links with probabilistic theories of categorization and ambiguity resolution in perception.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chater.n2006 Probabilistic models of language process.pdf}
}

@article{chater.n:2006trends,
  title = {Probabilistic Models of Cognition: {{Conceptual}} Foundations},
  shorttitle = {Probabilistic Models of Cognition},
  author = {Chater, Nick and Tenenbaum, Joshua B. and Yuille, Alan},
  year = {2006},
  month = jul,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {7},
  pages = {287--291},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2006.05.007},
  urldate = {2024-05-26},
  langid = {english},
  pmid = {16807064},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chater.n2006trends Probabilistic models of cognition Conce.pdf}
}

@book{chater.n:2008,
  title = {The Probabilistic Mind: Prospects for {{Bayesian}} Cognitive Science},
  shorttitle = {The Probabilistic Mind},
  editor = {Chater, Nick and Oaksford, Mike},
  year = {2008},
  month = mar,
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199216093.001.0001},
  urldate = {2024-05-26},
  abstract = {Abstract. The rational analysis method, first proposed by John R. Anderson, has been enormously influential in helping us understand high-level cognitive p},
  isbn = {978-0-19-169597-1},
  langid = {english}
}

@article{chatterjee.s:2018,
  title = {The Sample Size Required in Importance Sampling},
  author = {Chatterjee, Sourav and Diaconis, Persi},
  year = {2018},
  month = apr,
  journal = {The Annals of Applied Probability},
  volume = {28},
  number = {2},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/17-aap1326},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chatterjee.s2018 The sample size required in importance s.pdf}
}

@misc{chen.c:2023arxiv,
  title = {Accelerating Large Language Model Decoding with Speculative Sampling},
  author = {Chen, Charlie and Borgeaud, Sebastian and Irving, Geoffrey and Lespiau, Jean-Baptiste and Sifre, Laurent and Jumper, John},
  year = {2023},
  month = feb,
  number = {arXiv:2302.01318},
  eprint = {2302.01318},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-02-15},
  abstract = {We present speculative sampling, an algorithm for accelerating transformer decoding by enabling the generation of multiple tokens from each transformer call. Our algorithm relies on the observation that the latency of parallel scoring of short continuations, generated by a faster but less powerful draft model, is comparable to that of sampling a single token from the larger target model. This is combined with a novel modified rejection sampling scheme which preserves the distribution of the target model within hardware numerics. We benchmark speculative sampling with Chinchilla, a 70 billion parameter language model, achieving a 2-2.5x decoding speedup in a distributed setup, without compromising the sample quality or making modifications to the model itself.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,language model decoding,rejection sampling,speculative sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.c2023 Accelerating large language model decodi.pdf}
}

@inproceedings{chen.d:2014,
  title = {A Fast and Accurate Dependency Parser Using Neural Networks},
  booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Chen, Danqi and Manning, Christopher},
  year = {2014},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/v1/d14-1082},
  bdsk-url-2 = {https://doi.org/10.3115/v1/d14-1082},
  date-added = {2022-05-06 15:52:04 -0400},
  date-modified = {2022-05-06 15:53:01 -0400},
  keywords = {dependency parsing,dependency structures,parsing,stanford dependencies}
}

@inproceedings{chen.j:2024,
  title = {Language {{Model Based Unsupervised Dependency Parsing}} with {{Conditional Mutual Information}} and {{Grammatical Constraints}}},
  booktitle = {Proceedings of the 2024 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}} ({{Volume}} 1: {{Long Papers}})},
  author = {Chen, Junjie and He, Xiangheng and Miyao, Yusuke},
  editor = {Duh, Kevin and Gomez, Helena and Bethard, Steven},
  year = {2024},
  month = jun,
  pages = {6355--6366},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  urldate = {2024-06-24},
  abstract = {Previous methods based on Large Language Models (LLM) perform unsupervised dependency parsing by maximizing bi-lexical dependence scores. However, these previous methods adopt dependence scores that are difficult to interpret. These methods cannot incorporate grammatical constraints that previous grammar-based parsing research has shown beneficial to improving parsing performance. In this work, we apply Conditional Mutual Information (CMI), an interpretable metric, to measure the bi-lexical dependence and incorporate grammatical constraints into LLM-based unsupervised parsing. We incorporate Part-Of-Speech information as a grammatical constraint at the CMI estimation stage and integrate two additional grammatical constraints at the subsequent tree decoding stage. We find that the CMI score positively correlates with syntactic dependencies and has a stronger correlation with the syntactic dependencies than baseline scores. Our experiment confirms the benefits and applicability of the proposed grammatical constraints across five languages and eight datasets. The CMI parsing model outperforms state-of-the-art LLM-based models and similarly constrained grammar-based models. Our analysis reveals that the CMI model is strong in retrieving dependency relations with rich lexical interactions but is weak in retrieving relations with sparse lexical interactions, indicating a potential limitation in CMI-based unsupervised parsing methods.},
  keywords = {dependency parsing,dependency structures,pmi},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.j2024 Language Model Based Unsupervised Depend.pdf}
}

@inproceedings{chen.r:2021cogsci,
  title = {On Factors Influencing Typing Time: Insights from a Viral Online Typing Game},
  shorttitle = {On Factors Influencing Typing Time},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Chen, Robert and Levy, Roger and Eisape, Tiwalayo},
  year = {2021},
  volume = {43},
  urldate = {2023-09-08},
  abstract = {Context effects in human spoken language are well-documented and play a central role in the theory of language production. However, the role of context in written language production is far less well understood, even though a considerable proportion of the language produced by many people today is written. Here we analyze the factors predictive of English language typing times in a large, naturalistic corpus from the popular TypeRacer.com website. We find broad consistency with the major documented effects of linguistic context on spoken language production, suggesting potential modality-independence in the cognitive mechanisms underlying language production and/or similar optimization pressures on the production systems in both modalities.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.r2021cogsci On factors influencing typing time insi.pdf}
}

@inproceedings{chen.s:1995,
  title = {Bayesian Grammar Induction for Language Modeling},
  booktitle = {Proceedings of the 33rd Annual Meeting on {{Association}} for {{Computational Linguistics}}},
  author = {Chen, Stanley F.},
  year = {1995},
  month = jun,
  series = {{{ACL}} '95},
  pages = {228--235},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/981658.981689},
  urldate = {2022-07-04},
  abstract = {We describe a corpus-based induction algorithm for probabilistic context-free grammars. The algorithm employs a greedy heuristic search within a Bayesian framework, and a post-pass using the Inside-Outside algorithm. We compare the performance of our algorithm to n-gram models and the Inside-Outside algorithm in three language modeling tasks. In two of the tasks, the training data is generated by a probabilistic context-free grammar and in both tasks our algorithm outperforms the other techniques. The third task involves naturally-occurring data, and in this task our algorithm does not perform as well as n-gram models but vastly outperforms the Inside-Outside algorithm.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.s1995 Bayesian grammar induction for language.pdf}
}

@article{chen.s:2023,
  title = {The Effect of Context on Noisy-Channel Sentence Comprehension},
  author = {Chen, Sihan and Nathaniel, Sarah and Ryskin, Rachel and Gibson, Edward},
  year = {2023},
  month = sep,
  journal = {Cognition},
  volume = {238},
  pages = {105503},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2023.105503},
  urldate = {2023-07-28},
  abstract = {The process of sentence comprehension must allow for the possibility of noise in the input, e.g., from speaker error, listener mishearing, or environmental noise. Consequently, semantically implausible sentences such as The girl tossed the apple the boy are often interpreted as a semantically plausible alternative (e.g., The girl tossed the apple to the boy). Previous investigations of noisy-channel comprehension have relied exclusively on paradigms with isolated sentences. Because supportive contexts alter the expectations of possible interpretations, the noisy channel framework predicts that context should encourage more inference in interpreting implausible sentences, relative to null contexts (i.e. a lack of context) or unsupportive contexts. In the present work, we tested this prediction in four types of sentence constructions: two where inference is relatively frequent (double object - prepositional object), and two where inference is rare (active-passive). We found evidence that in the two sentence types that commonly elicit inference, supportive contexts encourage noisy-channel inferences about the intended meaning of implausible sentences more than non-supportive contexts or null contexts. These results suggest that noisy-channel inference may be more pervasive in everyday language processing than previously assumed based on work with isolated sentences.},
  langid = {english},
  keywords = {Context,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.s2023 The effect of context on noisy-channel s.pdf}
}

@inproceedings{chen.x:2016,
  title = {{{InfoGAN}}: {{Interpretable}} Representation Learning by Information Maximizing Generative Adversarial Nets},
  booktitle = {Advances in Neural Information Processing Systems 29: {{Annual}} Conference on Neural Information Processing Systems 2016, December 5-10, 2016, Barcelona, Spain},
  author = {Chen, Xi and Duan, Yan and Houthooft, Rein and Schulman, John and Sutskever, Ilya and Abbeel, Pieter},
  editor = {Lee, Daniel D. and Sugiyama, Masashi and {von Luxburg}, Ulrike and Guyon, Isabelle and Garnett, Roman},
  year = {2016},
  pages = {2172--2180},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChenCDHSSA16.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{chen.y:2005,
  title = {Another Look at Rejection Sampling through Importance Sampling},
  author = {Chen, Yuguo},
  year = {2005},
  month = may,
  journal = {Statistics \& Probability Letters},
  volume = {72},
  number = {4},
  pages = {277--283},
  issn = {0167-7152},
  doi = {10.1016/j.spl.2005.01.002},
  urldate = {2023-01-01},
  abstract = {We show that rejection sampling is inferior to the importance sampling algorithm in terms of the {$\chi$}2 distance between the proposal distribution and the target distribution. Similar conclusions are drawn for comparing rejection control with importance sampling.},
  langid = {english},
  keywords = {distance,Effective sample size,importance sampling,Importance sampling,Rejection control,rejection sampling,Rejection sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chen.y2005 Another look at rejection sampling throu.pdf}
}

@article{cheyette.s:2020,
  title = {A Unified Account of Numerosity Perception},
  author = {Cheyette, Samuel J. and Piantadosi, Steven T.},
  year = {2020},
  month = dec,
  journal = {Nature Human Behaviour},
  volume = {4},
  number = {12},
  pages = {1265--1272},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-020-00946-0},
  urldate = {2022-05-19},
  abstract = {People can identify the number of objects in sets of four or fewer items with near-perfect accuracy but exhibit linearly increasing error for larger sets. Some researchers have taken this discontinuity as evidence of two distinct representational systems. Here, we present a mathematical derivation showing that this behaviour is an optimal representation of cardinalities under a limited informational capacity, indicating that this behaviour can emerge from a single system. Our derivation predicts how the amount of information accessible to viewers should influence the perception of quantity for both large and small sets. In a series of four preregistered experiments (N\,=\,100 each), we varied the amount of information accessible to participants in number estimation. We find tight alignment between the model and human performance for both small and large quantities, implicating efficient representation as the common origin behind key phenomena of human and animal numerical cognition.},
  copyright = {2020 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computational models,Human behaviour,Object vision},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cheyette.s2020 A unified account of numerosity percepti.pdf}
}

@book{chierchia.g:2013,
  title = {Logic in Grammar: {{Polarity}}, Free Choice, and Intervention},
  author = {Chierchia, Gennaro},
  year = {2013},
  publisher = {Oxford},
  date-added = {2019-05-19 22:01:50 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {logic,mathematical linguistics,semantics}
}

@inproceedings{chierchia.g:2019,
  title = {Ultrametric Fitting by Gradient Descent},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Chierchia, Giovanni and Perret, Benjamin},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3175--3186},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ChierchiaP19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@unpublished{chomsky.n:1955,
  type = {Unpublished Manuscript},
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {[1975] 1955},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  keywords = {information theory,syntactic information,syntax}
}

@book{chomsky.n:1957,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {1957},
  publisher = {{Mouton and Co.}},
  address = {The Hague},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@article{chomsky.n:1957a,
  title = {Review of {{A Manual}} of {{Phonology}}},
  author = {Chomsky, Noam},
  year = {1957},
  journal = {International Journal of American Linguistics},
  volume = {23},
  number = {3},
  eprint = {1263617},
  eprinttype = {jstor},
  pages = {223--234},
  publisher = {University of Chicago Press},
  issn = {0020-7071},
  urldate = {2024-02-25},
  collaborator = {Hockett, Charles F.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chomsky.n1957a Review of A Manual of Phonology.pdf}
}

@book{chomsky.n:1965aspects,
  title = {Aspects of the Theory of Syntax},
  author = {Chomsky, Noam},
  year = {1965},
  publisher = {MIT Press},
  date-added = {2022-04-14 12:41:20 -0400},
  date-modified = {2022-04-14 12:42:21 -0400},
  keywords = {generative grammar,syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chomsky.n1965aspects Aspects of the theory of syntax 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/chomsky.n1965aspects Aspects of the theory of syntax.pdf}
}

@incollection{chomsky.n:1967,
  title = {Recent {{Contributions}} to the {{Theory}} of {{Innate Ideas}}},
  booktitle = {Proceedings of the {{Boston Colloquium}} for the {{Philosophy}} of {{Science}} 1964/1966: {{In Memory}} of {{Norwood Russell Hanson}}},
  author = {Chomsky, Noam},
  editor = {Cohen, Robert S. and Wartofsky, Marx W.},
  year = {1967},
  pages = {81--90},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-010-3508-8_4},
  urldate = {2024-04-12},
  abstract = {I think that it will be useful to separate two issues in the discussion of our present topic --- one is the issue of historical interpretation, namely, what in fact was the content of the classical doctrine of innate ideas, let us say, in Descartes and Leibniz; the second is the substantive issue, namely, in the light of the information presently available, what can we say about the prerequisites for the acquisition of knowledge --- what can we postulate regarding the psychologically a priori principles that determine the character of learning and the nature of what is acquired.},
  isbn = {978-94-010-3508-8},
  langid = {english}
}

@book{chomsky.n:1975,
  title = {The Logical Structure of Linguistic Theory},
  author = {Chomsky, Noam},
  year = {1975},
  publisher = {Springer},
  date-added = {2020-06-05 14:43:06 -0400},
  date-modified = {2020-06-05 14:44:48 -0400},
  project = {syntactic embedding},
  keywords = {syntax}
}

@incollection{chomsky.n:1980reviewSkinner,
  title = {A Review of {{B}}. {{F}}. {{Skinner}}'s Verbal Behavior},
  booktitle = {Readings in Philosophy of Psychology},
  author = {Chomsky, Noam},
  editor = {Block, Ned},
  year = {1980},
  month = jan,
  series = {The Language and Thought Series},
  volume = {1},
  pages = {48--66},
  publisher = {Harvard University Press},
  address = {Cambridge, MA and London, England},
  doi = {10.4159/harvard.9780674594623.c6},
  urldate = {2024-05-26},
  isbn = {978-0-674-59462-3}
}

@book{chomsky.n:1995,
  title = {The Minimalist Program},
  author = {Chomsky, Noam},
  year = {1995},
  publisher = {MIT Press},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@incollection{chomsky.n:1995a,
  title = {Bare Phrase Structure},
  booktitle = {Government and Binding Theory and the Minimalist Program},
  author = {Chomsky, Noam},
  editor = {Webelhuth, Gerth},
  year = {1995},
  pages = {383--349},
  publisher = {Blackwell},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  readinglist = {X-bar}
}

@book{chomsky.n:2002ss2e,
  title = {Syntactic Structures},
  author = {Chomsky, Noam},
  year = {2002},
  month = nov,
  edition = {2},
  publisher = {Mouton de Gruyter},
  doi = {10.1515/9783110218329},
  bdsk-url-2 = {https://doi.org/10.1515/9783110218329},
  date-added = {2021-09-24 08:27:37 -0400},
  date-modified = {2021-09-24 08:30:27 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chomsky.n2002ss2e Syntactic structures.pdf}
}

@book{chopin.n:2020,
  title = {An Introduction to Sequential {{Monte Carlo}}},
  author = {Chopin, Nicolas and Papaspiliopoulos, Omiros},
  year = {2020},
  month = oct,
  series = {Springer {{Series}} in {{Statistics}}},
  edition = {1},
  publisher = {Springer},
  address = {Cham, Switzerland},
  doi = {10.1007/978-3-030-47845-2},
  abstract = {Offers a general and gentle introduction to all aspects of particle filtering: the algorithms, their uses in different areas, their computer implementation in Python and the supporting theory Covers both the basics and more advanced, cutting-edge developments, such as PMCMC (particle Markov chain Monte Carlo) and SQMC (Sequential quasi-Monte Carlo) Comes with a freely available Python library (particles), which implements all the algorithms discussed in the book. Each chapter ends with a ``Python corner'' that discusses how the methods covered can be implemented in Python},
  isbn = {978-3-030-47844-5},
  langid = {english},
  keywords = {monte carlo,sampling,statistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/chopin.n2020 An introduction to sequential Monte Carl.pdf}
}

@article{chow.c:1968,
  title = {Approximating Discrete Probability Distributions with Dependence Trees},
  author = {Chow, C and Liu, Cong},
  year = {1968},
  journal = {IEEE transactions on Information Theory},
  volume = {14},
  number = {3},
  pages = {462--467},
  publisher = {IEEE},
  date-added = {2019-10-09 20:57:36 -0400},
  date-modified = {2019-10-09 20:58:15 -0400},
  project = {syntactic embedding},
  keywords = {dependency structures,mutual information}
}

@misc{chowdhery.a:2022PaLM,
  title = {{{PaLM}}: Scaling Language Modeling with Pathways},
  author = {Chowdhery, Aakanksha and Narang, Sharan and Devlin, Jacob and Bosma, Maarten and Mishra, Gaurav and Roberts, Adam and Barham, Paul and Chung, Hyung Won and Sutton, Charles and Gehrmann, Sebastian and Schuh, Parker and Shi, Kensen and Tsvyashchenko, Sasha and Maynez, Joshua and Rao, Abhishek and Barnes, Parker and Tay, Yi and Shazeer, Noam and Prabhakaran, Vinodkumar and Reif, Emily and Du, Nan and Hutchinson, Ben and Pope, Reiner and Bradbury, James and Austin, Jacob and Isard, Michael and {Gur-Ari}, Guy and Yin, Pengcheng and Duke, Toju and Levskaya, Anselm and Ghemawat, Sanjay and Dev, Sunipa and Michalewski, Henryk and Garcia, Xavier and Misra, Vedant and Robinson, Kevin and Fedus, Liam and Zhou, Denny and Ippolito, Daphne and Luan, David and Lim, Hyeontaek and Zoph, Barret and Spiridonov, Alexander and Sepassi, Ryan and Dohan, David and Agrawal, Shivani and Omernick, Mark and Dai, Andrew M. and Pillai, Thanumalayan Sankaranarayana and Pellat, Marie and Lewkowycz, Aitor and Moreira, Erica and Child, Rewon and Polozov, Oleksandr and Lee, Katherine and Zhou, Zongwei and Wang, Xuezhi and Saeta, Brennan and Diaz, Mark and Firat, Orhan and Catasta, Michele and Wei, Jason and {Meier-Hellstern}, Kathy and Eck, Douglas and Dean, Jeff and Petrov, Slav and Fiedel, Noah},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2204.02311},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2204.02311},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-04-19 13:50:14 -0400},
  date-modified = {2022-04-28 12:21:29 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@incollection{christensen.k:2016,
  title = {The Dead Ends of Language: The (Mis)Interpretation of a Grammatical Illusion},
  shorttitle = {The Dead Ends of Language},
  booktitle = {Let Us Have Articles Betwixt Us: {{Papers}} in {{Historical}} and {{Comparative Linguistics}} in {{Honour}} of {{Johanna L}}. {{Wood}}},
  author = {Christensen, Ken Ramsh{\o}j},
  editor = {Vikner, Sten and J{\o}rgensen, Henrik and Gelderen, Elly Van},
  year = {2016},
  pages = {129--159},
  publisher = {Department of English, University of Aarhus},
  doi = {10.7146/aul.119.107},
  urldate = {2023-08-01},
  isbn = {978-87-7507-359-7}
}

@article{christianson.k:2010,
  title = {Effects of Plausibility on Structural Priming},
  author = {Christianson, Kiel and Luke, Steven G. and Ferreira, Fernanda},
  year = {2010},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {36},
  number = {2},
  pages = {538--544},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1285},
  doi = {10.1037/a0018027},
  abstract = {We report a replication and extension of Ferreira (2003), in which it was observed that native adult English speakers misinterpret passive sentences that relate implausible but not impossible semantic relationships (e.g., The angler was caught by the fish) significantly more often than they do plausible passives or plausible or implausible active sentences. In the experiment reported here, participants listened to the same plausible and implausible passive and active sentences as in Ferreira (2003), answered comprehension questions, and then orally described line drawings of simple transitive actions. The descriptions were analyzed as a measure of structural priming (Bock, 1986). Question accuracy data replicated Ferreira (2003). Production data yielded an interaction: Passive descriptions were produced more often after plausible passives and implausible actives. We interpret these results as indicative of a language processor that proceeds along differentiated morphosyntactic and semantic routes. The processor may end up adjudicating between conflicting outputs from these routes by settling on a ``good enough'' representation that is not completely faithful to the input. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Comprehension,good enough processing,Language,plausibility,Priming,Semantics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/christianson.k2010 Effects of plausibility on structural pr.pdf}
}

@article{christianson.k:2016,
  title = {When Language Comprehension Goes Wrong for the Right Reasons: {{Good-enough}}, Underspecified, or Shallow Language Processing},
  shorttitle = {When Language Comprehension Goes Wrong for the Right Reasons},
  author = {Christianson, Kiel},
  year = {2016},
  month = may,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {5},
  pages = {817--828},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1080/17470218.2015.1134603},
  urldate = {2023-08-01},
  abstract = {This paper contains an overview of language processing that can be described as ``good enough'', ``underspecified'', or ``shallow''. The central idea is that a nontrivial proportion of misunderstanding, misinterpretation, and miscommunication can be attributed not to random error, but instead to processing preferences of the human language processing system. In other words, the very architecture of the language processor favours certain types of processing errors because in a majority of instances, this ``fast and frugal'', less effortful processing is good enough to support communication. By way of historical background, connections are made between this relatively recent facet of psycholinguistic study, other recent language processing models, and related concepts in other areas of cognitive science. Finally, the nine papers included in this special issue are introduced as representative of novel explorations of good-enough, or underspecified, language processing.},
  langid = {english}
}

@article{chung.f:1984,
  title = {On Optimal Linear Arrangements of Trees},
  author = {Chung, F.R.K.},
  year = {1984},
  journal = {Computers and Mathematics with Applications},
  volume = {10},
  number = {1},
  pages = {43--60},
  issn = {0898-1221},
  bdsk-url-2 = {https://doi.org/10.1016/0898-1221(84)90085-3},
  date-added = {2019-05-14 23:56:39 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,linearization}
}

@inproceedings{church.k:1990,
  title = {Word Association Norms, Mutual Information, and Lexicography},
  booktitle = {27th Annual Meeting of the Association for Computational Linguistics},
  author = {Church, Kenneth Ward and Hanks, Patrick},
  year = {1989},
  pages = {76--83},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, British Columbia, Canada},
  doi = {10.3115/981623.981633},
  bdsk-url-2 = {https://doi.org/10.3115/981623.981633}
}

@inproceedings{clark.c:2019,
  title = {Don't Take the Easy Way out: {{Ensemble}} Based Methods for Avoiding Known Dataset Biases},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Clark, Christopher and Yatskar, Mark and Zettlemoyer, Luke},
  year = {2019},
  pages = {4069--4082},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1418},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1418}
}

@inproceedings{clark.k:2019,
  title = {What Does {{BERT}} Look at? {{An}} Analysis of {{BERT}}'s Attention},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
  year = {2019},
  pages = {276--286},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4828},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4828}
}

@article{clark.t:2022,
  title = {Evidence for Availability Effects on Speaker Choice in the {{Russian}} Comparative Alternation},
  author = {Clark, Thomas and Wilcox, Ethan Gotlieb and Gibson, Edward and Levy, Roger},
  year = {2022},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {44},
  urldate = {2022-10-29},
  abstract = {When a language offers multiple options for expressing the same meaning, what principles govern a speaker's choice? Two well-known principles proposed for explaining wide-ranging speaker preference are Uniform Information Density and Availability-Based Production. Here we test the predictions of these theories in a previously uninvestigated case of speaker choice. Russian has two ways of expressing the comparative: an {\textbackslash}textsc\{explicit\} option ({\textbackslash}textit\{Ona bystree chem ja\}/She fast\{{\textbackslash}sc-comp\} than me\{{\textbackslash}sc-nom\}) and a {\textbackslash}textsc\{genitive\} option ({\textbackslash}textit\{Ona bystree menya/She fast\{{\textbackslash}sc-comp\} me\{{\textbackslash}sc-gen\}\}). We lay out several potential predictions of each theory for speaker choice in the Russian comparative construction, including effects of post-comparative word predictability, phrase length, syntactic complexity, and semantic association between the comparative adjective and subsequent noun. In a corpus study, we find that the explicit construction is used preferentially when the post-comparative noun phrase is longer, has a relative clause, and is less semantically associated with the comparative adjective. A follow-up production experiment using visual scene stimuli to elicit comparative sentences replicates the corpus finding that Russian native speakers prefer the explicit form when post-comparative phrases are longer. These findings offer no clear support for the predictions of Uniform Information Density, but are broadly supportive of Availability-Based Production, with the explicit option serving as an unreduced form that eases speakers' planning of complex or low-availability utterances. Code for this study is available at https://github.mit.edu/thclark/russian\_uid},
  langid = {english},
  keywords = {uniform information density}
}

@article{clark.t:2023,
  title = {A Cross-Linguistic Pressure for Uniform Information Density in Word Order},
  author = {Clark, Thomas Hikaru and Meister, Clara and Pimentel, Tiago and Hahn, Michael and Cotterell, Ryan and Futrell, Richard and Levy, Roger},
  year = {2023},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1048--1065},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00589},
  urldate = {2024-05-08},
  abstract = {While natural languages differ widely in both canonical word order and word order flexibility, their word orders still follow shared cross-linguistic statistical patterns, often attributed to functional pressures. In the effort to identify these pressures, prior work has compared real and counterfactual word orders. Yet one functional pressure has been overlooked in such investigations: The uniform information density (UID) hypothesis, which holds that information should be spread evenly throughout an utterance. Here, we ask whether a pressure for UID may have influenced word order patterns cross-linguistically. To this end, we use computational models to test whether real orders lead to greater information uniformity than counterfactual orders. In our empirical study of 10 typologically diverse languages, we find that: (i) among SVO languages, real word orders consistently have greater uniformity than reverse word orders, and (ii) only linguistically implausible counterfactual orders consistently exceed the uniformity of real orders. These findings are compatible with a pressure for information uniformity in the development and usage of natural languages.1},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/clark.t2023 A cross-linguistic pressure for uniform.pdf}
}

@article{clayards.m:2008,
  title = {Perception of Speech Reflects Optimal Use of Probabilistic Speech Cues},
  author = {Clayards, Meghan and Tanenhaus, Michael K. and Aslin, Richard N. and Jacobs, Robert A.},
  year = {2008},
  month = sep,
  journal = {Cognition},
  volume = {108},
  number = {3},
  pages = {804--809},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.04.004},
  urldate = {2024-05-26},
  abstract = {Listeners are exquisitely sensitive to fine-grained acoustic detail within phonetic categories for sounds and words. Here we show that this sensitivity is optimal given the probabilistic nature of speech cues. We manipulated the probability distribution of one probabilistic cue, voice onset time (VOT), which differentiates word initial labial stops in English (e.g., ``beach'' and ``peach''). Participants categorized words from distributions of VOT with wide or narrow variances. Uncertainty about word identity was measured by four-alternative forced-choice judgments and by the probability of looks to pictures. Both measures closely reflected the posterior probability of the word given the likelihood distributions of VOT, suggesting that listeners are sensitive to these distributions.},
  keywords = {Categorization,Ideal observer model,Speech perception,Word recognition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/clayards.m2008 Perception of speech reflects optimal us.pdf}
}

@misc{coecke.b:2010,
  title = {Mathematical Foundations for a Compositional Distributional Model of Meaning},
  author = {Coecke, Bob and Sadrzadeh, Mehrnoosh and Clark, Stephen},
  year = {2010},
  eprint = {1003.4394},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2019-08-06 08:49:05 +0300},
  date-modified = {2019-08-06 08:50:43 +0300},
  project = {syntactic embedding},
  keywords = {compositionality,distributional models}
}

@inproceedings{coenen.a:2019,
  title = {Visualizing and Measuring the Geometry of {{BERT}}},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Reif, Emily and Yuan, Ann and Wattenberg, Martin and Vi{\'e}gas, Fernanda B. and Coenen, Andy and Pearce, Adam and Kim, Been},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {8592--8600},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ReifYWVCPK19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{cohen.j:1994,
  title = {The Earth Is Round ({{p$<$.05}}).},
  author = {Cohen, Jacob},
  year = {1994},
  journal = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0003-066x.49.12.997},
  bdsk-url-2 = {https://doi.org/10.1037/0003-066x.49.12.997},
  date-added = {2021-08-08 11:16:17 -0400},
  date-modified = {2021-08-08 20:26:36 -0400}
}

@article{cohen.l:1981,
  title = {Can Human Irrationality Be Experimentally Demonstrated?},
  author = {Cohen, L. Jonathan},
  year = {1981},
  month = sep,
  journal = {Behavioral and Brain Sciences},
  volume = {4},
  number = {3},
  pages = {317--331},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X00009092},
  urldate = {2024-05-13},
  abstract = {The object of this paper is to show why recent research in the psychology of deductive and probabilistic reasoning does not have "bleak implications for human rationality," as has sometimes been supposed. The presence of fallacies in reasoning is evaluated by referring to normative criteria which ultimately derive their own credentials from a systematisation of the intuitions that agree with them. These normative criteria cannot be taken, as some have suggested, to constitute a part of natural science, nor can they be established by metamathematical proof. Since a theory of competence has to predict the very same intuitions, it must ascribe rationality to ordinary people.Accordingly, psychological research on this topic falls into four categories. In the first, experimenters investigate conditions under which their subjects suffer from genuine cognitive illusions. The search for explanations of such performance errors may then generate hypotheses about the ways in which the relevant information-processing mechanisms operate. In the second category, experimenters investigate circumstances in which their subjects exhibit mathematical or scientific ignorance: these are tests of the subjects' intelligence or education. In the third and fourth categories, experimenters impute a fallacy where none exists, either because they are applying the relevant normative criteria in an inappropriate way or because the normative criteria being applied are not the appropriate ones.},
  langid = {english},
  keywords = {competence,deduction,fallacy,intuition,probability judgments,rationality,reasoning}
}

@book{cohen.p:2014,
  title = {Applied Multiple Regression/Correlation Analysis for the Behavioral Sciences},
  author = {Cohen, Patricia and Cohen, Patricia and West, Stephen G. and Aiken, Leona S.},
  year = {2014},
  month = apr,
  edition = {2},
  publisher = {Psychology Press},
  address = {New York},
  doi = {10.4324/9781410606266},
  abstract = {This classic text on multiple regression is noted for its nonmathematical, applied, and data-analytic approach. Readers profit from its verbal-conceptual},
  isbn = {978-1-4106-0626-6}
}

@phdthesis{collins.m:1999phd,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael John},
  year = {1999},
  address = {Philadelphia, PA, USA},
  urldate = {2022-10-16},
  abstract = {Statistical models for parsing natural language have recently shown considerable success in broad-coverage domains. Ambiguity often leads to an input sentence having many possible parse trees; statistical approaches assign a probability to each tree, thereby ranking competing trees in order of plausibility. The probability for each candidate tree is calculated as a product of terms, each term corresponding to some sub-structure within the tree. The choice of parameterization is the choice of how to break down the tree. There are two critical questions regarding the parameterization of the problem: (1) What linguistic objects (e.g., context-free rules, parse moves) should the model's parameters be associated with? I.e., How should trees be decomposed into smaller fragments? (2) How can this choice be instantiated in a sound probabilistic model? This thesis argues that the locality of a lexical head's influence in a tree should motivate modeling choices in the parsing problem. In the final parsing models a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then follow naturally, leading to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The goals of the work are two-fold. First, we aim to advance the state of the art. We report tests on Wall Street Journal text showing that the models give improved accuracy over other methods in the literature. The models recover richer representations than previous approaches, adding the complement/adjunct distinction and information regarding wh-movement. Second, we aim to increase understanding of statistical parsing models. Each parameter type is motivated through tree examples where it provides discriminative information. An empirical study of prepositional phrase attachment ambiguity is used to investigate the effectiveness of dependency parameters for ambiguity resolution. A number of parsing models are tested, and we give a breakdown of their performance on different types of construction. Finally, we give a detailed comparison of the models to others in the literature.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780599258747},
  langid = {english},
  school = {University of Pennsylvania},
  keywords = {Applied sciences,Head-driven,Natural language,Parameterization,Parsing,Statistical models},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/collins.m1999phd Head-driven statistical models for natur.pdf}
}

@article{collins.m:2003,
  title = {Head-Driven Statistical Models for Natural Language Parsing},
  author = {Collins, Michael},
  year = {2003},
  month = dec,
  journal = {Computational Linguistics},
  volume = {29},
  number = {4},
  pages = {589--637},
  issn = {0891-2017},
  doi = {10.1162/089120103322753356},
  urldate = {2022-10-16},
  abstract = {This article describes three statistical models for natural language parsing. The models extend methods from probabilistic context-free grammars to lexicalized grammars, leading to approaches in which a parse tree is represented as the sequence of decisions corresponding to a head-centered, top-down derivation of the tree. Independence assumptions then lead to parameters that encode the X-bar schema, subcategorization, ordering of complements, placement of adjuncts, bigram lexical dependencies, wh-movement, and preferences for close attachment. All of these preferences are expressed by probabilities conditioned on lexical heads. The models are evaluated on the Penn Wall Street Journal Treebank, showing that their accuracy is competitive with other models in the literature. To gain a better understanding of the models, we also give results on different constituent types, as well as a breakdown of precision/recall results in recovering various types of dependencies. We analyze various characteristics of the models through experiments on parsing accuracy, by collecting frequencies of various structures in the treebank, and through linguistically motivated examples. Finally, we compare the models to others that have been applied to parsing the treebank, aiming to give some explanation of the difference in performance of the various models.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/collins.m2003 Head-driven statistical models for natur.pdf}
}

@inproceedings{collins.m:2004,
  title = {Incremental Parsing with the Perceptron Algorithm},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Association}} for {{Computational Linguistics}}},
  author = {Collins, Michael and Roark, Brian},
  year = {2004},
  pages = {111--118},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1218970},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218970}
}

@article{collins.m:2013,
  title = {Information Density and Dependency Length as Complementary Cognitive Models},
  author = {Collins, Michael Xavier},
  year = {2013},
  month = sep,
  volume = {43},
  number = {5},
  pages = {651--681},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10936-013-9273-3},
  bdsk-url-2 = {https://doi.org/10.1007/s10936-013-9273-3},
  date-added = {2021-10-18 22:13:42 -0400},
  date-modified = {2021-10-18 22:13:43 -0400}
}

@inproceedings{conneau.a:2018,
  title = {What You Can Cram into a Single \$\&!\#* Vector: {{Probing}} Sentence Embeddings for Linguistic Properties},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Conneau, Alexis and Kruszewski, German and Lample, Guillaume and Barrault, Lo{\"i}c and Baroni, Marco},
  year = {2018},
  pages = {2126--2136},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1198},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1198}
}

@misc{coon.j:2018,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2018},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-02-26 09:53:56 -0500},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@misc{coon.j:2019,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {2019},
  date-added = {2020-06-16 10:17:01 -0400},
  date-modified = {2020-06-16 10:17:01 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,phi features,quirky case}
}

@article{coon.j:2020,
  title = {Feature Gluttony},
  author = {Coon, Jessica and Keine, Stefan},
  year = {to appear},
  journal = {Linguistic Inquiry},
  doi = {10.1162/ling_a_00386},
  date-added = {2020-02-03 15:20:36 -0500},
  date-modified = {2020-06-16 10:21:03 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,hierarchy effects,person case constraint,phi features,quirky case}
}

@article{costa.f:2003,
  title = {Towards Incremental Parsing of Natural Language Using Recursive Neural Networks},
  author = {Costa, F.},
  year = {2003},
  journal = {Applied Intelligence},
  volume = {19},
  number = {1/2},
  pages = {9--25},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1023860521975},
  bdsk-url-2 = {https://doi.org/10.1023/a:1023860521975},
  date-added = {2021-06-22 15:15:28 -0400},
  date-modified = {2021-06-22 15:16:11 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/costa.f2003 Towards incremental parsing of natural l.pdf}
}

@book{cover.t:2006,
  title = {Elements of Information Theory},
  author = {Cover, Thomas M. and Thomas, Joy A.},
  year = {2006},
  edition = {2},
  publisher = {Wiley},
  doi = {10.1002/047174882x},
  bdsk-url-2 = {https://doi.org/10.1002/047174882x},
  date-added = {2022-04-28 11:28:46 -0400},
  date-modified = {2022-04-28 11:29:11 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cover.t2006 Elements of information theory.pdf}
}

@inproceedings{cramer.b:2007,
  title = {Limitations of Current Grammar Induction Algorithms},
  booktitle = {Proceedings of the {{ACL}} 2007 Student Research Workshop},
  author = {Cramer, Bart},
  year = {2007},
  pages = {43--48},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@article{crameri.f:2020,
  title = {The Misuse of Colour in Science Communication},
  author = {Crameri, Fabio and Shephard, Grace E. and Heron, Philip J.},
  year = {2020},
  month = oct,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {5444},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-19160-7},
  urldate = {2022-10-12},
  abstract = {The accurate representation of data is essential in science communication. However, colour maps that visually distort data through uneven colour gradients or are unreadable to those with colour-vision deficiency remain prevalent in science. These include, but are not limited to, rainbow-like and red--green colour maps. Here, we present a simple guide for the scientific use of colour. We show how scientifically derived colour maps report true data variations, reduce complexity, and are accessible for people with colour-vision deficiencies. We highlight ways for the scientific community to identify and prevent the misuse of colour in science, and call for a proactive step away from colour misuse among the community, publishers, and the press.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Scientific community,Software}
}

@article{crocker.m:2000,
  title = {Wide-Coverage Probabilistic Sentence Processing},
  author = {Crocker, Matthew W. and Brants, Thorsten},
  year = {2000},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {29},
  number = {6},
  pages = {647--669},
  issn = {1573-6555},
  doi = {10.1023/A:1026560822390},
  urldate = {2022-10-13},
  abstract = {This paper describes a fully implemented, broad-coverage model of human syntactic processing. The model uses probabilistic parsing techniques, which combine phrase structure, lexical category, and limited subcategory probabilities with an incremental, left-to-right ``pruning'' mechanism based on cascaded Markov models. The parameters of the system are established through a uniform training algorithm, which determines maximum-likelihood estimates from a parsed corpus. The probabilistic parsing mechanism enables the system to achieve good accuracy on typical, ``garden-variety'' language (i.e., when tested on corpora). Furthermore, the incremental probabilistic ranking of the preferred analyses during parsing also naturally explains observed human behavior for a range of garden-path structures. We do not make strong psychological claims about the specific probabilistic mechanism discussed here, which is limited by a number of practical considerations. Rather, we argue incremental probabilistic parsing models are, in general, extremely well suited to explaining this dual nature---generally good and occasionally pathological---of human linguistic performance.},
  langid = {english},
  keywords = {frequency,Markov models,probabilistic parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/crocker.m2000 Wide-coverage probabilistic sentence pro.pdf}
}

@techreport{cronbach.l:1953,
  type = {Technical Report},
  title = {A Consideration of Information Theory and Utility Theory as Tools for Psychometric Problems},
  author = {Cronbach, Lee J.},
  year = {1953},
  month = nov,
  number = {1, contract N60ri-07146, Office of Naval Research},
  institution = {University of Illinois},
  urldate = {2024-05-14}
}

@book{crooks.g:2019,
  title = {Field {{Guide}} to {{Continuous Probability Distributions}}},
  author = {Crooks, Gavin E.},
  year = {2019},
  edition = {1.0.0},
  publisher = {Berkeley Institute for Theoretical Science},
  urldate = {2022-06-22},
  abstract = {Over 170 continuous univariate probability distributions (and at least as many synonyms) organized into 20 families.},
  isbn = {978-1-73393-810-5},
  langid = {english}
}

@incollection{culicover.p:1999,
  title = {Syntactic Nuts: {{Hard}} Cases in Syntax},
  booktitle = {Syntactic Nuts: {{Hard}} Cases in Syntax.},
  author = {Culicover, Peter},
  year = {1999},
  series = {Foundations of Syntax},
  publisher = {Oxford University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{culicover.p:2005,
  title = {Simpler Syntax},
  booktitle = {Simpler Syntax},
  author = {Culicover, Peter and Jackendoff, Ray},
  year = {2005},
  publisher = {Oxford University Press},
  address = {Oxford},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:43 -0400}
}

@article{cuskley.c:2024,
  title = {The {{Limitations}} of {{Large Language Models}} for {{Understanding Human Language}} and {{Cognition}}},
  author = {Cuskley, Christine and Woods, Rebecca and Flaherty, Molly},
  year = {2024},
  month = aug,
  journal = {Open Mind},
  volume = {8},
  pages = {1058--1083},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00160},
  urldate = {2024-08-31},
  abstract = {Researchers have recently argued that the capabilities of Large Language Models (LLMs) can provide new insights into longstanding debates about the role of learning and/or innateness in the development and evolution of human language. Here, we argue on two grounds that LLMs alone tell us very little about human language and cognition in terms of acquisition and evolution. First, any similarities between human language and the output of LLMs are purely functional. Borrowing the ``four questions'' framework from ethology, we argue that what LLMs do is superficially similar, but how they do it is not. In contrast to the rich multimodal data humans leverage in interactive language learning, LLMs rely on immersive exposure to vastly greater quantities of unimodal text data, with recent multimodal efforts built upon mappings between images and text. Second, turning to functional similarities between human language and LLM output, we show that human linguistic behavior is much broader. LLMs were designed to imitate the very specific behavior of human writing; while they do this impressively, the underlying mechanisms of these models limit their capacities for meaning and naturalistic interaction, and their potential for dealing with the diversity in human language. We conclude by emphasising that LLMs are not theories of language, but tools that may be used to study language, and that can only be effectively applied with specific hypotheses to motivate research.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cuskley.c2024 The Limitations of Large Language Models.pdf}
}

@inproceedings{cusumano-towner.m:2018,
  title = {Incremental Inference for Probabilistic Programs},
  booktitle = {Proceedings of the 39th {{ACM SIGPLAN}} Conference on Programming Language Design and Implementation},
  author = {{Cusumano-Towner}, Marco and Bichsel, Benjamin and Gehr, Timon and Vechev, Martin and Mansinghka, Vikash K.},
  year = {2018},
  series = {{{PLDI}} 2018},
  pages = {571--585},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3192366.3192399},
  abstract = {We present a novel approach for approximate sampling in probabilistic programs based on incremental inference. The key idea is to adapt the samples for a program P into samples for a program Q, thereby avoiding the expensive sampling computation for program Q. To enable incremental inference in probabilistic programming, our work: (i) introduces the concept of a trace translator which adapts samples from P into samples of Q, (ii) phrases this translation approach in the context of sequential Monte Carlo (SMC), which gives theoretical guarantees that the adapted samples converge to the distribution induced by Q, and (iii) shows how to obtain a concrete trace translator by establishing a correspondence between the random choices of the two probabilistic programs. We implemented our approach in two different probabilistic programming systems and showed that, compared to methods that sample the program Q from scratch, incremental inference can lead to orders of magnitude increase in efficiency, depending on how closely related P and Q are.},
  date-added = {2022-05-03 21:06:22 -0400},
  date-modified = {2022-05-03 21:07:50 -0400},
  isbn = {978-1-4503-5698-5},
  keywords = {incremental computation,probabilistic programming,sequential Monte Carlo}
}

@article{cutter.m:2022replication,
  title = {Do Readers Maintain Word-Level Uncertainty during Reading? {{A}} Pre-Registered Replication Study},
  shorttitle = {Do Readers Maintain Word-Level Uncertainty during Reading?},
  author = {Cutter, Michael G. and Filik, Ruth and Paterson, Kevin B.},
  year = {2022},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {125},
  pages = {104336},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2022.104336},
  urldate = {2024-03-06},
  abstract = {We present a replication of Levy, Bicknell, Slattery, and Rayner (2009). In this prior study participants read sentences in which a perceptually confusable preposition (at; confusable with as) or non-confusable preposition (toward) was followed by a verb more likely to appear in the syntactic structure formed by replacing at with as (e.g. tossed) or a verb that was not more likely to appear in this structure (e.g. thrown). Readers experienced processing difficulty upon fixating verbs like tossed following at, but not toward. Levy et al. argued that this suggests readers maintained uncertainty about previously fixated words' identities. We argue that this finding has wide-ranging implications for language processing theories, and that a replication is required. On the basis of a Bayes Factor Design Analysis we conducted a replication study with 56 items and 72 participants in order to determine whether Levy et al.'s effects are replicable. Using Bayesian statistical techniques we show that in our dataset there is evidence against the existence of the interaction Levy et al. found, and thus conclude that this study is non-replicable.},
  keywords = {Eye-movements,Noisy-channel processing,Reading,Sentence processing,Word-level uncertainty},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cutter.m2022replication Do readers maintain word-level uncertain.pdf}
}

@article{cutter.m:2022replicationSPRT,
  title = {No Evidence of Word-Level Uncertainty in Younger and Older Adults in Self-Paced Reading},
  author = {Cutter, Michael G and Paterson, Kevin B and Filik, Ruth},
  year = {2022},
  month = jun,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {75},
  number = {6},
  pages = {1085--1093},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1177/17470218211045987},
  urldate = {2024-03-06},
  abstract = {In a self-paced reading study, we investigated whether older adults maintain a greater level of uncertainty about the identity of words in a sentence than younger adults, potentially due to deficits in visuo-perceptual processing of high-spatial frequencies associated with normal aging. In the experiment, 60 older adults and 60 younger adults read sentences in which an early preposition was either perceptually confusable with another word (at; confusable with as) or not (toward), and in which the reading of a subsequent ambiguous verb (e.g., tossed) should be affected by the confusability of the preposition, while the reading of an unambiguous verb (e.g., thrown) should not be. This design replicated that of an earlier study conducted by Levy et al. (2009) that found evidence in favour of participants maintaining uncertainty about the confusable preposition in go-past times during natural reading. However, in our study, there was no evidence that either younger or older adults maintained uncertainty about the identity of the perceptually confusable preposition, such that there was no interaction between the preposition's form and subsequent verb ambiguity in self-paced reading times, although we did observe a main effect of verb ambiguity. This represents a failure to replicate the effect observed by Levy et al. when using a different experimental paradigm, and we consider potential causes of our findings at both a methodological and theoretical level.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cutter.m2022replicationSPRT No evidence of word-level uncertainty in 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/cutter.m2022replicationSPRT No evidence of word-level uncertainty in.pdf}
}

@article{cutter.m:2024,
  title = {Eye-Movements during Reading and Noisy-Channel Inference Making},
  author = {Cutter, Michael G. and Paterson, Kevin B. and Filik, Ruth},
  year = {2024},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {137},
  pages = {104513},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104513},
  urldate = {2024-03-05},
  abstract = {This novel experiment investigates the relationship between readers' eye movements and their use of ``noisy channel'' inferences when reading implausible sentences, and how this might be affected by cognitive aging. Young (18--26~years) and older (65--87~years) adult participants read sentences which were either plausible or implausible. Crucially, readers could assign a plausible interpretation to the implausible sentences by inferring that a preposition (i.e., to) had been unintentionally omitted or included. Our results reveal that readers' fixation locations within such sentences are associated with the likelihood of them inferring the presence or absence of this critical preposition to reach a plausible interpretation. Moreover, our older adults were more likely to make these noisy-channel inferences than the younger adults, potentially because their poorer visual processing and greater linguistic experience promote such inference-making. We propose that the present findings provide novel experimental evidence for a perceptual contribution to noisy-channel inference-making during reading.},
  keywords = {Eye-movements during reading,Noisy-channel language processing,Oculomotor control,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/cutter.m2024 Eye-movements during reading and noisy-c.pdf}
}

@inproceedings{dai.z:2019,
  title = {Transformer-{{XL}}: Attentive Language Models beyond a Fixed-Length Context},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Dai, Zihang and Yang, Zhilin and Yang, Yiming and Carbonell, Jaime and Le, Quoc and Salakhutdinov, Ruslan},
  year = {2019},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/p19-1285},
  bdsk-url-2 = {https://doi.org/10.18653/v1/p19-1285},
  date-added = {2021-11-30 13:59:20 -0500},
  date-modified = {2021-11-30 13:59:33 -0500}
}

@misc{damani.m:2024arxiv,
  title = {Learning {{How Hard}} to {{Think}}: {{Input-Adaptive Allocation}} of {{LM Computation}}},
  shorttitle = {Learning {{How Hard}} to {{Think}}},
  author = {Damani, Mehul and Shenfeld, Idan and Peng, Andi and Bobu, Andreea and Andreas, Jacob},
  year = {2024},
  month = oct,
  number = {arXiv:2410.04707},
  eprint = {2410.04707},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2410.04707},
  urldate = {2024-10-17},
  abstract = {Computationally intensive decoding procedures--including search, reranking, and self-critique--can improve the quality of language model (LM) outputs in problems spanning code generation, numerical reasoning, and dialog. Existing work typically applies the same decoding procedure for every input to an LM. But not all inputs require the same amount of computation to process. Can we allocate decoding computation adaptively, using more resources to answer questions whose answers will be harder to compute? We present an approach that predicts the distribution of rewards given an input and computation budget, then allocates additional computation to inputs for which it is predicted to be most useful. We apply this approach in two decoding procedures: first, an adaptive best-of-k procedure that dynamically selects the number of samples to generate as input to a reranker; second, a routing procedure that dynamically responds to a query using a decoding procedure that is expensive but accurate, or one that is cheaper but less capable. Across a suite of programming, mathematics, and dialog tasks, we show that accurate computation-allocation procedures can be learned, and reduce computation by up to 50\% at no cost to response quality, or improve quality by up to 10\% at a fixed computational budget.},
  archiveprefix = {arXiv},
  keywords = {adaptive complexity,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/damani.m2024arxiv Learning How Hard to Think Input-Adapti.pdf}
}

@article{danon.g:2006,
  title = {Caseless Nominals and the Projection of {{DP}}},
  author = {Danon, Gabi},
  year = {2006},
  journal = {Natural Language \& Linguistic Theory},
  volume = {24},
  number = {4},
  pages = {977},
  issn = {1573-0859},
  doi = {10.1007/s11049-006-9005-6},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:43:43 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case,subject positions}
}

@article{danon.g:2011,
  title = {Agreement and {{DP-Internal}} Feature Distribution},
  author = {Danon, Gabi},
  year = {2011},
  journal = {Syntax (Oxford, England)},
  volume = {14},
  number = {4},
  pages = {297--317},
  doi = {10.1111/j.1467-9612.2011.00154.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9612.2011.00154.x},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-06-16 10:45:11 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,subject positions}
}

@inproceedings{darwin.o:2022,
  title = {On the {{Sequential Probability Ratio Test}} in {{Hidden Markov Models}}},
  booktitle = {33rd {{International Conference}} on {{Concurrency Theory}} ({{CONCUR}} 2022)},
  author = {Darwin, Oscar and Kiefer, Stefan},
  editor = {Klin, Bartek and Lasota, S{\l}awomir and Muscholl, Anca},
  year = {2022},
  series = {Leibniz {{International Proceedings}} in {{Informatics}} ({{LIPIcs}})},
  volume = {243},
  pages = {9:1--9:16},
  publisher = {Schloss Dagstuhl -- Leibniz-Zentrum f{\"u}r Informatik},
  address = {Dagstuhl, Germany},
  issn = {1868-8969},
  doi = {10.4230/LIPIcs.CONCUR.2022.9},
  urldate = {2023-11-07},
  isbn = {978-3-95977-246-4},
  keywords = {hidden Markov models,Markov chains,probabilistic systems,verification},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/darwin.o2022 On the Sequential Probability Ratio Test.pdf}
}

@inproceedings{dasgupta.s:2016,
  title = {A Cost Function for Similarity-Based Hierarchical Clustering},
  booktitle = {Proceedings of the 48th Annual {{ACM SIGACT}} Symposium on Theory of Computing, {{STOC}} 2016, Cambridge, {{MA}}, {{USA}}, June 18-21, 2016},
  author = {Dasgupta, Sanjoy},
  editor = {Wichs, Daniel and Mansour, Yishay},
  year = {2016},
  pages = {118--127},
  publisher = {ACM},
  doi = {10.1145/2897518.2897527},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/stoc/Dasgupta16.bib},
  timestamp = {Tue, 06 Nov 2018 00:00:00 +0100}
}

@inproceedings{dathathri.s:2020,
  title = {Plug and Play Language Models: {{A}} Simple Approach to Controlled Text Generation},
  shorttitle = {Plug and Play Language Models},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Dathathri, Sumanth and Madotto, Andrea and Lan, Janice and Hung, Jane and Frank, Eric and Molino, Piero and Yosinski, Jason and Liu, Rosanne},
  year = {2020},
  month = apr,
  urldate = {2022-07-11},
  abstract = {We control the topic and sentiment of text generation (almost) without any training.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dathathri.s2020 Plug and play language models A simple.pdf}
}

@misc{davies.m:2008COCA,
  title = {The {{Corpus}} of {{Contemporary American English}} ({{COCA}})},
  author = {Davies, Mark},
  year = {2008}
}

@misc{davies.m:2008COCAwordFreq,
  title = {Word Frequency Data from the {{Corpus}} of {{Contemporary American English}} ({{COCA}})},
  author = {Davies, Mark},
  year = {2008},
  urldate = {2023-11-22}
}

@article{dawid.a:2004,
  title = {Probability, Causality and the Empirical World: {{A Bayes}}--de {{Finetti}}--{{Popper}}--{{Borel}} Synthesis},
  author = {Dawid, A. P.},
  year = {2004},
  month = feb,
  journal = {Statistical Science},
  volume = {19},
  number = {1},
  publisher = {Institute of Mathematical Statistics},
  doi = {10.1214/088342304000000125},
  bdsk-url-2 = {https://doi.org/10.1214/088342304000000125},
  date-added = {2022-04-13 19:51:33 -0400},
  date-modified = {2022-04-13 19:51:34 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dawid.a2004 Probability, causality and the empirical.pdf}
}

@article{dayan.p:1995,
  title = {The {{Helmholtz}} Machine},
  author = {Dayan, Peter and Hinton, Geoffrey E. and Neal, Radford M. and Zemel, Richard S.},
  year = {1995},
  month = sep,
  journal = {Neural Computation},
  volume = {7},
  number = {5},
  pages = {889--904},
  issn = {0899-7667},
  doi = {10.1162/neco.1995.7.5.889},
  urldate = {2022-11-30},
  abstract = {Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterized stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns. We describe a way of finessing this combinatorial explosion by maximizing an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dayan.p1995 The Helmholtz machine.pdf}
}

@inproceedings{deal.a:2015,
  title = {Interaction and Satisfaction in {{$\varphi$}}-Agreement},
  booktitle = {Proceedings of {{NELS}}},
  author = {Deal, Amy Rose},
  year = {2015},
  volume = {45},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:25 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@article{decaire.r:2017,
  title = {On Optionality in {{Mohawk}} Noun Incorporation},
  author = {DeCaire, Ryan and Johns, Alana and Ku{\v c}erov{\'a}, Ivona},
  year = {2017},
  month = dec,
  journal = {Toronto Working Papers in Linguistics},
  volume = {39},
  issn = {1718-3510},
  urldate = {2022-05-30},
  abstract = {Noun incorporation is a phenomenon much discussed within Iroquoian language literature. In this paper, we consider noun incorporation in Mohawk, a language within the Iroquoian language family, and argue that what has often been considered to be optional noun incorporation is in fact primarily determined by the information structure of the clause. We show that with the exception of lexically-determined verbs that always or never incorporate, every verb may or may not incorporate its nominal object. We analyse the incorporated version as the default structure. The non-incorporated counterpart is licensed only under particular information-structure properties. We provide evidence that noun excorporation arises whenever the verb or the object noun is focused, and in turn moves to the left periphery.},
  copyright = {Copyright (c) 2017 Ryan DeCaire, Alana Johns, Ivona Ku{\v c}erov{\'a}},
  langid = {english},
  keywords = {excorporation,iroquoian,Mohawk,noun incorporation},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/decaire.r2017 On optionality in Mohawk noun incorporat.pdf}
}

@book{definetti.b:1972,
  title = {Probability, Induction and Statistics: The Art of Guessing},
  shorttitle = {Probability, Induction and Statistics},
  author = {{de Finetti}, Bruno},
  year = {1972},
  series = {Wiley Series in Probability and Mathematical Statistics},
  publisher = {Wiley},
  address = {London New York},
  isbn = {978-0-471-20140-3},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/definetti.b1972 Probability, induction and statistics t.djvu}
}

@book{definetti.b:1975trans,
  title = {Theory of Probability: A Critical Introductory Treatment},
  shorttitle = {Theory of Probability},
  author = {{de Finetti}, Bruno},
  editor = {Mach{\'i}, Antonio and Smith, Adrian},
  year = {2017},
  month = feb,
  series = {Wiley {{Series}} in {{Probability}} and {{Statistics}}},
  edition = {1},
  publisher = {Wiley},
  doi = {10.1002/9781119286387},
  urldate = {2024-05-13},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  isbn = {978-1-119-28637-0 978-1-119-28638-7},
  langid = {english}
}

@article{degen.j:2023,
  title = {The Rational Speech Act Framework},
  author = {Degen, Judith},
  year = {2023},
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {9},
  number = {Volume 9, 2023},
  pages = {519--540},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-031220-010811},
  urldate = {2024-05-15},
  abstract = {The past decade has seen the rapid development of a new approach to pragmatics that attempts to integrate insights from formal and experimental semantics and pragmatics, psycholinguistics, and computational cognitive science in the study of meaning: probabilistic pragmatics. The most influential probabilistic approach to pragmatics is the Rational Speech Act (RSA) framework. In this review, I demonstrate the basic mechanics and commitments of RSA as well as some of its standard extensions, highlighting the key features that have led to its success in accounting for a wide variety of pragmatic phenomena. Fundamentally, it treats language as probabilistic, informativeness as gradient, alternatives as context-dependent, and subjective prior beliefs (world knowledge) as a crucial facet of interpretation. It also provides an integrated account of the link between production and interpretation. I highlight key challenges for RSA, which include scalability, the treatment of the boundedness of cognition, and the incremental and compositional nature of language.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/degen.j2023 The rational speech act framework.pdf}
}

@inproceedings{dehaene.d:2019,
  title = {Iterative Energy-Based Projection on a Normal Data Manifold for Anomaly Localization},
  booktitle = {International {{Conference}} on {{Learning Representations}}, 2020},
  author = {Dehaene, David and Frigo, Oriel and Combrexelle, S{\'e}bastien and Eline, Pierre},
  year = {2019},
  month = sep,
  urldate = {2023-08-09},
  abstract = {Autoencoder reconstructions are widely used for the task of unsupervised anomaly localization. Indeed, an autoencoder trained on normal data is expected to only be able to reconstruct normal features of the data, allowing the segmentation of anomalous pixels in an image via a simple comparison between the image and its autoencoder reconstruction. In practice however, local defects added to a normal image can deteriorate the whole reconstruction, making this segmentation challenging. To tackle the issue, we propose in this paper a new approach for projecting anomalous data on a autoencoder-learned normal data manifold, by using gradient descent on an energy derived from the autoencoder's loss function. This energy can be augmented with regularization terms that model priors on what constitutes the user-defined optimal projection. By iteratively updating the input of the autoencoder, we bypass the loss of high-frequency information caused by the autoencoder bottleneck. This allows to produce images of higher quality than classic reconstructions. Our method achieves state-of-the-art results on various anomaly localization datasets. It also shows promising results at an inpainting task on the CelebA dataset.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dehaene.d2019 Iterative energy-based projection on a n.pdf}
}

@article{del-moral.p:2006,
  title = {Sequential {{Monte Carlo}} Samplers},
  author = {Del Moral, Pierre and Doucet, Arnaud and Jasra, Ajay},
  year = {2006},
  month = jun,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {68},
  number = {3},
  pages = {411--436},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9868.2006.00553.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2006.00553.x},
  date-added = {2022-04-30 17:47:24 -0400},
  date-modified = {2022-04-30 22:51:54 -0400},
  keywords = {particle filtering,sampling,sequential monte carlo},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/del-moral.p2006 Sequential Monte Carlo samplers 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/del-moral.p2006 Sequential Monte Carlo samplers.pdf}
}

@misc{deletang.g:2022arxiv,
  title = {Neural Networks and the {{Chomsky}} Hierarchy},
  author = {Del{\'e}tang, Gr{\'e}goire and Ruoss, Anian and {Grau-Moya}, Jordi and Genewein, Tim and Wenliang, Li Kevin and Catt, Elliot and Cundy, Chris and Hutter, Marcus and Legg, Shane and Veness, Joel and Ortega, Pedro A.},
  year = {2022},
  month = oct,
  number = {arXiv:2207.02098},
  eprint = {2207.02098},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-10-11},
  abstract = {Reliable generalization lies at the heart of safe ML and AI. However, understanding when and how neural networks generalize remains one of the most important unsolved problems in the field. In this work, we conduct an extensive empirical study (10250 models, 15 tasks) to investigate whether insights from the theory of computation can predict the limits of neural network generalization in practice. We demonstrate that grouping tasks according to the Chomsky hierarchy allows us to forecast whether certain architectures will be able to generalize to out-of-distribution inputs. This includes negative results where even extensive amounts of data and training time never lead to any non-trivial generalization, despite models having sufficient capacity to fit the training data perfectly. Our results show that, for our subset of tasks, RNNs and Transformers fail to generalize on non-regular tasks, LSTMs can solve regular and counter-language tasks, and only networks augmented with structured memory (such as a stack or memory tape) can successfully generalize on context-free and context-sensitive tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Formal Languages and Automata Theory,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/deletang.g2022 Neural networks and the Chomsky hierarch.pdf}
}

@book{delmoral.p:2004,
  title = {Feynman-{{Kac}} Formulae: Genealogical and Interacting Particle Systems with Applications},
  author = {Del Moral, Pierre},
  editor = {Gani, J. and Heyde, C. C. and Kurtz, T. G.},
  year = {2004},
  month = mar,
  series = {Probability and Its {{Applications}}},
  edition = {1},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-1-4684-9393-1},
  urldate = {2023-04-22},
  isbn = {978-1-4684-9393-1},
  keywords = {algorithms,Feynman-Kac formula,filtering problem,genetic algorithms,interacting particle system,Markov chain,Markov kernel,Markov process,Monte Carlo method,Statistical Physics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/delmoral.p2004 Feynman-Kac formulae genealogical and i.pdf}
}

@inproceedings{demarneffe.m:2006stanforddep,
  title = {Generating Typed Dependency Parses from Phrase Structure Parses},
  booktitle = {Proceedings of the Fifth International Conference on Language Resources and Evaluation ({{LREC}}'06)},
  author = {{de Marneffe}, Marie-Catherine and MacCartney, Bill and Manning, Christopher D.},
  year = {2006},
  publisher = {European Language Resources Association (ELRA)},
  address = {Genoa, Italy}
}

@inproceedings{demarneffe.m:2008,
  title = {The {{Stanford}} Typed Dependencies Representation},
  booktitle = {Coling 2008: {{Proceedings}} of the Workshop on Cross-Framework and Cross-Domain Parser Evaluation},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher D.},
  year = {2008},
  pages = {1--8},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK}
}

@manual{demarneffe.m:2008sdmanual,
  type = {Manual},
  title = {Stanford Typed Dependencies Manual},
  author = {{de Marneffe}, Marie-Catherine and Manning, Christopher},
  year = {2008},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-03-12 10:47:01 -0500},
  organization = {Stanford NLP},
  project = {syntactic embedding},
  version = {Stanford Parser v.3.7.0},
  keywords = {dependency structures,stanford dependencies}
}

@article{demberg.v:2008,
  title = {Data from Eye-Tracking Corpora as Evidence for Theories of Syntactic Processing Complexity},
  author = {Demberg, Vera and Keller, Frank},
  year = {2008},
  journal = {Cognition},
  volume = {109},
  number = {2},
  pages = {193--210},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.07.008},
  abstract = {We evaluate the predictions of two theories of syntactic processing complexity, dependency locality theory (DLT) and surprisal, against the Dundee Corpus, which contains the eye-tracking record of 10 participants reading 51,000 words of newspaper text. Our results show that DLT integration cost is not a significant predictor of reading times for arbitrary words in the corpus. However, DLT successfully predicts reading times for nouns. We also find evidence for integration cost effects at auxiliaries, not predicted by DLT. For surprisal, we demonstrate that an unlexicalized formulation of surprisal can predict reading times for arbitrary words in the corpus. Comparing DLT integration cost and surprisal, we find that the two measures are uncorrelated, which suggests that a complete theory will need to incorporate both aspects of processing complexity. We conclude that eye-tracking corpora, which provide reading time data for naturally occurring, contextualized sentences, can complement experimental evidence as a basis for theories of processing complexity.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2008.07.008},
  keywords = {Corpus data,Dependency locality theory,Eye-tracking,Processing complexity,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/demberg.v2008 Data from eye-tracking corpora as eviden.pdf}
}

@incollection{demberg.v:2019,
  title = {Cognitive Models of Syntax and Sentence Processing},
  booktitle = {Human {{Language}}: {{From Genes}} and {{Brains}} to {{Behavior}}},
  author = {Demberg, Vera and Keller, Frank},
  editor = {Hagoort, Peter},
  year = {2019},
  month = oct,
  pages = {293--312},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/10841.003.0027},
  urldate = {2022-10-22},
  isbn = {978-0-262-35386-1},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/demberg.v2019 Cognitive models of syntax and sentence.pdf}
}

@book{dennett.d:2002,
  title = {Content and Consciousness},
  author = {Dennett, Daniel C.},
  year = {2002},
  month = jan,
  edition = {2},
  publisher = {Routledge},
  doi = {10.4324/9780203005729},
  urldate = {2024-05-13},
  isbn = {978-1-134-84830-0},
  langid = {english}
}

@inproceedings{devarda.a:2023,
  title = {Scaling in {{Cognitive Modelling}}: A {{Multilingual Approach}} to {{Human Reading Times}}},
  shorttitle = {Scaling in {{Cognitive Modelling}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {{de Varda}, Andrea and Marelli, Marco},
  year = {2023},
  month = jul,
  pages = {139--149},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-11},
  abstract = {Neural language models are increasingly valued in computational psycholinguistics, due to their ability to provide conditional probability distributions over the lexicon that are predictive of human processing times. Given the vast array of available models, it is of both theoretical and methodological importance to assess what features of a model influence its psychometric quality. In this work we focus on parameter size, showing that larger Transformer-based language models generate probabilistic estimates that are less predictive of early eye-tracking measurements reflecting lexical access and early semantic integration. However, relatively bigger models show an advantage in capturing late eye-tracking measurements that reflect the full semantic and syntactic integration of a word into the current language context. Our results are supported by eye movement data in ten languages and consider four models, spanning from 564M to 4.5B parameters.},
  keywords = {surprisal theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/devarda.a2023 Scaling in Cognitive Modelling a Multil.pdf}
}

@article{devarda.a:2023a,
  title = {Cloze Probability, Predictability Ratings, and Computational Estimates for 205 {{English}} Sentences, Aligned with Existing {{EEG}} and Reading Time Data},
  author = {{de Varda}, Andrea Gregor and Marelli, Marco and Amenta, Simona},
  year = {2023},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02261-8},
  urldate = {2023-11-10},
  abstract = {We release a database of cloze probability values, predictability ratings, and computational estimates for a sample of 205 English sentences (1726 words), aligned with previously released word-by-word reading time data (both self-paced reading and eye-movement records; Frank et al., Behavior Research Methods, 45(4), 1182--1190. 2013) and EEG responses (Frank et al., Brain and Language, 140, 1--11. 2015). Our analyses show that predictability ratings are the best predictors of the EEG signal (N400, P600, LAN) self-paced reading times, and eye movement patterns, when spillover effects are taken into account. The computational estimates are particularly effective at explaining variance in the eye-tracking data without spillover. Cloze probability estimates have decent overall psychometric accuracy and are the best predictors of early fixation patterns (first fixation duration). Our results indicate that the choice of the best measurement of word predictability in context critically depends on the processing index being considered.},
  langid = {english},
  keywords = {cloze,Cloze probability,EEG,Predictability ratings,Prediction,reading time,surprisal,Surprisal estimates},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/devarda.a2023a Cloze probability, predictability rating.pdf}
}

@inproceedings{devlin.j:2019,
  title = {{{BERT}}: Pre-Training of Deep Bidirectional Transformers for Language Understanding},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1423},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/devlin.j2019 BERT pre-training of deep bidirectional.pdf}
}

@article{diaconis.p:2008,
  title = {The {{Markov}} Chain {{Monte Carlo}} Revolution},
  author = {Diaconis, Persi},
  year = {2008},
  month = nov,
  journal = {Bulletin of the American Mathematical Society},
  volume = {46},
  number = {2},
  pages = {179--205},
  publisher = {American Mathematical Society (AMS)},
  doi = {10.1090/s0273-0979-08-01238-x},
  bdsk-url-2 = {https://doi.org/10.1090/s0273-0979-08-01238-x},
  date-added = {2022-03-17 13:44:06 -0400},
  date-modified = {2022-03-17 13:44:07 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/diaconis.p2008 The Markov chain Monte Carlo revolution.pdf}
}

@inproceedings{dieng.a:2017,
  title = {Variational Inference via {$\chi$} Upper Bound Minimization},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Dieng, Adji Bousso and Tran, Dustin and Ranganath, Rajesh and Paisley, John and Blei, David},
  year = {2017},
  volume = {30},
  publisher = {Curran Associates, Inc.},
  urldate = {2023-01-01},
  keywords = {chi-squared divergence,CHIVI,variational inference},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dieng.a2017 Variational inference via χ upper bound.pdf}
}

@misc{dong.l:2019UniLM,
  title = {Unified Language Model Pre-Training for Natural Language Understanding and Generation},
  author = {Dong, Li and Yang, Nan and Wang, Wenhui and Wei, Furu and Liu, Xiaodong and Wang, Yu and Gao, Jianfeng and Zhou, Ming and Hon, Hsiao-Wuen},
  year = {2019},
  month = oct,
  number = {arXiv:1905.03197},
  eprint = {1905.03197},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dong.l2019UniLM Unified language model pre-training for.pdf}
}

@article{dotlacil.j:2021,
  title = {Parsing as a Cue-Based Retrieval Model},
  author = {Dotla{\v c}il, Jakub},
  year = {2021},
  journal = {Cognitive science},
  volume = {45},
  number = {8},
  pages = {e13020},
  issn = {1551-6709},
  doi = {10.1111/cogs.13020},
  urldate = {2022-07-01},
  abstract = {This paper develops a novel psycholinguistic parser and tests it against experimental and corpus reading data. The parser builds on the recent research into memory structures, which argues that memory retrieval is content-addressable and cue-based. It is shown that the theory of cue-based memory systems can be combined with transition-based parsing to produce a parser that, when combined with the cognitive architecture ACT-R, can model reading and predict online behavioral measures (reading times and regressions). The parser's modeling capacities are tested against self-paced reading experimental data (Grodner \& Gibson, 2005), eye-tracking experimental data (Staub, 2011), and a self-paced reading corpus (Futrell et al., 2018).},
  langid = {english},
  keywords = {ACT-R,Computational psycholinguistics,Cue-based retrieval,Memory retrieval,Modeling reading data,Processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/dotlacil.j2021 Parsing as a cue-based retrieval model.pdf}
}

@article{douc.r:2005,
  title = {Comparison of Resampling Schemes for Particle Filtering},
  author = {Douc, Randal and Capp{\'e}, Olivier and Moulines, Eric},
  year = {2005},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.CS/0507025},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.CS/0507025},
  copyright = {Assumed arXiv.org perpetual, non-exclusive license to distribute this article for submissions made before January 2004},
  date-added = {2022-03-29 20:05:42 -0400},
  date-modified = {2022-03-29 20:05:43 -0400},
  keywords = {and Science (cs.CE),Computational Engineering,Finance,FOS: Computer and information sciences},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/douc.r2005 Comparison of resampling schemes for par.pdf}
}

@incollection{doucet.a:2001,
  title = {An Introduction to Sequential {{Monte Carlo}} Methods},
  booktitle = {Sequential Monte Carlo Methods in Practice},
  author = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  pages = {3--14},
  publisher = {Springer New York},
  doi = {10.1007/978-1-4757-3437-9_1},
  date-added = {2021-03-22 19:46:53 -0400},
  date-modified = {2021-03-22 19:46:54 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/doucet.a2001 An introduction to sequential Monte Carl.pdf}
}

@book{doucet.a:2001smcbook,
  title = {Sequential {{Monte Carlo}} Methods in Practice},
  editor = {Doucet, Arnaud and Freitas, Nando and Gordon, Neil},
  year = {2001},
  series = {Statistics for Engineering and Information Science},
  publisher = {Springer},
  address = {New York},
  doi = {10.1007/978-1-4757-3437-9},
  bdsk-url-2 = {https://doi.org/10.1007/978-1-4757-3437-9},
  date-added = {2022-03-25 22:05:58 -0400},
  date-modified = {2022-03-25 22:10:04 -0400},
  file = {/Users/j/Zotfiles/doucet.a2001smcbook Sequential Monte Carlo methods in practi.pdf}
}

@incollection{doucet.a:2008,
  title = {A Tutorial on Particle Filtering and Smoothing: Fifteen Years Later},
  booktitle = {The {{Oxford Handbook}} of {{Nonlinear Filtering}}},
  author = {Doucet, Arnaud and Johansen, Adam M.},
  editor = {Crisan, Dan and Rozovski{\u \i}, Boris},
  year = {2008},
  month = dec,
  series = {Oxford {{Handbooks}}},
  pages = {656--704},
  publisher = {Oxford University Press},
  abstract = {Optimal estimation problems for non-linear non-Gaussian state-space models do not typically admit analytic solutions. Since their introduction in 1993, particle filtering methods have become a very popular class of algorithms to solve these estimation problems numerically in an online manner, i.e. recursively as observations become available, and are now routinely used in fields as diverse as computer vision,　econometrics, robotics and navigation. The objective of this tutorial is to provide a complete, up-to-date survey of this field as of 2008. Basic and advanced particle methods for filtering as well as smoothing are presented.},
  annotation = {Note: Version 1.1 -- December 2008 with typographical corrections March 2012},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/doucet.a2008 A tutorial on particle filtering and smo 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/doucet.a2008 A tutorial on particle filtering and smo.pdf}
}

@inproceedings{dozat.t:2016,
  title = {Deep Biaffine Attention for Neural Dependency Parsing},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Dozat, Timothy and Manning, Christopher D.},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/DozatM17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@incollection{drieghe.d:2011,
  title = {Parafoveal-on-Foveal Effects on Eye Movements during Reading},
  booktitle = {The Oxford Handbook of Eye Movements},
  author = {Drieghe, Denis},
  editor = {Liversedge, Simon P. and Gilchrist, Iain and Everling, Stefan},
  year = {2011},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780199539789.013.0046},
  bdsk-url-2 = {https://doi.org/10.1093/oxfordhb/9780199539789.013.0046},
  date-added = {2022-04-21 11:02:00 -0400},
  date-modified = {2022-04-21 11:03:00 -0400}
}

@inproceedings{du.l:2023,
  title = {A {{Measure-Theoretic Characterization}} of {{Tight Language Models}}},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Li and Torroba Hennigen, Lucas and Pimentel, Tiago and Meister, Clara and Eisner, Jason and Cotterell, Ryan},
  year = {2023},
  month = jul,
  pages = {9744--9770},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-24},
  abstract = {Language modeling, a central task in natural language processing, involves estimating a probability distribution over strings. In most cases, the estimated distribution sums to 1 over all finite strings. However, in some pathological cases, probability mass can ``leak'' onto the set of infinite sequences. In order to characterize the notion of leakage more precisely, this paper offers a measure-theoretic treatment of language modeling. We prove that many popular language model families are in fact tight, meaning that they will not leak in this sense. We also generalize characterizations of tightness proposed in previous works.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/du.l2023 A Measure-Theoretic Characterization of.pdf}
}

@inproceedings{du.w:2020,
  title = {Exploiting Syntactic Structure for Better Language Modeling: {{A}} Syntactic Distance Approach},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Du, Wenyu and Lin, Zhouhan and Shen, Yikang and O'Donnell, Timothy J. and Bengio, Yoshua and Zhang, Yue},
  year = {2020},
  pages = {6611--6628},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.591},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.591},
  date-modified = {2022-05-17 08:09:30 -0400}
}

@inproceedings{du.z:2022GLM,
  title = {{{GLM}}: {{General}} Language Model Pretraining with Autoregressive Blank Infilling},
  shorttitle = {Glm},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Du, Zhengxiao and Qian, Yujie and Liu, Xiao and Ding, Ming and Qiu, Jiezhong and Yang, Zhilin and Tang, Jie},
  year = {2022},
  month = may,
  pages = {320--335},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.26},
  urldate = {2023-05-31},
  abstract = {There have been various types of pretraining architectures including autoencoding models (e.g., BERT), autoregressive models (e.g., GPT), and encoder-decoder models (e.g., T5). However, none of the pretraining frameworks performs the best for all tasks of three main categories including natural language understanding (NLU), unconditional generation, and conditional generation. We propose a General Language Model (GLM) based on autoregressive blank infilling to address this challenge. GLM improves blank filling pretraining by adding 2D positional encodings and allowing an arbitrary order to predict spans, which results in performance gains over BERT and T5 on NLU tasks. Meanwhile, GLM can be pretrained for different types of tasks by varying the number and lengths of blanks. On a wide range of tasks across NLU, conditional and unconditional generation, GLM outperforms BERT, T5, and GPT given the same model sizes and data, and achieves the best performance from a single pretrained model with 1.25{\textbackslash}mbox{\textbackslash}times parameters of BERT Large , demonstrating its generalizability to different downstream tasks.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/du.z2022GLM GLM General language model pretraining.pdf}
}

@article{dupoux.e:2018,
  title = {Cognitive Science in the Era of Artificial Intelligence: A Roadmap for Reverse-Engineering the Infant Language-Learner},
  author = {Dupoux, Emmanuel},
  year = {2018},
  month = apr,
  journal = {Cognition},
  volume = {173},
  pages = {43--59},
  publisher = {Elsevier BV},
  doi = {10.1016/j.cognition.2017.11.008},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2017.11.008},
  date-added = {2021-10-04 22:09:38 -0400},
  date-modified = {2021-10-04 22:09:56 -0400}
}

@inproceedings{dyer.c:2016,
  title = {Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
  year = {2016},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/n16-1024},
  bdsk-url-2 = {https://doi.org/10.18653/v1/n16-1024},
  date-added = {2021-09-13 21:32:13 -0400},
  date-modified = {2021-09-13 21:32:16 -0400}
}

@article{earley.j:1970,
  title = {An Efficient Context-Free Parsing Algorithm},
  author = {Earley, Jay},
  year = {1970},
  month = feb,
  journal = {Communications of the ACM},
  volume = {13},
  number = {2},
  pages = {94--102},
  issn = {0001-0782},
  doi = {10.1145/362007.362035},
  urldate = {2022-06-13},
  abstract = {A parsing algorithm which seems to be the most efficient general context-free algorithm known is described. It is similar to both Knuth's LR(k) algorithm and the familiar top-down algorithm. It has a time bound proportional to n3 (where n is the length of the string being parsed) in general; it has an n2 bound for unambiguous grammars; and it runs in linear time on a large class of grammars, which seems to include most practical context-free programming language grammars. In an empirical comparison it appears to be superior to the top-down and bottom-up algorithms studied by Griffiths and Petrick.},
  keywords = {compilers,computational complexity,context-free grammar,parsing,syntax analysis},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/earley.j1970 An efficient context-free parsing algori.pdf}
}

@book{earman.j:1992,
  title = {Bayes or Bust? A Critical Examination of {{Bayesian}} Confirmation Theory},
  shorttitle = {Bayes or Bust?},
  author = {Earman, John},
  year = {1992},
  series = {A {{Bradford}} Book},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-51900-7},
  langid = {english}
}

@article{ebbinghaus.h:1885,
  title = {Memory: {{A Contribution}} to {{Experimental Psychology}}},
  shorttitle = {Memory},
  author = {Ebbinghaus, Hermann},
  translator = {Ruger, Henry A. and Bussenius, Clara E.},
  year = {2013},
  journal = {Annals of Neurosciences},
  series = {Annals {{Classics}}},
  volume = {20},
  number = {4},
  pages = {155--156},
  issn = {09727531, 09763260},
  doi = {10.5214/ans.0972.7531.200408},
  urldate = {2022-06-13},
  abstract = {The language of life as well as of science in attributing a memory to the mind attempts to point out the facts and their interpretation somewhat as follows: Mental states of every kind, -- sensations, feelings, ideas, -- which were at one time present in consciousness and then have disappeared from it, have not with their disappearance absolutely ceased to exist. Although the inwardly-turned look may no longer be able to find them, nevertheless they have not been utterly destroyed and annulled, but in a certain manner they continue to exist, stored up, so to speak, in the memory. We cannot, of course, directly observe their present existence, but it is revealed by the effects which come to our knowledge with a certainty like that with which we infer the existence of the stars below the horizon. These effects are of different kinds.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ebbinghaus.h1885 Memory A Contribution to Experimental P.pdf}
}

@article{eberhard.k:1995,
  title = {Eye Movements as a Window into Real-Time Spoken Language Comprehension in Natural Contexts},
  author = {Eberhard, Kathleen M. and {Spivey-Knowlton}, Michael J. and Sedivy, Julie C. and Tanenhaus, Michael K.},
  year = {1995},
  month = nov,
  journal = {Journal of Psycholinguistic Research},
  volume = {24},
  number = {6},
  pages = {409--436},
  issn = {1573-6555},
  doi = {10.1007/BF02143160},
  urldate = {2022-10-13},
  abstract = {When listeners follow spoken instructions to manipulate real objects, their eye movements to the objects are closely time locked to the referring words. We review five experiments showing that this time-locked characteristic of eye movements provides a detailed profile of the processes that underlie real-time spoken language comprehension. Together, the first four experiments showed that listerners immediately integrated lexical, sublexical, and prosodic information in the spoken input with information from the visual context to reduce the set of referents to the intended one. The fifth experiment demonstrated that a visual referential context affected the initial structuring of the linguistic input, eliminating even strong syntactic preferences that result in clear garden paths when the referential context is introduced linguistically. We argue that context affected the earliest moments of language processing because it was highly accessible and relevant to the behavioral goals of the listener.},
  langid = {english},
  keywords = {Cognitive Psychology,Initial Structure,Language Comprehension,Language Processing,Real Object},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/eberhard.k1995 Eye movements as a window into real-time.pdf}
}

@article{efraimidis.p:2006,
  title = {Weighted Random Sampling with a Reservoir},
  author = {Efraimidis, Pavlos S. and Spirakis, Paul G.},
  year = {2006},
  month = mar,
  journal = {Information Processing Letters},
  volume = {97},
  number = {5},
  pages = {181--185},
  issn = {0020-0190},
  doi = {10.1016/j.ipl.2005.11.003},
  urldate = {2022-11-06},
  abstract = {In this work, a new algorithm for drawing a weighted random sample of size m from a population of n weighted items, where m{$\leq$}n, is presented. The algorithm can generate a weighted random sample in one-pass over unknown populations.},
  langid = {english},
  keywords = {Data streams,Parallel algorithms,Randomized algorithms,Reservoir sampling,Weighted random sampling}
}

@article{ehrlich.s:1981,
  title = {Contextual Effects on Word Perception and Eye Movements during Reading},
  author = {Ehrlich, Susan F. and Rayner, Keith},
  year = {1981},
  journal = {Journal of Memory and Language},
  volume = {20},
  number = {6},
  pages = {641},
  publisher = {Academic Press},
  doi = {10.1016/S0022-5371(81)90220-6},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ehrlich.s1981 Contextual effects on word perception an.pdf}
}

@inproceedings{eisape.t:2020,
  title = {Cloze {{Distillation}}: {{Improving Neural Language Models}} with {{Human Next-Word Prediction}}},
  shorttitle = {Cloze {{Distillation}}},
  booktitle = {Proceedings of the 24th {{Conference}} on {{Computational Natural Language Learning}}},
  author = {Eisape, Tiwalayo and Zaslavsky, Noga and Levy, Roger},
  editor = {Fern{\'a}ndez, Raquel and Linzen, Tal},
  year = {2020},
  month = nov,
  pages = {609--619},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.conll-1.49},
  urldate = {2024-08-01},
  abstract = {Contemporary autoregressive language models (LMs) trained purely on corpus data have been shown to capture numerous features of human incremental processing. However, past work has also suggested dissociations between corpus probabilities and human next-word predictions. Here we evaluate several state-of-the-art language models for their match to human next-word predictions and to reading time behavior from eye movements. We then propose a novel method for distilling the linguistic information implicit in human linguistic predictions into pre-trained LMs: Cloze Distillation. We apply this method to a baseline neural LM and show potential improvement in reading time prediction and generalization to held-out human cloze data.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/eisape.t2020 Cloze Distillation Improving Neural Lan.pdf}
}

@inproceedings{eisner.j:1996,
  title = {Three New Probabilistic Models for Dependency Parsing: An Exploration},
  booktitle = {{{COLING}} 1996 Volume 1: {{The}} 16th International Conference on Computational Linguistics},
  author = {Eisner, Jason M.},
  year = {1996}
}

@techreport{eisner.j:1997,
  type = {Technical Report},
  title = {An Empirical Comparison of Probability Models for Dependency Grammar},
  author = {Eisner, Jason M.},
  year = {1997},
  number = {IRCS-96-11},
  institution = {Institute for Research in Cognitive Science, University of Pennsylvania},
  keywords = {dependency parsing,parsing algorithm,projective dependencies,technical report},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/eisner.j1997 An empirical comparison of probability m.pdf}
}

@inproceedings{eisner.j:2016,
  title = {Inside-Outside and Forward-Backward Algorithms Are Just Backprop (Tutorial Paper)},
  booktitle = {Proceedings of the {{Workshop}} on {{Structured Prediction}} for {{NLP}}},
  author = {Eisner, Jason},
  year = {2016},
  month = nov,
  pages = {1--17},
  publisher = {Association for Computational Linguistics},
  address = {Austin, TX},
  doi = {10.18653/v1/W16-5901},
  urldate = {2022-07-05},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/eisner.j2016 Inside-outside and forward-backward algo.pdf}
}

@article{ellis.k:2022,
  title = {Synthesizing Theories of Human Language with {{Bayesian}} Program Induction},
  author = {Ellis, Kevin and Albright, Adam and {Solar-Lezama}, Armando and Tenenbaum, Joshua B. and O'Donnell, Timothy J.},
  year = {2022},
  month = aug,
  journal = {Nature Communications},
  volume = {13},
  number = {1},
  pages = {5024},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-32012-w},
  urldate = {2024-04-13},
  abstract = {Automated, data-driven construction and evaluation of scientific models and theories is a long-standing challenge in artificial intelligence. We present a framework for algorithmically synthesizing models of a basic part of human language: morpho-phonology, the system that builds word forms from sounds. We integrate Bayesian inference with program synthesis and representations inspired by linguistic theory and cognitive models of learning and discovery. Across 70 datasets from 58 diverse languages, our system synthesizes human-interpretable models for core aspects of each language's morpho-phonology, sometimes approaching models posited by human linguists. Joint inference across all 70 data sets automatically synthesizes a meta-model encoding interpretable cross-language typological tendencies. Finally, the same algorithm captures few-shot learning dynamics, acquiring new morphophonological rules from just one or a few examples. These results suggest routes to more powerful machine-enabled discovery of interpretable models in linguistics and other scientific domains.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ellis.k2022 Synthesizing theories of human language.pdf}
}

@article{elvira.v:2017,
  title = {Adapting the Number of Particles in Sequential {{Monte Carlo}} Methods through an Online Scheme for Convergence Assessment},
  author = {Elvira, V{\'i}ctor and M{\'i}guez, Joaqu{\'i}n and Djuri{\'c}, Petar M.},
  year = {2017},
  month = apr,
  journal = {IEEE Transactions on Signal Processing},
  volume = {65},
  number = {7},
  pages = {1781--1794},
  issn = {1941-0476},
  doi = {10.1109/TSP.2016.2637324},
  abstract = {Particle filters are broadly used to approximate posterior distributions of hidden states in state-space models by means of sets of weighted particles. While the convergence of the filter is guaranteed when the number of particles tends to infinity, the quality of the approximation is usually unknown but strongly dependent on the number of particles. In this paper, we propose a novel method for assessing the convergence of particle filters in an online manner, as well as a simple scheme for the online adaptation of the number of particles based on the convergence assessment. The method is based on a sequential comparison between the actual observations and their predictive probability distributions approximated by the filter. We provide a rigorous theoretical analysis of the proposed methodology and, as an example of its practical use, we present simulations of a simple algorithm for the dynamic and online adaptation of the number of particles during the operation of a particle filter on a stochastic version of the Lorenz 63 system.},
  keywords = {Adaptation models,adaptive complexity,computational complexity,Computational modeling,Convergence,convergence analysis,convergence assessment,Heuristic algorithms,Monte Carlo methods,Particle filtering,predictive distribution,Probability density function,sequential Monte Carlo,Signal processing algorithms},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/elvira.v2017 Adapting the number of particles in sequ.pdf}
}

@article{elvira.v:2022,
  title = {Rethinking the Effective Sample Size},
  author = {Elvira, V{\'i}ctor and Martino, Luca and Robert, Christian P.},
  year = {2022},
  journal = {International Statistical Review},
  volume = {90},
  number = {3},
  pages = {525--550},
  issn = {1751-5823},
  doi = {10.1111/insr.12500},
  urldate = {2022-12-08},
  abstract = {The effective sample size (ESS) is widely used in sample-based simulation methods for assessing the quality of a Monte Carlo approximation of a given distribution and of related integrals. In this paper, we revisit the approximation of the ESS in the specific context of importance sampling. The derivation of this approximation, that we will denote as ESS{\textasciicircum}, is partially available in a 1992 foundational technical report of Augustine Kong. This approximation has been widely used in the last 25 years due to its simplicity as a practical rule of thumb in a wide variety of importance sampling methods. However, we show that the multiple assumptions and approximations in the derivation of ESS{\textasciicircum} make it difficult to be considered even as a reasonable approximation of the ESS. We extend the discussion of the ESS{\textasciicircum} in the multiple importance sampling setting, we display numerical examples and we discuss several avenues for developing alternative metrics. This paper does not cover the use of ESS for Markov chain Monte Carlo algorithms.},
  langid = {english},
  keywords = {Bayesian inference,effective sample size,importance sampling,Monte Carlo methods},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/elvira.v2022 Rethinking the effective sample size.pdf}
}

@article{engbert.r:2005,
  title = {{{SWIFT}}: {{A}} Dynamical Model of Saccade Generation during Reading.},
  author = {Engbert, Ralf and Nuthmann, Antje and Richter, Eike M. and Kliegl, Reinhold},
  year = {2005},
  journal = {Psychological Review},
  volume = {112},
  number = {4},
  pages = {777--813},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0033-295x.112.4.777},
  bdsk-url-2 = {https://doi.org/10.1037/0033-295x.112.4.777},
  date-added = {2021-06-02 14:39:17 -0400},
  date-modified = {2021-06-02 14:39:49 -0400},
  keywords = {eye-tracking,frequency effects,predictability}
}

@article{engelmann.f:2013,
  title = {A Framework for Modeling the Interaction of Syntactic Processing and Eye Movement Control},
  author = {Engelmann, Felix and Vasishth, Shravan and Engbert, Ralf and Kliegl, Reinhold},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {452--474},
  issn = {17568757},
  doi = {10.1111/tops.12026},
  urldate = {2022-08-28},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/engelmann.f2013 A framework for modeling the interaction.pdf}
}

@phdthesis{engelmann.f:2016PhD,
  title = {Toward an Integrated Model of Sentence Processing in Reading},
  author = {Engelmann, Felix},
  year = {2016},
  address = {Potsdam, Germany},
  urldate = {2022-10-12},
  abstract = {In experiments investigating sentence processing, eye movement measures such as fixation durations and regression proportions while reading are commonly used to draw conclusions about processing difficulties. However, these measures are the result of an interaction of multiple cognitive levels and processing strategies and thus are only indirect indicators of processing difficulty. In order to properly interpret an eye movement response, one has to understand the underlying principles of adaptive processing such as trade-off mechanisms between reading speed and depth of comprehension that interact with task demands and individual differences. Therefore, it is necessary to establish explicit models of the respective mechanisms as well as their causal relationship with observable behavior. There are models of lexical processing and eye movement control on the one side and models on sentence parsing and memory processes on the other. However, no model so far combines both sides with explicitly defined linking assumptions. In this thesis, a model is developed that integrates oculomotor control with a parsing mechanism and a theory of cue-based memory retrieval. On the basis of previous empirical findings and independently motivated principles, adaptive, resource-preserving mechanisms of underspecification are proposed both on the level of memory access and on the level of syntactic parsing. The thesis first investigates the model of cue-based retrieval in sentence comprehension of Lewis \& Vasishth (2005) with a comprehensive literature review and computational modeling of retrieval interference in dependency processing. The results reveal a great variability in the data that is not explained by the theory. Therefore, two principles, 'distractor prominence' and 'cue confusion', are proposed as an extension to the theory, thus providing a more adequate description of systematic variance in empirical results as a consequence of experimental design, linguistic environment, and individual differences. In the remainder of the thesis, four interfaces between parsing and eye movement control are defined: Time Out, Reanalysis, Underspecification, and Subvocalization. By comparing computationally derived predictions with experimental results from the literature, it is investigated to what extent these four interfaces constitute an appropriate elementary set of assumptions for explaining specific eye movement patterns during sentence processing. Through simulations, it is shown how this system of in itself simple assumptions results in predictions of complex, adaptive behavior.  In conclusion, it is argued that, on all levels, the sentence comprehension mechanism seeks a balance between necessary processing effort and reading speed on the basis of experience, task demands, and resource limitations. Theories of linguistic processing therefore need to be explicitly defined and implemented, in particular with respect to linking assumptions between observable behavior and underlying cognitive processes. The comprehensive model developed here integrates multiple levels of sentence processing that hitherto have only been studied in isolation. The model is made publicly available as an expandable framework for future studies of the interactions between parsing, memory access, and eye movement control.},
  copyright = {https://rightsstatements.org/vocab/InC/1.0/},
  langid = {english},
  school = {Universit{\"a}t Potsdam},
  keywords = {ACT-R},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/engelmann.f2016PhD Toward an integrated model of sentence p.pdf}
}

@article{engelmann.f:2019,
  title = {The Effect of Prominence and Cue Association on Retrieval Processes: A Computational Account},
  shorttitle = {The Effect of Prominence and Cue Association on Retrieval Processes},
  author = {Engelmann, Felix and J{\"a}ger, Lena A. and Vasishth, Shravan},
  year = {2019},
  journal = {Cognitive Science},
  volume = {43},
  number = {12},
  pages = {e12800},
  issn = {1551-6709},
  doi = {10.1111/cogs.12800},
  urldate = {2022-10-12},
  abstract = {We present a comprehensive empirical evaluation of the ACT-R--based model of sentence processing developed by Lewis and Vasishth (2005) (LV05). The predictions of the model are compared with the results of a recent meta-analysis of published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject--verb dependencies (J{\"a}ger, Engelmann, \& Vasishth, 2017). The comparison shows that the model has only partial success in explaining the data; and we propose that its prediction space is restricted by oversimplifying assumptions. We then implement a revised model that takes into account differences between individual experimental designs in terms of the prominence of the target and the distractor in memory- and context-dependent cue-feature associations. The predictions of the original and the revised model are quantitatively compared with the results of the meta-analysis. Our simulations show that, compared to the original LV05 model, the revised model accounts for the data better. The results suggest that effects of prominence and variable cue-feature associations need to be considered in the interpretation of existing empirical results and in the design and planning of future experiments. With regard to retrieval interference in sentence processing and to the broader field of psycholinguistic studies, we conclude that well-specified models in tandem with high-powered experiments are needed in order to uncover the underlying cognitive processes.},
  langid = {english},
  keywords = {ACT-R,Computational modeling,Cue-based retrieval,Dependency completion,Retrieval interference,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/engelmann.f2019 The effect of prominence and cue associa.pdf}
}

@article{ennever.t:2017,
  title = {A Replicable Acoustic Measure of Lenition and the Nature of Variability in {{Gurindji}} Stops},
  author = {Ennever, Thomas and Meakins, Felicity and Round, Erich R.},
  year = {2017},
  month = aug,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {8},
  number = {1},
  publisher = {Open Library of the Humanities},
  doi = {10.5334/labphon.18},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.18},
  date-added = {2022-05-10 10:59:44 -0400},
  date-modified = {2022-05-10 10:59:54 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ennever.t2017 A replicable acoustic measure of lenitio.pdf}
}

@article{erickson.t:1981,
  title = {From Words to Meaning: {{A}} Semantic Illusion},
  shorttitle = {From Words to Meaning},
  author = {Erickson, Thomas D. and Mattson, Mark E.},
  year = {1981},
  month = oct,
  journal = {Journal of Verbal Learning and Verbal Behavior},
  volume = {20},
  number = {5},
  pages = {540--551},
  issn = {0022-5371},
  doi = {10.1016/S0022-5371(81)90165-1},
  urldate = {2024-05-28},
  abstract = {How are the meanings of individual words combined to form a more global description of meaning? This paper describes a phenomenon which sheds some light on one aspects of this process. Consider the following question: How many animals of each kind did Moses take on the Ark? Most people answer ``two'' even though they know quite well that it was Noah and not Moses who sailed the Ark. This illusion occurs even when time pressure is eliminated and subjects are told that questions may be ``wrong'' and given an example of a question with an inconsistent name in it. Two explanations of this illusion---that people are skipping over the name or that the focus of the question is leading them astry---are eliminated. Results indicate that it is important that the inconsistent name share semantic featuers with the correct name. An explanation of the illusion is developed.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/erickson.t1981 From words to meaning A semantic illusi.pdf}
}

@misc{erion.g:2019,
  title = {Learning Explainable Models Using Attribution Priors},
  author = {Erion, Gabriel and Janizek, Joseph D. and Sturmfels, Pascal and Lundberg, Scott and Lee, Su-In},
  year = {2019},
  eprint = {1906.10670},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:04:57 -0400},
  date-modified = {2019-07-05 11:06:04 -0400},
  project = {syntactic embedding},
  keywords = {gradient attribution priors}
}

@article{estes.w:1959,
  title = {Foundations of Linear Models},
  author = {Estes, William K and Suppes, Patrick},
  year = {1959},
  journal = {Studies in mathematical learning theory},
  pages = {137--179},
  publisher = {Stanford University Press Stanford},
  date-added = {2021-05-31 13:41:13 -0400},
  date-modified = {2021-05-31 13:41:26 -0400},
  keywords = {probability matching}
}

@book{fano.r:1961,
  title = {Transmission of Information: A Statistical Theory of Communications},
  author = {Fano, Robert M},
  year = {1961},
  edition = {1},
  publisher = {MIT Press},
  address = {Cambdridge, Mass},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-07 09:14:32 -0400}
}

@incollection{farmer.t:2012,
  title = {Individual Differences in Sentence Processing},
  booktitle = {The {{Cambridge Handbook}} of {{Psycholinguistics}}},
  author = {Farmer, Thomas A. and Misyak, Jennifer B. and Christiansen, Morten H.},
  editor = {McRae, Ken and Joanisse, Marc and Spivey, Michael},
  year = {2012},
  series = {Cambridge {{Handbooks}} in {{Psychology}}},
  pages = {353--364},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139029377.018},
  urldate = {2022-10-13},
  isbn = {978-0-521-86064-2},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/farmer.t2012 Individual differences in sentence proce.pdf}
}

@inproceedings{faruqui.m:2018,
  title = {{{WikiAtomicEdits}}: {{A Multilingual Corpus}} of {{Wikipedia Edits}} for {{Modeling Language}} and {{Discourse}}},
  shorttitle = {{{WikiAtomicEdits}}},
  booktitle = {Proceedings of the 2018 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Faruqui, Manaal and Pavlick, Ellie and Tenney, Ian and Das, Dipanjan},
  year = {2018},
  month = oct,
  pages = {305--315},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1028},
  urldate = {2023-09-07},
  abstract = {We release a corpus of 43 million atomic edits across 8 languages. These edits are mined from Wikipedia edit history and consist of instances in which a human editor has inserted a single contiguous phrase into, or deleted a single contiguous phrase from, an existing sentence. We use the collected data to show that the language generated during editing differs from the language that we observe in standard corpora, and that models trained on edits encode different aspects of semantics and discourse than models trained on raw text. We release the full corpus as a resource to aid ongoing research in semantics, discourse, and representation learning.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/faruqui.m2018 WikiAtomicEdits A Multilingual Corpus o.pdf}
}

@article{fasiolo.m:2020,
  title = {Fast Calibrated Additive Quantile Regression},
  author = {Fasiolo, Matteo and Wood, Simon N. and Zaffran, Margaux and Nedellec, Rapha{\"e}l and Goude, Yannig},
  year = {2020},
  month = mar,
  journal = {Journal of the American Statistical Association},
  volume = {116},
  number = {535},
  pages = {1402--1412},
  publisher = {Informa UK Limited},
  issn = {1537-274X},
  doi = {10.1080/01621459.2020.1725521},
  date-added = {2022-03-10 22:30:30 -0500},
  date-modified = {2022-03-10 22:30:33 -0500}
}

@article{fearnhead.p:2018review,
  title = {Particle {{Filters}} and {{Data Assimilation}}},
  author = {Fearnhead, Paul and K{\"u}nsch, Hans R.},
  year = {2018},
  month = mar,
  journal = {Annual Review of Statistics and Its Application},
  volume = {5},
  number = {1},
  pages = {421--449},
  issn = {2326-8298, 2326-831X},
  doi = {10.1146/annurev-statistics-031017-100232},
  urldate = {2024-09-30},
  abstract = {State-space models can be used to incorporate subject knowledge on the underlying dynamics of a time series by the introduction of a latent Markov state process. A user can specify the dynamics of this process together with how the state relates to partial and noisy observations that have been made. Inference and prediction then involve solving a challenging inverse problem: calculating the conditional distribution of quantities of interest given the observations. This article reviews Monte Carlo algorithms for solving this inverse problem, covering methods based on the particle filter and the ensemble Kalman filter. We discuss the challenges posed by models with high-dimensional states, joint estimation of parameters and the state, and inference for the history of the state process. We also point out some potential new developments that will be important for tackling cutting-edge filtering applications.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/fearnhead.p2018review Particle Filters and Data Assimilation.pdf}
}

@misc{feder.a:2021,
  title = {Causal Inference in Natural Language Processing: Estimation, Prediction, Interpretation and Beyond},
  author = {Feder, Amir and Keith, Katherine A. and Manzoor, Emaad and Pryzant, Reid and Sridhar, Dhanya and {Wood-Doughty}, Zach and Eisenstein, Jacob and Grimmer, Justin and Reichart, Roi and Roberts, Margaret E. and Stewart, Brandon M. and Veitch, Victor and Yang, Diyi},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2109.00725},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2109.00725},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-10 11:31:28 -0400},
  date-modified = {2022-05-10 11:31:44 -0400},
  keywords = {causality,machine learning,natural language processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/feder.a2021 Causal inference in natural language pro.pdf}
}

@article{federmeier.k:1999,
  title = {A Rose by Any Other Name: Long-Term Memory Structure and Sentence Processing},
  shorttitle = {A Rose by Any Other Name},
  author = {Federmeier, Kara D. and Kutas, Marta},
  year = {1999},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {41},
  number = {4},
  pages = {469--495},
  issn = {0749-596X},
  doi = {10.1006/jmla.1999.2660},
  urldate = {2023-12-12},
  abstract = {The effects of sentential context and semantic memory structure during on-line sentence processing were examined by recording event-related brain potentials as individuals read pairs of sentences for comprehension. The first sentence established an expectation for a particular exemplar of a semantic category, while the second ended with (1) that expected exemplar, (2) an unexpected exemplar from the same (expected) category, or (3) an unexpected item from a different (unexpected) category. Expected endings elicited a positivity between 250 and 550 ms while all unexpected endings elicited an N400, which was significantly smaller to items from the expected category. This N400 reduction varied with the strength of the contextually induced expectation: unexpected, categorically related endings elicited smaller N400s in more constraining contexts, despite their poorer fit to context (lower plausibility). This pattern of effects is best explained as reflecting the impact of context-independent long-term memory structure on sentence processing. The results thus suggest that physical and functional similarities that hold between objects in the world---i.e., category structure---influence neural organization and, in turn, routine language comprehension processes.},
  keywords = {categorization,event-related potentials,N400,sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/federmeier.k1999 A rose by any other name long-term memo.pdf}
}

@article{federmeier.k:2007,
  title = {Thinking Ahead: {{The}} Role and Roots of Prediction in Language Comprehension},
  shorttitle = {Thinking Ahead},
  author = {Federmeier, Kara D.},
  year = {2007},
  journal = {Psychophysiology},
  volume = {44},
  number = {4},
  pages = {491--505},
  issn = {1469-8986},
  doi = {10.1111/j.1469-8986.2007.00531.x},
  urldate = {2023-12-12},
  abstract = {Reviewed are studies using event-related potentials to examine when and how sentence context information is used during language comprehension. Results suggest that, when it can, the brain uses context to predict features of likely upcoming items. However, although prediction seems important for comprehension, it also appears susceptible to age-related deterioration and can be associated with processing costs. The brain may address this trade-off by employing multiple processing strategies, distributed across the two cerebral hemispheres. In particular, left hemisphere language processing seems to be oriented toward prediction and the use of top-down cues, whereas right hemisphere comprehension is more bottom-up, biased toward the veridical maintenance of information. Such asymmetries may arise, in turn, because language comprehension mechanisms are integrated with language production mechanisms only in the left hemisphere (the PARLO framework).},
  langid = {english},
  keywords = {Aging,Event-related potentials,Hemispheric differences,Language,N400,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/federmeier.k2007 Thinking ahead The role and roots of pr.pdf}
}

@article{fedorenko.e:2004,
  title = {Verbal Working Memory in Sentence Comprehension},
  author = {Fedorenko, Evelina and Gibson, Edward and Rohde, Douglas},
  year = {2004},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {26},
  number = {26},
  urldate = {2022-10-26},
  abstract = {Author(s): Fedorenko, Evelina; Gibson, Edward; Rohde, Douglas},
  langid = {english},
  keywords = {uniform information density},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/fedorenko.e2004 Verbal working memory in sentence compre.pdf}
}

@inproceedings{feller.w:1949,
  title = {On the Theory of Stochastic Processes, with Particular Reference to Applications},
  booktitle = {Proceedings of the [{{First}}] Berkeley Symposium on Mathematical Statistics and Probability},
  author = {Feller, William},
  year = {1949},
  pages = {403--432},
  keywords = {diffusion processes}
}

@article{fenk.a:1980,
  title = {Konstanz Im Kurzzeitged{\"a}chtnis - Konstanz Im Sprachlichen Informationsflu{\ss}?},
  author = {Fenk, August and {Fenk-Oczlon}, Gertraud},
  year = {1980},
  month = jan,
  journal = {Zeitschrift f{\"u}r experimentelle und angewandte Psychologie},
  volume = {27},
  number = {3},
  pages = {400--414},
  date-added = {2021-10-26 14:23:04 -0400},
  date-modified = {2021-10-27 09:20:37 -0400}
}

@inproceedings{fernandez-monsalve.i:2012,
  title = {Lexical Surprisal as a General Predictor of Reading Time},
  booktitle = {Proceedings of the 13th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Fernandez Monsalve, Irene and Frank, Stefan L. and Vigliocco, Gabriella},
  year = {2012},
  month = apr,
  pages = {398--408},
  publisher = {Association for Computational Linguistics},
  address = {Avignon, France},
  date-added = {2021-11-29 10:29:43 -0500},
  date-modified = {2021-11-29 10:29:44 -0500}
}

@article{ferreira.f:2001,
  title = {Misinterpretations of {{Garden-Path Sentences}}: {{Implications}} for {{Models}} of {{Sentence Processing}} and {{Reanalysis}}},
  shorttitle = {Misinterpretations of {{Garden-Path Sentences}}},
  author = {Ferreira, Fernanda and Christianson, Kiel and Hollingworth, Andrew},
  year = {2001},
  month = jan,
  journal = {Journal of Psycholinguistic Research},
  volume = {30},
  number = {1},
  pages = {3--20},
  issn = {1573-6555},
  doi = {10.1023/A:1005290706460},
  urldate = {2022-06-11},
  abstract = {Theories of sentence comprehension have addressed both initial parsing processes and mechanisms responsible for reanalysis. Three experiments are summarized that were designed to investigate the reanalysis and interpretation of relatively difficult garden-path sentences (e.g., While Anna dressed the baby spit up on the bed). After reading such sentences, participants correctly believed that the baby spit up on the bed; however, they often confidently, yet incorrectly, believed that Anna dressed the baby. These results demonstrate that garden-path reanalysis is not an all-or-nothing process and that thematic roles initially assigned for the subordinate clause verb are not consistently revised. The implications of the partial reanalysis phenomenon for Fodor and Inoue's (1998) model of reanalysis and sentence processing are discussed. In addition, we discuss the possibility that language processing often creates ``good enough'' structures rather than ideal structures.},
  langid = {english},
  keywords = {parsing,reanalysis,semantics,syntactic ambiguity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ferreira.f2001 Misinterpretations of Garden-Path Senten.pdf}
}

@article{ferreira.f:2002,
  title = {Good-Enough Representations in Language Comprehension},
  author = {Ferreira, Fernanda and Bailey, Karl G.D. and Ferraro, Vittoria},
  year = {2002},
  month = feb,
  journal = {Current Directions in Psychological Science},
  volume = {11},
  number = {1},
  pages = {11--15},
  publisher = {SAGE Publications Inc},
  issn = {0963-7214},
  doi = {10.1111/1467-8721.00158},
  urldate = {2022-06-11},
  abstract = {People comprehend utterances rapidly and without conscious effort. Traditional theories assume that sentence processing is algorithmic and that meaning is derived compositionally. The language processor is believed to generate representations of the linguistic input that are complete, detailed, and accurate. However, recent findings challenge these assumptions. Investigations of the misinterpretation of both garden-path and passive sentences have yielded support for the idea that the meaning people obtain for a sentence is often not a reflection of its true content. Moreover, incorrect interpretations may persist even after syntactic reanalysis has taken place. Our good-enough approach to language comprehension holds that language processing is sometimes only partial and that semantic representations are often incomplete. Future work will elucidate the conditions under which sentence processing is simply good enough.},
  langid = {english},
  keywords = {language comprehension,linguistic ambiguity,satisficing,syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ferreira.f2002 Good-enough representations in language.pdf}
}

@incollection{ferreira.f:2016,
  title = {Chapter {{Six}} - {{Prediction}}, {{Information Structure}}, and {{Good-Enough Language Processing}}},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Ferreira, Fernanda and Lowder, Matthew W.},
  editor = {Ross, Brian H.},
  year = {2016},
  month = jan,
  volume = {65},
  pages = {217--247},
  publisher = {Academic Press},
  doi = {10.1016/bs.plm.2016.04.002},
  urldate = {2022-06-14},
  abstract = {The good-enough language processing approach emphasizes people's tendency to generate superficial and even inaccurate interpretations of sentences. At the same time, a number of researchers have argued that prediction plays a key role in comprehension, allowing people to anticipate features of the input and even specific upcoming words based on sentential constraint. In this chapter, we review evidence from our lab supporting both approaches, even though at least superficially these two perspectives seem incompatible. We then argue that what allows us to link good-enough processing and prediction is the concept of information structure, which states that sentences are organized to convey both given or presupposed information, and new or focused information. Our fundamental proposal is that given or presupposed information is processed in a good-enough manner, while new or focused information is the target of the comprehender's prediction efforts. The result is a theory that brings together three different literatures that have been treated almost entirely independently, and which can be evaluated using a combination of behavioral, computational, and neural methods.},
  langid = {english},
  keywords = {Comprehension,Good-enough processing,Information structure,Language processing,Prediction}
}

@article{ferrer-i-cancho.r:2015,
  title = {The Placement of the Head That Minimizes Online Memory: A Complex Systems Approach},
  author = {{Ferrer-i-Cancho}, Ramon},
  year = {2015},
  journal = {Language Dynamics and Change},
  volume = {5},
  number = {1},
  pages = {114--137},
  publisher = {Brill},
  date-added = {2019-05-15 00:10:35 -0400},
  date-modified = {2019-06-17 21:57:08 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,memory}
}

@inproceedings{finkel.j:2008,
  title = {Efficient, Feature-Based, Conditional Random Field Parsing},
  booktitle = {Proceedings of {{ACL-08}}: {{HLT}}},
  author = {Finkel, Jenny Rose and Kleeman, Alex and Manning, Christopher D.},
  year = {2008},
  month = jun,
  pages = {959--967},
  publisher = {Association for Computational Linguistics},
  address = {Columbus, Ohio},
  keywords = {pruning}
}

@article{flake.j:2020,
  title = {Measurement {{Schmeasurement}}: {{Questionable Measurement Practices}} and {{How}} to {{Avoid Them}}},
  shorttitle = {Measurement {{Schmeasurement}}},
  author = {Flake, Jessica Kay and Fried, Eiko I.},
  year = {2020},
  month = dec,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {3},
  number = {4},
  pages = {456--465},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245920952393},
  urldate = {2024-09-23},
  abstract = {In this article, we define questionable measurement practices (QMPs) as decisions researchers make that raise doubts about the validity of the measures, and ultimately the validity of study conclusions. Doubts arise for a host of reasons, including a lack of transparency, ignorance, negligence, or misrepresentation of the evidence. We describe the scope of the problem and focus on how transparency is a part of the solution. A lack of measurement transparency makes it impossible to evaluate potential threats to internal, external, statistical-conclusion, and construct validity. We demonstrate that psychology is plagued by a measurement schmeasurement attitude: QMPs are common, hide a stunning source of researcher degrees of freedom, and pose a serious threat to cumulative psychological science, but are largely ignored. We address these challenges by providing a set of questions that researchers and consumers of scientific research can consider to identify and avoid QMPs. Transparent answers to these measurement questions promote rigorous research, allow for thorough evaluations of a study's inferences, and are necessary for meaningful replication studies.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/flake.j2020 Measurement Schmeasurement Questionable.pdf}
}

@inproceedings{flock.f:2014,
  title = {{{WikiWho}}: Precise and Efficient Attribution of Authorship of Revisioned Content},
  shorttitle = {{{WikiWho}}},
  booktitle = {Proceedings of the 23rd International Conference on {{World}} Wide Web},
  author = {Fl{\"o}ck, Fabian and Acosta, Maribel},
  year = {2014},
  month = apr,
  series = {{{WWW}} '14},
  pages = {843--854},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/2566486.2568026},
  urldate = {2023-10-16},
  abstract = {Revisioned text content is present in numerous collaboration platforms on the Web, most notably Wikis. To track authorship of text tokens in such systems has many potential applications; the identification of main authors for licensing reasons or tracing collaborative writing patterns over time, to name some. In this context, two main challenges arise. First, it is critical for such an authorship tracking system to be precise in its attributions, to be reliable for further processing. Second, it has to run efficiently even on very large datasets, such as Wikipedia. As a solution, we propose a graph-based model to represent revisioned content and an algorithm over this model that tackles both issues effectively. We describe the optimal implementation and design choices when tuning it to a Wiki environment. We further present a gold standard of 240 tokens from English Wikipedia articles annotated with their origin. This gold standard was created manually and confirmed by multiple independent users of a crowdsourcing platform. It is the first gold standard of this kind and quality and our solution achieves an average of 95\% precision on this data set. We also perform a first-ever precision evaluation of the state-of-the-art algorithm for the task, exceeding it by over 10\% on average. Our approach outperforms the execution time of the state-of-the-art by one order of magnitude, as we demonstrate on a sample of over 240 English Wikipedia articles. We argue that the increased size of an optional materialization of our results by about 10\% compared to the baseline is a favorable trade-off, given the large advantage in runtime performance.},
  isbn = {978-1-4503-2744-2},
  keywords = {authorship,collaborative writing,community-driven content creation,content modeling,online collaboration,version control,wikipedia},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/flock.f2014 WikiWho precise and efficient attributi.pdf}
}

@misc{flock.f:2017arxiv,
  title = {{{TokTrack}}: {{A Complete Token Provenance}} and {{Change Tracking Dataset}} for the {{English Wikipedia}}},
  shorttitle = {{{TokTrack}}},
  author = {Fl{\"o}ck, Fabian and Erdogan, Kenan and Acosta, Maribel},
  year = {2017},
  month = mar,
  number = {arXiv:1703.08244},
  eprint = {1703.08244},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-28},
  abstract = {We present a dataset that contains every instance of all tokens ({\textasciitilde} words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. We show how this data enables, on token-level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics, in the process gaining several novel insights.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/flock.f2017arxiv TokTrack A Complete Token Provenance an.pdf}
}

@misc{flock.f:2017dataset,
  title = {{{TokTrack}}: {{A Complete Token Provenance}} and {{Change Tracking Dataset}} for the {{English Wikipedia}}},
  shorttitle = {{{TokTrack}}},
  author = {Fl{\"o}ck, Fabian and Erdogan, Kenan and Acosta, Maribel},
  year = {2017},
  month = jul,
  doi = {10.5281/zenodo.834557},
  urldate = {2023-09-28},
  abstract = {Fixes in version 1.1 (= Zenodo's "version 2") *In 20161101-revisions-part1-12-1728.csv, missing first data line is added. *In Current\_content and Deleted\_content files, some token values ('str' column) which contain regular quotes ('"') are fixed. *In Current\_content and Deleted\_content files, some wrong revision ID values for 'origin\_rev\_id', 'in' and 'out' columns are fixed. ------ This dataset contains every instance of all tokens ({$\approx$} words) ever written in undeleted, non-redirect English Wikipedia articles until October 2016, in total 13,545,349,787 instances. Each token is annotated with (i) the article revision it was originally created in, and (ii) lists with all the revisions in which the token was ever deleted and (potentially) re-added and re-deleted from its article, enabling a complete and straightforward tracking of its history. This data would be exceedingly hard to create by an average potential user as it is (i) very expensive to compute and as (ii) accurately tracking the history of each token in revisioned documents is a non-trivial task. Adapting a state-of-the-art algorithm, we have produced a dataset that allows for a range of analyses and metrics, already popular in research and going beyond, to be generated on complete-Wikipedia scale; ensuring quality and allowing researchers to forego expensive text-comparison computation, which so far has hindered scalable usage. This dataset, its creation process and use cases are described in a dedicated dataset paper of the same name, published at the ICWSM 2017 conference. In this paper, we show how this data enables, on token level, computation of provenance, measuring survival of content over time, very detailed conflict metrics, and fine-grained interactions of editors like partial reverts, re-additions and other metrics. Tokenization used: https://gist.github.com/faflo/3f5f30b1224c38b1836d63fa05d1ac94 Toy example for how the token metadata is generated: https://gist.github.com/faflo/8bd212e81e594676f8d002b175b79de8 Be sure to read the ReadMe.txt or - even more detailed - the supporting paper which is referenced under "related identifiers".},
  langid = {english}
}

@article{floridi.l:2020,
  title = {{{GPT-3}}: {{Its}} Nature, Scope, Limits, and Consequences},
  author = {Floridi, Luciano and Chiriatti, Massimo},
  year = {2020},
  journal = {Minds and Machines},
  volume = {30},
  number = {4},
  pages = {681--694},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s11023-020-09548-1},
  bdsk-url-2 = {https://doi.org/10.1007/s11023-020-09548-1},
  date-added = {2021-06-05 22:29:51 -0400},
  date-modified = {2021-06-05 22:29:53 -0400}
}

@article{fodor.j:1978,
  title = {Parsing Strategies and Constraints on Transformations},
  author = {Fodor, Janet Dean},
  year = {1978},
  journal = {Linguistic Inquiry},
  volume = {9},
  number = {3},
  eprint = {4178071},
  eprinttype = {jstor},
  pages = {427--473},
  publisher = {The MIT Press},
  issn = {0024-3892},
  urldate = {2023-02-22},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/fodor.j1978 Parsing strategies and constraints on tr.pdf}
}

@book{folland.g:1999,
  title = {Real {{Analysis}}: {{Modern Techniques}} and {{Their Applications}}, 2nd {{Edition}} {\textbar} {{Wiley}}},
  shorttitle = {Real {{Analysis}}},
  author = {Folland, Gerald B.},
  year = {1999},
  month = apr,
  series = {Pure and {{Applied Mathematics}}: {{A Wiley Series}} of {{Texts}}, {{Monographs}} and {{Tracts}}},
  edition = {2nd Edition},
  urldate = {2022-06-23},
  abstract = {An in-depth look at real analysis and its applications-now expanded and revised. This new edition of the widely used analysis book continues to cover real analysis in greater detail and at a more advanced level than most books on the subject. Encompassing several subjects that underlie much of modern analysis, the book focuses on measure and integration theory, point set topology, and the basics of functional analysis. It illustrates the use of the general theories and introduces readers to other branches of analysis such as Fourier analysis, distribution theory, and probability theory. This edition is bolstered in content as well as in scope-extending its usefulness to students outside of pure analysis as well as those interested in dynamical systems. The numerous exercises, extensive bibliography, and review chapter on sets and metric spaces make Real Analysis: Modern Techniques and Their Applications, Second Edition invaluable for students in graduate-level analysis courses. New features include: * Revised material on the n-dimensional Lebesgue integral. * An improved proof of Tychonoffs theorem. * Expanded material on Fourier analysis. * A newly written chapter devoted to distributions and differential equations. * Updated material on Hausdorff dimension and fractal dimension.},
  isbn = {978-0-471-31716-6},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/folland.g1999 Real Analysis Modern Techniques and The.djvu}
}

@article{forster.k:2009,
  title = {The Maze Task: {{Measuring}} Forced Incremental Sentence Processing Time},
  shorttitle = {The Maze Task},
  author = {Forster, Kenneth I. and Guerrera, Christine and Elliot, Lisa},
  year = {2009},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {41},
  number = {1},
  pages = {163--171},
  issn = {1554-3528},
  doi = {10.3758/BRM.41.1.163},
  urldate = {2023-10-22},
  abstract = {The maze task is an online measure of sentence processing time that provides an alternative to the standard moving window version of self-paced reading. Rather than each word of the sentence being presented in succession, two words are presented at the same time, and the participant must choose which word is a grammatical continuation of the sentence. This procedure forces the reader into an incremental mode of processing in which each word must be fully integrated with the preceding context before the next word can be considered. Previous research with this technique has not considered whether it is sufficiently sensitive to syntactic complexity effects or to garden path effects. Four experiments are reported demonstrating that reliable differences in processing time for subject relatives and object relatives can be obtained, and that this technique generates garden path effects that correspond closely with the data from eyetracking experiments, but without the spillover effects that are sometimes obtained with eyetracking. It is also shown that the task is sensitive to word frequency effects, producing estimates well in excess of those found with eyetracking.},
  langid = {english},
  keywords = {Ambiguous Word,Lexical Decision Task,Maze Task,Object Relative,Relative Clause}
}

@inproceedings{fossum.v:2012,
  title = {Sequential vs. {{Hierarchical}} Syntactic Models of Human Incremental Sentence Processing},
  booktitle = {Proceedings of the 3rd Workshop on Cognitive Modeling and Computational Linguistics ({{CMCL}} 2012)},
  author = {Fossum, Victoria and Levy, Roger},
  year = {2012},
  month = jun,
  pages = {61--69},
  publisher = {Association for Computational Linguistics},
  address = {Montr{\'e}al, Canada},
  date-added = {2021-11-29 10:02:26 -0500},
  date-modified = {2021-11-29 10:02:27 -0500}
}

@inproceedings{foster.a:2019,
  title = {Variational Bayesian Optimal Experimental Design},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Foster, Adam and Jankowiak, Martin and Bingham, Eli and Horsfall, Paul and Teh, Yee Whye and Rainforth, Tom and Goodman, Noah D.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {14036--14047},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/FosterJBHTRG19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{fox.c:2012,
  title = {A Tutorial on Variational {{Bayesian}} Inference},
  author = {Fox, Charles W. and Roberts, Stephen J.},
  year = {2012},
  month = aug,
  journal = {Artificial Intelligence Review},
  volume = {38},
  number = {2},
  pages = {85--95},
  issn = {1573-7462},
  doi = {10.1007/s10462-011-9236-8},
  urldate = {2022-06-27},
  abstract = {This tutorial describes the mean-field variational Bayesian approximation to inference in graphical models, using modern machine learning terminology rather than statistical physics concepts. It begins by seeking to find an approximate mean-field distribution close to the target joint in the KL-divergence sense. It then derives local node updates and reviews the recent Variational Message Passing framework.},
  langid = {english},
  keywords = {Mean-field,Tutorial,Variational Bayes},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/fox.c2012 A tutorial on variational Bayesian infer.pdf}
}

@article{fox.d:2003,
  title = {Adapting the Sample Size in Particle Filters through {{KLD-Sampling}}},
  author = {Fox, Dieter},
  year = {2003},
  month = dec,
  journal = {The International Journal of Robotics Research},
  volume = {22},
  number = {12},
  pages = {985--1003},
  publisher = {SAGE Publications},
  doi = {10.1177/0278364903022012001},
  bdsk-url-2 = {https://doi.org/10.1177/0278364903022012001},
  date-added = {2022-05-05 09:45:16 -0400},
  date-modified = {2022-05-05 09:46:04 -0400},
  keywords = {bayes filtering,KLD-sampling,particle filtering}
}

@article{frank.s:2009cogsci,
  title = {Surprisal-Based Comparison between a Symbolic and a Connectionist Model of Sentence Processing},
  author = {Frank, Stefan L.},
  year = {2009},
  journal = {Proceedings of the 31st Annual Meeting of the Cognitive Science Society},
  urldate = {2022-10-12},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/frank.s2009cogsci Surprisal-based comparison between a sym.pdf}
}

@article{frank.s:2011,
  title = {Insensitivity of the Human Sentence-Processing System to Hierarchical Structure},
  author = {Frank, Stefan L. and Bod, Rens},
  year = {2011},
  month = may,
  journal = {Psychological Science},
  volume = {22},
  number = {6},
  pages = {829--834},
  publisher = {SAGE Publications},
  doi = {10.1177/0956797611409589},
  bdsk-url-2 = {https://doi.org/10.1177/0956797611409589},
  date-added = {2021-11-29 10:04:00 -0500},
  date-modified = {2021-11-29 10:04:02 -0500}
}

@article{frank.s:2013corpus,
  title = {Reading Time Data for Evaluating Broad-Coverage Models of {{English}} Sentence Processing},
  author = {Frank, Stefan L. and Monsalve, Irene Fernandez and Thompson, Robin L. and Vigliocco, Gabriella},
  year = {2013},
  month = feb,
  journal = {Behavior Research Methods},
  volume = {45},
  number = {4},
  pages = {1182--1190},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-012-0313-y},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-012-0313-y},
  date-added = {2021-09-16 13:31:43 -0400},
  date-modified = {2021-12-15 09:07:19 -0500}
}

@inproceedings{frank.s:2013surp,
  title = {Word Surprisal Predicts {{N400}} Amplitude during Reading},
  booktitle = {Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2013},
  pages = {878--883},
  publisher = {Association for Computational Linguistics},
  address = {Sofia, Bulgaria},
  date-added = {2021-12-15 09:07:40 -0500},
  date-modified = {2021-12-15 09:07:44 -0500}
}

@article{frank.s:2015,
  title = {The {{ERP}} Response to the Amount of Information Conveyed by Words in Sentences},
  author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
  year = {2015},
  month = jan,
  journal = {Brain and Language},
  volume = {140},
  pages = {1--11},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2014.10.006},
  urldate = {2024-10-17},
  abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence's hierarchical structure for generating expectations about the upcoming word.},
  keywords = {Entropy,Event-related potentials,Information theory,Reading,Sentence comprehension,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/frank.s2015 The ERP response to the amount of inform.pdf}
}

@article{frank.s:2021,
  title = {Toward Computational Models of Multilingual Sentence Processing},
  author = {Frank, Stefan L.},
  year = {2021},
  journal = {Language Learning},
  volume = {71},
  number = {S1},
  pages = {193--218},
  issn = {1467-9922},
  doi = {10.1111/lang.12406},
  urldate = {2022-10-22},
  abstract = {Although computational models can simulate aspects of human sentence processing, research on this topic has remained almost exclusively limited to the single language case. The current review presents an overview of the state of the art in computational cognitive models of sentence processing, and discusses how recent sentence-processing models can be used to study bi- and multilingualism. Recent results from cognitive modeling and computational linguistics suggest that phenomena specific to bilingualism can emerge from systems that have no dedicated components for handling multiple languages. Hence, accounting for human bi-/multilingualism may not require models that are much more sophisticated than those for the monolingual case.},
  langid = {english},
  keywords = {computational models,multilingualism,neural networks,probabilistic grammars,sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/frank.s2021 Toward computational models of multiling.pdf}
}

@article{frazier.l:1978,
  title = {The Sausage Machine: {{A}} New Two-Stage Parsing Model},
  shorttitle = {The Sausage Machine},
  author = {Frazier, Lyn and Fodor, Janet Dean},
  year = {1978},
  month = jan,
  journal = {Cognition},
  volume = {6},
  number = {4},
  pages = {291--325},
  issn = {0010-0277},
  doi = {10.1016/0010-0277(78)90002-1},
  urldate = {2022-06-12},
  abstract = {It is proposed that the human sentence parsing device assigns phrase structure to word strings in two steps. The first stage parser assigns lexical and phrasal nodes to substrings of roughly six words. The second stage parser then adds higher nodes to link these phrasal packages together into a complete phrase marker. This model of the parser is compared with ATN models, and with the two-stage models of Kimball (1973) and Fodor, Bever and Garrett (1974). Our assumption that the units which are shunted from the first stage to the second stage are defined by their length, rather than by their syntactic type, explains the effects of constituent length on perceptual complexity in center embedded sentences and in sentences of the kind that fall under Kimball's principle of Right Association. The particular division of labor between the two parsing units allows us to explain, without appeal to any ad hoc parsing strategies, why the parser makes certain `shortsighted' errors even though, in general, it is able to make intelligent use of all the information that is available to it. R{\'e}sum{\'e} Dans cet article on propose un m{\'e}canisme de segmentation des {\'e}nonc{\'e}s qui assigne en deux {\'e}tapes une structure syntagmatique aux suites de mots. La premi{\`e}re m{\'e}thode de segmentation assigne des noeuds lexicaux et syntagmatiques {\`a} des suites de 6 mots environ. La seconde ajoute des noeuds {\`a} un niveau sup{\'e}rieur pour lier ces blocs syntagmatiques et obtenir ainsi un marqueur syntagmatique complet. Ce mod{\`e}le de segmentation est compar{\'e} d'une part aux mod{\`e}les ATN et d'autre part au mod{\`e}le en deux {\'e}tapes de Kimball (1973) et Fodor, Bever et Garrett (1974). Nous pensons que les unit{\'e}s qui passent du ler au 2{\`e} niveau sont caract{\'e}ris{\'e}es par leur longueur plut{\^o}t que par leur forme syntaxique. Ceci expliquerait les effects de la longueur des constituants sur la complexit{\'e} perceptuelle des phrases enclass{\'e}es et des phrases du type de celles qui tombent sous le principe de l'association {\`a} droite de Kimball. La distinction sp{\'e}cifique du travail entre les deux unit{\'e}s de segmentation permet d'expliquer, sans faire intervenir des strat{\'e}gies ad hoc, certaines erreurs de segmentation m{\^e}me si, en g{\'e}n{\'e}ral, il est possible de faire un usage intelligent de toutes les informations disponibles.},
  langid = {english}
}

@article{frazier.l:1982,
  title = {Making and Correcting Errors during Sentence Comprehension: Eye Movements in the Analysis of Structurally Ambiguous Sentences},
  author = {Frazier, Lyn and Rayner, Keith},
  year = {1982},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {14},
  number = {2},
  pages = {178--210},
  publisher = {Elsevier BV},
  doi = {10.1016/0010-0285(82)90008-1},
  bdsk-url-2 = {https://doi.org/10.1016/0010-0285(82)90008-1},
  date-added = {2022-05-06 15:36:26 -0400},
  date-modified = {2022-05-06 15:36:46 -0400},
  keywords = {eye-tracking,regressions}
}

@article{frazier.l:1987,
  title = {Syntactic Processing: {{Evidence}} from {{Dutch}}},
  shorttitle = {Syntactic Processing},
  author = {Frazier, Lyn},
  year = {1987},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  volume = {5},
  number = {4},
  pages = {519--559},
  issn = {1573-0859},
  doi = {10.1007/BF00138988},
  urldate = {2022-10-13},
  langid = {english},
  keywords = {Artificial Intelligence,eager processing,Syntactic Processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/frazier.l1987 Syntactic processing Evidence from Dutc.pdf}
}

@inproceedings{freer.f:2010,
  title = {When Are Probabilistic Programs Probably Computationally Tractable?},
  booktitle = {{{NIPS}} Workshop on {{Monte Carlo}} Methods for Modern Applications},
  author = {Freer, Cameron and Mansinghka, Vikash K. and Roy, Daniel},
  year = {2010},
  month = dec,
  address = {Whistler, British Columbia, Canada},
  date-added = {2021-03-09 22:52:16 -0500},
  date-modified = {2022-04-14 10:57:16 -0400},
  keywords = {probabilistic programming,rejection sampling,sampling,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/freer.f2010 When are probabilistic programs probably.pdf}
}

@article{frinsel.f:2024,
  title = {Capturing Individual Differences in Sentence Processing: {{How}} Reliable Is the Self-Paced Reading Task?},
  shorttitle = {Capturing Individual Differences in Sentence Processing},
  author = {Frinsel, Felicity F. and Christiansen, Morten H.},
  year = {2024},
  journal = {Behavior Research Methods},
  doi = {10.3758/s13428-024-02355-x},
  urldate = {2024-02-21},
  abstract = {Advances in research on language processing have originally come from group-level comparisons, but there is now a growing interest in individual differences. To investigate individual differences, tasks that have shown robust group-level differences are often used with the implicit assumption that they will also be reliable when used as an individual differences measure. Here, we examined whether one of the primary tasks used in psycholinguistic research on language processing, the self-paced reading task, can reliably measure individual differences in relative clause processing. We replicated the well-established effects of relative clauses at the group level, with object relative clauses being more difficult to process than subject relative clauses. However, when using difference scores, the reliability of the size of the relative clause effect was close to zero because the self-paced reading times for the different relative clause types were highly correlated within individuals. Nonetheless, we found that the self-paced reading task can be used to reliably capture individual differences in overall reading speed as well as key sentence regions when the two types of relative clause sentences are considered separately. Our results indicate that both the reliability and validity of different sentence regions need to be assessed to determine whether and when self-paced reading can be used to examine individual differences in language processing.},
  langid = {english}
}

@article{friston.k:2005,
  title = {A Theory of Cortical Responses},
  author = {Friston, Karl},
  year = {2005},
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {360},
  number = {1456},
  issn = {0962-8436},
  doi = {10.1098/rstb.2005.1622},
  urldate = {2023-08-18},
  abstract = {This article concerns the nature of evoked brain responses and the principles underlying their generation. We start with the premise that the sensory brain has evolved to represent or infer the causes of changes in its sensory inputs. The problem of inference is well formulated in statistical terms. The statistical fundaments of inference may therefore afford important constraints on neuronal implementation. By formulating the original ideas of Helmholtz on perception, in terms of modern-day statistical theories, one arrives at a model of perceptual inference and learning that can explain a remarkable range of neurobiological facts. It turns out that the problems of inferring the causes of sensory input (perceptual inference) and learning the relationship between input and cause (perceptual learning) can be resolved using exactly the same principle. Specifically, both inference and learning rest on minimizing the brain's free energy, as defined in statistical physics. Furthermore, inference and learning can proceed in a biologically plausible fashion. Cortical responses can be seen as the brains attempt to minimize the free energy induced by a stimulus and thereby encode the most likely cause of that stimulus. Similarly, learning emerges from changes in synaptic efficacy that minimize the free energy, averaged over all stimuli encountered. The underlying scheme rests on empirical Bayes and hierarchical models of how sensory input is caused. The use of hierarchical models enables the brain to construct prior expectations in a dynamic and context-sensitive fashion. This scheme provides a principled way to understand many aspects of cortical organization and responses. The aim of this article is to encompass many apparently unrelated anatomical, physiological and psychophysical attributes of the brain within a single theoretical perspective. In terms of cortical architectures, the theoretical treatment predicts that sensory cortex should be arranged hierarchically, that connections should be reciprocal and that forward and backward connections should show a functional asymmetry (forward connections are driving, whereas backward connections are both driving and modulatory). In terms of synaptic physiology, it predicts associative plasticity and, for dynamic models, spike-timing-dependent plasticity. In terms of electrophysiology, it accounts for classical and extra classical receptive field effects and long-latency or endogenous components of evoked cortical responses. It predicts the attenuation of responses encoding prediction error with perceptual learning and explains many phenomena such as repetition suppression, mismatch negativity (MMN) and the P300 in electroencephalography. In psychophysical terms, it accounts for the behavioural correlates of these physiological phenomena, for example, priming and global precedence. The final focus of this article is on perceptual learning as measured with the MMN and the implications for empirical studies of coupling among cortical areas using evoked sensory responses.},
  langid = {english},
  keywords = {empirical Bayes,predictive coding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/friston.k2005 A theory of cortical responses.pdf}
}

@article{friston.k:2012,
  title = {A Free Energy Principle for Biological Systems},
  author = {Friston, Karl},
  year = {2012},
  month = nov,
  journal = {Entropy},
  volume = {14},
  number = {11},
  pages = {2100--2121},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e14112100},
  urldate = {2022-06-10},
  abstract = {This paper describes a free energy principle that tries to explain the ability of biological systems to resist a natural tendency to disorder. It appeals to circular causality of the sort found in synergetic formulations of self-organization (e.g., the slaving principle) and models of coupled dynamical systems, using nonlinear Fokker Planck equations. Here, circular causality is induced by separating the states of a random dynamical system into external and internal states, where external states are subject to random fluctuations and internal states are not. This reduces the problem to finding some (deterministic) dynamics of the internal states that ensure the system visits a limited number of external states; in other words, the measure of its (random) attracting set, or the Shannon entropy of the external states is small. We motivate a solution using a principle of least action based on variational free energy (from statistical physics) and establish the conditions under which it is formally equivalent to the information bottleneck method. This approach has proved useful in understanding the functional architecture of the brain. The generality of variational free energy minimisation and corresponding information theoretic formulations may speak to interesting applications beyond the neurosciences; e.g., in molecular or evolutionary biology.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {Bayesian,ergodicity,free energy,random dynamical system,self-organization,surprise},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/friston.k2012 A free energy principle for biological s.pdf}
}

@article{fromkin.v:1971,
  title = {The Non-Anomalous Nature of Anomalous Utterances},
  author = {Fromkin, Victoria A.},
  year = {1971},
  journal = {Language},
  volume = {47},
  number = {1},
  eprint = {412187},
  eprinttype = {jstor},
  pages = {27--52},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/412187},
  urldate = {2022-06-24},
  abstract = {An analysis of speech errors provides evidence for the psychological reality of theoretical linguistic concepts such as distinctive features, morpheme structure constraints, abstract underlying forms, phonological rules, and syntactic and semantic features. Furthermore, such errors reveal that linguistic performance is highly rule-governed, and that in many cases it is grammatical rules which constrain or monitor actual speech production. While a model of linguistic competence is independent of temporal constraints, a model of linguistic performance must provide information as to the sequencing of events in real time. To explain the occurrence of particular kinds of errors, a specific ordering of rules is posited, which ordering may or may not coincide with the organization of a grammar.},
  keywords = {speech errors}
}

@misc{fu.z:2023arxiv,
  title = {Decoder-Only or Encoder-Decoder? {{Interpreting}} Language Model as a Regularized Encoder-Decoder},
  shorttitle = {Decoder-Only or Encoder-Decoder?},
  author = {Fu, Zihao and Lam, Wai and Yu, Qian and So, Anthony Man-Cho and Hu, Shengding and Liu, Zhiyuan and Collier, Nigel},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04052},
  eprint = {2304.04052},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-25},
  abstract = {The sequence-to-sequence (seq2seq) task aims at generating the target sequence based on the given input source sequence. Traditionally, most of the seq2seq task is resolved by the Encoder-Decoder framework which requires an encoder to encode the source sequence and a decoder to generate the target text. Recently, a bunch of new approaches have emerged that apply decoder-only language models directly to the seq2seq task. Despite the significant advancements in applying language models to the seq2seq task, there is still a lack of thorough analysis on the effectiveness of the decoder-only language model architecture. This paper aims to address this gap by conducting a detailed comparison between the encoder-decoder architecture and the decoder-only language model framework through the analysis of a regularized encoder-decoder structure. This structure is designed to replicate all behaviors in the classical decoder-only language model but has an encoder and a decoder making it easier to be compared with the classical encoder-decoder structure. Based on the analysis, we unveil the attention degeneration problem in the language model, namely, as the generation step number grows, less and less attention is focused on the source sequence. To give a quantitative understanding of this problem, we conduct a theoretical sensitivity analysis of the attention output with respect to the source input. Grounded on our analysis, we propose a novel partial attention language model to solve the attention degeneration problem. Experimental results on machine translation, summarization, and data-to-text generation tasks support our analysis and demonstrate the effectiveness of our proposed model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/fu.z2023 Decoder-only or encoder-decoder Interpr.pdf}
}

@inproceedings{futrell.r:2017,
  title = {Noisy-Context Surprisal as a Human Sentence Processing Cost Model},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Volume}} 1, Long Papers},
  author = {Futrell, Richard and Levy, Roger},
  year = {2017},
  pages = {688--698},
  publisher = {Association for Computational Linguistics},
  address = {Valencia, Spain}
}

@article{futrell.r:2017L2,
  title = {L2 Processing as Noisy Channel Language Comprehension},
  author = {Futrell, Richard and Gibson, Edward},
  year = {2017},
  month = aug,
  journal = {Bilingualism: Language and Cognition},
  volume = {20},
  number = {4},
  pages = {683--684},
  issn = {1366-7289, 1469-1841},
  doi = {10.1017/S1366728916001061},
  urldate = {2024-03-06},
  abstract = {The thesis in this paper is that L2 speakers differ from L1 speakers in their ability to do memory storage and retrieval about linguistic structure. We would like to suggest it is possible to go farther than this thesis and develop a computational-level theory which explains why this mechanistic difference between L2 and L1 speakers exists. For this purpose, we believe a noisy channel model (Shannon, 1948; Levy, 2008; Levy, Bicknell, Slattery \& Rayner, 2009; Gibson, Bergen \& Piantadosi, 2013) could be a good start. Under the reasonable assumption that L2 speakers have a less precise probabilistic representation of the syntax of their L2 language than L1 speakers do, the noisy channel model straightforwardly predicts that L2 comprehenders will depend more on world knowledge and discourse factors when interpreting and recalling utterances (cf. Gibson, Sandberg, Fedorenko, Bergen \& Kiran, 2015, for this assumption applied to language processing for persons with aphasia). Also, under the assumption that L2 speakers assume a higher error rate than L1 speakers do, the noisy channel model predicts that they will be more affected by alternative parses which are not directly compatible with the form of an utterance.},
  langid = {english},
  keywords = {L2,multilingual,noisy channel},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2017L2 L2 processing as noisy channel language.pdf}
}

@phdthesis{futrell.r:2017phd,
  title = {Memory and Locality in Natural Language},
  author = {Futrell, Richard},
  year = {2017},
  abstract = {I explore the hypothesis that the universal properties of human languages can be explained in terms of efficient communication given fixed human information processing constraints. I argue that under short-term memory constraints, optimal languages should exhibit information locality: words that depend on each other, both in their interpretation and in their statistical distribution, should be close to each other in linear order. The informationtheoretic approach to natural language motivates a study of quantitative syntax in Chapter 2, focusing on word order flexibility. In Chapter 3, I show comprehensive corpus evidence from over 40 languages that word order in grammar and usage is shaped by working memory constraints in the form of dependency locality: a pressure for syntactically linked words to be close. In Chapter 4, I develop a new formal model of language processing cost, called noisy-context surprisal, based on rational inference over noisy memory representations. This model unifies surprisal and memory effects and derives dependency locality effects as a subset of information locality effects. I show that the new processing model also resolves a long-standing paradox in the psycholinguistic literature, structural forgetting, where the effects of memory appear to be language-dependent. In the conclusion I discuss connections to probabilistic grammars, endocentricity, duality of patterning, incremental planning, and deep reinforcement learning.},
  date-added = {2022-04-11 23:46:38 -0400},
  date-modified = {2022-04-11 23:50:11 -0400},
  school = {Massachusetts Institute of Technology / Massachusetts Institute of Technology. Department of Brain and Cognitive Sciences},
  keywords = {information theory,noisy channel coding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2017phd Memory and locality in natural language.pdf}
}

@inproceedings{futrell.r:2018,
  title = {The {{Natural Stories}} Corpus},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven and Fedorenko, Evelina},
  year = {2018},
  publisher = {European Language Resources Association (ELRA)},
  address = {Miyazaki, Japan},
  date-modified = {2021-12-02 00:12:56 -0500}
}

@inproceedings{futrell.r:2019,
  title = {Syntactic Dependencies Correspond to Word Pairs with High Mutual Information},
  booktitle = {Proceedings of the Fifth International Conference on Dependency Linguistics (Depling, {{SyntaxFest}} 2019)},
  author = {Futrell, Richard and Qian, Peng and Gibson, Edward and Fedorenko, Evelina and Blank, Idan},
  year = {2019},
  pages = {3--13},
  publisher = {Association for Computational Linguistics},
  address = {Paris, France},
  doi = {10.18653/v1/W19-7703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-7703},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2019 Syntactic dependencies correspond to wor.pdf}
}

@article{futrell.r:2020,
  title = {Lossy-Context Surprisal: {{An}} Information-Theoretic Model of Memory Effects in Sentence Processing},
  author = {Futrell, Richard and Gibson, Edward and Levy, Roger},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {3},
  pages = {e12814},
  publisher = {Wiley Online Library},
  doi = {10.1111/cogs.12814},
  date-added = {2020-03-27 16:35:14 -0400},
  date-modified = {2022-04-20 13:48:30 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lossy context surprisal,mutual information,processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 3.pdf;/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2020 Lossy-context surprisal An information- 4.pdf;/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2020 Lossy-context surprisal An information-.pdf}
}

@article{futrell.r:2021,
  title = {The {{Natural Stories}} Corpus: A Reading-Time Corpus of {{English}} Texts Containing Rare Syntactic Constructions},
  shorttitle = {The {{Natural Stories}} Corpus},
  author = {Futrell, Richard and Gibson, Edward and Tily, Harry J. and Blank, Idan and Vishnevetsky, Anastasia and Piantadosi, Steven T. and Fedorenko, Evelina},
  year = {2021},
  month = mar,
  journal = {Language Resources and Evaluation},
  volume = {55},
  number = {1},
  pages = {63--77},
  issn = {1574-0218},
  doi = {10.1007/s10579-020-09503-7},
  urldate = {2022-06-09},
  abstract = {It is now a common practice to compare models of human language processing by comparing how well they predict behavioral and neural measures of processing difficulty, such as reading times, on corpora of rich naturalistic linguistic materials. However, many of these corpora, which are based on naturally-occurring text, do not contain many of the low-frequency syntactic constructions that are often required to distinguish between processing theories. Here we describe a new corpus consisting of English texts edited to contain many low-frequency syntactic constructions while still sounding fluent to native speakers. The corpus is annotated with hand-corrected Penn Treebank-style parse trees and includes self-paced reading time data and aligned audio recordings. We give an overview of the content of the corpus, review recent work using the corpus, and release the data.},
  langid = {english},
  keywords = {cognitive modeling,psycholinguistics,reading time,self-paced reading time},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2021 The Natural Stories corpus a reading-ti.pdf}
}

@article{futrell.r:2023cogsci,
  title = {An Information-Theoretic Account of Availability Effects in Language Production},
  author = {Futrell, Richard},
  year = {2023},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {45},
  number = {45},
  urldate = {2023-07-28},
  abstract = {I present a computational-level model of language production in terms of a combination of information theory and control theory in which words are chosen incrementally in order to maximize communicative value subject to an information-theoretic capacity constraint. The theory generally predicts a tradeoff between ease of production and communicative accuracy. I apply the theory to two cases of apparent availability effects in language production, in which words are selected on the basis of their accessibility to a speaker who has not yet perfectly planned the rest of the utterance. Using corpus data on English relative clause complementizer dropping from Levy \&amp; Jaeger (2007) and experimental data on Mandarin noun classifier choice from Zhan \&amp; Levy (2019), I show that the theory reproduces the observed phenomena, providing an alternative account to Uniform Information Density (UID) and a promising general model of language production which is tightly linked to emerging theories in computational neuroscience.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/futrell.r2023cogsci An information-theoretic account of avai.pdf}
}

@incollection{gale.w:1994,
  title = {What Is Wrong with Adding One?},
  booktitle = {Corpus-{{Based Research}} into {{Language}}},
  author = {Gale, William and Church, Kenneth},
  year = {1994},
  month = jan,
  series = {Language and {{Computers}}},
  volume = {12},
  pages = {189--198},
  publisher = {Brill},
  doi = {10.1163/9789004653566_015},
  urldate = {2024-08-02},
  langid = {english},
  keywords = {Applied Linguistics,Languages and Linguistics}
}

@incollection{gamut.l:1991,
  title = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  booktitle = {Logic, Language, and Meaning Volume {{II}}: {{Intensional}} Logic and Logical Grammar},
  author = {Gamut, L. T. F.},
  year = {1991},
  publisher = {University of Chicago Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@misc{gao.l:2020ThePile,
  title = {The Pile: {{An 800GB}} Dataset of Diverse Text for Language Modeling},
  author = {Gao, Leo and Biderman, Stella and Black, Sid and Golding, Laurence and Hoppe, Travis and Foster, Charles and Phang, Jason and He, Horace and Thite, Anish and Nabeshima, Noa and Presser, Shawn and Leahy, Connor},
  year = {2020},
  eprint = {2101.00027},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-11-30 10:16:24 -0500},
  date-modified = {2021-11-30 10:19:58 -0500}
}

@misc{gao.l:2021blogGPT3sizes,
  title = {On the Sizes of {{OpenAI API}} Models},
  author = {Gao, Leo},
  year = {2021},
  month = may,
  journal = {EleutherAI Blog},
  urldate = {2021-12-13},
  date-added = {2021-12-13 19:27:28 -0500},
  date-modified = {2021-12-13 19:29:08 -0500},
  howpublished = {Blog post}
}

@book{garner.w:1962,
  title = {Uncertainty and Structure as Psychological Concepts},
  author = {Garner, Wendell R.},
  year = {1962},
  series = {Uncertainty and Structure as Psychological Concepts},
  publisher = {Wiley},
  address = {Oxford, England},
  abstract = {A mathematical basis stemming from information theory has been used "to develop ideas about and an understanding of some psychological problems." The 1st 4 chapters discuss uncertainty in reference to perceptual discrimination and information transmission. Chapter 5 deals with "the partitioning of structure and meaning." Pattern perception, language redundancy, other sequential behavior, and concept formation are considered in Chapters 6 through 10. Chapter 11 is a "Final Commentary." (PsycINFO Database Record (c) 2016 APA, all rights reserved)}
}

@misc{gastaldi.j:2024arxiv,
  title = {The {{Foundations}} of {{Tokenization}}: {{Statistical}} and {{Computational Concerns}}},
  shorttitle = {The {{Foundations}} of {{Tokenization}}},
  author = {Gastaldi, Juan Luis and Terilla, John and Malagutti, Luca and DuSell, Brian and Vieira, Tim and Cotterell, Ryan},
  year = {2024},
  month = jul,
  number = {arXiv:2407.11606},
  eprint = {2407.11606},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.11606},
  urldate = {2024-08-06},
  abstract = {Tokenization - the practice of converting strings of characters over an alphabet into sequences of tokens over a vocabulary - is a critical yet under-theorized step in the NLP pipeline. Notably, it remains the only major step not fully integrated into widely used end-to-end neural models. This paper aims to address this theoretical gap by laying the foundations of tokenization from a formal perspective. By articulating and extending basic properties about the category of stochastic maps, we propose a unified framework for representing and analyzing tokenizer models. This framework allows us to establish general conditions for the use of tokenizers. In particular, we formally establish the necessary and sufficient conditions for a tokenizer model to preserve the consistency of statistical estimators. Additionally, we discuss statistical and computational concerns crucial for the design and implementation of tokenizer models. The framework and results advanced in this paper represent a step toward a robust theoretical foundation for neural language modeling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,tokenization},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gastaldi.j2024arxiv The Foundations of Tokenization Statist.pdf}
}

@inproceedings{gauthier.j:2020syntaxgym,
  title = {{{SyntaxGym}}: {{An}} Online Platform for Targeted Evaluation of Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: {{System}} Demonstrations},
  author = {Gauthier, Jon and Hu, Jennifer and Wilcox, Ethan and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {70--76},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-demos.10},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-demos.10}
}

@inproceedings{gauthier.j:2023,
  title = {The Neural Dynamics of Word Recognition and Integration},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Gauthier, Jon and Levy, Roger},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {980--995},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  urldate = {2023-12-12},
  abstract = {Listeners recognize and integrate words in rapid and noisy everyday speech by combining expectations about upcoming content with incremental sensory evidence. We present a computational model of word recognition which formalizes this perceptual process in Bayesian decision theory. We fit this model to explain scalp EEG signals recorded as subjects passively listened to a fictional story, revealing both the dynamics of the online auditory word recognition process and the neural correlates of the recognition and integration of words. The model reveals distinct neural processing of words depending on whether or not they can be quickly recognized. While all words trigger a neural response characteristic of probabilistic integration --- voltage modulations predicted by a word's surprisal in context --- these modulations are amplified for words which require more than roughly 150 ms of input to be recognized. We observe no difference in the latency of these neural responses according to words' recognition times. Our results support a two-part model of speech comprehension, combining an eager and rapid process of word recognition with a temporally independent process of word integration. However, we also developed alternative models of the scalp EEG signal not incorporating word recognition dynamics which showed similar performance improvements. We discuss potential future modeling steps which may help to separate these hypotheses.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gauthier.j2023 The neural dynamics of word recognition.pdf}
}

@incollection{gazdar.g:1985,
  title = {Generalized Phrase Structure Grammar},
  booktitle = {Generalized Phrase Structure Grammar},
  author = {Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K. and Sag, Ivan A.},
  year = {1985},
  publisher = {Harvard University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:14 -0400},
  keywords = {GPSG}
}

@inproceedings{geertzen.j:2014EFCamDat,
  title = {Automatic Linguistic Annotation of Large Scale {{L2}} Databases: {{The EF-Cambridge Open Language Database}} ({{EFCamDat}})},
  booktitle = {Selected {{Proceedings}} of the 2012 {{Second Language Research Forum}}: {{Building Bridges}} between {{Disciplines}}},
  author = {Geertzen, Jeroen and Alexopoulou, Theodora and Korhonen, Anna},
  year = {2014},
  pages = {240--254},
  publisher = {Cascadilla Proceedings Project},
  address = {University of Pittsburgh and Carnegie Mellon University},
  isbn = {978-1-57473-464-5},
  keywords = {dataset,error correction}
}

@book{gelman.a:2006book1,
  title = {Data Analysis Using Regression and Multilevel/Hierarchical Models},
  author = {Gelman, Andrew and Hill, Jennifer},
  year = {2006},
  month = dec,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/CBO9780511790942},
  urldate = {2024-05-21},
  abstract = {Data Analysis Using Regression and Multilevel/Hierarchical Models, first published in 2007, is a comprehensive manual for the applied researcher who wants to perform data analysis using linear and nonlinear regression and multilevel models. The book introduces a wide variety of models, whilst at the same time instructing the reader in how to fit these models using available software packages. The book illustrates the concepts by working through scores of real data examples that have arisen from the authors' own applied research, with programming codes provided for each one. Topics covered include causal inference, including regression, poststratification, matching, regression discontinuity, and instrumental variables, as well as multilevel logistic regression and missing-data imputation. Practical tips regarding building, fitting, and understanding are provided throughout.},
  copyright = {https://www.cambridge.org/core/terms},
  isbn = {978-0-521-86706-1 978-0-521-68689-1 978-0-511-79094-2},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gelman.a2006book1 Data analysis using regression and multi.pdf}
}

@book{gelman.a:2014bda3,
  title = {Bayesian Data Analysis},
  author = {Gelman, Andrew and Carlin, John B. and Stern, Hal S. and Dunson, David B. and Vehtari, Aki and Rubin, Donald B.},
  year = {2014},
  series = {Texts in Statistical Science Series},
  edition = {Third edition},
  publisher = {{CRC Press, Taylor and Francis Group}},
  address = {Boca Raton, FL, USA},
  abstract = {Preface This book is intended to have three roles and to serve three associated audiences: an introductory text on Bayesian inference starting from first principles, a graduate text on effective current approaches to Bayesian modeling and computation in statistics and related fields, and a handbook of Bayesian methods in applied statistics for general users of and researchers in applied statistics. Although introductory in its early sections, the book is definitely not elementary in the sense of a first text in statistics. The mathematics used in our book is basic probability and statistics, elementary calculus, and linear algebra. A review of probability notation is given in Chapter 1 along with a more detailed list of topics assumed to have been studied. The practical orientation of the book means that the reader's previous experience in probability, statistics, and linear algebra should ideally have included strong computational components. To write an introductory text alone would leave many readers with only a taste of the conceptual elements but no guidance for venturing into genuine practical applications, beyond those where Bayesian methods agree essentially with standard non-Bayesian analyses. On the other hand, we feel it would be a mistake to present the advanced methods without first introducing the basic concepts from our data-analytic perspective. Furthermore, due to the nature of applied statistics, a text on current Bayesian methodology would be incomplete without a variety of worked examples drawn from real applications. To avoid cluttering the main narrative, there are bibliographic notes at the end of each chapter and references at the end of the book},
  isbn = {978-1-4398-4095-5},
  langid = {english},
  file = {/Users/j/Zotfiles/gelman.a2014bda3 Bayesian data analysis.pdf}
}

@article{geman.s:1984,
  title = {Stochastic {{Relaxation}}, {{Gibbs Distributions}}, and the {{Bayesian Restoration}} of {{Images}}},
  author = {Geman, Stuart and Geman, Donald},
  year = {1984},
  month = nov,
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  volume = {PAMI-6},
  number = {6},
  pages = {721--741},
  issn = {1939-3539},
  doi = {10.1109/TPAMI.1984.4767596},
  abstract = {We make an analogy between images and statistical mechanics systems. Pixel gray levels and the presence and orientation of edges are viewed as states of atoms or molecules in a lattice-like physical system. The assignment of an energy function in the physical system determines its Gibbs distribution. Because of the Gibbs distribution, Markov random field (MRF) equivalence, this assignment also determines an MRF image model. The energy function is a more convenient and natural mechanism for embodying picture attributes than are the local characteristics of the MRF. For a range of degradation mechanisms, including blurring, nonlinear deformations, and multiplicative or additive noise, the posterior distribution is an MRF with a structure akin to the image model. By the analogy, the posterior distribution defines another (imaginary) physical system. Gradual temperature reduction in the physical system isolates low energy states (``annealing''), or what is the same thing, the most probable states under the Gibbs distribution. The analogous operation under the posterior distribution yields the maximum a posteriori (MAP) estimate of the image given the degraded observations. The result is a highly parallel ``relaxation'' algorithm for MAP estimation. We establish convergence properties of the algorithm and we experiment with some simple pictures, for which good restorations are obtained at low signal-to-noise ratios.},
  date-added = {2021-03-17 15:08:02 -0400},
  date-modified = {2021-03-17 15:09:10 -0400},
  keywords = {Additive noise,Annealing,bayesian,Bayesian methods,Deformable models,Degradation,Energy states,Gibbs distribution,gibbs sampling,image restoration,Image restoration,line process,MAP estimate,Markov random field,markov random fields,Markov random fields,relaxation,scene modeling,spatial degradation,stochastic processes,Stochastic processes,Temperature distribution},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/geman.s1984 Stochastic Relaxation, Gibbs Distributio.pdf}
}

@misc{genewein.t:2013,
  title = {Abstraction in Decision-Makers with Limited Information Processing Capabilities},
  author = {Genewein, Tim and Braun, Daniel A.},
  year = {2013},
  month = dec,
  number = {arXiv:1312.4353},
  eprint = {1312.4353},
  primaryclass = {cs, math, stat},
  institution = {arXiv},
  doi = {10.48550/arXiv.1312.4353},
  urldate = {2022-06-09},
  abstract = {A distinctive property of human and animal intelligence is the ability to form abstractions by neglecting irrelevant information which allows to separate structure from noise. From an information theoretic point of view abstractions are desirable because they allow for very efficient information processing. In artificial systems abstractions are often implemented through computationally costly formations of groups or clusters. In this work we establish the relation between the free-energy framework for decision making and rate-distortion theory and demonstrate how the application of rate-distortion for decision-making leads to the emergence of abstractions. We argue that abstractions are induced due to a limit in information processing capacity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Information Theory,Statistics - Machine Learning},
  annotation = {note: Presented at the NIPS 2013 Workshop on Planning with Information Constraints},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/genewein.t2013 Abstraction in decision-makers with limi.pdf}
}

@article{genewein.t:2015,
  title = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making: An Information-Theoretic Optimality Principle},
  shorttitle = {Bounded Rationality, Abstraction, and Hierarchical Decision-Making},
  author = {Genewein, Tim and Leibfried, Felix and {Grau-Moya}, Jordi and Braun, Daniel Alexander},
  year = {2015},
  journal = {Frontiers in Robotics and AI},
  volume = {2},
  issn = {2296-9144},
  urldate = {2022-06-07},
  abstract = {Abstraction and hierarchical information processing are hallmarks of human and animal intelligence underlying the unrivaled flexibility of behavior in biological systems. Achieving such flexibility in artificial systems is challenging, even with more and more computational power. Here, we investigate the hypothesis that abstraction and hierarchical information processing might in fact be the consequence of limitations in information-processing power. In particular, we study an information-theoretic framework of bounded rational decision-making that trades off utility maximization against information-processing costs. We apply the basic principle of this framework to perception-action systems with multiple information-processing nodes and derive bounded-optimal solutions. We show how the formation of abstractions and decision-making hierarchies depends on information-processing costs. We illustrate the theoretical ideas with example simulations and conclude by formalizing a mathematically unifying optimization principle that could potentially be extended to more complex systems.},
  keywords = {bounded rationality,information theory,lossy compression,rate-distortion theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/genewein.t2015 Bounded rationality, abstraction, and hi 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/genewein.t2015 Bounded rationality, abstraction, and hi.pdf}
}

@inproceedings{georgi.d:2011,
  title = {Deriving the Distribution of Person Portmanteaux by Relativized Probing},
  booktitle = {Proceedings of the North-Eastern Linguistic Society},
  author = {Georgi, Doreen},
  year = {2011},
  volume = {42},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:19 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@incollection{gershman.s:2012,
  title = {Perception, Action and Utility: {{The}} Tangled Skein},
  booktitle = {Principles of Brain Dynamics: {{Global}} State Interactions},
  author = {Gershman, Samuel J. and Daw, Nathaniel D.},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  series = {Computational {{Neuroscience Series}}},
  pages = {293--312},
  publisher = {MIT Press},
  doi = {10.7551/mitpress/9108.003.0015},
  isbn = {978-0-262-01764-0},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gershman.s2012 Perception, action and utility The tang.pdf}
}

@article{gershman.s:2015,
  title = {Computational Rationality: {{A}} Converging Paradigm for Intelligence in Brains, Minds, and Machines},
  shorttitle = {Computational Rationality},
  author = {Gershman, Samuel J. and Horvitz, Eric J. and Tenenbaum, Joshua B.},
  year = {2015},
  month = jul,
  journal = {Science},
  volume = {349},
  number = {6245},
  pages = {273--278},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.aac6076},
  urldate = {2024-05-14},
  abstract = {After growing up together, and mostly growing apart in the second half of the 20th century, the fields of artificial intelligence (AI), cognitive science, and neuroscience are reconverging on a shared view of the computational foundations of intelligence that promotes valuable cross-disciplinary exchanges on questions, methods, and results. We chart advances over the past several decades that address challenges of perception and action under uncertainty through the lens of computation. Advances include the development of representations and inferential procedures for large-scale probabilistic inference and machinery for enabling reflection and decisions about tradeoffs in effort, precision, and timeliness of computations. These tools are deployed toward the goal of computational rationality: identifying decisions with highest expected utility, while taking into consideration the costs of computation in complex real-world problems in which most relevant calculations can only be approximated. We highlight key concepts with examples that show the potential for interchange between computer science, cognitive science, and neuroscience.},
  copyright = {http://www.sciencemag.org/about/science-licenses-journal-article-reuse},
  langid = {english}
}

@article{gershman.s:2019,
  title = {What Does the Free Energy Principle Tell Us about the Brain?},
  author = {Gershman, Samuel J.},
  year = {2019},
  month = jan,
  doi = {10.48550/arXiv.1901.07945},
  urldate = {2022-07-27},
  abstract = {The free energy principle has been proposed as a unifying account of brain function. It is closely related, and in some cases subsumes, earlier unifying ideas such as Bayesian inference, predictive coding, and active learning. This article clarifies these connections, teasing apart distinctive and shared predictions.},
  langid = {english},
  keywords = {Bayesian brain,free energy principle,inference,predictive coding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gershman.s2019 What does the free energy principle tell.pdf}
}

@unpublished{gershman.s:2021draft,
  type = {Draft for the {{Oxford}} Handbook of Human Memory},
  title = {The Rational Analysis of Memory},
  author = {Gershman, Samuel J.},
  year = {2021},
  abstract = {This chapter surveys rational models of memory, which posit that memory is optimized to store information that will be needed in the future, subject to the constraint that information can only be stored with a limited amount of precision. This optimization problem can be formalized using the framework of rate-distortion theory, which addresses the trade-off between memory precision and task performance. The design principles that emerge from this framework shed light on numerous regularities of memory, as well as how cognitive and environmental factors shape these regularities.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gershman.s2021draft The rational analysis of memory.pdf}
}

@article{gershman.s:2023,
  title = {The Molecular Memory Code and Synaptic Plasticity: {{A}} Synthesis},
  shorttitle = {The Molecular Memory Code and Synaptic Plasticity},
  author = {Gershman, Samuel J.},
  year = {2023},
  month = feb,
  journal = {Biosystems},
  volume = {224},
  pages = {104825},
  issn = {0303-2647},
  doi = {10.1016/j.biosystems.2022.104825},
  urldate = {2023-08-18},
  abstract = {The most widely accepted view of memory in the brain holds that synapses are the storage sites of memory, and that memories are formed through associative modification of synapses. This view has been challenged on conceptual and empirical grounds. As an alternative, it has been proposed that molecules within the cell body are the storage sites of memory, and that memories are formed through biochemical operations on these molecules. This paper proposes a synthesis of these two views, grounded in a computational model of memory. Synapses are conceived as storage sites for the parameters of an approximate posterior probability distribution over latent causes. Intracellular molecules are conceived as storage sites for the parameters of a generative model. The model stipulates how these two components work together as part of an integrated algorithm for learning and inference.},
  keywords = {Free energy,Inference,Learning,Memory,Synaptic plasticity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gershman.s2023 The molecular memory code and synaptic p.pdf}
}

@article{gibbs.a:2002,
  title = {On Choosing and Bounding Probability Metrics},
  author = {Gibbs, Alison L. and Su, Francis Edward},
  year = {2002},
  journal = {International Statistical Review},
  volume = {70},
  number = {3},
  pages = {419--435},
  issn = {1751-5823},
  doi = {10.1111/j.1751-5823.2002.tb00178.x},
  urldate = {2022-12-22},
  abstract = {When studying convergence of measures, an important issue is the choice of probability metric. We provide a summary and some new results concerning bounds among some important probability metrics/distances that are used by statisticians and probabilists. Knowledge of other metrics can provide a means of deriving bounds for another one in an applied problem. Considering other metrics can also provide alternate insights. We also give examples that show that rates of convergence can strongly depend on the metric chosen. Careful consideration is necessary when choosing a metric.},
  langid = {english},
  keywords = {Discrepancy,Hellinger distance,probability metrics,Probability metrics,Prokhorov metric,Rates of convergence,relative entropy,Relative entropy,Wasserstein distance},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gibbs.a2002 On choosing and bounding probability met.pdf}
}

@phdthesis{gibson.e:1991phd,
  title = {A Computational Theory of Human Linguistic Processing: {{Memory}} Limitations and Processing Breakdown},
  shorttitle = {A Computational Theory of Human Linguistic Processing},
  author = {Gibson, Edward},
  year = {1991},
  address = {United States -- Pennsylvania},
  urldate = {2022-06-14},
  abstract = {This thesis gives a theory of sentence comprehension that attempts to explain a number of linguistic performance effects, including garden-path effects, preferred readings for ambiguous input and processing overload effects. It is hypothesized that the human parser heuristically determines its options based upon evaluation of possible representations with respect to lexical, syntactic, semantic and pragmatic properties, each of which is associated with a weight. Processing overload effects are explained by the assumption of the existence of a maximum load corresponding to the limited capacity of short term memory: a structure becomes unacceptable at a particular parse state if the combination of the processing weights associated with its properties at that state is greater than the available capacity. Furthermore, it is assumed that the language processor is an automatic device that maintains only the best of the set of all compatible representations for an input string. This thesis assumes a formulation of representational evaluation within a parallel framework: one structure is preferred over another if the processing load associated with the first structure is markedly lower than the processing load associated with the second. Thus a garden path effect results if the unpreferred structure is necessary for a successful parse of the input. Four properties of linguistic representations are presented within this framework. The first two--the Properties of Thematic Reception and Transmission--derivable from the \${\textbackslash}theta\$-Criterion from Government-Binding (GB) Theory (Chomsky (1981)); the third--the Property of Lexical Requirement--derivable from the Projection Principle of GB Theory; and the fourth--the Property of Recency Preference--prefers local attachments over more distant attachments (cf. Kimball (1973), Frazier (1979)). This thesis shows how these properties interact to give a partially unified theory of many performance effects.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798617014688},
  langid = {english},
  school = {Carnegie Mellon University},
  keywords = {Applied sciences,computational linguistics,Language,literature and linguistics,memory,parsing,processing,psycholinguistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gibson.e1991phd A computational theory of human linguist.pdf}
}

@article{gibson.e:1998,
  title = {Linguistic Complexity: Locality of Syntactic Dependencies},
  author = {Gibson, Edward},
  year = {1998},
  month = aug,
  journal = {Cognition},
  volume = {68},
  number = {1},
  pages = {1--76},
  issn = {0010-0277},
  doi = {10.1016/S0010-0277(98)00034-1},
  abstract = {This paper proposes a new theory of the relationship between the sentence processing mechanism and the available computational resources. This theory -- the Syntactic Prediction Locality Theory (SPLT) -- has two components: an integration cost component and a component for the memory cost associated with keeping track of obligatory syntactic requirements. Memory cost is hypothesized to be quantified in terms of the number of syntactic categories that are necessary to complete the current input string as a grammatical sentence. Furthermore, in accordance with results from the working memory literature both memory cost and integration cost are hypothesized to be heavily influenced by locality (1) the longer a predicted category must be kept in memory before the prediction is satisfied, the greater is the cost for maintaining that prediction; and (2) the greater the distance between an incoming word and the most local head or dependent to which it attaches, the greater the integration cost. The SPLT is shown to explain a wide range of processing complexity phenomena not previously accounted for under a single theory, including (1) the lower complexity of subject-extracted relative clauses compared to object-extracted relative clauses, (2) numerous processing overload effects across languages, including the unacceptability of multiply center-embedded structures, (3) the lower complexity of cross-serial dependencies relative to center-embedded dependencies, (4) heaviness effects, such that sentences are easier to understand when larger phrases are placed later and (5) numerous ambiguity effects, such as those which have been argued to be evidence for the Active Filler Hypothesis.},
  keywords = {Computational resources,Linguistic complexity,Sentence processing,Syntactic dependency}
}

@article{gibson.e:1999,
  title = {Memory Limitations and Structural Forgetting: The Perception of Complex Ungrammatical Sentences as Grammatical},
  author = {Gibson, Edward and Thomas, James},
  year = {1999},
  month = jun,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {3},
  pages = {225--248},
  publisher = {Informa UK Limited},
  doi = {10.1080/016909699386293},
  bdsk-url-2 = {https://doi.org/10.1080/016909699386293},
  date-added = {2022-04-19 22:36:47 -0400},
  date-modified = {2022-04-19 22:36:48 -0400}
}

@incollection{gibson.e:2000,
  title = {The Dependency Locality Theory: {{A}} Distance-Based Theory of Linguistic Complexity},
  shorttitle = {The Dependency Locality Theory},
  booktitle = {Image, Language, Brain:  {{Papers}} from the First Mind Articulation Project Symposium},
  author = {Gibson, Edward},
  editor = {Marantz, Alec and Miyashita, Yasushi and O'Neil, Wayne},
  year = {2000},
  pages = {94--126},
  publisher = {The MIT Press},
  address = {Cambridge, MA, US},
  abstract = {Discusses the dependency locality theory (DLT) of human computational resources in sentence parsing that relies on 2 kinds of resource use. One of the key ideas underlying the theory is locality, such that the cost of integrating 2 elements (such as a head and a dependent, or a pronominal referent to its antecedent) depends on the distance between the 2. The remainder of the chapter reviews some empirical observations regarding the proceeding difficulty associated with unambiguous structures. It is shown that the DLT accounts for the complexity effect in these structures as well as preferences in ambiguous structures.},
  isbn = {978-0-262-13371-5},
  keywords = {Linguistics,Psychological Theories,Sentence Comprehension,Sentence Structure,Syntax}
}

@article{gibson.e:2013,
  title = {A Noisy-Channel Account of Crosslinguistic Word-Order Variation},
  author = {Gibson, Edward and Piantadosi, Steven T. and Brink, Kimberly and Bergen, Leon and Lim, Eunice and Saxe, Rebecca},
  year = {2013},
  month = may,
  journal = {Psychological Science},
  volume = {24},
  number = {7},
  pages = {1079--1088},
  publisher = {SAGE Publications},
  doi = {10.1177/0956797612463705},
  bdsk-url-2 = {https://doi.org/10.1177/0956797612463705},
  date-added = {2022-04-19 22:26:57 -0400},
  date-modified = {2022-05-02 14:46:14 -0400},
  keywords = {noisy channel coding}
}

@article{gibson.e:2013pnas,
  title = {Rational Integration of Noisy Evidence and Prior Semantic Expectations in Sentence Interpretation},
  author = {Gibson, Edward and Bergen, Leon and Piantadosi, Steven T.},
  year = {2013},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {110},
  number = {20},
  pages = {8051--8056},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1216438110},
  keywords = {noisy channel coding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gibson.e2013pnas Rational integration of noisy evidence a.pdf}
}

@inproceedings{gildea.d:2007,
  title = {Optimizing Grammars for Minimum Dependency Length},
  booktitle = {Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics},
  author = {Gildea, Daniel and Temperley, David},
  year = {2007},
  pages = {184--191},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@article{gildea.d:2010,
  title = {Do Grammars Minimize Dependency Length?},
  author = {Gildea, Daniel and Temperley, David},
  year = {2010},
  journal = {Cognitive Science},
  volume = {34},
  number = {2},
  pages = {286--310},
  publisher = {Wiley Online Library},
  date-added = {2019-05-14 23:50:31 -0400},
  date-modified = {2019-06-17 21:56:52 -0400},
  project = {syntactic embedding},
  keywords = {DL minimization,projectivity}
}

@inproceedings{giulianelli.m:2023,
  title = {Information {{Value}}: {{Measuring Utterance Predictability}} as {{Distance}} from {{Plausible Alternatives}}},
  shorttitle = {Information {{Value}}},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Giulianelli, Mario and Wallbridge, Sarenne and Fern{\'a}ndez, Raquel},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {5633--5653},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.343},
  urldate = {2024-10-17},
  abstract = {We present information value, a measure which quantifies the predictability of an utterance relative to a set of plausible alternatives. We introduce a method to obtain interpretable estimates of information value using neural text generators, and exploit their psychometric predictive power to investigate the dimensions of predictability that drive human comprehension behaviour. Information value is a stronger predictor of utterance acceptability in written and spoken dialogue than aggregates of token-level surprisal and it is complementary to surprisal for predicting eye-tracked reading times.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/giulianelli.m2023 Information Value Measuring Utterance P.pdf}
}

@inproceedings{giulianelli.m:2024,
  title = {On the Proper Treatment of Tokenization in Psycholinguistics},
  booktitle = {Proceedings of the 2024 Conference on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Giulianelli, Mario and Malagutti, Luca and Gastaldi, Juan Luis and DuSell, Brian and Vieira, Tim and Cotterell, Ryan},
  year = {2024},
  publisher = {Association for Computational Linguistics},
  address = {Miami, Florida, USA}
}

@misc{giulianelli.m:2024arxiv,
  title = {Generalized {{Measures}} of {{Anticipation}} and {{Responsivity}} in {{Online Language Processing}}},
  author = {Giulianelli, Mario and Opedal, Andreas and Cotterell, Ryan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.10728},
  eprint = {2409.10728},
  publisher = {arXiv},
  urldate = {2024-10-13},
  abstract = {We introduce a generalization of classic information-theoretic measures of predictive uncertainty in online language processing, based on the simulation of expected continuations of incremental linguistic contexts. Our framework provides a formal definition of anticipatory and responsive measures, and it equips experimenters with the tools to define new, more expressive measures beyond standard next-symbol entropy and surprisal. While extracting these standard quantities from language models is convenient, we demonstrate that using Monte Carlo simulation to estimate alternative responsive and anticipatory measures pays off empirically: New special cases of our generalized formula exhibit enhanced predictive power compared to surprisal for human cloze completion probability as well as ELAN, LAN, and N400 amplitudes, and greater complementarity with surprisal in predicting reading times.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Theory,Mathematics - Information Theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/giulianelli.m2024arxiv Generalized Measures of Anticipation and.pdf}
}

@misc{giulianelli.m:2024psyarxiv,
  title = {Incremental {{Alternative Sampling}} as a {{Lens}} into the {{Temporal}} and {{Representational Resolution}} of {{Linguistic Prediction}}},
  author = {Giulianelli, Mario and Wallbridge, Sarenne and Cotterell, Ryan and Fern{\'a}ndez, Raquel},
  year = {2024},
  month = jul,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/fhp84},
  urldate = {2024-10-17},
  abstract = {This study presents a new model of processing difficulty rooted in resource allocation theory, Incremental Alternative Sampling (IAS).  Differential difficulty for a linguistic unit is estimated with respect to a set of plausible alternatives. Compared to a surprisal-based model, it prescribes a more efficient use of a comprehender's predicted continuations of partial linguistic stimuli thanks to (i) an expressive representation function that captures different levels of linguistic processing and (ii) the bootstrapping of long-horizon prediction error. Our results show that IAS estimates of processing difficulty, computed with autoregressive language models via Monte Carlo estimation, have greater predictive power than surprisal extracted from the same language models for most neural and behavioural responses under analysis---including reading times, event-related brain potentials, cloze and predictability judgements.  Perhaps more importantly, IAS estimates provide insight into the nature of the predictive mechanisms that generate those responses during language comprehension.  Variability in neural and behavioural responses is well explained by different combinations of the representational and temporal resolution of prediction. Processing difficulty calculated at varying representational domains reflects known relations to lexical, constructional, and structural levels of linguistic processing, and forecast horizons are determined by a combination of experimental task setup and naturalness of the stimulus. Beyond enriching psycholinguistic models, IAS can also provide insights into the information processing mechanisms of computational language models.  Our analysis of next-word surprisal under the lenses of IAS reveals that, despite the metric's seemingly narrow focus on the upcoming word, language model surprisal implicitly captures anticipatory processing of multiple future lexical items.},
  langid = {american},
  keywords = {Cognitive cost,Cognitive modelling,Incremental language processing,Information theory,Language comprehension,Language models,Linguistics,Predictive uncertainty}
}

@article{gleitman.l:2019,
  title = {The Impossibility of Language Acquisition (and How They Do It)},
  author = {Gleitman, Lila R. and Liberman, Mark Y. and McLemore, Cynthia A. and Partee, Barbara H.},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {1--24},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011718-011640},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011640},
  date-added = {2021-08-17 09:52:01 -0400},
  date-modified = {2021-08-17 09:52:03 -0400}
}

@misc{godfrey.j:1993switchboard,
  title = {Switchboard-1 Release 2},
  author = {Godfrey, John J. and Holliman, Edward},
  year = {1993},
  number = {LDC97S62},
  publisher = {Linguistic Data Consortium},
  doi = {10.35111/SW3H-RW02},
  bdsk-url-2 = {https://doi.org/10.35111/SW3H-RW02},
  date-added = {2022-05-06 14:21:03 -0400},
  date-modified = {2022-05-06 14:23:45 -0400},
  howpublished = {Web Download},
  keywords = {dataset,speech errors}
}

@inproceedings{gogate.v:2007,
  title = {{{SampleSearch}}: A Scheme That Searches for Consistent Samples},
  booktitle = {Proceedings of the Eleventh International Conference on Artificial Intelligence and Statistics},
  author = {Gogate, Vibhav and Dechter, Rina},
  editor = {Meila, Marina and Shen, Xiaotong},
  year = {2007-03-21/2007-03-24},
  series = {Proceedings of Machine Learning Research},
  volume = {2},
  pages = {147--154},
  publisher = {PMLR},
  address = {San Juan, Puerto Rico},
  abstract = {Sampling from belief networks which have a substantial number of zero probabilities is problematic. MCMC algorithms like Gibbs sampling do not converge and importance sampling schemes generate many zero weight samples that are rejected, yielding an inefficient sampling process (the rejection problem). In this paper, we propose to augment importance sampling with systematic constraint-satisfaction search in order to overcome the rejection problem. The resulting SampleSearch scheme can be made unbiased by using a computationally expensive weighting scheme. To overcome this an approximation is proposed such that the resulting estimator is asymptotically unbiased. Our empirical results demonstrate the potential of our new scheme.},
  date-added = {2022-05-05 09:35:36 -0400},
  date-modified = {2022-05-05 09:37:37 -0400},
  pdf = {http://proceedings.mlr.press/v2/gogate07a/gogate07a.pdf},
  keywords = {sample search}
}

@book{goldberg.y:2017,
  title = {Neural Network Methods for Natural Language Processing},
  author = {Goldberg, Yoav},
  year = {2017},
  publisher = {{Morgan and Claypool Publishers}},
  date-added = {2019-05-17 21:08:13 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  keywords = {machine learning,neural networks,recurrent neural networks,sequence to sequence models,word embeddings}
}

@misc{goldberg.y:2019,
  title = {Assessing {{BERT}}'s Syntactic Abilities},
  author = {Goldberg, Yoav},
  year = {2019},
  eprint = {1901.05287},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@article{goldstein.a:2021,
  title = {Thinking Ahead: Spontaneous Prediction in Context as a Keystone of Language in Humans and Machines},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  year = {2021},
  journal = {bioRxiv : the preprint server for biology},
  eprint = {https://www.biorxiv.org/content/early/2021/03/19/2020.12.02.403477.full.pdf},
  publisher = {Cold Spring Harbor Laboratory},
  doi = {10.1101/2020.12.02.403477},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM's next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM's contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.Competing Interest StatementThe authors have declared no competing interest.},
  bdsk-url-2 = {https://doi.org/10.1101/2020.12.02.403477},
  date-added = {2021-06-09 15:53:23 -0400},
  date-modified = {2021-06-09 15:53:24 -0400},
  elocation-id = {2020.12.02.403477},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/goldstein.a2021 Thinking ahead spontaneous prediction i.pdf}
}

@article{gomez.p:2008,
  title = {The Overlap Model: {{A}} Model of Letter Position Coding.},
  shorttitle = {The Overlap Model},
  author = {Gomez, Pablo and Ratcliff, Roger and Perea, Manuel},
  year = {2008},
  journal = {Psychological Review},
  volume = {115},
  number = {3},
  pages = {577--600},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/a0012667},
  urldate = {2023-12-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gomez.p2008 The overlap model A model of letter pos.pdf}
}

@inproceedings{goodkind.a:2018,
  title = {Predictive Power of Word Surprisal for Reading Times Is a Linear Function of Language Model Quality},
  booktitle = {Proceedings of the 8th {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}} ({{CMCL}} 2018)},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/w18-0102},
  bdsk-url-2 = {https://doi.org/10.18653/v1/w18-0102},
  date-added = {2021-11-29 10:00:16 -0500},
  date-modified = {2021-11-29 10:00:18 -0500}
}

@misc{goodkind.a:2021,
  title = {Local Word Statistics Affect Reading Times Independently of Surprisal},
  author = {Goodkind, Adam and Bicknell, Klinton},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2103.04469},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2103.04469},
  copyright = {Creative Commons Attribution 4.0 International},
  date-added = {2022-05-09 17:20:07 -0400},
  date-modified = {2022-05-09 17:21:18 -0400},
  keywords = {causal bottleneck},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/goodkind.a2021 Local word statistics affect reading tim.pdf}
}

@article{goodman.j:1999,
  title = {Semiring Parsing},
  author = {Goodman, Joshua},
  year = {1999},
  journal = {Computational Linguistics},
  volume = {25},
  number = {4},
  pages = {573--606}
}

@article{goodman.j:2001,
  title = {A Bit of Progress in Language Modeling},
  author = {Goodman, Joshua T.},
  year = {2001},
  journal = {Computer Speech \& Language},
  volume = {15},
  number = {4},
  pages = {403--434},
  issn = {0885-2308},
  doi = {10.1006/csla.2001.0174},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-06-09 15:53:09 -0400},
  opturl = {http://www.sciencedirect.com/science/article/pii/S0885230801901743}
}

@inproceedings{goodwin.e:2020,
  title = {Probing {{Linguistic Systematicity}}},
  booktitle = {Proceedings of the 58th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Goodwin, Emily and Sinha, Koustuv and O'Donnell, Timothy J.},
  year = {2020},
  month = jul,
  pages = {1958--1969},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.177},
  urldate = {2022-05-19},
  abstract = {Recently, there has been much interest in the question of whether deep natural language understanding (NLU) models exhibit systematicity, generalizing such that units like words make consistent contributions to the meaning of the sentences in which they appear. There is accumulating evidence that neural models do not learn systematically. We examine the notion of systematicity from a linguistic perspective, defining a set of probing tasks and a set of metrics to measure systematic behaviour. We also identify ways in which network architectures can generalize non-systematically, and discuss why such forms of generalization may be unsatisfying. As a case study, we perform a series of experiments in the setting of natural language inference (NLI). We provide evidence that current state-of-the-art NLU systems do not generalize systematically, despite overall high performance.},
  keywords = {natural language inference,natural logic,systematicity}
}

@inproceedings{gorla.j:2007,
  title = {Two Approaches for Building an Unsupervised Dependency Parser and Their Other Applications},
  booktitle = {{{PROCEEDINGS OF THE NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE}}},
  author = {Gorla, Jagadeesh and Goyal, Amit and Sangal, Rajeev},
  year = {2007},
  volume = {22},
  pages = {1860},
  date-added = {2020-04-23 11:09:55 -0400},
  date-modified = {2020-04-23 11:10:41 -0400},
  organization = {Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information,unsupervised parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gorla.j2007 Two approaches for building an unsupervi.pdf}
}

@article{gottwald.s:2020,
  title = {The Two Kinds of Free Energy and the {{Bayesian}} Revolution},
  author = {Gottwald, Sebastian and Braun, Daniel A.},
  year = {2020},
  month = dec,
  journal = {PLOS Computational Biology},
  volume = {16},
  number = {12},
  pages = {e1008420},
  publisher = {Public Library of Science},
  issn = {1553-7358},
  doi = {10.1371/journal.pcbi.1008420},
  urldate = {2022-07-11},
  abstract = {The concept of free energy has its origins in 19th century thermodynamics, but has recently found its way into the behavioral and neural sciences, where it has been promoted for its wide applicability and has even been suggested as a fundamental principle of understanding intelligent behavior and brain function. We argue that there are essentially two different notions of free energy in current models of intelligent agency, that can both be considered as applications of Bayesian inference to the problem of action selection: one that appears when trading off accuracy and uncertainty based on a general maximum entropy principle, and one that formulates action selection in terms of minimizing an error measure that quantifies deviations of beliefs and policies from given reference models. The first approach provides a normative rule for action selection in the face of model uncertainty or when information processing capabilities are limited. The second approach directly aims to formulate the action selection problem as an inference problem in the context of Bayesian brain theories, also known as Active Inference in the literature. We elucidate the main ideas and discuss critical technical and conceptual issues revolving around these two notions of free energy that both claim to apply at all levels of decision-making, from the high-level deliberation of reasoning down to the low-level information processing of perception.},
  langid = {english},
  keywords = {Decision making,Entropy,Free energy,Helmholtz free energy,Information processing,Kullback Leibler divergence,Optimization,Probability distribution},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gottwald.s2020 The two kinds of free energy and the Bay.pdf}
}

@misc{goyal.k:2021,
  title = {Exposing the Implicit Energy Networks behind Masked Language Models via {{Metropolis}}--{{Hastings}}},
  author = {Goyal, Kartik and Dyer, Chris and {Berg-Kirkpatrick}, Taylor},
  year = {2021},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2106.02736},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2106.02736},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 12:27:57 -0400},
  date-modified = {2022-04-09 00:57:41 -0400},
  keywords = {energy networks,masked language models,metropolis hastings},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/goyal.k2021 Exposing the implicit energy networks be.pdf}
}

@article{graf.t:2017,
  title = {Relative Clauses as a Benchmark for Minimalist Parsing},
  author = {Graf, Thomas and Monette, James and Zhang, Chong},
  year = {2017},
  month = jul,
  journal = {Journal of Language Modelling},
  volume = {5},
  number = {1},
  issn = {2299-8470, 2299-856X},
  doi = {10.15398/jlm.v5i1.157},
  urldate = {2022-09-30},
  abstract = {Minimalist grammars have been used recently in a series of papers to explain well-known contrasts in human sentence processing in terms of subtle structural differences. These proposals combine a top-down parser with complexity metrics that relate parsing difficulty to memory usage. So far, though, there has been no large-scale exploration of the space of viable metrics. Building on this earlier work, we compare the ability of 1600 metrics to derive several processing effects observed with relative clauses, many of which have been proven difficult to unify. We show that among those 1600 candidates, a few metrics (and only a few) can provide a unified account of all these contrasts. This is a welcome result for two reasons: First, it provides a novel account of extensively studied psycholinguistic data. Second, it significantly limits the number of viable metrics that may be applied to other phenomena, thus reducing theoretical indeterminacy.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/graf.t2017 Relative clauses as a benchmark for mini.pdf}
}

@incollection{grice.h:1975,
  title = {Logic and Conversation},
  booktitle = {Speech {{Acts}}},
  author = {Grice, H. P.},
  editor = {Cole, Peter and Morgan, Jerry L.},
  year = {1975},
  month = dec,
  pages = {41--58},
  publisher = {BRILL},
  doi = {10.1163/9789004368811_003},
  urldate = {2022-09-30},
  isbn = {978-90-04-36881-1 978-90-04-36857-6}
}

@misc{griffith.v:2012,
  title = {Quantifying Synergistic Mutual Information},
  author = {Griffith, Virgil and Koch, Christof},
  year = {2012},
  eprint = {1205.4265},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2020-07-06 08:48:33 -0400},
  date-modified = {2020-07-06 08:49:07 -0400},
  project = {information-compositionality}
}

@incollection{griffiths.t:2008primer,
  title = {A Primer on Probabilistic Inference},
  booktitle = {The Probabilistic Mind: Prospects for {{Bayesian}} Cognitive Science},
  author = {Griffiths, Thomas L. and Yuille, Alan},
  editor = {Chater, Nick and Oaksford, Mike},
  year = {2008},
  month = mar,
  pages = {0},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780199216093.003.0002},
  urldate = {2024-05-26},
  abstract = {This chapter provides the technical introduction to Bayesian methods. Probabilistic models of cognition are often referred to as Bayesian models, reflecting the central role that Bayesian inference plays in reasoning under uncertainty. It introduces the basic ideas of Bayesian inference and discusses how it can be used in different contexts. Probabilistic models provide a unique opportunity to develop a rational account of human cognition that combines statistical learning with structured representations. It recommends the EM algorithm and Markov chain Monte Carlo to estimate the parameters of models that incorporate latent variables, and to work with complicated probability distributions of the kind that often arise in Bayesian inference.},
  isbn = {978-0-19-921609-3}
}

@article{griffiths.t:2015,
  title = {Rational Use of Cognitive Resources: {{Levels}} of Analysis between the Computational and the Algorithmic},
  author = {Griffiths, Thomas L. and Lieder, Falk and Goodman, Noah D.},
  year = {2015},
  journal = {Topics in cognitive science},
  volume = {7},
  number = {2},
  pages = {217--229},
  publisher = {Wiley Online Library},
  doi = {10.1111/tops.12142},
  isbn = {1756-8757}
}

@article{grodner.d:2003,
  title = {Against Repair-Based Reanalysis in Sentence Comprehension},
  author = {Grodner, Daniel and Gibson, Edward and Argaman, Vered and Babyonyshev, Maria},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {141--166},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1023/a:1022496223965},
  bdsk-url-2 = {https://doi.org/10.1023/a:1022496223965},
  date-added = {2021-03-18 11:28:51 -0400},
  date-modified = {2021-03-18 11:30:16 -0400},
  keywords = {reading time,self-paced reading}
}

@misc{groeneveld.d:2024OLMo,
  title = {{{OLMo}}: Accelerating the Science of Language Models},
  shorttitle = {{{OLMo}}},
  author = {Groeneveld, Dirk and Beltagy, Iz and Walsh, Pete and Bhagia, Akshita and Kinney, Rodney and Tafjord, Oyvind and Jha, Ananya Harsh and Ivison, Hamish and Magnusson, Ian and Wang, Yizhong and Arora, Shane and Atkinson, David and Authur, Russell and Chandu, Khyathi Raghavi and Cohan, Arman and Dumas, Jennifer and Elazar, Yanai and Gu, Yuling and Hessel, Jack and Khot, Tushar and Merrill, William and Morrison, Jacob and Muennighoff, Niklas and Naik, Aakanksha and Nam, Crystal and Peters, Matthew E. and Pyatkin, Valentina and Ravichander, Abhilasha and Schwenk, Dustin and Shah, Saurabh and Smith, Will and Strubell, Emma and Subramani, Nishant and Wortsman, Mitchell and Dasigi, Pradeep and Lambert, Nathan and Richardson, Kyle and Zettlemoyer, Luke and Dodge, Jesse and Lo, Kyle and Soldaini, Luca and Smith, Noah A. and Hajishirzi, Hannaneh},
  year = {2024},
  month = feb,
  number = {arXiv:2402.00838},
  eprint = {2402.00838},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-15},
  abstract = {Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@book{grune.d:2008,
  title = {Parsing Techniques},
  author = {Grune, Dick and Jacobs, Ceriel J. H.},
  year = {2008},
  publisher = {Springer New York},
  doi = {10.1007/978-0-387-68954-8},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-68954-8},
  date-added = {2022-03-31 10:23:54 -0400},
  date-modified = {2022-03-31 10:24:43 -0400},
  keywords = {book,parsing}
}

@misc{grunwald.p:2004,
  title = {Shannon Information and Kolmogorov Complexity},
  author = {Grunwald, Peter and Vitanyi, Paul},
  year = {2004},
  eprint = {cs/0410002},
  archiveprefix = {arXiv},
  date-added = {2019-09-13 08:19:19 -0400},
  date-modified = {2019-09-13 08:20:10 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity,mutual information,rate-distortion theory,shannon entropy}
}

@misc{grunwald.p:2004a,
  title = {A Tutorial Introduction to the Minimum Description Length Principle},
  author = {Grunwald, Peter},
  year = {2004},
  eprint = {math/0406077},
  archiveprefix = {arXiv},
  date-added = {2020-02-20 11:40:38 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {minimum description length,mutual information},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/grunwald.p2004a A tutorial introduction to the minimum d.pdf}
}

@article{gryszka.k:2021,
  title = {From Biased Coin to Any Discrete Distribution},
  author = {Gryszka, Karol},
  year = {2021},
  month = sep,
  journal = {Periodica Mathematica Hungarica},
  volume = {83},
  number = {1},
  pages = {71--80},
  issn = {1588-2829},
  doi = {10.1007/s10998-020-00363-w},
  urldate = {2023-12-24},
  abstract = {In this note we construct an algorithm generating any discrete distribution with an arbitrary coin (and, as a result, with arbitrary initial distribution). The coin need not be fair and the target distribution can be supported on a countable set.},
  langid = {english},
  keywords = {60E05,biased coin,coin flip,discrete distribution,Discrete probability measure,expected value,Primary 65C10,Secondary 60A10,simulation of distribution},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/gryszka.k2021 From biased coin to any discrete distrib.pdf}
}

@inproceedings{gulordava.k:2018,
  title = {Colorless Green Recurrent Networks Dream Hierarchically},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
  year = {2018},
  pages = {1195--1205},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1108},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1108}
}

@article{gutknecht.a:2021,
  title = {Bits and Pieces: Understanding Information Decomposition from Part-Whole Relationships and Formal Logic},
  author = {Gutknecht, A. J. and Wibral, M. and Makkeh, A.},
  year = {2021},
  month = jul,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {477},
  number = {2251},
  pages = {20210110},
  publisher = {The Royal Society},
  doi = {10.1098/rspa.2021.0110},
  bdsk-url-2 = {https://doi.org/10.1098/rspa.2021.0110},
  date-added = {2022-04-18 11:14:51 -0400},
  date-modified = {2022-04-18 11:15:12 -0400},
  keywords = {partial information decomposition}
}

@book{hacking.i:2006,
  title = {The Emergence of Probability: {{A}} Philosophical Study of Early Ideas about Probability, Induction and Statistical Inference},
  author = {Hacking, Ian},
  year = {2006},
  edition = {2},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9780511817557},
  date-added = {2021-02-16 15:11:17 -0500},
  date-modified = {2021-02-16 15:11:20 -0500}
}

@inproceedings{hagiwara.m:2020,
  title = {{{GitHub Typo Corpus}}: {{A Large-Scale Multilingual Dataset}} of {{Misspellings}} and {{Grammatical Errors}}},
  shorttitle = {{{GitHub Typo Corpus}}},
  booktitle = {Proceedings of the {{Twelfth Language Resources}} and {{Evaluation Conference}}},
  author = {Hagiwara, Masato and Mita, Masato},
  year = {2020},
  month = may,
  pages = {6761--6768},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  urldate = {2023-08-24},
  abstract = {The lack of large-scale datasets has been a major hindrance to the development of NLP tasks such as spelling correction and grammatical error correction (GEC). As a complementary new resource for these tasks, we present the GitHub Typo Corpus, a large-scale, multilingual dataset of misspellings and grammatical errors along with their corrections harvested from GitHub, a large and popular platform for hosting and sharing git repositories. The dataset, which we have made publicly available, contains more than 350k edits and 65M characters in more than 15 languages, making it the largest dataset of misspellings to date. We also describe our process for filtering true typo edits based on learned classifiers on a small annotated subset, and demonstrate that typo edits can be identified with F1 0.9 using a very simple classifier with only three features. The detailed analyses of the dataset show that existing spelling correctors merely achieve an F-measure of approx. 0.5, suggesting that the dataset serves as a new, rich source of spelling errors that complement existing datasets.},
  isbn = {979-10-95546-34-4},
  langid = {english},
  keywords = {grammatical error correction,spelling correction,typos},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hagiwara.m2020 GitHub Typo Corpus A Large-Scale Multil.pdf}
}

@inproceedings{hahn.m:2018cogsci,
  title = {An Information-Theoretic Explanation of Adjective Ordering Preferences},
  booktitle = {Proceedings of the 40th Annual Meeting of the {{Cognitive Science Society}}},
  author = {Hahn, Michael and Degen, Judith and Goodman, Noah and Jurafsky, Dan and Futrell, Richard},
  editor = {Kalish, Charles and Rau, Martina and Zhu, Jerry and Rogers, Timothy},
  year = {2018},
  pages = {1766--1772},
  publisher = {Cognitive Science Society},
  address = {Madison, Wisconsin, USA},
  abstract = {Across languages, adjectives are subject to ordering restrictions. Recent research shows that these are predicted by adjective subjectivity, but the question remains open why this is the case. We first conduct a corpus study and not only replicate the subjectivity effect, but also find a previously undocumented effect of mutual information between adjectives and nouns. We then describe a rational model of adjective use in which listeners explicitly reason about judgments made by different speakers, formalizing the notion of subjectivity as agreement between speakers. We show that, once incremental processing is combined with memory limitations, our model predicts effects both of subjectivity and mutual information. We confirm the adequacy of our model by evaluating it on corpus data, finding that it correctly predicts ordering in unseen data with an accuracy of 96.2\%. This suggests that adjective ordering can be explained by general principles of human communication and language processing.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2018cogsci An information-theoretic explanation of.pdf}
}

@unpublished{hahn.m:2019,
  type = {Unpublished Manuscript},
  title = {Estimating Predictive Rate-Distortion Curves via Neural Variational Inference},
  author = {Hahn, Michael and Futrell, Richard},
  year = {2019},
  date-added = {2019-06-11 14:15:47 -0400},
  date-modified = {2019-06-17 21:56:11 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory,variational inference}
}

@inproceedings{hahn.m:2019cogsci,
  title = {Character-Based {{Surprisal}} as a {{Model}} of {{Reading Difficulty}} in the {{Presence}} of {{Errors}}.},
  booktitle = {Proceedings of the 41th {{Annual Meeting}} of the {{Cognitive Science Society}}, {{CogSci}} 2019: {{Creativity}} + {{Cognition}} + {{Computation}}, {{Montreal}}, {{Canada}}, {{July}} 24-27, 2019},
  author = {Hahn, Michael and Keller, Frank and Bisk, Yonatan and Belinkov, Yonatan},
  year = {2019},
  pages = {401--407},
  address = {Montr{\'e}al, Canada},
  doi = {10.48550/arXiv.1902.00595},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2019cogsci Character-based Surprisal as a Model of.pdf}
}

@article{hahn.m:2021,
  title = {Modeling Word and Morpheme Order in Natural Language as an Efficient Trade-off of Memory and Surprisal.},
  author = {Hahn, Michael and Degen, Judith and Futrell, Richard},
  year = {2021},
  month = apr,
  journal = {Psychological Review},
  volume = {128},
  number = {4},
  pages = {726},
  publisher = {US: American Psychological Association},
  issn = {1939-1471},
  doi = {10.1037/rev0000269},
  urldate = {2023-03-27}
}

@article{hahn.m:2022,
  title = {Morpheme Ordering across Languages Reflects Optimization for Processing Efficiency},
  author = {Hahn, Michael and Mathew, Rebecca and Degen, Judith},
  year = {2022},
  month = feb,
  journal = {Open Mind},
  volume = {5},
  pages = {208--232},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00051},
  urldate = {2023-03-27},
  abstract = {The ordering of morphemes in a word displays well-documented regularities across languages. Previous work has explained these in terms of notions such as semantic scope, relevance, and productivity. Here, we test a recently formulated processing theory of the ordering of linguistic units, the efficient tradeoff hypothesis (Hahn et al., 2021). The claim of the theory is that morpheme ordering can partly be explained by the optimization of a tradeoff between memory and surprisal. This claim has received initial empirical support from two languages. In this work, we test this idea more extensively using data from four additional agglutinative languages with significant amounts of morphology, and by considering nouns in addition to verbs. We find that the efficient tradeoff hypothesis predicts ordering in most cases with high accuracy, and accounts for cross-linguistic regularities in noun and verb inflection. Our work adds to a growing body of work suggesting that many ordering properties of language arise from a pressure for efficient language processing.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2022 Morpheme ordering across languages refle.pdf}
}

@article{hahn.m:2022PNAS,
  title = {A Resource-Rational Model of Human Processing of Recursive Linguistic Structure},
  author = {Hahn, Michael and Futrell, Richard and Levy, Roger and Gibson, Edward},
  year = {2022},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {43},
  pages = {e2122602119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2122602119},
  urldate = {2022-11-28},
  abstract = {A major goal of psycholinguistic theory is to account for the cognitive constraints limiting the speed and ease of language comprehension and production. Wide-ranging evidence demonstrates a key role for linguistic expectations: A word's predictability, as measured by the information-theoretic quantity of surprisal, is a major determinant of processing difficulty. But surprisal, under standard theories, fails to predict the difficulty profile of an important class of linguistic patterns: the nested hierarchical structures made possible by recursion in human language. These nested structures are better accounted for by psycholinguistic theories of constrained working memory capacity. However, progress on theory unifying expectation-based and memory-based accounts has been limited. Here we present a unified theory of a rational trade-off between precision of memory representations with ease of prediction, a scaled-up computational implementation using contemporary machine learning methods, and experimental evidence in support of the theory's distinctive predictions. We show that the theory makes nuanced and distinctive predictions for difficulty patterns in nested recursive structures predicted by neither expectation-based nor memory-based theories alone. These predictions are confirmed 1) in two language comprehension experiments in English, and 2) in sentence completions in English, Spanish, and German. More generally, our framework offers computationally explicit theory and methods for understanding how memory constraints and prediction interact in human language comprehension and production.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2022PNAS A resource-rational model of human proce 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2022PNAS A resource-rational model of human proce.pdf}
}

@misc{hahn.m:2023arxiv,
  title = {A Theory of Emergent In-Context Learning as Implicit Structure Induction},
  author = {Hahn, Michael and Goyal, Navin},
  year = {2023},
  month = mar,
  number = {arXiv:2303.07971},
  eprint = {2303.07971},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-23},
  abstract = {Scaling large language models (LLMs) leads to an emergent capacity to learn in-context from example demonstrations. Despite progress, theoretical understanding of this phenomenon remains limited. We argue that in-context learning relies on recombination of compositional operations found in natural language data. We derive an information-theoretic bound showing how in-context learning abilities arise from generic next-token prediction when the pretraining distribution has sufficient amounts of compositional structure, under linguistically motivated assumptions. A second bound provides a theoretical justification for the empirical success of prompting LLMs to output intermediate steps towards an answer. To validate theoretical predictions, we introduce a controlled setup for inducing in-context learning; unlike previous approaches, it accounts for the compositional nature of language. Trained transformers can perform in-context learning for a range of tasks, in a manner consistent with the theoretical results. Mirroring real-world LLMs in a miniature setup, in-context learning emerges when scaling parameters and data, and models perform better when prompted to output intermediate steps. Probing shows that in-context learning is supported by a representation of the input's compositional structure. Taken together, these results provide a step towards theoretical understanding of emergent behavior in large language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@article{hahn.m:2023cognition,
  title = {Modeling Task Effects in Human Reading with Neural Network-Based Attention},
  author = {Hahn, Michael and Keller, Frank},
  year = {2023},
  month = jan,
  journal = {Cognition},
  volume = {230},
  pages = {105289},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105289},
  urldate = {2023-03-23},
  abstract = {Research on human reading has long documented that reading behavior shows task-specific effects, but it has been challenging to build general models predicting what reading behavior humans will show in a given task. We introduce NEAT, a computational model of the allocation of attention in human reading, based on the hypothesis that human reading optimizes a tradeoff between economy of attention and success at a task. Our model is implemented using contemporary neural network modeling techniques, and makes explicit and testable predictions about how the allocation of attention varies across different tasks. We test this in an eyetracking study comparing two versions of a reading comprehension task, finding that our model successfully accounts for reading behavior across the tasks. Our work thus provides evidence that task effects can be modeled as optimal adaptation to task demands.},
  langid = {english},
  keywords = {Computational modeling,Reading,Task effects},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hahn.m2023cognition Modeling task effects in human reading w.pdf}
}

@inproceedings{hale.j:2001,
  title = {A Probabilistic {{Earley}} Parser as a Psycholinguistic Model},
  booktitle = {Second Meeting of the {{North American}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Hale, John T.},
  year = {2001},
  date-modified = {2022-04-20 13:49:59 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hale.j2001 A probabilistic Earley parser as a psych.pdf}
}

@article{hale.j:2003,
  title = {The Information Conveyed by Words in Sentences},
  author = {Hale, John T.},
  year = {2003},
  journal = {Journal of Psycholinguistic Research},
  volume = {32},
  number = {2},
  pages = {101--123},
  publisher = {Springer},
  doi = {10.1023/A:1022492123056},
  date-added = {2021-04-08 14:24:37 -0400},
  date-modified = {2022-04-20 13:49:43 -0400},
  keywords = {entropy reduction}
}

@phdthesis{hale.j:2003phd,
  title = {Grammar, Uncertainty and Sentence Processing},
  author = {Hale, John T.},
  year = {2003},
  address = {Baltimore, Maryland},
  abstract = {Toward a probabilistic theory of human sentence processing, this dissertation proposes a definition of computational work done in the course of analyzing sentences generated by formal grammars. It applies the idea of entropy from information theory to the set of derivations compatible with an initial substring of a sentence. Given a probabilistic grammar, this permits the set of such compatible derivations to be viewed as a random variable, and the change in uncertainty about the outcomes to be calculated. This definition of computational work is examined as a cognitive model of human sentence processing difficulty. To apply the model, a variety of existing syntactic proposals for English sentences are cast as probabilistic Generalized Phrase Structure Grammars (Gazdar et al., 1985) and probabilistic Minimalist Grammars (Stabler, 1997). It is shown that the amount of predicted processing effort in relative clauses correlates with the Accessibility Hierarchy of relativized grammatical relations (Keenan and Comrie, 1977) on a Kaynian (1994) view of relative clause structure. Results from three new on-line sentence reading experiments suggest that while genitivity has the role suggested by the Accessibility Hierarchy, extraction from oblique does not. Evidence is also found for a direct object/indirect object processing asymmetry, which can be derived from the proposed cognitive model under the assumption of a lexicalized probabilistic grammar.},
  date-added = {2022-04-14 15:27:00 -0400},
  date-modified = {2022-04-14 15:31:29 -0400},
  isbn = {978-0-496-55064-7},
  school = {Johns Hopkins University},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hale.j2003phd Grammar, uncertainty and sentence proces.pdf}
}

@inproceedings{hale.j:2004,
  title = {The Information-Processing Difficulty of Incremental Parsing},
  booktitle = {Proceedings of the Workshop on Incremental Parsing: {{Bringing}} Engineering and Cognition Together},
  author = {Hale, John T.},
  year = {2004},
  month = jul,
  pages = {58--65},
  publisher = {Association for Computational Linguistics},
  address = {Barcelona, Spain},
  date-added = {2022-04-14 13:31:29 -0400},
  date-modified = {2022-04-20 13:50:05 -0400}
}

@article{hale.j:2006,
  title = {Uncertainty about the Rest of the Sentence},
  author = {Hale, John T.},
  year = {2006},
  journal = {Cognitive Science},
  volume = {30},
  number = {4},
  pages = {643--672},
  publisher = {Wiley},
  doi = {10.1207/s15516709cog0000_64},
  bdsk-url-2 = {https://doi.org/10.1207/s15516709cog0000{$_6$}4},
  date-added = {2021-03-18 10:37:45 -0400},
  date-modified = {2022-04-20 13:50:10 -0400},
  keywords = {entropy reduction,processing}
}

@article{hale.j:2011,
  title = {What a Rational Parser Would Do},
  author = {Hale, John T.},
  year = {2011},
  month = apr,
  journal = {Cognitive Science},
  volume = {35},
  number = {3},
  pages = {399--443},
  issn = {03640213},
  doi = {10.1111/j.1551-6709.2010.01145.x},
  urldate = {2022-10-24},
  abstract = {This article examines cognitive process models of human sentence comprehension based on the idea of informed search. These models are rational in the sense that they strive to find a good syntactic analysis quickly. Informed search derives a new account of garden pathing that handles traditional counterexamples. It supports a symbolic explanation for local coherence as well as an algorithmic account of entropy reduction. The models are expressed in a broad framework for theories of human sentence comprehension.},
  langid = {english},
  keywords = {entropy reduction,parsing algorithm,rational analysis,sentence processing}
}

@book{hale.j:2014,
  title = {Automaton Theories of Human Sentence Comprehension},
  author = {Hale, John T.},
  year = {2014},
  month = sep,
  series = {{{CSLI Studies}} in {{Computational Linguistics}}},
  publisher = {{CSLI Publications, Center for the Study of Language and Information}},
  address = {Stanford, CA},
  urldate = {2022-07-01},
  abstract = {Different kinds of grammars may actually be used in models of perceptual processing. By relating grammars to cognitive architecture, John T. Hale shows step-by-step how incremental parsing works and how specific learning rules might lead to frequency-sensitive preferences. Along the way, this book reconsiders garden-pathing, the parallel/serial distinction and information-theoretical complexity metrics, such as surprisal. This book is a must for cognitive scientists of language.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hale.j2014 Automaton theories of human sentence com.pdf}
}

@article{hale.j:2016,
  title = {Information-Theoretical {{Complexity Metrics}}},
  author = {Hale, John T.},
  year = {2016},
  journal = {Language and Linguistics Compass},
  volume = {10},
  number = {9},
  pages = {397--412},
  issn = {1749-818X},
  doi = {10.1111/lnc3.12196},
  urldate = {2024-05-03},
  abstract = {Information-theoretical complexity metrics are auxiliary hypotheses that link theories of parsing and grammar to potentially observable measurements such as reading times and neural signals. This review article considers two such metrics, Surprisal and Entropy Reduction, which are respectively built upon the two most natural notions of `information value' for an observed event (Blachman ). This review sketches their conceptual background and touches on their relationship to other theories in cognitive science. It characterizes them as `lenses' through which theorists `see' the information-processing consequences of linguistic grammars. While these metrics are not themselves parsing algorithms, the review identifies candidate mechanisms that have been proposed for both of them.},
  copyright = {{\copyright} 2016 The Author Language and Linguistics Compass {\copyright} 2016 John Wiley \& Sons Ltd},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hale.j2016 Information-theoretical Complexity Metri.pdf}
}

@inproceedings{hale.j:2018,
  title = {Finding Syntax in Human Encephalography with Beam Search},
  booktitle = {Proceedings of the 56th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Hale, John T. and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan},
  year = {2018},
  month = jul,
  pages = {2727--2736},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1254},
  urldate = {2023-08-18},
  abstract = {Recurrent neural network grammars (RNNGs) are generative models of (tree , string ) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hale.j2018 Finding syntax in human encephalography.pdf}
}

@inproceedings{hall.d:2014,
  title = {On Substance in Phonology},
  booktitle = {Proceedings of the 2014 Annual Conference of the {{Canadian Linguistic Association}}},
  author = {Hall, Daniel Currie},
  year = {2014},
  date-added = {2019-06-17 08:36:30 -0400},
  date-modified = {2019-06-17 08:37:21 -0400},
  keywords = {substance free phonology}
}

@article{halle.m:1994,
  title = {Some Key Features of {{Distributed Morphology}}},
  author = {Halle, Morris and Marantz, Alec},
  year = {1994},
  journal = {MIT working papers in linguistics},
  volume = {21},
  number = {275},
  pages = {88},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@inproceedings{haller.p:2024,
  title = {On Language Models' Cognitive Biases in Reading Time Prediction},
  booktitle = {{{ICML}} 2024 Workshop on {{LLMs}} and Cognition},
  author = {Haller, Patrick and Bolliger, Lena Sophia and J{\"a}ger, Lena Ann},
  year = {2024},
  address = {Vienna, Austria}
}

@misc{hamilton.w:2017,
  title = {Representation Learning on Graphs: {{Methods}} and Applications},
  author = {Hamilton, William L. and Ying, Rex and Leskovec, Jure},
  year = {2017},
  eprint = {1709.05584},
  primaryclass = {cs.SI},
  archiveprefix = {arXiv},
  date-added = {2021-08-03 10:09:17 -0400},
  date-modified = {2021-08-03 10:10:59 -0400},
  project = {syntactic embedding},
  keywords = {graph embedding}
}

@inproceedings{hao.y:2020,
  title = {Probabilistic Predictions of People Perusing: Evaluating Metrics of Language Model Performance for Psycholinguistic Modeling},
  shorttitle = {Probabilistic Predictions of People Perusing},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Hao, Yiding and Mendelsohn, Simon and Sterneck, Rachel and Martinez, Randi and Frank, Robert},
  year = {2020},
  month = nov,
  pages = {75--86},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.cmcl-1.10},
  urldate = {2022-10-13},
  abstract = {By positing a relationship between naturalistic reading times and information-theoretic surprisal, surprisal theory (Hale, 2001; Levy, 2008) provides a natural interface between language models and psycholinguistic models. This paper re-evaluates a claim due to Goodkind and Bicknell (2018) that a language model's ability to model reading times is a linear function of its perplexity. By extending Goodkind and Bicknell's analysis to modern neural architectures, we show that the proposed relation does not always hold for Long Short-Term Memory networks, Transformers, and pre-trained models. We introduce an alternate measure of language modeling performance called predictability norm correlation based on Cloze probabilities measured from human subjects. Our new metric yields a more robust relationship between language model quality and psycholinguistic modeling performance that allows for comparison between models with different training configurations.},
  keywords = {psychometrics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hao.y2020 Probabilistic predictions of people peru.pdf}
}

@incollection{harb.b:2005,
  title = {Approximating the Best-Fit Tree under {{L}} p Norms},
  booktitle = {Approximation, Randomization and Combinatorial Optimization. {{Algorithms}} and Techniques},
  author = {Harb, Boulos and Kannan, Sampath and McGregor, Andrew},
  year = {2005},
  pages = {123--133},
  publisher = {Springer},
  date-added = {2019-07-17 17:56:30 -0400},
  date-modified = {2019-07-17 17:56:53 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{harley.h:2002,
  title = {Person and Number in Pronouns: {{A}} Feature-Geometric Analysis},
  author = {Harley, Heidi and Ritter, Elizabeth},
  year = {2002},
  journal = {Language},
  volume = {78},
  number = {3},
  pages = {482--526},
  publisher = {Linguistic Society of America},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:21:59 -0400},
  project = {Icelandic gluttony},
  keywords = {feature geometry,phi features,pronouns}
}

@inproceedings{harremoes.p:2007,
  title = {The Information Bottleneck Revisited or How to Choose a Good Distortion Measure},
  booktitle = {2007 {{IEEE}} International Symposium on Information Theory},
  author = {Harremo{\"e}s, Peter and Tishby, Naftali},
  year = {2007},
  pages = {566--570},
  doi = {10.1109/ISIT.2007.4557285},
  date-added = {2019-05-15 00:02:01 -0400},
  date-modified = {2020-07-22 11:17:35 -0400},
  organization = {IEEE},
  project = {syntactic embedding},
  keywords = {information bottleneck,KL divergence},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/harremoes.p2007 The information bottleneck revisited or.pdf}
}

@article{harringtonstack.c:2018,
  title = {A Failure to Replicate Rapid Syntactic Adaptation in Comprehension},
  author = {Harrington Stack, Caoimhe M. and James, Ariel N. and Watson, Duane G.},
  year = {2018},
  month = aug,
  journal = {Memory \& Cognition},
  volume = {46},
  number = {6},
  pages = {864--877},
  issn = {1532-5946},
  doi = {10.3758/s13421-018-0808-6},
  urldate = {2024-05-26},
  abstract = {Language comprehension requires successfully navigating linguistic variability. One hypothesis for how listeners manage variability is that they rapidly update their expectations of likely linguistic events in new contexts. This process, called adaptation, allows listeners to better predict the upcoming linguistic input. In previous work, Fine, Jaeger, Farmer, and Qian (PLoS ONE, 8, e77661, 2013) found evidence for syntactic adaptation. Subjects repeatedly encountered sentences in which a verb was temporarily ambiguous between main verb (MV) and reduced relative clause (RC) interpretations. They found that subjects who had higher levels of exposure to the unexpected RC interpretation of the sentences had an easier time reading the RC sentences but a more difficult time reading the MV sentences. They concluded that syntactic adaptation occurs rapidly in unexpected structures and also results in difficulty with processing the previously expected alternative structures. This article presents two experiments. Experiment 1 was designed as a follow-up to Fine et al.'s study and failed to find evidence of adaptation. A power analysis of Fine et al.'s raw data revealed that a similar study would need double the items and four times the subjects to reach 95\% power. In Experiment 2 we designed a close replication of Fine et al.'s experiment using these sample size guidelines. No evidence of rapid syntactic adaptation was found in this experiment. The failure to find evidence of adaptation in both experiments calls into question the robustness of the effect.},
  langid = {english},
  keywords = {Adaptation,Replication,Sentence processing,Syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/harringtonstack.c2018 A failure to replicate rapid syntactic a.pdf}
}

@book{harris.z:1951,
  title = {Structural Linguistics},
  author = {Harris, Zellig},
  year = {1951},
  publisher = {University of Chicago Press},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/harris.z1951 Structural linguistics.pdf}
}

@book{harris.z:1991,
  title = {A {{Theory}} of {{Language}} and {{Information}}: {{A Mathematical Approach}}},
  shorttitle = {A {{Theory}} of {{Language}} and {{Information}}},
  author = {Harris, Zellig},
  year = {1991},
  month = feb,
  publisher = {Oxford University Press},
  doi = {10.1093/oso/9780198242246.001.0001},
  urldate = {2024-04-11},
  abstract = {Abstract. Professor Harris presents a formal theory of language structure, in which syntax is characterized as an orderly system of departures from random},
  isbn = {978-1-383-01348-1},
  langid = {english}
}

@inproceedings{hartmann.j:2016,
  title = {Evading Agreement: {{A}} New Perspective on Low Nominative Agreement in {{Icelandic}}},
  booktitle = {Proceedings of the 46th Annual Meeting of the North East Linguistic Society ({{NELS}})},
  author = {Hartmann, Jutta M and Heycock, Caroline},
  editor = {Hammerly, Christopher and Prickett, Brandon},
  year = {2016},
  volume = {2},
  pages = {67--80},
  publisher = {GLSA Publications},
  date-added = {2020-01-22 18:01:44 -0500},
  date-modified = {2020-02-01 19:41:52 -0500},
  project = {Icelandic gluttony},
  keywords = {invisible dative,low nominative}
}

@article{hartmann.j:2022,
  title = {Person Effects in Agreement with {{Icelandic}} Low Nominatives: {{An}} Experimental Investigation},
  shorttitle = {Person Effects in Agreement with {{Icelandic}} Low Nominatives},
  author = {Hartmann, Jutta M. and Heycock, Caroline},
  year = {2022},
  month = dec,
  journal = {Natural Language \& Linguistic Theory},
  issn = {1573-0859},
  doi = {10.1007/s11049-022-09564-z},
  urldate = {2023-01-12},
  abstract = {This paper investigates agreement---in particular person agreement---in two configurations in Icelandic where there are two potential controllers of agreement and where at least in some cases agreement is with the lower of the two (a~``low nominative''). One case is the Dative-Nominative construction, where there is a dative subject and a lower nominative argument. The other is the Specificational Copular Clause (SCC) construction, where there are two nominative arguments. A much-discussed aspect of agreement in the former case is that agreement in number with the low nominative is generally possible, but agreement in person is at best highly restricted, leading in some cases to ineffability. This person effect has been claimed to be ameliorated by syncretism in the agreement paradigm, but there is limited data available substantiating this effect, which is however crucial to deciding between two recent types of account. This paper reports on a pair of experimental rating studies on the Dat-Nom and SCC configurations in Icelandic. We show that, taken together, the two sets of data provide evidence against the Person Licensing Condition and in favour of an account of the Dat-Nom construction in terms of morphological conflict arising from double agreement, although we show that the ameliorating effect of morphological syncretism, while real, is limited. Further, we show that there is no evidence of double agreement in the copular clauses investigated. We argue that full agreement with the low nominative here arises if the first nominal can move out of the domain of agreement entirely. The possibility of agreement with the initial nominal we suggest indicates that nominatives, unlike datives, cause the search of the agreement probe to terminate.},
  langid = {english},
  keywords = {Agreement,Copula,hierarchy effects,icelandic,Icelandic,Person Case Constraint,Person Licensing Condition,Syncretism},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hartmann.j2022 Person effects in agreement with Iceland.pdf}
}

@article{hartshorne.j:2015,
  title = {When {{Does Cognitive Functioning Peak}}? {{The Asynchronous Rise}} and {{Fall}} of {{Different Cognitive Abilities Across}} the {{Life Span}}},
  shorttitle = {When {{Does Cognitive Functioning Peak}}?},
  author = {Hartshorne, Joshua K. and Germine, Laura T.},
  year = {2015},
  month = apr,
  journal = {Psychological Science},
  volume = {26},
  number = {4},
  pages = {433--443},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1177/0956797614567339},
  urldate = {2024-03-19},
  abstract = {Understanding how and when cognitive change occurs over the life span is a prerequisite for understanding normal and abnormal development and aging. Most studies of cognitive change are constrained, however, in their ability to detect subtle, but theoretically informative life-span changes, as they rely on either comparing broad age groups or sparse sampling across the age range. Here, we present convergent evidence from 48,537 online participants and a comprehensive analysis of normative data from standardized IQ and memory tests. Our results reveal considerable heterogeneity in when cognitive abilities peak: Some abilities peak and begin to decline around high school graduation; some abilities plateau in early adulthood, beginning to decline in subjects' 30s; and still others do not peak until subjects reach their 40s or later. These findings motivate a nuanced theory of maturation and age-related decline, in which multiple, dissociable factors differentially affect different domains of cognition.},
  langid = {english},
  keywords = {cognitive aging},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hartshorne.j2015 When Does Cognitive Functioning Peak Th.pdf}
}

@book{hastie.t:1990,
  title = {Generalized Additive Models},
  author = {Hastie, T.J. and Tibshirani, R.J.},
  year = {1990},
  month = oct,
  publisher = {Routledge},
  doi = {10.1201/9780203753781},
  bdsk-url-2 = {https://doi.org/10.1201/9780203753781},
  date-added = {2022-01-05 22:06:57 -0500},
  date-modified = {2022-01-05 22:10:25 -0500}
}

@article{havelka.j:2007,
  title = {Mathematical Properties of Dependency Trees and Their Application to Natural Language Syntax},
  author = {Havelka, Ji{\v r}{\'i}},
  year = {2007},
  publisher = {Univerzita Karlova, Matematicko-fyzik{\'a}ln{\'i} fakulta},
  date-added = {2020-02-26 18:33:58 -0500},
  date-modified = {2020-02-26 18:36:25 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projectivity}
}

@inproceedings{heafield.k:2011,
  title = {{{KenLM}}: Faster and Smaller Language Model Queries},
  shorttitle = {Kenlm},
  booktitle = {Proceedings of the {{Sixth Workshop}} on {{Statistical Machine Translation}}},
  author = {Heafield, Kenneth},
  year = {2011},
  month = jul,
  pages = {187--197},
  publisher = {Association for Computational Linguistics},
  address = {Edinburgh, Scotland},
  urldate = {2023-03-03}
}

@article{heathcote.a:2012,
  title = {Linear Deterministic Accumulator Models of Simple Choice},
  author = {Heathcote, Andrew and Love, Jonathon},
  year = {2012},
  journal = {Frontiers in Psychology},
  volume = {3},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2012.00292},
  urldate = {2022-08-13},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/heathcote.a2012 Linear deterministic accumulator models.pdf}
}

@article{heckerman.d:2000,
  title = {Dependency Networks for Inference, Collaborative Filtering, and Data Visualization},
  author = {Heckerman, David and Chickering, David Maxwell and Meek, Christopher and Rounthwaite, Robert and Kadie, Carl},
  year = {2000},
  month = oct,
  journal = {Journal of Machine Learning Research},
  volume = {1},
  number = {Oct},
  pages = {49--75},
  issn = {ISSN 1533-7928},
  urldate = {2023-12-27},
  abstract = {We describe a graphical model for probabilistic relationships--an alternative to the Bayesian network--called a dependency network. The graph of a dependency network, unlike a Bayesian network, is potentially cyclic. The probability component of a dependency network, like a Bayesian network, is a set of conditional distributions, one for each node given its parents. We identify several basic properties of this representation and describe a computationally efficient procedure for learning the graph and probability components from data. We describe the application of this representation to probabilistic inference, collaborative filtering (the task of predicting preferences), and the visualization of acausal predictive relationships.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/heckerman.d2000 Dependency networks for inference, colla.pdf}
}

@article{heilbron.m:2022,
  title = {A Hierarchy of Linguistic Predictions during Natural Language Comprehension},
  author = {Heilbron, Micha and Armeni, Kristijan and Schoffelen, Jan-Mathijs and Hagoort, Peter and {de Lange}, Floris P.},
  year = {2022},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {32},
  pages = {e2201968119},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2201968119},
  urldate = {2022-10-29},
  abstract = {Understanding spoken language requires transforming ambiguous acoustic streams into a hierarchy of representations, from phonemes to meaning. It has been suggested that the brain uses prediction to guide the interpretation of incoming input. However, the role of prediction in language processing remains disputed, with disagreement about both the ubiquity and representational nature of predictions. Here, we address both issues by analyzing brain recordings of participants listening to audiobooks, and using a deep neural network (GPT-2) to precisely quantify contextual predictions. First, we establish that brain responses to words are modulated by ubiquitous predictions. Next, we disentangle model-based predictions into distinct dimensions, revealing dissociable neural signatures of predictions about syntactic category (parts of speech), phonemes, and semantics. Finally, we show that high-level (word) predictions inform low-level (phoneme) predictions, supporting hierarchical predictive processing. Together, these results underscore the ubiquity of prediction in language processing, showing that the brain spontaneously predicts upcoming language at multiple levels of abstraction.}
}

@book{heim.i:1998,
  title = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {1998},
  publisher = {Blackwell},
  date-added = {2019-05-19 21:59:01 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {0-631-19712-5},
  keywords = {semantics}
}

@incollection{heim.i:2000,
  title = {Semantics in Generative Grammar},
  booktitle = {Semantics in Generative Grammar},
  author = {Heim, Irene and Kratzer, Angelika},
  year = {2000},
  publisher = {Blackwell Publishing},
  address = {Malden, MA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:16:57 -0400},
  keywords = {Semantics}
}

@misc{heiss.a:2021,
  title = {A Guide to Correctly Calculating Posterior Predictions and Average Marginal Effects with Multilievel {{Bayesian}} Models},
  author = {Heiss, Andrew},
  year = {2021},
  month = nov,
  journal = {Andrew Heiss's blog},
  doi = {10.59350/wbn93-edb02},
  urldate = {2024-05-22},
  abstract = {At the end of my previous post on beta and zero-inflated-beta regression, I included an example of a multilevel model that predicted the proportion of women members of parliament based on whether a country implements gender-based quotas for their legislatures, along with a few different control variables. I also included random effects for year and region in order to capture time- and geography-specific trends.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english}
}

@article{heng.j:2020,
  title = {Controlled Sequential {{Monte Carlo}}},
  author = {Heng, Jeremy and Bishop, Adrian N. and Deligiannidis, George and Doucet, Arnaud},
  year = {2020},
  month = oct,
  journal = {The Annals of Statistics},
  volume = {48},
  number = {5},
  pages = {2904--2929},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/19-AOS1914},
  urldate = {2024-10-03},
  abstract = {Sequential Monte Carlo methods, also known as particle methods, are a popular set of techniques for approximating high-dimensional probability distributions and their normalizing constants. These methods have found numerous applications in statistics and related fields; for example, for inference in nonlinear non-Gaussian state space models, and in complex static models. Like many Monte Carlo sampling schemes, they rely on proposal distributions which crucially impact their performance. We introduce here a class of controlled sequential Monte Carlo algorithms, where the proposal distributions are determined by approximating the solution to an associated optimal control problem using an iterative scheme. This method builds upon a number of existing algorithms in econometrics, physics and statistics for inference in state space models, and generalizes these methods so as to accommodate complex static models. We provide a theoretical analysis concerning the fluctuation and stability of this methodology that also provides insight into the properties of related algorithms. We demonstrate significant gains over state-of-the-art methods at a fixed computational complexity on a variety of applications.},
  keywords = {62F12,62M05,62M10,annealed importance sampling,approximate dynamic programming,Normalizing constants,optimal control,reinforcement learning,state space models},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/heng.j2020 Controlled sequential Monte Carlo.pdf}
}

@inproceedings{hewitt.j:2019,
  title = {A Structural Probe for Finding Syntax in Word Representations},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Hewitt, John and Manning, Christopher D.},
  year = {2019},
  pages = {4129--4138},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1419},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1419}
}

@inproceedings{hewitt.j:2019selectivity,
  title = {Designing and Interpreting Probes with Control Tasks},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Hewitt, John and Liang, Percy},
  year = {2019},
  pages = {2733--2743},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1275},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D19-1275}
}

@article{hick.w:1952,
  title = {On the {{Rate}} of {{Gain}} of {{Information}}},
  author = {Hick, W. E.},
  year = {1952},
  month = mar,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {4},
  number = {1},
  pages = {11--26},
  publisher = {SAGE Publications},
  issn = {0033-555X},
  doi = {10.1080/17470215208416600},
  urldate = {2024-05-03},
  abstract = {The analytical methods of information theory are applied to the data obtained in certain choice-reaction-time experiments. Two types of experiment were performed: (a) a conventional choice-reaction experiment, with various numbers of alternatives up to ten, and with a negligible proportion of errors, and (b) a ten-choice experiment in which the subjects deliberately reduced their reaction time by allowing themselves various proportions of errors. The principal finding is that the rate of gain of information is, on the average, constant with respect to time, within the duration of one perceptual-motor act, and has a value of the order of five ``bits'' per second. The distribution of reaction times among the ten stimuli in the second experiment is shown to be related to the objective uncertainty as to which response will be given to each stimulus. The distribution of reaction times among the responses is also related to the same uncertainty. This is further evidence that information is intimately concerned with reaction time. Some possible conceptual models of the process are considered, but tests against the data are inconclusive.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hick.w1952 On the Rate of Gain of Information.pdf}
}

@article{hidaka.s:2013,
  title = {A Computational Model Associating Learning Process, Word Attributes, and Age of Acquisition},
  author = {Hidaka, Shohei},
  year = {2013},
  month = nov,
  journal = {PLOS ONE},
  volume = {8},
  number = {11},
  pages = {e76242},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0076242},
  urldate = {2022-09-28},
  abstract = {We propose a new model-based approach linking word learning to the age of acquisition (AoA) of words; a new computational tool for understanding the relationships among word learning processes, psychological attributes, and word AoAs as measures of vocabulary growth. The computational model developed describes the distinct statistical relationships between three theoretical factors underpinning word learning and AoA distributions. Simply put, this model formulates how different learning processes, characterized by change in learning rate over time and/or by the number of exposures required to acquire a word, likely result in different AoA distributions depending on word type. We tested the model in three respects. The first analysis showed that the proposed model accounts for empirical AoA distributions better than a standard alternative. The second analysis demonstrated that the estimated learning parameters well predicted the psychological attributes, such as frequency and imageability, of words. The third analysis illustrated that the developmental trend predicted by our estimated learning parameters was consistent with relevant findings in the developmental literature on word learning in children. We further discuss the theoretical implications of our model-based approach.},
  langid = {english},
  keywords = {Children,Learning,Learning curves,Linguistic morphology,Semantics,Social psychology,Statistical distributions,Vocabulary},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hidaka.s2013 A computational model associating learni.pdf}
}

@inproceedings{hinton.g:1986,
  title = {Learning Distributed Representations of Concepts},
  booktitle = {Proceedings of the Eighth Annual Conference of the Cognitive Science Society},
  author = {Hinton, Geoffrey E.},
  year = {1986},
  volume = {1},
  pages = {12},
  address = {Amherst, MA}
}

@inproceedings{ho.j:2020,
  title = {Denoising Diffusion Probabilistic Models},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  year = {2020},
  volume = {33},
  pages = {6840--6851},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-07-07},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ho.j2020 Denoising diffusion probabilistic models.pdf}
}

@article{hochreiter.s:1997,
  title = {Long Short-Term Memory},
  author = {Hochreiter, Sepp and Schmidhuber, J{\"u}rgen},
  year = {1997},
  month = nov,
  journal = {Neural Computation},
  volume = {9},
  number = {8},
  pages = {1735--1780},
  publisher = {MIT Press - Journals},
  doi = {10.1162/neco.1997.9.8.1735},
  bdsk-url-2 = {https://doi.org/10.1162/neco.1997.9.8.1735},
  date-added = {2021-12-01 18:30:30 -0500},
  date-modified = {2021-12-01 18:30:32 -0500}
}

@article{hockett.c:1953,
  title = {Review of the Mathematical Theory of Communication},
  author = {Hockett, Charles F.},
  year = {1953},
  journal = {Language},
  volume = {29},
  number = {1},
  eprint = {410457},
  eprinttype = {jstor},
  pages = {69--93},
  publisher = {Linguistic Society of America},
  issn = {0097-8507},
  doi = {10.2307/410457},
  urldate = {2024-05-03},
  collaborator = {Shannon, Claude E. and Weaver, Warren},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hockett.c1953 Review of the mathematical theory of com.pdf}
}

@book{hodges.w:2008,
  title = {Model Theory},
  author = {Hodges, Wilfrid},
  year = {2008},
  month = jun,
  edition = {1st edition},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  isbn = {978-0-521-06636-5},
  langid = {english}
}

@article{hoffman.m:2013,
  title = {Stochastic Variational Inference},
  author = {Hoffman, Matthew D. and Blei, David M. and Wang, Chong and Paisley, John},
  year = {2013},
  journal = {Journal of Machine Learning Research},
  volume = {14},
  number = {40},
  pages = {1303--1347},
  issn = {1533-7928},
  urldate = {2022-06-30},
  abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoffman.m2013 Stochastic variational inference.pdf}
}

@incollection{hofmann.m:2017,
  title = {Benchmarking N-Grams, Topic Models and Recurrent Neural Networks by Cloze Completions, {{EEGs}} and Eye Movements},
  booktitle = {Cognitive {{Approach}} to {{Natural Language Processing}}},
  author = {Hofmann, Markus J. and Biemann, Chris and Remus, Steffen},
  year = {2017},
  pages = {197--215},
  publisher = {Elsevier},
  doi = {10.1016/B978-1-78548-253-3.50010-X},
  urldate = {2022-08-28},
  isbn = {978-1-78548-253-3},
  langid = {english}
}

@article{hofmann.m:2022,
  title = {Language Models Explain Word Reading Times Better than Empirical Predictability},
  author = {Hofmann, Markus J. and Remus, Steffen and Biemann, Chris and Radach, Ralph and Kuchinke, Lars},
  year = {2022},
  month = feb,
  journal = {Frontiers in Artificial Intelligence},
  volume = {4},
  pages = {730570},
  issn = {2624-8212},
  doi = {10.3389/frai.2021.730570},
  urldate = {2022-08-28},
  abstract = {Though there is a strong consensus that word length and frequency are the most important single-word features determining visual-orthographic access to the mental lexicon, there is less agreement as how to best capture syntactic and semantic factors. The traditional approach in cognitive reading research assumes that word predictability from sentence context is best captured by cloze completion probability (CCP) derived from human performance data. We review recent research suggesting that probabilistic language models provide deeper explanations for syntactic and semantic effects than CCP. Then we compare CCP with three probabilistic language models for predicting word viewing times in an English and a German eye tracking sample: (1) Symbolic n-gram models consolidate syntactic and semantic short-range relations by computing the probability of a word to occur, given two preceding words. (2) Topic models rely on subsymbolic representations to capture long-range semantic similarity by word co-occurrence counts in documents. (3) In recurrent neural networks (RNNs), the subsymbolic units are trained to predict the next word, given all preceding words in the sentences. To examine lexical retrieval, these models were used to predict single fixation durations and gaze durations to capture rapidly successful and standard lexical access, and total viewing time to capture late semantic integration. The linear item-level analyses showed greater correlations of all language models with all eye-movement measures than CCP. Then we examined non-linear relations between the different types of predictability and the reading times using generalized additive models. N-gram and RNN probabilities of the present word more consistently predicted reading performance compared with topic models or CCP. For the effects of last-word probability on current-word viewing times, we obtained the best results with n-gram models. Such count-based models seem to best capture short-range access that is still underway when the eyes move on to the subsequent word. The prediction-trained RNN models, in contrast, better predicted early preprocessing of the next word. In sum, our results demonstrate that the different language models account for differential cognitive processes during reading. We discuss these algorithmically concrete blueprints of lexical consolidation as theoretically deep explanations for human reading.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hofmann.m2022 Language models explain word reading tim.pdf}
}

@article{holly.j:2001,
  title = {Pictures of Ultrametric Spaces, the p-Adic Numbers, and Valued Fields},
  author = {Holly, Jan E},
  year = {2001},
  journal = {The American Mathematical Monthly},
  volume = {108},
  number = {8},
  pages = {721--728},
  publisher = {Taylor \& Francis},
  date-added = {2019-06-11 08:49:52 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry,ultrametric}
}

@article{holmberg.a:2003,
  title = {Agreement and Movement in {{Icelandic}} Raising Constructions},
  author = {Holmberg, Anders and Hr{\'o}arsd{\'o}ttir, {\TH}orbj{\"o}rg},
  year = {2003},
  journal = {Lingua. International review of general linguistics. Revue internationale de linguistique g{\'e}n{\'e}rale},
  volume = {113},
  number = {10},
  pages = {997--1019},
  publisher = {Elsevier},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:24:18 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@inproceedings{hoogeboom.e:2021,
  title = {Argmax Flows and Multinomial Diffusion: Learning Categorical Distributions},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Hoogeboom, Emiel and Nielsen, Didrik and Jaini, Priyank and Forr{\'e}, Patrick and Welling, Max},
  editor = {Beygelzimer, A. and Dauphin, Y. and Liang, P. and Vaughan, J. Wortman},
  year = {2021},
  date-added = {2022-03-31 10:59:19 -0400},
  date-modified = {2022-03-31 11:00:13 -0400},
  keywords = {denoising}
}

@phdthesis{hoover.i:2022phd,
  title = {Effective Equidistribution on {{Hilbert}} Modular Varieties},
  author = {Hoover, Ian},
  year = {2022},
  month = aug,
  doi = {2345/bc-ir:109520},
  urldate = {2022-09-03},
  abstract = {We compute effective error rates for the equidistribution of translates of diagonal orbits on Hilbert modular varieties. The translation is determined by \$n\$ real parameters and our results require the assumption that all parameters are non-zero. The error rate is given in explicit polynomial terms of the translation parameters and Sobolev type norms of the test functions. The effective equidistribution is applied to give counting estimates for binary quadratic forms of square discriminant over real number rings.},
  langid = {english},
  school = {Boston College},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.i2022phd Effective equidistribution on Hilbert mo.pdf}
}

@misc{hoover.j:2020wccflhandout,
  type = {Handout},
  title = {Accounting for Variation in Number Agreement in {{Icelandic DAT-NOM}} Constructions},
  author = {Hoover, Jacob Louis},
  year = {2020},
  month = mar,
  doi = {10.14288/1.0389856},
  howpublished = {WCCFL2020 Talk handout},
  peerreview = {no},
  project = {Icelandic gluttony},
  keywords = {agreement,feature geometry}
}

@misc{hoover.j:2021arxiv,
  title = {Linguistic Dependencies and Statistical Dependence},
  author = {Hoover, Jacob Louis and Sordoni, Alessandro and Du, Wenyu and O'Donnell, Timothy J.},
  year = {2021},
  eprint = {2104.08685},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2104.08685},
  urldate = {2022-05-18},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \${\textbackslash}approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  archiveprefix = {arXiv},
  copyright = {All rights reserved},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2021arxiv Linguistic dependencies and statistical.pdf}
}

@inproceedings{hoover.j:2021emnlp,
  title = {Linguistic Dependencies and Statistical Dependence},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Hoover, Jacob Louis and Du, Wenyu and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2021},
  month = nov,
  pages = {2941--2963},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.234},
  urldate = {2022-05-19},
  abstract = {Are pairs of words that tend to occur together also likely to stand in a linguistic dependency? This empirical question is motivated by a long history of literature in cognitive science, psycholinguistics, and NLP. In this work we contribute an extensive analysis of the relationship between linguistic dependencies and statistical dependence between words. Improving on previous work, we introduce the use of large pretrained language models to compute contextualized estimates of the pointwise mutual information between words (CPMI). For multiple models and languages, we extract dependency trees which maximize CPMI, and compare to gold standard linguistic dependencies. Overall, we find that CPMI dependencies achieve an unlabelled undirected attachment score of at most \${\textbackslash}approx 0.5\$. While far above chance, and consistently above a non-contextualized PMI baseline, this score is generally comparable to a simple baseline formed by connecting adjacent words. We analyze which kinds of linguistic dependencies are best captured in CPMI dependencies, and also find marked differences between the estimates of the large pretrained language models, illustrating how their different training schemes affect the type of dependencies they capture.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {db},
  keywords = {dependency grammar,linguistic dependencies},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2021emnlp Linguistic dependencies and statistical.pdf}
}

@inproceedings{hoover.j:2021wccfl,
  title = {Accounting for Variation in Number Agreement in Icelandic Dative-Nominative Constructions},
  booktitle = {Proceedings of the 38th {{West Coast Conference}} on {{Formal Linguistics}}},
  author = {Hoover, Jacob Louis},
  editor = {Soo, Rachel and Chow, Una Y. and Nederveen, Sander},
  year = {2021},
  month = jun,
  pages = {231--241},
  publisher = {Cascadilla Proceedings Project},
  address = {Somerville, Mass., USA},
  abstract = {Icelandic dative-nominative constructions exhibit a syntactic hierarchy effect known as the Person Restriction: only third person nominatives may control agreement. In these constructions, there is variation between speakers in the extent to which the verb agrees with the nominative for number. Sigur{\dh}sson \& Holmberg (2008) explain this variation as arising due to differences between varieties in the timing of subject raising, using a split phi-probe. This paper revises their approach, using the feature gluttony mechanism for Agree developed in Coon \& Keine (2020), and a split phi-probe in which person probing precedes number probing. Within this framework, the observed variation can be captured by allowing variability two independent parameters: the timing of EPP subject raising, and the visibility of a number feature on dative DPs. The proposed mechanism describes the variation, including predicting the observed optional agreement in certain cases that previous literature had struggled to account for, and makes additional predictions about the differences between varieties in cases of syncretism within the verbal paradigm. An investigation into these predictions should allow this already well-studied area of Icelandic grammar to continue to be a useful test-case for crosslinguistic assumptions about the mechanism of Agree, and the status of dative arguments.},
  copyright = {All rights reserved},
  openaccess = {true},
  peerreview = {no},
  annotation = {Note: lingref.com document 3568},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2021wccfl Accounting for variation in number agree.pdf}
}

@misc{hoover.j:2022amlap,
  type = {Poster},
  title = {With Better Language Models, Processing Time Is Superlinear in Surprisal},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and O'Donnell, Timothy J.},
  year = {2022},
  month = sep,
  address = {York, England},
  collaborator = {Piantadosi, Steven T.},
  copyright = {All rights reserved},
  peerreview = {no}
}

@misc{hoover.j:2022psyarxiv,
  title = {The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/qjnpv},
  abstract = {Words that are more surprising given context take longer to process. However, no incremental parsing algorithm has been shown to directly predict this phenomenon. In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisal---those that involve repeatedly sampling from the prior. Our first contribution is to show that simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase. These two predictions stand in contrast with literature on surprisal theory (Hale, 2001; Levy, 2008), which assumes that the expected processing cost increases linearly with surprisal, and makes no prediction about variance. In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal. We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.},
  copyright = {Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International License (CC-BY-NC-ND)},
  langid = {american},
  peerreview = {db},
  keywords = {Linguistics,parsing algorithms,sampling,sentence processing,Social and Behavioral Sciences,surprisal},
  note = {To appear in Open Mind: Discoveries in Cognitive Science (2023)},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2022psyarxiv The plausibility of sampling as an algor.pdf}
}

@article{hoover.j:2023,
  title = {The Plausibility of Sampling as an Algorithmic Theory of Sentence Processing},
  author = {Hoover, Jacob Louis and Sonderegger, Morgan and Piantadosi, Steven T. and O'Donnell, Timothy J.},
  year = {2023},
  month = jul,
  journal = {Open Mind: Discoveries in Cognitive Science},
  volume = {7},
  pages = {350--391},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00086},
  urldate = {2023-07-21},
  abstract = {Words that are more surprising given context take longer to process. However, no incremental parsing algorithm has been shown to directly predict this phenomenon. In this work, we focus on a class of algorithms whose runtime does naturally scale in surprisal---those that involve repeatedly sampling from the prior. Our first contribution is to show that simple examples of such algorithms predict runtime to increase superlinearly with surprisal, and also predict variance in runtime to increase. These two predictions stand in contrast with literature on surprisal theory (Hale, 2001; Levy, 2008a) which assumes that the expected processing cost increases linearly with surprisal, and makes no prediction about variance. In the second part of this paper, we conduct an empirical study of the relationship between surprisal and reading time, using a collection of modern language models to estimate surprisal. We find that with better language models, reading time increases superlinearly in surprisal, and also that variance increases. These results are consistent with the predictions of sampling-based algorithms.},
  openaccess = {true},
  pmcid = {PMC10449406},
  keywords = {processing algorithms,sentence processing,surprisal theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2023 The plausibility of sampling as an algor.pdf}
}

@misc{hoover.j:2023CPL,
  title = {When Does Unpredictable Not Mean Difficult?},
  author = {Hoover, Jacob Louis},
  year = {2023},
  month = dec,
  address = {MIT Computational Psycholinguistics Laboratory},
  collaborator = {O'Donnell, Timothy J.}
}

@phdthesis{hoover.j:2024phd,
  title = {The Cost of Information: Looking beyond Predictability in Language Processing},
  author = {Hoover, Jacob Louis},
  year = {2024},
  month = aug,
  address = {Montr{\'e}al, Canada},
  langid = {english},
  school = {McGill University},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hoover.j2024phd The cost of information looking beyond.pdf}
}

@article{howes.d:1957,
  title = {On the Relation between the Intelligibility and Frequency of Occurrence of English Words},
  author = {Howes, Davis},
  year = {1957},
  month = feb,
  journal = {The Journal of the Acoustical Society of America},
  volume = {29},
  number = {2},
  pages = {296--305},
  issn = {0001-4966, 1520-8524},
  doi = {10.1121/1.1908862},
  urldate = {2024-05-03},
  abstract = {The threshold of intelligibility for a word in a wide-spectrum noise is shown to be a decreasing function of the frequency with which the word occurs in general linguistic usage (word frequency). The drop in threshold is about 4.5 db per logarithmic unit of word frequency. This rate is independent of the length of the word, although the thresholds for words of given frequency of occurrence are lower for long words.             The effect of restricting the listener's alternatives in an intelligibility test to a specified number of words is calculated from this relationship. These calculations come within 1 db of published experimental data. Theoretical functions relating intelligibility threshold to word length are also calculated from the word-frequency effect, on the assumption that listeners can discriminate the length of a word at levels too low for it to be identified. These functions are in general agreement with the experimental results.             Implications for intelligibility testing procedures are discussed.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/howes.d1957 On the relation between the intelligibil.pdf}
}

@book{howson.c:2006,
  title = {Scientific Reasoning: The {{Bayesian}} Approach},
  shorttitle = {Scientific Reasoning},
  author = {Howson, Colin and Urbach, Peter},
  year = {2006},
  edition = {3},
  publisher = {Open Court Publishing},
  abstract = {In this clearly reasoned defense of Bayes's Theorem -- that probability can be used to reasonably justify scientific theories -- Colin Howson and Peter Urbach examine the way in which scientists appeal to probability arguments, and demonstrate that the classical approach to statistical inference is full of flaws. Arguing the case for the Bayesian method with little more than basic algebra, the authors show that it avoids the difficulties of the classical system. The book also refutes the major criticisms leveled against Bayesian logic, especially that it is too subjective. This newly updated edition of this classic textbook is also suitable for college courses.},
  googlebooks = {3JusAwAAQBAJ},
  isbn = {978-0-8126-9578-6},
  langid = {english},
  keywords = {Philosophy / General,Philosophy / History & Surveys / Modern,Science / Philosophy & Social Aspects}
}

@article{hoyer.p:2004,
  title = {Non-Negative Matrix Factorization with Sparseness Constraints},
  author = {Hoyer, Patrik O},
  year = {2004},
  journal = {Journal of machine learning research},
  volume = {5},
  number = {Nov},
  pages = {1457--1469},
  date-added = {2020-07-26 15:09:12 -0400},
  date-modified = {2020-07-26 15:10:47 -0400},
  project = {syntactic embedding},
  keywords = {sparseness}
}

@misc{htut.p:2019arxiv,
  title = {Do Attention Heads in {{BERT}} Track Syntactic Dependencies?},
  author = {Htut, Phu Mon and Phang, Jason and Bordia, Shikha and Bowman, Samuel R.},
  year = {2019},
  month = nov,
  number = {arXiv:1911.12246},
  eprint = {1911.12246},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-05},
  abstract = {We investigate the extent to which individual attention heads in pretrained transformer language models, such as BERT and RoBERTa, implicitly capture syntactic dependency relations. We employ two methods---taking the maximum attention weight and computing the maximum spanning tree---to extract implicit dependency relations from the attention weights of each layer/head, and compare them to the ground-truth Universal Dependency (UD) trees. We show that, for some UD relation types, there exist heads that can recover the dependency type significantly better than baselines on parsed English text, suggesting that some self-attention heads act as a proxy for syntactic structure. We also analyze BERT fine-tuned on two datasets---the syntax-oriented CoLA and the semantics-oriented MNLI---to investigate whether fine-tuning affects the patterns of their self-attention, but we do not observe substantial differences in the overall dependency relations extracted using our methods. Our results suggest that these models have some specialist attention heads that track individual dependency types, but no generalist head that performs holistic parsing significantly better than a trivial baseline, and that analyzing attention weights directly may not reveal much of the syntactic knowledge that BERT-style models are known to learn.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/htut.p2019arxiv Do attention heads in BERT track syntact.pdf}
}

@inproceedings{hu.j:2020systematic,
  title = {A Systematic Assessment of Syntactic Generalization in Neural Language Models},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger},
  year = {2020},
  pages = {1725--1744},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.158},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.158}
}

@inproceedings{hu.j:2022,
  title = {Predicting Scalar Diversity with Context-Driven Uncertainty over Alternatives},
  booktitle = {Proceedings of the {{Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Hu, Jennifer and Levy, Roger and Schuster, Sebastian},
  year = {2022},
  month = may,
  pages = {68--74},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.cmcl-1.8},
  urldate = {2023-05-26},
  abstract = {Scalar implicature (SI) arises when a speaker uses an expression (e.g., ``some'') that is semantically compatible with a logically stronger alternative on the same scale (e.g., ``all''), leading the listener to infer that they did not intend to convey the stronger meaning. Prior work has demonstrated that SI rates are highly variable across scales, raising the question of what factors determine the SI strength for a particular scale. Here, we test the hypothesis that SI rates depend on the listener's confidence in the underlying scale, which we operationalize as uncertainty over the distribution of possible alternatives conditioned on the context. We use a T5 model fine-tuned on a text infilling task to estimate this distribution. We find that scale uncertainty predicts human SI rates, measured as entropy over the sampled alternatives and over latent classes among alternatives in sentence embedding space. Furthermore, we do not find a significant effect of the surprisal of the strong scalemate. Our results suggest that pragmatic inferences depend on listeners' context-driven uncertainty over alternatives.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hu.j2022 Predicting scalar diversity with context.pdf}
}

@misc{hu.j:2023arxiv,
  title = {Expectations over Unspoken Alternatives Predict Pragmatic Inferences},
  author = {Hu, Jennifer and Levy, Roger and Degen, Judith and Schuster, Sebastian},
  year = {2023},
  month = apr,
  number = {arXiv:2304.04758},
  eprint = {2304.04758},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.04758},
  urldate = {2023-04-28},
  abstract = {Scalar inferences (SI) are a signature example of how humans interpret language based on unspoken alternatives. While empirical studies have demonstrated that human SI rates are highly variable -- both within instances of a single scale, and across different scales -- there have been few proposals that quantitatively explain both cross- and within-scale variation. Furthermore, while it is generally assumed that SIs arise through reasoning about unspoken alternatives, it remains debated whether humans reason about alternatives as linguistic forms, or at the level of concepts. Here, we test a shared mechanism explaining SI rates within and across scales: context-driven expectations about the unspoken alternatives. Using neural language models to approximate human predictive distributions, we find that SI rates are captured by the expectedness of the strong scalemate as an alternative. Crucially, however, expectedness robustly predicts cross-scale variation only under a meaning-based view of alternatives. Our results suggest that pragmatic inferences arise from context-driven expectations over alternatives, and these expectations operate at the level of concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hu.j2023arxiv Expectations over unspoken alternatives.pdf}
}

@phdthesis{hu.j:2023phd,
  title = {Neural Language Models and Human Linguistic Knowledge},
  author = {Hu, Jennifer},
  year = {2023},
  month = may,
  address = {Cambridge, MA, USA},
  urldate = {2024-05-15},
  abstract = {Language is one of the hallmarks of intelligence, demanding explanation in a theory of human cognition. However, language presents unique practical challenges for quantitative empirical research, making many linguistic theories difficult to test at naturalistic scales. Artificial neural network language models (LMs) provide a new tool for studying language with mathematical precision and control, as they exhibit remarkably sophisticated linguistic behaviors while being fully intervenable. While LMs differ from humans in many ways, the learning outcomes of these models can reveal the behaviors that may emerge through expressive statistical learning algorithms applied to linguistic input.     In this thesis, I demonstrate this approach through three case studies using LMs to investigate open questions in language acquisition and comprehension. First, I use LMs to perform controlled manipulations of language learning, and find that syntactic generalizations depend more on a learner's inductive bias than on training data size. Second, I use LMs to explain systematic variation in scalar inferences by approximating human listeners' expectations over unspoken alternative sentences (e.g., "The bill was supported overwhelmingly" implies that the bill was not supported unanimously). Finally, I show that LMs and humans exhibit similar behaviors on a set of non-literal comprehension tasks which are hypothesized to require social reasoning (e.g., inferring a speaker's intended meaning from ironic statements). These findings suggest that certain aspects of linguistic knowledge could emerge through domain-general prediction mechanisms, while other aspects may require specific inductive biases and conceptual structures.},
  copyright = {Attribution 4.0 International (CC BY 4.0)},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/hu.j2023phd Neural language models and human linguis.pdf}
}

@inproceedings{hu.x:2021,
  title = {{{R2D2}}: Recursive Transformer Based on Differentiable Tree for Interpretable Hierarchical Language Modeling},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Hu, Xiang and Mi, Haitao and Wen, Zujie and Wang, Yafang and Su, Yi and Zheng, Jing and {de Melo}, Gerard},
  year = {2021},
  month = aug,
  pages = {4897--4908},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.379},
  abstract = {Human language understanding operates at multiple levels of granularity (e.g., words, phrases, and sentences) with increasing levels of abstraction that can be hierarchically combined. However, existing deep models with stacked layers do not explicitly model any sort of hierarchical process. In this paper, we propose a recursive Transformer model based on differentiable CKY style binary trees to emulate this composition process, and we extend the bidirectional language model pre-training objective to this architecture, attempting to predict each word given its left and right abstraction nodes. To scale up our approach, we also introduce an efficient pruning and growing algorithm to reduce the time complexity and enable encoding in linear time. Experimental results on language modeling and unsupervised parsing show the effectiveness of our approach.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.379},
  date-added = {2022-04-28 10:19:40 -0400},
  date-modified = {2022-04-28 10:19:58 -0400},
  keywords = {chart parsing,pruning}
}

@misc{hu.x:2022arxiv,
  title = {Fast-{{R2D2}}: A Pretrained Recursive Neural Network Based on Pruned {{CKY}} for Grammar Induction and Text Representation},
  shorttitle = {Fast-{{R2D2}}},
  author = {Hu, Xiang and Mi, Haitao and Li, Liang and {de Melo}, Gerard},
  year = {2022},
  month = nov,
  number = {arXiv:2203.00281},
  eprint = {2203.00281},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2203.00281},
  urldate = {2024-05-15},
  abstract = {Recently CKY-based models show great potential in unsupervised grammar induction thanks to their human-like encoding paradigm, which runs recursively and hierarchically, but requires \$O(n{\textasciicircum}3)\$ time-complexity. Recursive Transformer based on Differentiable Trees (R2D2) makes it possible to scale to large language model pre-training even with complex tree encoder by introducing a heuristic pruning method. However, the rule-based pruning approach suffers from local optimum and slow inference issues. In this paper, we fix those issues in a unified method. We propose to use a top-down parser as a model-based pruning method, which also enables parallel encoding during inference. Typically, our parser casts parsing as a split point scoring task, which first scores all split points for a given sentence, and then recursively splits a span into two by picking a split point with the highest score in the current span. The reverse order of the splits is considered as the order of pruning in R2D2 encoder. Beside the bi-directional language model loss, we also optimize the parser by minimizing the KL distance between tree probabilities from parser and R2D2. Our experiments show that our Fast-R2D2 improves performance significantly in grammar induction and achieves competitive results in downstream classification tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@inproceedings{huang.c:2021,
  title = {A Variational Perspective on Diffusion-Based Generative Models and Score Matching},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Huang, Chin-Wei and Lim, Jae Hyun and Courville, Aaron C},
  editor = {Ranzato, M. and Beygelzimer, A. and Dauphin, Y. and Liang, P.S. and Vaughan, J. Wortman},
  year = {2021},
  volume = {34},
  pages = {22863--22876},
  publisher = {Curran Associates, Inc.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.c2021 A variational perspective on diffusion-b.pdf}
}

@article{huang.k:2021,
  title = {Using Eye Tracking to Investigate Failure to Notice Word Transpositions in Reading},
  author = {Huang, Kuan-Jung and Staub, Adrian},
  year = {2021},
  month = nov,
  journal = {Cognition},
  volume = {216},
  pages = {104846},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104846},
  urldate = {2023-10-29},
  abstract = {Previous research (Mirault, Snell, \& Grainger, 2018) has demonstrated that subjects sometimes incorrectly judge an ungrammatical sentence as grammatical when it is created by the transposition of two words in a grammatical sentence (e.g., The white was cat big). Here we present two eye-tracking experiments designed to assess the prevalence of this phenomenon in a more natural reading task, and to explore theoretical explanations. Readers failed to notice transpositions at about the same rate as in Mirault et al. (2018). Failure to notice the transposition was more common when both words were short, and when readers' eyes skipped, rather than directly fixated, one of the two words. The status of the transposed words as open- or closed-class did not have a reliable effect. The transposed words caused disruption in the eye movement record only on trials when participants ultimately judged the sentence to be ungrammatical, not when they judged the sentence to be grammatical. We argue that the results are not entirely consistent with the account offered by Mirault et al. (2018), which attributes failure to notice transpositions to parallel processing of adjacent words, or with a late, post-perceptual rational inference account (Gibson, Bergen, \& Piantadosi, 2013). We propose that word recognition is serial, but post-lexical integration of each word into its context may not be perfectly incremental.},
  keywords = {Eye movements,Incremental processing,noisy channel,Sentence comprehension,Word position coding,word transposition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.k2021 Using eye tracking to investigate failur.pdf}
}

@misc{huang.k:2022HSP,
  type = {Talk},
  title = {{{SPR}} Mega-Benchmark Shows Surprisal Tracks Construction- but Not Item-Level Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2022},
  address = {Santa Cruz, California, USA},
  keywords = {eye-tracking,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.k2022HSP SPR mega-benchmark shows surprisal track 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/huang.k2022HSP SPR mega-benchmark shows surprisal track.pdf}
}

@misc{huang.k:2023psyarxiv,
  title = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty: Evidence from a Large-Scale Benchmark},
  shorttitle = {Surprisal Does Not Explain Syntactic Disambiguation Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2023},
  month = apr,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/z38u6},
  urldate = {2023-04-22},
  abstract = {Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences - one of the major concerns of psycholinguistics - be explained by predictability, as estimated using computational language models? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. We find that the predictions of language models with two different architectures sharply diverge from the reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that prediction is most likely insufficient on its own to explain human syntactic processing.},
  langid = {american},
  keywords = {Language models,Linguistics,Prediction,Psycholinguistics and Neurolinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.k2023psyarxiv Surprisal does not explain syntactic dis.pdf}
}

@article{huang.k:2024JML,
  title = {Large-Scale Benchmark Yields No Evidence That Language Model Surprisal Explains Syntactic Disambiguation Difficulty},
  author = {Huang, Kuan-Jung and Arehalli, Suhas and Kugemoto, Mari and Muxica, Christian and Prasad, Grusha and Dillon, Brian and Linzen, Tal},
  year = {2024},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {137},
  pages = {104510},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104510},
  urldate = {2024-03-03},
  abstract = {Prediction has been proposed as an overarching principle that explains human information processing in language and beyond. To what degree can processing difficulty in syntactically complex sentences -- one of the major concerns of psycholinguistics -- be explained by predictability, as estimated using computational language models, and operationalized as surprisal (negative log probability)? A precise, quantitative test of this question requires a much larger scale data collection effort than has been done in the past. We present the Syntactic Ambiguity Processing Benchmark, a dataset of self-paced reading times from 2000 participants, who read a diverse set of complex English sentences. This dataset makes it possible to measure processing difficulty associated with individual syntactic constructions, and even individual sentences, precisely enough to rigorously test the predictions of computational models of language comprehension. By estimating the function that relates surprisal to reading times from filler items included in the experiment, we find that the predictions of language models with two different architectures sharply diverge from the empirical reading time data, dramatically underpredicting processing difficulty, failing to predict relative difficulty among different syntactic ambiguous constructions, and only partially explaining item-wise variability. These findings suggest that next-word prediction is most likely insufficient on its own to explain human syntactic processing.},
  keywords = {Language models,Prediction,Sentence processing,suprisal theory,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.k2024JML Large-scale benchmark yields no evidence.pdf}
}

@phdthesis{huang.l:2008phd,
  title = {Forest-Based Algorithms in Natural Language Processing},
  author = {Huang, Liang},
  year = {2008},
  date-added = {2022-03-31 09:58:59 -0400},
  date-modified = {2022-04-26 21:20:55 -0400},
  school = {University of Pennsylvania},
  keywords = {parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huang.l2008phd Forest-based algorithms in natural langu.pdf}
}

@inproceedings{huang.l:2010,
  title = {Dynamic Programming for Linear-Time Incremental Parsing},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Huang, Liang and Sagae, Kenji},
  year = {2010},
  month = jul,
  pages = {1077--1086},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  date-added = {2022-04-26 21:01:34 -0400},
  date-modified = {2022-04-26 21:02:08 -0400},
  keywords = {dependency parsing,dynamic programming}
}

@article{huddleston.r:2002,
  title = {The Cambridge Grammar of English},
  author = {Huddleston, Rodney and Pullum, Geoffrey K and others},
  year = {2002},
  journal = {Language. Cambridge: Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{hudson.r:1984,
  title = {Word Grammar},
  author = {Hudson, Richard A},
  year = {1984},
  publisher = {Blackwell Oxford},
  date-added = {2021-07-17 10:26:33 -0400},
  date-modified = {2021-07-17 10:48:55 -0400}
}

@article{hughes.b:2004,
  title = {Trees and Ultrametric Spaces: A Categorical Equivalence},
  author = {Hughes, Bruce},
  year = {2004},
  journal = {Advances in Mathematics},
  volume = {189},
  number = {1},
  pages = {148--191},
  publisher = {Elsevier},
  date-added = {2019-07-10 18:13:04 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@article{huijben.i:2022,
  title = {A Review of the {{Gumbel-max}} Trick and Its Extensions for Discrete Stochasticity in Machine Learning},
  author = {Huijben, Iris A.M. and Kool, Wouter and Paulus, Max Benedikt and Sloun, Ruud JG Van},
  year = {2022},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  pages = {1--1},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/tpami.2022.3157042},
  bdsk-url-2 = {https://doi.org/10.1109/tpami.2022.3157042},
  date-added = {2022-04-10 19:20:21 -0400},
  date-modified = {2022-04-10 19:26:14 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/huijben.i2022 A review of the Gumbel-max trick and its.pdf}
}

@article{hupkes.d:2018,
  title = {Visualisation and'diagnostic Classifiers' Reveal How Recurrent and Recursive Neural Networks Process Hierarchical Structure},
  author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
  year = {2018},
  journal = {Journal of Artificial Intelligence Research},
  volume = {61},
  pages = {907--926},
  date-added = {2019-06-17 18:51:15 -0400},
  date-modified = {2019-06-17 18:52:12 -0400},
  project = {syntactic embedding},
  keywords = {implicit information probing}
}

@inproceedings{hwa.r:1999,
  title = {Supervised Grammar Induction Using Training Data with Limited Constituent Information},
  booktitle = {Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics},
  author = {Hwa, Rebecca},
  year = {1999},
  pages = {73--79},
  publisher = {Association for Computational Linguistics},
  address = {College Park, Maryland, USA},
  doi = {10.3115/1034678.1034699},
  bdsk-url-2 = {https://doi.org/10.3115/1034678.1034699}
}

@inproceedings{icard.t:2014cogsci,
  title = {Toward Boundedly Rational Analysis},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Icard, Thomas},
  year = {2014},
  volume = {36}
}

@unpublished{icard.t:2023draft,
  type = {Book Draft},
  title = {Resource Rationality},
  author = {Icard, Thomas},
  year = {2023},
  month = sep
}

@article{itti.l:2009,
  title = {Bayesian Surprise Attracts Human Attention},
  author = {Itti, Laurent and Baldi, Pierre},
  year = {2009},
  month = jun,
  journal = {Vision Research},
  series = {Visual {{Attention}}: {{Psychophysics}}, Electrophysiology and Neuroimaging},
  volume = {49},
  number = {10},
  pages = {1295--1306},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2008.09.007},
  urldate = {2022-08-07},
  abstract = {We propose a formal Bayesian definition of surprise to capture subjective aspects of sensory information. Surprise measures how data affects an observer, in terms of differences between posterior and prior beliefs about the world. Only data observations which substantially affect the observer's beliefs yield surprise, irrespectively of how rare or informative in Shannon's sense these observations are. We test the framework by quantifying the extent to which humans may orient attention and gaze towards surprising events or items while watching television. To this end, we implement a simple computational model where a low-level, sensory form of surprise is computed by simple simulated early visual neurons. Bayesian surprise is a strong attractor of human attention, with 72\% of all gaze shifts directed towards locations more surprising than the average, a figure rising to 84\% when focusing the analysis onto regions simultaneously selected by all observers. The proposed theory of surprise is applicable across different spatio-temporal scales, modalities, and levels of abstraction.},
  langid = {english},
  keywords = {Attention,Bayes theorem,Eye movements,Free viewing,Information theory,KL divergence,Natural vision,Novelty,Saliency,surprisal theory,Surprise,vision},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/itti.l2009 Bayesian surprise attracts human attenti.pdf}
}

@incollection{jackendoff.r:2002,
  title = {Foundations of Language},
  booktitle = {Foundations of Language},
  author = {Jackendoff, Ray},
  year = {2002},
  publisher = {Oxford University Press},
  address = {New York},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:52 -0400},
  group = {Cognitive Science, Language},
  readinglist = {Thesis},
  keywords = {Parallel Architecture}
}

@article{jacobs.c:2024,
  title = {Constraint Satisfaction in Large Language Models},
  author = {Jacobs, Cassandra L. and MacDonald, Maryellen C.},
  year = {2024},
  month = jun,
  journal = {Language, Cognition and Neuroscience},
  volume = {0},
  number = {0},
  pages = {1--18},
  publisher = {Routledge},
  issn = {2327-3798},
  doi = {10.1080/23273798.2024.2364339},
  urldate = {2024-10-11},
  abstract = {Constraint satisfaction theories were prominent in the late 20th century and emphasized continuous, rich interaction between many sources of information in a linguistic signal unfolding over time. A major challenge was rigorously capturing these highly interactive comprehension processes and yielding explicit predictions, because the important constraints were numerous and changed in prominence from one context to the next. Connectionist models were conceptually well-suited to this, but researchers had insufficient computing power and lacked sufficiently large corpora to bring these models to bear. These limitations no longer hold, and large language models (LLMs) offer an opportunity to test constraint satisfaction ideas about human language comprehension. We consider how LLMs can be applied to study interactive processes with lexical ambiguity resolution as a test case. We argue that further study of LLMs can advance theories of constraint satisfaction, though gaps remain in our understanding of how people and LLMs combine linguistic information.},
  keywords = {ambiguity,connectionism,constraint satisfaction,Language comprehension,large language models}
}

@phdthesis{jaeger.t:2006phd,
  title = {Redundancy and Syntactic Reduction in Spontaneous Speech},
  author = {Jaeger, T. Florian},
  year = {2006},
  address = {Stanford, CA},
  school = {Stanford University}
}

@article{jager.g:2012,
  title = {Formal Language Theory: Refining the {{Chomsky}} Hierarchy},
  shorttitle = {Formal Language Theory},
  author = {J{\"a}ger, Gerhard and Rogers, James},
  year = {2012},
  month = jul,
  journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
  volume = {367},
  number = {1598},
  pages = {1956--1970},
  publisher = {Royal Society},
  doi = {10.1098/rstb.2012.0077},
  urldate = {2023-03-09},
  abstract = {The first part of this article gives a brief overview of the four levels of the Chomsky hierarchy, with a special emphasis on context-free and regular languages. It then recapitulates the arguments why neither regular nor context-free grammar is sufficiently expressive to capture all phenomena in the natural language syntax. In the second part, two refinements of the Chomsky hierarchy are reviewed, which are both relevant to the extant research in cognitive science: the mildly context-sensitive languages (which are located between context-free and context-sensitive languages), and the sub-regular hierarchy (which distinguishes several levels of complexity within the class of regular languages).},
  keywords = {artificial grammar learning,complexity,formal language theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jager.g2012 Formal language theory refining the Cho.pdf}
}

@article{jager.l:2015,
  title = {Retrieval Interference in Reflexive Processing: Experimental Evidence from {{Mandarin}}, and Computational Modeling},
  shorttitle = {Retrieval Interference in Reflexive Processing},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2015},
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  urldate = {2022-10-12},
  abstract = {We conducted two eye-tracking experiments investigating the processing of the Mandarin reflexive ziji in order to tease apart structurally constrained accounts from standard cue-based accounts of memory retrieval. In both experiments, we tested whether structurally inaccessible distractors that fulfill the animacy requirement of ziji influence processing times at the reflexive. In Experiment 1, we manipulated animacy of the antecedent and a structurally inaccessible distractor intervening between the antecedent and the reflexive. In conditions where the accessible antecedent mismatched the animacy cue, we found inhibitory interference whereas in antecedent-match conditions, no effect of the distractor was observed. In Experiment 2, we tested only antecedent-match configurations and manipulated locality of the reflexive-antecedent binding (Mandarin allows non-local binding). Participants were asked to hold three distractors (animate vs. inanimate nouns) in memory while reading the target sentence. We found slower reading times when animate distractors were held in memory (inhibitory interference). Moreover, we replicated the locality effect reported in previous studies. These results are incompatible with structure-based accounts. However, the cue-based ACT-R model of Lewis and Vasishth (2005) cannot explain the observed pattern either. We therefore extend the original ACT-R model and show how this model not only explains the data presented in this article, but is also able to account for previously unexplained patterns in the literature on reflexive processing.},
  keywords = {ACT-R},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jager.l2015 Retrieval interference in reflexive proc.pdf}
}

@article{jager.l:2015a,
  title = {Teasing Apart Retrieval and Encoding Interference in the Processing of Anaphors},
  author = {J{\"a}ger, Lena A. and Benz, Lena and Roeser, Jens and Dillon, Brian W. and Vasishth, Shravan},
  year = {2015},
  month = jun,
  journal = {Frontiers in Psychology},
  volume = {6},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00506},
  urldate = {2023-10-29}
}

@article{jager.l:2017,
  title = {Similarity-Based Interference in Sentence Comprehension: {{Literature}} Review and {{Bayesian}} Meta-Analysis},
  shorttitle = {Similarity-Based Interference in Sentence Comprehension},
  author = {J{\"a}ger, Lena A. and Engelmann, Felix and Vasishth, Shravan},
  year = {2017},
  month = jun,
  journal = {Journal of Memory and Language},
  volume = {94},
  pages = {316--339},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2017.01.004},
  urldate = {2023-01-08},
  abstract = {We report a comprehensive review of the published reading studies on retrieval interference in reflexive-/reciprocal-antecedent and subject-verb dependencies. We also provide a quantitative random-effects meta-analysis of eyetracking and self-paced reading studies. We show that the empirical evidence is only partly consistent with cue-based retrieval as implemented in the ACT-R-based model of sentence processing by Lewis and Vasishth (2005) (LV05) and that there are important differences between the reviewed dependency types. In non-agreement subject-verb dependencies, there is evidence for inhibitory interference in configurations where the correct dependent fully matches the retrieval cues. This is consistent with the LV05 cue-based retrieval account. By contrast, in subject-verb agreement as well as in reflexive-/reciprocal-antecedent dependencies, no evidence for inhibitory interference is found in configurations with a fully cue-matching subject/antecedent. In configurations with only a partially cue-matching subject or antecedent, the meta-analysis reveals facilitatory interference in subject-verb agreement and inhibitory interference in reflexives/reciprocals. The former is consistent with the LV05 account, but the latter is not. Moreover, the meta-analysis reveals that (i) interference type (proactive versus retroactive) leads to different effects in the reviewed dependency types and (ii) the prominence of the distractor strongly influences the interference effect. In sum, the meta-analysis suggests that the LV05 needs important modifications to account for the unexplained interference patterns and the differences between the dependency types. More generally, the meta-analysis provides a quantitative empirical basis for comparing the predictions of competing accounts of retrieval processes in sentence comprehension.},
  langid = {english},
  keywords = {Agreement,Bayesian meta-analysis,Cue-based retrieval,Interference,Reflexives,Syntactic dependency processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jager.l2017 Similarity-based interference in sentenc.pdf}
}

@misc{jakulin.a:2003,
  title = {Quantifying and Visualizing Attribute Interactions},
  author = {Jakulin, Aleks and Bratko, Ivan},
  year = {2003},
  eprint = {cs/0308002},
  archiveprefix = {arXiv},
  date-added = {2021-07-19 22:11:58 -0400},
  date-modified = {2021-07-19 22:12:25 -0400},
  keywords = {co-information,interaction information,multivariate mututal information}
}

@inproceedings{jang.e:2017,
  title = {Categorical Reparameterization with {{Gumbel-softmax}}},
  booktitle = {5th International Conference on Learning Representations, Conference Track Proceedings},
  author = {Jang, Eric and Gu, Shixiang and Poole, Ben},
  year = {2017},
  month = apr,
  publisher = {OpenReview.net},
  address = {Toulon, France},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/JangGP17.bib},
  date-added = {2022-04-10 19:10:47 -0400},
  date-modified = {2022-04-10 19:20:19 -0400},
  timestamp = {Thu, 25 Jul 2019 14:26:04 +0200}
}

@article{jarnik.v:1930,
  title = {{O jist{\'e}m probl{\'e}mu minim{\'a}ln{\'i}m. (Z dopisu panu O. Bor{\u u}vkovi) [On a certain problem of minimization. (From a letter to Mr. O. Bor{\u u}vka).]}},
  author = {Jarn{\'i}k, Vojt{\v e}ch},
  year = {1930},
  journal = {Pr{\'a}ce moravsk{\'e} p{\v r}{\'i}rodov{\v e}deck{\'e} spole{\v c}nosti},
  volume = {6},
  number = {4},
  pages = {57--63},
  doi = {10338.dmlcz/500726},
  urldate = {2021-02-05},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  langid = {czech}
}

@misc{jasra.a:2013,
  title = {The Alive Particle Filter},
  author = {Jasra, Ajay and Lee, Anthony and Yau, Christopher and Zhang, Xiaole},
  year = {2013},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1304.0151},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1304.0151},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-05 09:49:28 -0400},
  date-modified = {2022-05-05 09:49:46 -0400},
  keywords = {particle filtering},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jasra.a2013 The alive particle filter.pdf}
}

@incollection{jegerski.j:2013,
  title = {Self-Paced Reading},
  booktitle = {Research {{Methods}} in {{Second Language Psycholinguistics}}},
  author = {Jegerski, Jill},
  year = {2013},
  pages = {20--49},
  publisher = {Routledge},
  abstract = {Self-Paced Reading - 1},
  isbn = {978-0-203-12343-0}
}

@article{jelinek.f:1976,
  title = {Continuous Speech Recognition by Statistical Methods},
  author = {Jelinek, F.},
  year = {1976},
  month = apr,
  journal = {Proceedings of the IEEE},
  volume = {64},
  number = {4},
  pages = {532--556},
  issn = {1558-2256},
  doi = {10.1109/PROC.1976.10159},
  abstract = {Statistical methods useful in automatic recognition of continuous speech are described. They concern modeling of a speaker and of an acoustic processor, extraction of the models' statistical parameters and hypothesis search procedures and likelihood computations of linguistic decoding. Experimental results are presented that indicate the power of the methods.},
  keywords = {Acoustic devices,Automatic speech recognition,Decoding,Loudspeakers,Natural languages,noisy-channel,Signal processing,Speech processing,Speech recognition,Statistical analysis,Statistics}
}

@book{jespersen.o:1922,
  title = {Language: Its Nature Development and Origin},
  author = {Jespersen, Otto},
  year = {1922},
  publisher = {George Allen \& Unwin Ltd.},
  address = {London},
  urldate = {2024-05-03}
}

@misc{jiang.a:2023Mistral,
  title = {Mistral {{7B}}},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Mensch, Arthur and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Bressand, Florian and Lengyel, Gianna and Lample, Guillaume and Saulnier, Lucile and Lavaud, L{\'e}lio Renard and Lachaux, Marie-Anne and Stock, Pierre and Scao, Teven Le and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06825},
  eprint = {2310.06825},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.06825},
  urldate = {2024-02-01},
  abstract = {We introduce Mistral 7B v0.1, a 7-billion-parameter language model engineered for superior performance and efficiency. Mistral 7B outperforms Llama 2 13B across all evaluated benchmarks, and Llama 1 34B in reasoning, mathematics, and code generation. Our model leverages grouped-query attention (GQA) for faster inference, coupled with sliding window attention (SWA) to effectively handle sequences of arbitrary length with a reduced inference cost. We also provide a model fine-tuned to follow instructions, Mistral 7B -- Instruct, that surpasses the Llama 2 13B -- Chat model both on human and automated benchmarks. Our models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{jiang.a:2024Mixtral,
  title = {Mixtral of Experts},
  author = {Jiang, Albert Q. and Sablayrolles, Alexandre and Roux, Antoine and Mensch, Arthur and Savary, Blanche and Bamford, Chris and Chaplot, Devendra Singh and de las Casas, Diego and Hanna, Emma Bou and Bressand, Florian and Lengyel, Gianna and Bour, Guillaume and Lample, Guillaume and Lavaud, L{\'e}lio Renard and Saulnier, Lucile and Lachaux, Marie-Anne and Stock, Pierre and Subramanian, Sandeep and Yang, Sophia and Antoniak, Szymon and Scao, Teven Le and Gervet, Th{\'e}ophile and Lavril, Thibaut and Wang, Thomas and Lacroix, Timoth{\'e}e and Sayed, William El},
  year = {2024},
  month = jan,
  number = {arXiv:2401.04088},
  eprint = {2401.04088},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.04088},
  urldate = {2024-02-01},
  abstract = {We introduce Mixtral 8x7B, a Sparse Mixture of Experts (SMoE) language model. Mixtral has the same architecture as Mistral 7B, with the difference that each layer is composed of 8 feedforward blocks (i.e. experts). For every token, at each layer, a router network selects two experts to process the current state and combine their outputs. Even though each token only sees two experts, the selected experts can be different at each timestep. As a result, each token has access to 47B parameters, but only uses 13B active parameters during inference. Mixtral was trained with a context size of 32k tokens and it outperforms or matches Llama 2 70B and GPT-3.5 across all evaluated benchmarks. In particular, Mixtral vastly outperforms Llama 2 70B on mathematics, code generation, and multilingual benchmarks. We also provide a model fine-tuned to follow instructions, Mixtral 8x7B - Instruct, that surpasses GPT-3.5 Turbo, Claude-2.1, Gemini Pro, and Llama 2 70B - chat model on human benchmarks. Both the base and instruct models are released under the Apache 2.0 license.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@inproceedings{jin.l:2020,
  title = {Memory-Bounded Neural Incremental Parsing for Psycholinguistic Prediction},
  booktitle = {Proceedings of the 16th International Conference on Parsing Technologies and the {{IWPT}} 2020 Shared Task on Parsing into Enhanced Universal Dependencies},
  author = {Jin, Lifeng and Schuler, William},
  year = {2020},
  month = jul,
  pages = {48--61},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.iwpt-1.6},
  abstract = {Syntactic surprisal has been shown to have an effect on human sentence processing, and can be predicted from prefix probabilities of generative incremental parsers. Recent state-of-the-art incremental generative neural parsers are able to produce accurate parses and surprisal values but have unbounded stack memory, which may be used by the neural parser to maintain explicit in-order representations of all previously parsed words, inconsistent with results of human memory experiments. In contrast, humans seem to have a bounded working memory, demonstrated by inhibited performance on word recall in multi-clause sentences (Bransford and Franks, 1971), and on center-embedded sentences (Miller and Isard,1964). Bounded statistical parsers exist, but are less accurate than neural parsers in predict-ing reading times. This paper describes a neural incremental generative parser that is able to provide accurate surprisal estimates and can be constrained to use a bounded stack. Results show that the accuracy gains of neural parsers can be reliably extended to psycholinguistic modeling without risk of distortion due to un-bounded working memory.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.iwpt-1.6},
  date-added = {2021-09-13 19:25:38 -0400},
  date-modified = {2021-09-13 19:25:40 -0400}
}

@book{johnson.d:1980,
  title = {Arc Pair Grammar},
  author = {Johnson, David E. and Postal, Paul M.},
  year = {1980},
  publisher = {Princeton University Press},
  address = {Princeton, New Jersey},
  area = {Linguistics, Syntax},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{johnson.m:2004,
  title = {A {{TAG-based}} Noisy-Channel Model of Speech Repairs},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Johnson, Mark and Charniak, Eugene},
  year = {2004},
  month = jul,
  pages = {33--39},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1218960},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1218960},
  date-added = {2022-05-03 15:15:11 -0400},
  date-modified = {2022-05-03 15:18:08 -0400},
  keywords = {noisy channel coding,tree adjoining grammars,tree transducers}
}

@phdthesis{johnson.m:2014phd,
  title = {Bayesian Time Series Models and Scalable Inference},
  author = {Johnson, Matthew James},
  year = {2014},
  urldate = {2022-07-05},
  abstract = {With large and growing datasets and complex models, there is an increasing need for scalable Bayesian inference. We describe two lines of work to address this need. In the first part, we develop new algorithms for inference in hierarchical Bayesian time series models based on the hidden Markov model (HMM), hidden semi-Markov model (HSMM), and their Bayesian nonparametric extensions. The HMM is ubiquitous in Bayesian time series models, and it and its Bayesian nonparametric extension, the hierarchical Dirichlet process hidden Markov model (HDP-HMM), have been applied in many settings. HSMMs and HDP-HSMMs extend these dynamical models to provide state-specific duration modeling, but at the cost of increased computational complexity for inference, limiting their general applicability. A challenge with all such models is scaling inference to large datasets. We address these challenges in several ways. First, we develop classes of duration models for which HSMM message passing complexity scales only linearly in the observation sequence length. Second, we apply the stochastic variational inference (SVI) framework to develop scalable inference for the HMM, HSMM, and their nonparametric extensions. Third, we build on these ideas to define a new Bayesian nonparametric model that can capture dynamics at multiple timescales while still allowing efficient and scalable inference. In the second part of this thesis, we develop a theoretical framework to analyze a special case of a highly parallelizable sampling strategy we refer to as Hogwild Gibbs sampling. Thorough empirical work has shown that Hogwild Gibbs sampling works very well for inference in large latent Dirichlet allocation models (LDA), but there is little theory to understand when it may be effective in general. By studying Hogwild Gibbs applied to sampling from Gaussian distributions we develop analytical results as well as a deeper understanding of its behavior, including its convergence and correctness in some regimes.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/johnson.m2014phd Bayesian time series models and scalable 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/johnson.m2014phd Bayesian time series models and scalable.pdf}
}

@article{johnson.r:2007,
  title = {Transposed-Letter Effects in Reading: {{Evidence}} from Eye Movements and Parafoveal Preview.},
  shorttitle = {Transposed-Letter Effects in Reading},
  author = {Johnson, Rebecca L. and Perea, Manuel and Rayner, Keith},
  year = {2007},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {33},
  number = {1},
  pages = {209--229},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.33.1.209},
  urldate = {2023-12-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/johnson.r2007 Transposed-letter effects in reading Ev.pdf}
}

@article{johnson.r:2009,
  title = {The Quiet Clam Is Quite Calm: {{Transposed-letter}} Neighborhood Effects on Eye Movements during Reading.},
  shorttitle = {The Quiet Clam Is Quite Calm},
  author = {Johnson, Rebecca L.},
  year = {2009},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {35},
  number = {4},
  pages = {943--969},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/a0015572},
  urldate = {2023-12-06},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/johnson.r2009 The quiet clam is quite calm Transposed.pdf}
}

@article{johnson.s:1967,
  title = {Hierarchical Clustering Schemes},
  author = {Johnson, Stephen C},
  year = {1967},
  journal = {Psychometrika},
  volume = {32},
  number = {3},
  pages = {241--254},
  publisher = {Springer-Verlag},
  date-added = {2019-06-15 10:38:05 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {1},
  keywords = {hierarchical clustering,ultrametric}
}

@article{jozefowicz.r:2016,
  title = {Exploring the Limits of Language Modeling},
  author = {J{\'o}zefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
  year = {2016},
  journal = {CoRR},
  volume = {abs/1602.02410},
  eprint = {1602.02410},
  archiveprefix = {arXiv},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/journals/corr/JozefowiczVSSW16},
  date-added = {2019-06-23 21:20:27 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {convolutional,language modeling,LSTM},
  timestamp = {Mon, 13 Aug 2018 16:48:43 +0200},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jozefowicz.r2016 Exploring the limits of language modelin.pdf}
}

@article{jurafsky.d:1996,
  title = {A Probabilistic Model of Lexical and Syntactic Access and Disambiguation},
  author = {Jurafsky, Daniel},
  year = {1996},
  journal = {Cognitive Science},
  volume = {20},
  number = {2},
  pages = {137--194},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2002_1},
  urldate = {2022-07-23},
  abstract = {The problems of access---retrieving linguistic structure from some mental grammar ---and disambiguation---choosing among these structures to correctly parse ambiguous linguistic input---are fundamental to language understanding. The literature abounds with psychological results on lexical access, the access of idioms, syntactic rule access, parsing preferences, syntactic disambiguation, and the processing of garden-path sentences. Unfortunately, it has been difficult to combine models which account for these results to build a general, uniform model of access and disambiguation at the lexical, idiomatic, and syntactic levels. For example, psycholinguistic theories of lexical access and idiom access and parsing theories of syntactic rule access have almost no commonality in methodology or coverage of psycholinguistic data. This article presents a single probabilistic algorithm which models both the access and disambiguation of linguistic knowledge. The algorithm is based on a parallel parser which ranks constructions for access, and interpretations for disambiguation, by their conditional probability. Low-ranked constructions and interpretations are pruned through beam-search; this pruning accounts, among other things, for the garden-path effect. I show that this motivated probabilistic treatment accounts for a wide variety of psycholinguistic results, arguing for a more uniform representation of linguistic knowledge and for the use of probabilistically-enriched grammars and interpreters as models of human knowledge of and processing of language.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/jurafsky.d1996 A probabilistic model of lexical and syn.pdf}
}

@incollection{jurafsky.d:2003,
  title = {Probabilistic Modeling in Psycholinguistics: Linguistic Comprehension and Production},
  shorttitle = {Probabilistic Modeling in Psycholinguistics},
  booktitle = {Probabilistic {{Linguistics}}},
  author = {Jurafsky, Dan},
  editor = {Bod, Rens and Hay, Jennifer and Jannedy, Stefanie},
  year = {2003},
  month = apr,
  pages = {39--96},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/5582.003.0006},
  urldate = {2024-05-15},
  isbn = {978-0-262-26885-1},
  langid = {english}
}

@book{jurafsky.d:2009slp2,
  title = {Speech and Language Processing: {{An}} Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2009},
  edition = {2},
  publisher = {Pearson Prentice Hall},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{jurafsky.d:2024slp3draft,
  title = {Speech and Language Processing},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2024},
  edition = {3}
}

@incollection{jurafsky.d:2024slp3draft:ngramLMs,
  title = {N-Gram {{Language Models}}},
  booktitle = {Speech and {{Language Processing}}},
  author = {Jurafsky, Daniel and Martin, James H.},
  year = {2024},
  month = feb,
  edition = {3}
}

@inproceedings{kahane.s:1997,
  title = {Bubble Trees and Syntactic Representations},
  booktitle = {Proceedings of Mathematics of Language (Mol5) Meeting},
  author = {Kahane, Sylvain},
  year = {1997},
  pages = {70--76},
  date-added = {2021-07-17 10:51:30 -0400},
  date-modified = {2021-07-17 10:52:15 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kahane.s1997 Bubble trees and syntactic representatio.pdf}
}

@misc{kahardipraja.p:2021,
  title = {Towards {{Incremental Transformers}}: {{An Empirical Analysis}} of {{Transformer Models}} for {{Incremental NLU}}},
  shorttitle = {Towards {{Incremental Transformers}}},
  author = {Kahardipraja, Patrick and Madureira, Brielen and Schlangen, David},
  year = {2021},
  month = sep,
  number = {arXiv:2109.07364},
  eprint = {2109.07364},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2109.07364},
  urldate = {2022-05-18},
  abstract = {Incremental processing allows interactive systems to respond based on partial inputs, which is a desirable property e.g. in dialogue agents. The currently popular Transformer architecture inherently processes sequences as a whole, abstracting away the notion of time. Recent work attempts to apply Transformers incrementally via restart-incrementality by repeatedly feeding, to an unchanged model, increasingly longer input prefixes to produce partial outputs. However, this approach is computationally costly and does not scale efficiently for long sequences. In parallel, we witness efforts to make Transformers more efficient, e.g. the Linear Transformer (LT) with a recurrence mechanism. In this work, we examine the feasibility of LT for incremental NLU in English. Our results show that the recurrent LT model has better incremental performance and faster inference speed compared to the standard Transformer and LT with restart-incrementality, at the cost of part of the non-incremental (full sequence) quality. We show that the performance drop can be mitigated by training the model to wait for right context before committing to an output and that training with input prefixes is beneficial for delivering correct partial outputs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kahardipraja.p2021 Towards Incremental Transformers An Emp.pdf}
}

@article{kalin.l:2018,
  title = {Licensing and {{Differential Object Marking}}: {{The}} View from {{Neo-Aramaic}}},
  author = {Kalin, Laura},
  year = {2018},
  journal = {Syntax (Oxford, England)},
  volume = {21},
  number = {2},
  pages = {112--159},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:40:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features,quirky case}
}

@article{kamide.y:1999,
  title = {Incremental Pre-Head Attachment in {{Japanese}} Parsing},
  author = {Kamide, Yuki and Mitchell, Don C.},
  year = {1999},
  month = oct,
  journal = {Language and Cognitive Processes},
  volume = {14},
  number = {5-6},
  pages = {631--662},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/016909699386211},
  urldate = {2022-10-13},
  abstract = {The present study addresses the question of whether structural analyses of verb-arguments are postponed up until the head verb has been processed (head-driven parsing accounts) or initiated prior to the appearance of the verb (pre-head attachment accounts). To explore this question in relation to a head-final language, a Japanese dative argument attachment ambiguity was examined in both a questionnaire study (Experiment 1) and a self-paced reading test (Experiment 2). The data suggested that the dative argument attachment ambiguity is resolved in the manner predicted by pre-head attachment accounts. The results were incompatible with most variants of the head-driven parsing model, and were not of the form currently predicted by constraint-satisfaction models. We end by discussing the general theoretical implications of the findings.},
  keywords = {eager processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kamide.y1999 Incremental pre-head attachment in Japan.pdf}
}

@article{kamide.y:2008,
  title = {Anticipatory {{Processes}} in {{Sentence Processing}}},
  author = {Kamide, Yuki},
  year = {2008},
  journal = {Language and Linguistics Compass},
  volume = {2},
  number = {4},
  pages = {647--670},
  issn = {1749-818X},
  doi = {10.1111/j.1749-818X.2008.00072.x},
  urldate = {2022-06-11},
  abstract = {Anticipation is an essential ability for the human cognitive system to survive in its surrounding environment. The present article will review previous research on anticipatory processes in sentence processing (comprehension). I start by pointing out past research carried out with inadequate methods, then move on to reviewing recent research with relatively new, more appropriate methods, specifically, the so-called `visual-world' eye-tracking paradigm, and neuropsychological techniques. I then discuss remaining unresolved issues, both methodological and theoretical.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kamide.y2008 Anticipatory Processes in Sentence Proce.pdf}
}

@article{kartsaklis.d:2019,
  title = {Linguistic Matrix Theory},
  author = {Kartsaklis, Dimitrios and Ramgoolam, Sanjaye and Sadrzadeh, Mehrnoosh},
  year = {2019},
  journal = {Annales de l'Institut Henri Poincar{\'e} D},
  publisher = {European Mathematical Publishing House},
  issn = {2308-5827},
  date-added = {2019-08-06 08:52:19 +0300},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {physics}
}

@article{katz.j:2019,
  title = {The Phonetics and Phonology of Lenition: A {{Campidanese Sardinian}} Case Study},
  author = {Katz, Jonah and Pitzanti, Gianmarco},
  year = {2019},
  month = sep,
  journal = {Laboratory Phonology: Journal of the Association for Laboratory Phonology},
  volume = {10},
  number = {1},
  pages = {16},
  publisher = {Open Library of the Humanities},
  doi = {10.5334/labphon.184},
  bdsk-url-2 = {https://doi.org/10.5334/labphon.184},
  date-added = {2022-05-10 10:57:54 -0400},
  date-modified = {2022-05-10 10:58:06 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/katz.j2019 The phonetics and phonology of lenition.pdf}
}

@article{kawabata.t:1992,
  title = {The Structure of the {{I-measure}} of a {{Markov}} Chain},
  author = {Kawabata, T. and Yeung, R.W.},
  year = {1992},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {38},
  number = {3},
  pages = {1146--1149},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.135658},
  bdsk-url-2 = {https://doi.org/10.1109/18.135658},
  date-added = {2021-09-21 17:47:00 -0400},
  date-modified = {2021-09-21 17:47:01 -0400}
}

@incollection{kay.p:2005,
  title = {Argument Structure Constructions and the {{Argument}}--{{Adjunct}} Distinction},
  booktitle = {Grammatical {{Constructions}}: {{Back}} to the Roots},
  author = {Kay, Paul},
  editor = {Fried, Mirjam and Boas, Hans C.},
  year = {2005},
  volume = {4},
  pages = {71--98},
  publisher = {John Benjamins Publishing Company},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:32 -0400},
  readinglist = {Adjunction},
  keywords = {Argument/Modifier}
}

@inproceedings{kazantseva.a:2018,
  title = {Kawenn{\'o}n:Nis: The {{Wordmaker}} for {{Kanyen}}'k{\'e}ha},
  shorttitle = {Kawenn{\'o}n:Nis},
  booktitle = {Proceedings of the {{Workshop}} on {{Computational Modeling}} of {{Polysynthetic Languages}}},
  author = {Kazantseva, Anna and Maracle, Owennatekha Brian and Maracle, Ronkwe'tiy{\'o}hstha Josiah and Pine, Aidan},
  year = {2018},
  month = aug,
  pages = {53--64},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA},
  urldate = {2022-06-04},
  abstract = {In this paper we describe preliminary work on Kawenn{\'o}n:nis, a verb conjugator for Kanyen'k{\'e}ha (Ohsweken dialect). The project is the result of a collaboration between Onkwawenna Kentyohkwa Kanyen'k{\'e}ha immersion school and the Canadian National Research Council's Indigenous Language Technology lab. The purpose of Kawenn{\'o}n:nis is to build on the educational successes of the Onkwawenna Kentyohkwa school and develop a tool that assists students in learning how to conjugate verbs in Kanyen'k{\'e}ha; a skill that is essential to mastering the language. Kawenn{\'o}n:nis is implemented with both web and mobile front-ends that communicate with an application programming interface that in turn communicates with a symbolic language model implemented as a finite state transducer. Eventually, it will serve as a foundation for several other applications for both Kanyen'k{\'e}ha and other Iroquoian languages.},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kazantseva.a2018 Kawennónnis the Wordmaker for Kanyen'k.pdf}
}

@article{kellert.o:2023,
  title = {Probing Sociodemographic Influence on Code-Switching and Language Choice in {{Quebec}} with Geolocation of Tweets},
  author = {Kellert, Olga},
  year = {2023},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {14},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1137038},
  urldate = {2024-07-20},
  abstract = {{$<$}p{$>$}This paper investigates the influence of the relative size of speech communities on language use in multilingual regions and cities. Due to peoples' everyday mobility inside a city, it is still unclear whether the size of a population matters for language use on a sub-city scale. By testing the correlation between the size of a population and language use on various spatial scales, this study will contribute to a better understanding of the extent to which sociodemographic factors influence language use. The present study investigates two particular phenomena that are common to multilingual speakers, namely language mixing or Code-Switching and using multiple languages without mixing. Demographic information from a Canadian census will make predictions about the intensity of Code-Switching and language use by multilinguals in cities of Quebec and neighborhoods of Montreal. Geolocated tweets will be used to identify where these linguistic phenomena occur the most and the least. My results show that the intensity of Code-Switching and the use of English by bilinguals is influenced by the size of anglophone and francophone populations on various spatial scales such as the city level, land use level (city center vs. periphery of Montreal), and large urban zones on the sub-city level, namely the western and eastern urban zones of Montreal. However, the correlation between population figures and language use is difficult to measure and evaluate on a much smaller sub-urban scale such as the city block scale due to factors such as population figures missing from the census and people's mobility. A qualitative evaluation of language use on a small spatial scale seems to suggest that other social influences such as the location context or topic of discussion are much more important predictors for language use than population figures. Methods will be suggested for testing this hypothesis in future research. I conclude that geographic space can provide us information about the relation between language use in multilingual cities and sociodemographic factors such as a speech community's size and that social media is a valuable alternative data source for sociolinguistic research that offers new insights into the mechanisms of language use such as Code-Switching.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Bilingualism -,code-switching,Geolocation,Language contact,Quebec,Twitter},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kellert.o2023 Probing sociodemographic influence on co.pdf}
}

@phdthesis{kelley.p:2018phd,
  title = {More People Understand {{Eschers}} than the Linguist Does: {{The}} Causes and Effects of Grammatical Illusions},
  shorttitle = {More People Understand Eschers than the Linguist Does},
  author = {Kelley, Patrick},
  year = {2018},
  address = {East Lansing, Michigan},
  urldate = {2023-02-22},
  abstract = {A grammatical illusion can be defined as a sentence that seems acceptable, but structurally, the sentence is ungrammatical. Grammatical illusions provide a challenge for linguists to understand why we do not immediately reject illusions like we do for most ungrammatical sentences. One type of illusion that has stirred several ongoing debates is the Escher Sentence. This dissertation focuses on the source of the illusory effect, or the reason why people fail to consistently reject these sentences. This dissertation explores the properties of Escher Sentences, the reason why they are illusory in nature, and what this contributes to our understanding of the parser. Six Experiments were designed to test the acceptability judgments, interpretations, and neurophysiological responses to these sentences. I conclude that Escher Sentences are recognized by the parser as ungrammatical, but because of the structure of these sentences, the parser is tricked into using a coercive operation to force Escher Sentences to have an acceptable interpretation. Escher Sentences gives us potential insight into the constraints of the parser in processing language while at the same time highlighting the parser's strategies in resolving computations that are ungrammatical.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9780355930733},
  langid = {english},
  school = {Michigan State University},
  keywords = {Escher,grammatical illusions,Illusion,Language,literature and linguistics,Parser,Processing,Psychology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kelley.p2018 More people understand Eschers than the.pdf}
}

@inproceedings{kennedy.a:2003,
  title = {The {{Dundee}} Corpus},
  booktitle = {Proceedings of the 12th {{European}} Conference on Eye Movement},
  author = {Kennedy, Alan and Hill, Robin and Pynte, Jo{\"e}l},
  year = {2003},
  date-added = {2021-06-02 17:24:08 -0400},
  date-modified = {2021-06-02 17:25:50 -0400}
}

@article{kennedy.a:2013,
  title = {Frequency and Predictability Effects in the {{Dundee Corpus}}: {{An}} Eye Movement Analysis},
  author = {Kennedy, Alan and Pynte, Jo{\"e}l and Murray, Wayne S. and Paul, Shirley-Anne},
  year = {2013},
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {66},
  number = {3},
  pages = {601--618},
  publisher = {SAGE Publications},
  doi = {10.1080/17470218.2012.676054},
  bdsk-url-2 = {https://doi.org/10.1080/17470218.2012.676054},
  date-added = {2021-06-02 17:10:01 -0400},
  date-modified = {2021-06-02 17:10:34 -0400}
}

@book{keynes.j:1921,
  title = {A Treatise on Probability},
  author = {Keynes, John Maynard},
  year = {1921},
  publisher = {Macmillan And Co.,},
  urldate = {2024-05-14},
  langid = {english},
  lccn = {21020432},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/keynes.j1921 A treatise on probability.pdf}
}

@misc{kim.t:2020chartbased,
  title = {Chart-Based Zero-Shot Constituency Parsing on Multiple Languages},
  author = {Kim, Taeuk and Li, Bowen and Lee, Sang-goo},
  year = {2020},
  eprint = {2004.13805},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{kim.t:2020pretrained,
  title = {Are Pre-Trained Language Models Aware of Phrases? {{Simple}} but Strong Baselines for Grammar Induction},
  booktitle = {8th International Conference on Learning Representations, {{ICLR}} 2020, Addis Ababa, Ethiopia, April 26-30, 2020},
  author = {Kim, Taeuk and Choi, Jihun and Edmiston, Daniel and Lee, Sang-goo},
  year = {2020},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KimCEL20.bib},
  timestamp = {Thu, 07 May 2020 01:00:00 +0200}
}

@inproceedings{kim.y:2015,
  title = {Character-Aware Neural Language Models},
  booktitle = {Proceedings of the Thirtieth {{AAAI}} Conference on Artificial Intelligence, February 12-17, 2016, Phoenix, Arizona, {{USA}}},
  author = {Kim, Yoon and Jernite, Yacine and Sontag, David A. and Rush, Alexander M.},
  editor = {Schuurmans, Dale and Wellman, Michael P.},
  year = {2016},
  pages = {2741--2749},
  publisher = {AAAI Press},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aaai/KimJSR16.bib},
  timestamp = {Fri, 15 Nov 2019 00:00:00 +0100}
}

@inproceedings{kingma.d:2013,
  title = {Auto-Encoding Variational Bayes},
  booktitle = {2nd International Conference on Learning Representations, {{ICLR}} 2014, Banff, {{AB}}, Canada, April 14-16, 2014, Conference Track Proceedings},
  author = {Kingma, Diederik P. and Welling, Max},
  editor = {Bengio, Yoshua and LeCun, Yann},
  year = {2014},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/journals/corr/KingmaW13.bib},
  timestamp = {Fri, 29 Mar 2019 00:00:00 +0100},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kingma.d2013 Auto-encoding variational bayes.pdf}
}

@phdthesis{kingma.d:2017,
  title = {Variational Inference \& Deep Learning: {{A}} New Synthesis},
  author = {Kingma, Diederik P},
  year = {2017},
  date-added = {2019-10-08 21:58:23 -0400},
  date-modified = {2021-03-12 11:48:12 -0500},
  project = {syntactic embedding},
  school = {University of Amsterdam},
  keywords = {autoencoders,variational inference},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kingma.d2017 Variational inference & deep learning A.pdf}
}

@inproceedings{kipf.t:2017,
  title = {Semi-Supervised Classification with Graph Convolutional Networks},
  booktitle = {5th International Conference on Learning Representations, {{ICLR}} 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings},
  author = {Kipf, Thomas N. and Welling, Max},
  year = {2017},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/KipfW17.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{kitaev.n:2018,
  title = {Constituency Parsing with a Self-Attentive Encoder},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kitaev, Nikita and Klein, Dan},
  year = {2018},
  pages = {2676--2686},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1249},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1249},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kitaev.n2018 Constituency parsing with a self-attenti.pdf}
}

@inproceedings{kitaev.n:2019,
  title = {Multilingual {{Constituency Parsing}} with {{Self-Attention}} and {{Pre-Training}}},
  booktitle = {Proceedings of the 57th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Kitaev, Nikita and Cao, Steven and Klein, Dan},
  year = {2019},
  month = jul,
  pages = {3499--3505},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1340},
  urldate = {2022-05-18},
  abstract = {We show that constituency parsing benefits from unsupervised pre-training across a variety of languages and a range of pre-training conditions. We first compare the benefits of no pre-training, fastText, ELMo, and BERT for English and find that BERT outperforms ELMo, in large part due to increased model capacity, whereas ELMo in turn outperforms the non-contextual fastText embeddings. We also find that pre-training is beneficial across all 11 languages tested; however, large model sizes (more than 100 million parameters) make it computationally expensive to train separate models for each language. To address this shortcoming, we show that joint multilingual pre-training and fine-tuning allows sharing all but a small number of parameters between ten languages in the final model. The 10x reduction in model size compared to fine-tuning one model per language causes only a 3.2\% relative error increase in aggregate. We further explore the idea of joint fine-tuning and show that it gives low-resource languages a way to benefit from the larger datasets of other languages. Finally, we demonstrate new state-of-the-art results for 11 languages, including English (95.8 F1) and Chinese (91.8 F1).},
  keywords = {parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kitaev.n2019 Multilingual Constituency Parsing with S.pdf}
}

@inproceedings{kitaev.n:2022,
  title = {Learned {{Incremental Representations}} for {{Parsing}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Kitaev, Nikita and Lu, Thomas and Klein, Dan},
  year = {2022},
  month = may,
  pages = {3086--3095},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  urldate = {2022-05-18},
  abstract = {We present an incremental syntactic representation that consists of assigning a single discrete label to each word in a sentence, where the label is predicted using strictly incremental processing of a prefix of the sentence, and the sequence of labels for a sentence fully determines a parse tree. Our goal is to induce a syntactic representation that commits to syntactic choices only as they are incrementally revealed by the input, in contrast with standard representations that must make output choices such as attachments speculatively and later throw out conflicting analyses. Our learned representations achieve 93.72 F1 on the Penn Treebank with as few as 5 bits per word, and at 8 bits per word they achieve 94.97 F1, which is comparable with other state of the art parsing models when using the same pre-trained embeddings. We also provide an analysis of the representations learned by our system, investigating properties such as the interpretable syntactic features captured by the system and mechanisms for deferred resolution of syntactic ambiguities.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kitaev.n2022 Learned Incremental Representations for.pdf}
}

@article{klafka.j:2021,
  title = {Characterizing the {{Typical Information Curves}} of {{Diverse Languages}}},
  author = {Klafka, Josef and Yurovsky, Daniel},
  year = {2021},
  month = oct,
  journal = {Entropy},
  volume = {23},
  number = {10},
  pages = {1300},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {1099-4300},
  doi = {10.3390/e23101300},
  urldate = {2023-11-07},
  abstract = {Optimal coding theories of language predict that speakers will keep the amount of information in their utterances relatively uniform under the constraints imposed by their language, but how much do these constraints influence information structure, and how does this influence vary across languages? We present a novel method for characterizing the information structure of sentences across a diverse set of languages. While the structure of English is broadly consistent with the shape predicted by optimal coding, many languages are not consistent with this prediction. We proceed to show that the characteristic information curves of languages are partly related to a variety of typological features from phonology to word order. These results present an important step in the direction of exploring upper bounds for the extent to which linguistic codes can be optimal for communication.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {communication,dynamic time warping,language development,n-grams,surprisal,typical information curves,typology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/klafka.j2021 Characterizing the Typical Information C.pdf}
}

@inproceedings{klein.d:2002parserFactored,
  title = {Fast Exact Inference with a Factored Model for Natural Language Parsing},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Klein, Dan and Manning, Christopher D},
  editor = {Becker, S. and Thrun, S. and Obermayer, K.},
  year = {2002},
  volume = {15},
  publisher = {MIT Press},
  date-added = {2022-05-06 15:57:44 -0400},
  date-modified = {2022-05-06 16:01:12 -0400},
  keywords = {stanford dependencies,stanford parser},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/klein.d2002parserFactored Fast exact inference with a factored mod.pdf}
}

@inproceedings{klein.d:2003parserPCFG,
  title = {Accurate Unlexicalized Parsing},
  booktitle = {Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics},
  author = {Klein, Dan and Manning, Christopher D.},
  year = {2003},
  month = jul,
  pages = {423--430},
  publisher = {Association for Computational Linguistics},
  address = {Sapporo, Japan},
  doi = {10.3115/1075096.1075150},
  bdsk-url-2 = {https://doi.org/10.3115/1075096.1075150},
  date-added = {2022-05-06 16:00:23 -0400},
  date-modified = {2022-05-06 16:00:53 -0400}
}

@inproceedings{klein.d:2004induction,
  title = {Corpus-Based Induction of Syntactic Structure: {{Models}} of Dependency and Constituency},
  booktitle = {Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({{ACL-04}})},
  author = {Klein, Dan and Manning, Christopher},
  year = {2004},
  pages = {478--485},
  address = {Barcelona, Spain},
  doi = {10.3115/1218955.1219016},
  bdsk-url-2 = {https://doi.org/10.3115/1218955.1219016}
}

@article{kliegl.r:2004,
  title = {Length, Frequency, and Predictability Effects of Words on Eye Movements in Reading},
  author = {Kliegl, Reinhold and Grabner, Ellen and Rolfs, Martin and Engbert, Ralf},
  year = {2004},
  journal = {European Journal of Cognitive Psychology},
  volume = {16},
  number = {1-2},
  pages = {262--284},
  publisher = {Informa UK Limited},
  doi = {10.1080/09541440340000213},
  bdsk-url-2 = {https://doi.org/10.1080/09541440340000213},
  date-added = {2021-06-02 17:15:22 -0400},
  date-modified = {2021-06-02 17:15:24 -0400}
}

@inproceedings{kneser.r:1995,
  title = {Improved Backing-off for {{M-gram}} Language Modeling},
  booktitle = {1995 {{International Conference}} on {{Acoustics}}, {{Speech}}, and {{Signal Processing}}},
  author = {Kneser, R. and Ney, H.},
  year = {1995},
  month = may,
  volume = {1},
  pages = {181-184 vol.1},
  issn = {1520-6149},
  doi = {10.1109/ICASSP.1995.479394},
  urldate = {2024-08-02},
  abstract = {In stochastic language modeling, backing-off is a widely used method to cope with the sparse data problem. In case of unseen events this method backs off to a less specific distribution. In this paper we propose to use distributions which are especially optimized for the task of backing-off. Two different theoretical derivations lead to distributions which are quite different from the probability distributions that are usually used for backing-off. Experiments show an improvement of about 10\% in terms of perplexity and 5\% in terms of word error rate.},
  keywords = {Error analysis,History,Interpolation,Laboratories,Probability distribution,Smoothing methods,Stochastic processes,Training data}
}

@article{kollar.t:2017,
  title = {Generalized Grounding Graphs: {{A}} Probabilistic Framework for Understanding Grounded Commands},
  author = {Kollar, Thomas and Tellex, Stefanie and Walter, Matthew and Huang, Albert and Bachrach, Abraham and Hemachandra, Sachi and Brunskill, Emma and Banerjee, Ashis and Roy, Deb and Teller, Seth and others},
  year = {2017},
  journal = {arXiv preprint arXiv:1712.01097},
  eprint = {1712.01097},
  archiveprefix = {arXiv},
  date-added = {2020-07-28 16:14:45 -0400},
  date-modified = {2020-07-28 16:15:35 -0400},
  project = {syntactic embedding},
  keywords = {robotics,semantics}
}

@article{kolmogorov.a:1968,
  title = {Logical Basis for Information Theory and Probability Theory},
  author = {Kolmogorov, Andrei},
  year = {1968},
  journal = {IEEE Transactions on Information Theory},
  volume = {14},
  number = {5},
  pages = {662--664},
  publisher = {IEEE},
  date-added = {2019-09-13 08:11:08 -0400},
  date-modified = {2019-09-13 08:11:46 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@article{kolmogorov.a:1968a,
  title = {Three Approaches to the Quantitative Definition of Information},
  author = {Kolmogorov, Andrei Nikolaevich},
  year = {1968},
  journal = {International journal of computer mathematics},
  volume = {2},
  number = {1-4},
  pages = {157--168},
  publisher = {Taylor \& Francis},
  date-added = {2019-09-13 08:14:37 -0400},
  date-modified = {2019-09-13 08:15:41 -0400},
  project = {information-entropy},
  keywords = {algorithmic complexity,information theory,kolmogorov complexity}
}

@techreport{kong.a:1992,
  type = {Technical Report},
  title = {A Note on Importance Sampling Using Standardized Weights},
  author = {Kong, Augustine},
  year = {1992},
  month = jul,
  number = {348},
  institution = {Department of Statistics, University of Chicago},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kong.a1992 A note on importance sampling using stan.pdf}
}

@article{kong.a:1994,
  title = {Sequential Imputations and {{Bayesian}} Missing Data Problems},
  author = {Kong, Augustine and Liu, Jun S. and Wong, Wing Hung},
  year = {1994},
  journal = {Journal of the American Statistical Association},
  volume = {89},
  number = {425},
  eprint = {https://www.tandfonline.com/doi/pdf/10.1080/01621459.1994.10476469},
  pages = {278--288},
  publisher = {Taylor \& Francis},
  doi = {10.1080/01621459.1994.10476469},
  abstract = {Abstract For missing data problems, Tanner and Wong have described a data augmentation procedure that approximates the actual posterior distribution of the parameter vector by a mixture of complete data posteriors. Their method of constructing the complete data sets is closely related to the Gibbs sampler. Both required iterations, and, similar to the EM algorithm, convergence can be slow. We introduce in this article an alternative procedure that involves imputing the missing data sequentially and computing appropriate importance sampling weights. In many applications this new procedure works very well without the need for iterations. Sensitivity analysis, influence analysis, and updating with new data can be performed cheaply. Bayesian prediction and model selection can also be incorporated. Examples taken from a wide range of applications are used for illustration.},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1994.10476469},
  date-added = {2022-05-07 10:36:04 -0400},
  date-modified = {2022-05-07 10:36:22 -0400},
  keywords = {sequential importance sampling,sequential imputation,sequential Monte Carlo}
}

@inproceedings{konieczny.l:2003,
  title = {Anticipation of Clause-Final Heads. {{Evidence}} from Eye-Tracking and {{SRNs}}},
  booktitle = {Proceedings of the 4th {{International Conference}} on {{Cognitive Science}}},
  author = {Konieczny, Lars and D{\"o}ring, Philipp},
  year = {2003-06-02/2003-06-04},
  pages = {13--17},
  publisher = {Springer Berlin Heidelberg},
  address = {Melbourne, Australia and St. Petersburg, Russia},
  abstract = {In a Simple Recurrent Network simulation and an eye- tracking study, we investigated the processing of clause- final verbs. Following the integration cost hypothesis (Gibson, 1998), processing verbs should be the harder, the more complement integrations have to take place. In contrast, probabilistic prediction-based models, like Simple Recurrent Networks (SRNs, Elman, 1990), might anticipate verbs the better, the more dependents have been encountered beforehand. We trained SRNs with a subset of the German language to establish basic dependency relationships between verbs and their arguments in both verb-second and verb-final constructions. The test results established a clear anticipation hypothesis: the more arguments precede the verb, the lower the prediction error and hence, predicted reading times. The data from an eye-tracking experiment confirm the anticipation hypothesis: Clause final verbs are read faster when an additional Dative, instead of a noun-modifying Genitive, is read beforehand. Adverbial PP-adjuncts, in contrast to Noun-modifying PPs, however, did not affect reading times. In general, the results support a restricted anticipation hypothesis.}
}

@inproceedings{koo.t:2007,
  title = {Structured Prediction Models via the Matrix-Tree Theorem},
  booktitle = {Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning ({{EMNLP-CoNLL}})},
  author = {Koo, Terry and Globerson, Amir and Carreras, Xavier and Collins, Michael},
  year = {2007},
  pages = {141--150},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic}
}

@inproceedings{koo.t:2024,
  title = {Automata-Based Constraints for Language Model Decoding},
  booktitle = {First {{Conference}} on {{Language Modeling}}},
  author = {Koo, Terry and Liu, Frederick and He, Luheng},
  year = {2024},
  month = aug,
  address = {Philadelphia, PA, USA},
  urldate = {2024-10-10},
  abstract = {Language models (LMs) are often expected to generate strings in some formal language; for example, structured data, API calls, or code snippets. Although LMs can be tuned to improve their adherence to formal syntax, this does not *guarantee* conformance, especially with smaller LMs suitable for large-scale deployment. In addition, tuning requires significant resources, making it impractical for uncommon or task-specific formats. To prevent downstream parsing errors we would ideally *constrain* the LM to only produce valid output, but this is severely complicated by tokenization, which is typically both ambiguous and misaligned with the formal grammar. We solve these issues through the application of automata theory, deriving an efficient closed-form solution for the *regular languages*, a broad class of formal languages with many practical applications, including API calls or schema-guided JSON and YAML. We also discuss pragmatic extensions for coping with the issue of high branching factor, and extend our techniques to *deterministic context-free languages*, which similarly admit an efficient closed-form solution. Previous work on this topic (Willard and Louf, 2023) layers bespoke solutions onto automata, leading to problems with speed, correctness, and extensibility. Instead, we reformulate the entire task in terms of automata so we can leverage well-studied and well-optimized algorithms. Our system compiles constraints {\textasciitilde}7,000x faster, is provably correct, and can be extended in a modular fashion.},
  langid = {english}
}

@inproceedings{kool.w:2019,
  title = {Stochastic Beams and Where to Find Them: {{The Gumbel-top-k}} Trick for Sampling Sequences without Replacement},
  shorttitle = {Stochastic Beams and Where to Find Them},
  booktitle = {Proceedings of the 36th {{International Conference}} on {{Machine Learning}}},
  author = {Kool, Wouter and Hoof, Herke Van and Welling, Max},
  year = {2019},
  month = may,
  pages = {3499--3508},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-11-06},
  abstract = {The well-known Gumbel-Max trick for sampling from a categorical distribution can be extended to sample {$k$}kk elements without replacement. We show how to implicitly apply this 'Gumbel-Top-{$k$}kk' trick on a factorized distribution over sequences, allowing to draw exact samples without replacement using a Stochastic Beam Search. Even for exponentially large domains, the number of model evaluations grows only linear in {$k$}kk and the maximum sampled sequence length. The algorithm creates a theoretical connection between sampling and (deterministic) beam search and can be used as a principled intermediate alternative. In a translation task, the proposed method compares favourably against alternatives to obtain diverse yet good quality translations. We show that sequences sampled without replacement can be used to construct low-variance estimators for expected sentence-level BLEU score and model entropy.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kool.w2019 Stochastic beams and where to find them 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/kool.w2019 Stochastic beams and where to find them.pdf}
}

@book{kozen.d:1997,
  title = {Automata and Computability},
  author = {Kozen, Dexter C.},
  year = {1997},
  publisher = {Springer New York},
  address = {New York, NY},
  doi = {10.1007/978-1-4612-1844-9},
  urldate = {2023-01-24},
  isbn = {978-1-4612-7309-7 978-1-4612-1844-9},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kozen.d1997 Automata and computability.pdf}
}

@book{kracht.m:2003,
  title = {The Mathematics of Language},
  author = {Kracht, Marcus},
  year = {2003},
  series = {Studies in Generative Grammar},
  number = {63},
  publisher = {Mouton De Gruyter},
  date-added = {2019-05-19 21:51:49 -0400},
  date-modified = {2019-06-13 08:09:06 -0400},
  isbn = {3-11-017620-3 978-3-11-017620-9},
  keywords = {automata,complexity,formal languages,mathematical linguistics,model theory}
}

@article{kubler.s:2009,
  title = {Dependency Parsing},
  author = {K{\"u}bler, Sandra and McDonald, Ryan and Nivre, Joakim},
  year = {2009},
  journal = {Synthesis lectures on human language technologies},
  volume = {1},
  number = {1},
  pages = {1--127},
  publisher = {Morgan \& Claypool Publishers},
  date-added = {2020-02-26 14:44:36 -0500},
  date-modified = {2020-02-26 14:45:01 -0500},
  project = {syntactic embedding},
  keywords = {dependency parsing,parsing algorithm}
}

@article{kucerova.i:2016,
  title = {Long-Distance Agreement in {{Icelandic}}: Locality Restored},
  author = {Ku{\v c}erov{\'a}, Ivona},
  year = {2016},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {19},
  number = {1},
  pages = {49--74},
  publisher = {Springer},
  date-added = {2020-02-26 09:11:49 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,object shift,phi features},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kucerova.i2016 Long-distance agreement in Icelandic lo.pdf}
}

@book{kuhlmann.m:2010,
  title = {Dependency Structures and Lexicalized Grammars: {{An}} Algebraic Approach},
  author = {Kuhlmann, Marco},
  year = {2010},
  volume = {6270},
  publisher = {Springer},
  date-added = {2020-02-26 18:37:01 -0500},
  date-modified = {2021-07-16 11:22:03 -0400},
  isbn = {978-3-642-14568-1},
  project = {syntactic embedding},
  keywords = {dependency parsing,dependency structures,projective dependencies,projectivity}
}

@article{kullback.s:1951,
  title = {On Information and Sufficiency},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  journal = {The Annals of Mathematical Statistics},
  volume = {22},
  number = {1},
  pages = {79--86},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  urldate = {2022-10-11},
  abstract = {The Annals of Mathematical Statistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kullback.s1951 On information and sufficiency.pdf}
}

@book{kullback.s:1959,
  title = {Information Theory and Statistics},
  author = {Kullback, Solomon},
  year = {[1968] 1959},
  edition = {1968 Dover republication of 1959 (Wiley) first edition},
  publisher = {Peter Smith},
  address = {New York, NY, USA},
  isbn = {978-0-8446-5625-0},
  langid = {english},
  keywords = {Information theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kullback.s1959 Information theory and statistics.djvu}
}

@article{kumar.m:2023,
  title = {Bayesian Surprise Predicts Human Event Segmentation in Story Listening},
  author = {Kumar, Manoj and Goldstein, Ariel and Michelmann, Sebastian and Zacks, Jeffrey M. and Hasson, Uri and Norman, Kenneth A.},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {10},
  pages = {e13343},
  issn = {1551-6709},
  doi = {10.1111/cogs.13343},
  urldate = {2024-02-10},
  abstract = {Event segmentation theory posits that people segment continuous experience into discrete events and that event boundaries occur when there are large transient increases in prediction error. Here, we set out to test this theory in the context of story listening, by using a deep learning language model (GPT-2) to compute the predicted probability distribution of the next word, at each point in the story. For three stories, we used the probability distributions generated by GPT-2 to compute the time series of prediction error. We also asked participants to listen to these stories while marking event boundaries. We used regression models to relate the GPT-2 measures to the human segmentation data. We found that event boundaries are associated with transient increases in Bayesian surprise but not with a simpler measure of prediction error (surprisal) that tracks, for each word in the story, how strongly that word was predicted at the previous time point. These results support the hypothesis that prediction error serves as a control mechanism governing event segmentation and point to important differences between operational definitions of prediction error.},
  langid = {english},
  keywords = {Bayesian surprise,Entropy,Event segmentation,GPT-2,Narratives,Surprise},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kumar.m2023 Bayesian surprise predicts human event s.pdf}
}

@inproceedings{kuncoro.a:2018,
  title = {{{LSTMs}} Can Learn Syntax-Sensitive Dependencies Well, but Modeling Structure Makes Them Better},
  booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John T. and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
  year = {2018},
  pages = {1426--1436},
  publisher = {Association for Computational Linguistics},
  address = {Melbourne, Australia},
  doi = {10.18653/v1/P18-1132},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P18-1132},
  date-modified = {2022-04-20 13:50:17 -0400}
}

@article{kuperberg.g:2016,
  title = {What Do We Mean by Prediction in Language Comprehension?},
  author = {Kuperberg, Gina R. and Jaeger, T. Florian},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  eprint = {https://doi.org/10.1080/23273798.2015.1102299},
  pages = {32--59},
  publisher = {Routledge},
  doi = {10.1080/23273798.2015.1102299},
  date-modified = {2021-06-05 22:29:28 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kuperberg.g2016 What do we mean by prediction in languag.pdf}
}

@inproceedings{kuribayashi.t:2021,
  title = {Lower Perplexity Is Not Always Human-Like},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Ito, Takumi and Yoshida, Ryo and Asahara, Masayuki and Inui, Kentaro},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.acl-long.405},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.405},
  date-added = {2021-12-02 19:40:09 -0500},
  date-modified = {2021-12-02 19:40:25 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kuribayashi.t2021 Lower perplexity is not always human-lik.pdf}
}

@inproceedings{kuribayashi.t:2022,
  title = {Context Limitations Make Neural Language Models More Human-Like},
  booktitle = {Proceedings of the 2022 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Kuribayashi, Tatsuki and Oseki, Yohei and Brassard, Ana and Inui, Kentaro},
  year = {2022},
  month = dec,
  pages = {10421--10436},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  urldate = {2023-04-30},
  abstract = {Language models (LMs) have been used in cognitive modeling as well as engineering studies---they compute information-theoretic complexity metrics that simulate humans' cognitive load during reading.This study highlights a limitation of modern neural LMs as the model of choice for this purpose: there is a discrepancy between their context access capacities and that of humans.Our results showed that constraining the LMs' context access improved their simulation of human reading behavior.We also showed that LM-human gaps in context access were associated with specific syntactic constructions; incorporating syntactic biases into LMs' context access might enhance their cognitive plausibility.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kuribayashi.t2022 Context limitations make neural language.pdf}
}

@inproceedings{kurihara.k:2004,
  title = {An Application of the Variational {{Bayesian}} Approach to Probabilistic Context-Free Grammars},
  booktitle = {In {{International Joint Conference}} on {{Natural Language Processing Workshop Beyond Shallow Analyses}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  year = {2004},
  abstract = {We present an efficient learning algorithm for probabilistic context-free grammars based on the variational Bayesian approach. Although the maximum likelihood method has traditionally been used for learning probabilistic language models, Bayesian learning is, in principle, less likely to cause overfitting problems than the maximum likelihood method. We show that the computational complexity of our algorithm is equal to that of the Inside-Outside algorithm. We also report results of experiments to compare precisions of the Inside-Outside algorithm and our algorithm. 1},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kurihara.k2004 An application of the variational Bayesi.pdf}
}

@incollection{kurihara.k:2006,
  title = {Variational {{Bayesian}} Grammar Induction for Natural Language},
  booktitle = {Grammatical {{Inference}}: {{Algorithms}} and {{Applications}}},
  author = {Kurihara, Kenichi and Sato, Taisuke},
  editor = {Sakakibara, Yasubumi and Kobayashi, Satoshi and Sato, Kengo and Nishino, Tetsuro and Tomita, Etsuji},
  year = {2006},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {84--96},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/11872436_8},
  abstract = {This paper presents a new grammar induction algorithm for probabilistic context-free grammars (PCFGs). There is an approach to PCFG induction that is based on parameter estimation. Following this approach, we apply the variational Bayes to PCFGs. The variational Bayes (VB) is an approximation of Bayesian learning. It has been empirically shown that VB is less likely to cause overfitting. Moreover, the free energy of VB has been successfully used in model selection. Our algorithm can be seen as a generalization of PCFG induction algorithms proposed before. In the experiments, we empirically show that induced grammars achieve better parsing results than those of other PCFG induction algorithms. Based on the better parsing results, we give examples of recursive grammatical structures found by the proposed algorithm.},
  isbn = {978-3-540-45265-2},
  langid = {english},
  keywords = {Bayesian Learning,Noun Phrase,Parse Tree,Training Corpus,Wall Street Journal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/kurihara.k2006 Variational Bayesian grammar induction f.pdf}
}

@article{kutas.m:1984,
  title = {Brain Potentials during Reading Reflect Word Expectancy and Semantic Association},
  author = {Kutas, M. and Hillyard, S. A.},
  year = {1984},
  month = jan,
  journal = {Nature},
  volume = {307},
  number = {5947},
  pages = {161--163},
  issn = {0028-0836},
  doi = {10.1038/307161a0},
  abstract = {The neuroelectric activity of the human brain that accompanies linguistic processing can be studied through recordings of event-related potentials (e.r.p. components) from the scalp. The e.r.ps triggered by verbal stimuli have been related to several different aspects of language processing. For example, the N400 component, peaking around 400 ms post-stimulus, appears to be a sensitive indicator of the semantic relationship between a word and the context in which it occurs. Words that complete sentences in a nonsensical fashion elicit much larger N400 waves than do semantically appropriate words or non-semantic irregularities in a text. In the present study, e.r.ps were recorded in response to words that completed meaningful sentences. The amplitude of the N400 component of the e.r.p. was found to be an inverse function of the subject's expectancy for the terminal word as measured by its 'Cloze probability'. In addition, unexpected words that were semantically related to highly expected words elicited lower N400 amplitudes. These findings suggest N400 may reflect processes of semantic priming or activation.},
  langid = {english},
  pmid = {6690995},
  keywords = {Brain,ERP,Evoked Potentials,Humans,N400,Reading,surprisal theory,Verbal Learning}
}

@article{kuznetsova.a:2017lmerTest,
  title = {{{{\textbf{lmerTest}}}} Package: {{Tests}} in {{Linear Mixed Effects Models}}},
  shorttitle = {{{{\textbf{lmerTest}}}} {{Package}}},
  author = {Kuznetsova, Alexandra and Brockhoff, Per B. and Christensen, Rune H. B.},
  year = {2017},
  journal = {Journal of Statistical Software},
  volume = {82},
  number = {13},
  issn = {1548-7660},
  doi = {10.18637/jss.v082.i13},
  urldate = {2024-02-25},
  langid = {english}
}

@article{lago.s:2021,
  title = {The {{Reading Signatures}} of {{Agreement Attraction}}},
  author = {Lago, Sol and Acu{\~n}a Fari{\~n}a, Carlos and Meseguer, Enrique},
  year = {2021},
  month = nov,
  journal = {Open Mind},
  volume = {5},
  pages = {132--153},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00047},
  urldate = {2023-10-29},
  abstract = {The comprehension of subject-verb agreement shows ``attraction effects,'' which reveal that number computations can be derailed by nouns that are grammatically unlicensed to control agreement with a verb. However, previous results are mixed regarding whether attraction affects the processing of grammatical and ungrammatical sentences alike. In a large-sample eye-tracking replication of Lago et al. (2015), we support this ``grammaticality asymmetry'' by showing that the reading profiles associated with attraction depend on sentence grammaticality. In ungrammatical sentences, attraction affected both fixation durations and regressive eye-movements at the critical disagreeing verb. Meanwhile, both grammatical and ungrammatical sentences showed effects of the attractor noun number prior to the verb, in the first- and second-pass reading of the subject phrase. This contrast suggests that attraction effects in comprehension have at least two different sources: the first reflects verb-triggered processes that operate mainly in ungrammatical sentences. The second source reflects difficulties in the encoding of the subject phrase, which disturb comprehension in both grammatical and ungrammatical sentences.},
  keywords = {agreement attraction},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lago.s2021 The Reading Signatures of Agreement Attr.pdf}
}

@incollection{lai.l:2021,
  title = {Policy Compression: {{An}} Information Bottleneck in Action Selection},
  shorttitle = {Chapter {{Five}} - {{Policy}} Compression},
  booktitle = {Psychology of {{Learning}} and {{Motivation}}},
  author = {Lai, Lucy and Gershman, Samuel J.},
  editor = {Federmeier, Kara D.},
  year = {2021},
  month = jan,
  series = {The {{Psychology}} of {{Learning}} and {{Motivation}}},
  volume = {74},
  pages = {195--232},
  publisher = {Academic Press},
  doi = {10.1016/bs.plm.2021.02.004},
  urldate = {2022-11-30},
  abstract = {The brain has evolved to produce a diversity of behaviors under stringent computational resource constraints. Given this limited capacity, how do biological agents balance reward maximization against the costs of representing complex action policies? In this chapter, we examine behavioral evidence for this reward-complexity trade-off. First, we introduce a theoretical framework that formalizes the idea of policy compression, or the reduction in cognitive cost of representing action policies by making them simpler. We then describe how a wide range of behavioral phenomena, including stochasticity, perseveration, response time, state and action chunking, and navigation are brought together under this framework. Finally, we discuss how our model can be used to probe the neural underpinnings of policy compression and their dysfunction in psychiatric illness.},
  langid = {english},
  keywords = {Action selection,Rational behavior,Reinforcement learning,Resource-rationality}
}

@article{lake.b:2017,
  title = {Building Machines That Learn and Think like People},
  author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
  year = {2017},
  month = jan,
  journal = {Behavioral and Brain Sciences},
  volume = {40},
  pages = {e253},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X16001837},
  urldate = {2024-06-04},
  abstract = {Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lake.b2017 Building machines that learn and think l.pdf}
}

@article{lambek.j:1958,
  title = {The Mathematics of Sentence Structure},
  author = {Lambek, Joachim},
  year = {1958},
  journal = {The American Mathematical Monthly},
  volume = {65},
  number = {3},
  eprint = {2310058},
  eprinttype = {jstor},
  pages = {154--170},
  publisher = {Taylor \& Francis, Ltd. on behalf of the Mathematical Association of America},
  doi = {10.1080/00029890.1958.11989160},
  bdsk-url-2 = {https://doi.org/10.1080/00029890.1958.11989160},
  date-added = {2019-08-26 14:46:48 -0400},
  date-modified = {2021-06-25 00:48:42 -0400},
  keywords = {category theory,pregroup grammar}
}

@inproceedings{lambek.j:1999,
  title = {Type Grammar Revisited},
  booktitle = {International Conference on Logical Aspects of Computational Linguistics},
  author = {Lambek, Joachim},
  year = {1999},
  pages = {1--27},
  date-added = {2019-08-26 22:09:20 -0400},
  date-modified = {2019-08-26 22:09:55 -0400},
  organization = {Springer},
  keywords = {pregroup grammar}
}

@article{lambek.j:2001,
  title = {Type Grammars as Pregroups},
  author = {Lambek, Joachim},
  year = {2001},
  journal = {Grammars},
  volume = {4},
  pages = {21--39},
  date-added = {2019-08-26 21:51:00 -0400},
  date-modified = {2019-08-26 21:51:54 -0400},
  keywords = {pregroup grammar}
}

@article{lambek.j:2012,
  title = {Logic and Grammar},
  author = {Lambek, Joachim},
  year = {2012},
  journal = {Studia Logica: An International Journal for Symbolic Logic},
  volume = {100},
  number = {4},
  eprint = {23262129},
  eprinttype = {jstor},
  pages = {667--681},
  publisher = {Springer},
  issn = {00393215, 15728730},
  abstract = {Grammar can be formulated as a kind of substructural propositional logic. In support of this claim, we survey bare Gentzen style deductive systems and two kinds of non-commutative linear logic: intuitionistic and compact bilinear logic. We also glance at their categorical refinements.},
  date-added = {2019-08-26 21:59:20 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  keywords = {pregroup grammar}
}

@inproceedings{lample.g:2019,
  title = {Cross-Lingual Language Model Pretraining},
  booktitle = {Advances in Neural Information Processing Systems 32 ({{NeurIPS}} 2019)},
  author = {Conneau, Alexis and Lample, Guillaume},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  month = dec,
  pages = {7057--7067},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/ConneauL19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@book{lanchier.n:2017,
  title = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  publisher = {Springer International},
  doi = {10.1007/978-3-319-50038-6},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6},
  date-added = {2022-04-07 10:01:23 -0400},
  date-modified = {2022-04-14 10:21:03 -0400}
}

@incollection{lanchier.n:2017ch1,
  title = {Basics of Measure and Probability Theory},
  booktitle = {Stochastic Modeling},
  author = {Lanchier, Nicolas},
  year = {2017},
  series = {Universitext},
  pages = {3--24},
  publisher = {Springer International},
  doi = {10.1007/978-3-319-50038-6_1},
  bdsk-url-2 = {https://doi.org/10.1007/978-3-319-50038-6{$_{1}$}},
  date-added = {2022-04-07 10:02:11 -0400},
  date-modified = {2022-04-14 10:20:52 -0400}
}

@article{laplace.p:1986trans,
  title = {Memoir on the Probability of the Causes of Events},
  author = {Laplace, Pierre Simon},
  translator = {Stiegler, S. M.},
  year = {1986},
  journal = {Statistical Science},
  volume = {1},
  number = {3},
  eprint = {2245476},
  eprinttype = {jstor},
  pages = {364--378},
  publisher = {Institute of Mathematical Statistics},
  issn = {0883-4237},
  urldate = {2024-05-15},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/laplace.p1986trans Memoir on the probability of the causes.pdf}
}

@article{lari.k:1991,
  title = {Applications of Stochastic Context-Free Grammars Using the inside-Outside Algorithm},
  author = {Lari, K. and Young, S. J.},
  year = {1991},
  month = jul,
  journal = {Computer Speech \& Language},
  volume = {5},
  number = {3},
  pages = {237--257},
  issn = {0885-2308},
  doi = {10.1016/0885-2308(91)90009-F},
  urldate = {2022-07-04},
  abstract = {This paper describes two applications in speech recognition of the use of stochastic context-free grammars (SCFGs) trained automatically via the Inside-Outside Algorithm. First, SCFGs are used to model VQ encoded speech for isolated word recognition and are compared directly to HMMs used for the same task. It is shown that SCFGs can model this low-level VQ data accurately and that a regular grammar based pre-training algorithm is effective both for reducing training time and obtaining robust solutions. Second, an SCFG is inferred from a transcription of the speech used to train a phoneme-based recognizer in an attempt to model phonotactic constraints. When used as a language model, this SCFG gives improved performance over a comparable regular grammar or bigram.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lari.k1991 Applications of stochastic context-free.pdf}
}

@article{lau.j:2017,
  title = {Grammaticality, Acceptability, and Probability: A Probabilistic View of Linguistic Knowledge},
  shorttitle = {Grammaticality, Acceptability, and Probability},
  author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
  year = {2017},
  journal = {Cognitive Science},
  volume = {41},
  number = {5},
  pages = {1202--1241},
  issn = {1551-6709},
  doi = {10.1111/cogs.12414},
  urldate = {2024-05-26},
  abstract = {The question of whether humans represent grammatical knowledge as a binary condition on membership in a set of well-formed sentences, or as a probabilistic property has been the subject of debate among linguists, psychologists, and cognitive scientists for many decades. Acceptability judgments present a serious problem for both classical binary and probabilistic theories of grammaticality. These judgements are gradient in nature, and so cannot be directly accommodated in a binary formal grammar. However, it is also not possible to simply reduce acceptability to probability. The acceptability of a sentence is not the same as the likelihood of its occurrence, which is, in part, determined by factors like sentence length and lexical frequency. In this paper, we present the results of a set of large-scale experiments using crowd-sourced acceptability judgments that demonstrate gradience to be a pervasive feature in acceptability judgments. We then show how one can predict acceptability judgments on the basis of probability by augmenting probabilistic language models with an acceptability measure. This is a function that normalizes probability values to eliminate the confounding factors of length and lexical frequency. We describe a sequence of modeling experiments with unsupervised language models drawn from state-of-the-art machine learning methods in natural language processing. Several of these models achieve very encouraging levels of accuracy in the acceptability prediction task, as measured by the correlation between the acceptability measure scores and mean human acceptability values. We consider the relevance of these results to the debate on the nature of grammatical competence, and we argue that they support the view that linguistic knowledge can be intrinsically probabilistic.},
  copyright = {Copyright {\copyright} 2016 Cognitive Science Society, Inc.},
  langid = {english},
  keywords = {Grammaticality,Probabilistic modeling,Syntactic knowledge},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lau.j2017 Grammaticality, acceptability, and proba.pdf}
}

@inproceedings{laverghetta.a:2022,
  title = {Predicting Human Psychometric Properties Using Computational Language Models},
  booktitle = {Quantitative {{Psychology}}},
  author = {Laverghetta, Antonio and Nighojkar, Animesh and Mirzakhalov, Jamshidbek and Licato, John},
  editor = {Wiberg, Marie and Molenaar, Dylan and Gonz{\'a}lez, Jorge and Kim, Jee-Seon and Hwang, Heungsun},
  year = {2022},
  series = {Springer {{Proceedings}} in {{Mathematics}} \& {{Statistics}}},
  pages = {151--169},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-031-04572-1_12},
  abstract = {Transformer-based language models (LMs) continue to achieve state-of-the-art performance on natural language processing (NLP) benchmarks, including tasks designed to mimic human-inspired ``commonsense'' competencies. To better understand the degree to which LMs can be said to have certain linguistic reasoning skills, researchers are beginning to adapt the tools and concepts from psychometrics. But to what extent can benefits flow in the other direction? In other words, can LMs be of use in predicting the psychometric properties of test items, when those items are given to human participants? If so, the benefit for psychometric practitioners is enormous, as it can reduce the need for multiple rounds of empirical testing. We gather responses from numerous human participants and LMs (transformer- and non-transformer-based) on a broad diagnostic test of linguistic competencies. We then use the human responses to calculate standard psychometric properties of the items in the diagnostic test, using the human responses and the LM responses separately. We then determine how well these two sets of predictions correlate. We find that transformer-based LMs predict the human psychometric data consistently well across most categories, suggesting that they can be used to gather human-like psychometric data without the need for extensive human trials.},
  isbn = {978-3-031-04572-1},
  langid = {english},
  keywords = {Classical test theory,Item response theory,Natural language processing,psychometrics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/laverghetta.a2022 Predicting human psychometric properties.pdf}
}

@article{lavi-rotbain.o:2023,
  title = {Zipfian Distributions in Child-Directed Speech},
  author = {{Lavi-Rotbain}, Ori and Arnon, Inbal},
  year = {2023},
  month = jan,
  journal = {Open Mind},
  volume = {7},
  pages = {1--30},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00070},
  urldate = {2023-05-26},
  abstract = {Across languages, word frequency and rank follow a power law relation, forming a distribution known as the Zipfian distribution. There is growing experimental evidence that this well-studied phenomenon may be beneficial for language learning. However, most investigations of word distributions in natural language have focused on adult-to-adult speech: Zipf's law has not been thoroughly evaluated in child-directed speech (CDS) across languages. If Zipfian distributions facilitate learning, they should also be found in CDS. At the same time, several unique properties of CDS may result in a less skewed distribution. Here, we examine the frequency distribution of words in CDS in three studies. We first show that CDS is Zipfian across 15 languages from seven language families. We then show that CDS is Zipfian from early on (six-months) and across development for five languages with sufficient longitudinal data. Finally, we show that the distribution holds across different parts of speech: Nouns, verbs, adjectives and prepositions follow a Zipfian distribution. Together, the results show that the input children hear is skewed in a particular way from early on, providing necessary (but not sufficient) support for the postulated learning advantage of such skew. They highlight the need to study skewed learning environments experimentally.}
}

@book{lazore.d:1993,
  title = {The {{Mohawk}} Language Standardisation Project Conference Report, Aug. 9-10, 1993},
  author = {Lazore, Dorothy Karihw{\'e}nhawe},
  editor = {Jacobs, Annette Kaia'tit{\'a}hkhe and Thompson, Nancy Kahawin{\'o}nkie and Leaf, Minnie Kai{\`a}:khons},
  year = {1993},
  month = aug,
  publisher = {{Literacy and Basic Skills Section, Ministry of Education and Training}},
  address = {Toronto},
  date-added = {2022-05-03 17:07:15 -0400},
  date-modified = {2022-05-03 17:17:25 -0400},
  isbn = {0-7778-6105-4},
  langid = {english},
  organization = {Mohawk Language Standardisation Conference (1993 : Tyendinaga Indian Reserve)},
  keywords = {kanien'keha,mohawk language,standardization}
}

@article{lebesgue.h:1902,
  title = {{Int{\'e}grale, Longueur, Aire}},
  author = {Lebesgue, H.},
  year = {1902},
  month = dec,
  journal = {Annali di Matematica Pura ed Applicata (1898-1922)},
  volume = {7},
  number = {1},
  pages = {231--359},
  issn = {0373-3114},
  doi = {10.1007/BF02420592},
  urldate = {2022-06-22},
  langid = {french}
}

@inproceedings{lebrun.b:2022,
  title = {Evaluating Distributional Distortion in Neural Language Modeling},
  booktitle = {International Conference on Learning Representations},
  author = {LeBrun, Benjamin and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022}
}

@article{leemis.l:2008,
  title = {Univariate {{Distribution Relationships}}},
  author = {Leemis, Lawrence M. and McQueston, Jacquelyn T.},
  year = {2008},
  month = feb,
  journal = {The American Statistician},
  volume = {62},
  number = {1},
  pages = {45--53},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1198/000313008X270448},
  urldate = {2022-06-20},
  abstract = {Probability distributions are traditionally treated separately in introductory mathematical statistics textbooks. A figure is presented here that shows properties that individual distributions possess and many of the relationships between these distributions.},
  keywords = {Asymptotic relationships,Distribution properties,Limiting distributions,Stochastic parameters,Transformations},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/leemis.l2008 Univariate Distribution Relationships.pdf}
}

@book{legate.j:2014,
  title = {Voice and v: {{Lessons}} from Acehnese},
  author = {Legate, Julie Anne},
  year = {2014},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9780262028141.001.0001},
  bdsk-url-2 = {https://doi.org/10.7551/mitpress/9780262028141.001.0001},
  date-added = {2021-03-22 00:34:12 -0400},
  date-modified = {2021-03-22 13:11:36 -0400},
  keywords = {argument structure,voice}
}

@article{legate.j:2020,
  title = {On Passives of Passives},
  author = {Legate, Julie Anne and Akku{\c s}, Faruk and {\v S}ereikait{\.e}, Milena and Ringe, Don},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {4},
  pages = {771--818},
  publisher = {Project Muse},
  doi = {10.1353/lan.2020.0062},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0062},
  date-added = {2021-03-20 12:21:19 -0400},
  date-modified = {2021-03-20 12:21:35 -0400},
  keywords = {argument structure,passives}
}

@article{leivada.e:2020,
  title = {Language {{Processing}} at {{Its Trickiest}}: {{Grammatical Illusions}} and {{Heuristics}} of {{Judgment}}},
  shorttitle = {Language {{Processing}} at {{Its Trickiest}}},
  author = {Leivada, Evelina},
  year = {2020},
  month = sep,
  journal = {Languages},
  volume = {5},
  number = {3},
  pages = {29},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2226-471X},
  doi = {10.3390/languages5030029},
  urldate = {2023-10-20},
  abstract = {Humans are intuitively good at providing judgments about what forms part of their native language and what does not. Although such judgments are robust, consistent, and reliable, human cognition is demonstrably fallible to illusions of various types. Language is no exception. In the linguistic domain, several types of sentences have been shown to trick the parser into giving them a high acceptability judgment despite their ill-formedness. One example is the so-called comparative illusion (`More people have been to Troms{\o} than I have'). To this day, comparative illusions have been tested mainly with monolingual, neurotypical speakers of English. The present research aims to broaden our understanding of this phenomenon by putting it to test in two populations that differ in one crucial factor: the number of languages they speak. A timed acceptability judgment task was administered to monolingual speakers of Standard Greek and bi(dia)lectal speakers of Standard and Cypriot Greek. The results are not fully in line with any of the semantic re-analyses proposed for the illusion so far, hence a new proposal is offered about what interpretation induces the illusion, appreciating the influence of both grammatical processing and cognitive heuristics. Second, the results reveal an effect of developmental trajectory. This effect may be linked to an enhanced ability to spot the illusion in bi(dia)lectals, but several factors can be identified as possible culprits behind this result. After discussing each of them, it is argued that having two grammars may facilitate the setting of a higher processing threshold, something that would entail decreased fallibility to grammatical illusions.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {acceptability judgments,bilectalism,grammatical illusions,parsing,processing,reaction times},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/leivada.e2020 Language Processing at Its Trickiest Gr.pdf}
}

@article{lemoine.n:2019,
  title = {Moving beyond Noninformative Priors: Why and How to Choose Weakly Informative Priors in {{Bayesian}} Analyses},
  shorttitle = {Moving beyond Noninformative Priors},
  author = {Lemoine, Nathan P.},
  year = {2019},
  journal = {Oikos},
  volume = {128},
  number = {7},
  pages = {912--928},
  issn = {1600-0706},
  doi = {10.1111/oik.05985},
  urldate = {2024-05-23},
  abstract = {Throughout the last two decades, Bayesian statistical methods have proliferated throughout ecology and evolution. Numerous previous references established both philosophical and computational guidelines for implementing Bayesian methods. However, protocols for incorporating prior information, the defining characteristic of Bayesian philosophy, are nearly nonexistent in the ecological literature. Here, I hope to encourage the use of weakly informative priors in ecology and evolution by providing a `consumer's guide' to weakly informative priors. The first section outlines three reasons why ecologists should abandon noninformative priors: 1) common flat priors are not always noninformative, 2) noninformative priors provide the same result as simpler frequentist methods, and 3) noninformative priors suffer from the same high type I and type M error rates as frequentist methods. The second section provides a guide for implementing informative priors, wherein I detail convenient `reference' prior distributions for common statistical models (i.e. regression, ANOVA, hierarchical models). I then use simulations to visually demonstrate how informative priors influence posterior parameter estimates. With the guidelines provided here, I hope to encourage the use of weakly informative priors for Bayesian analyses in ecology. Ecologists can and should debate the appropriate form of prior information, but should consider weakly informative priors as the new `default' prior for any Bayesian model.},
  copyright = {{\copyright} 2019 The Authors},
  langid = {english},
  keywords = {Bayesian statistics,frequentist statistics,Markov chain Monte Carlo,vague priors},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lemoine.n2019 Moving beyond noninformative priors why.pdf}
}

@misc{lenth.r:2024emmeans,
  title = {{{{\textbf{emmeans}}}}: Estimated Marginal Means, Aka Least-Squares Means},
  author = {Lenth, Russell V.},
  year = {2024}
}

@book{levelt.w:1974,
  title = {Formal Grammars in Linguistics and Psycholinguistics: {{Volume}} 3: {{Psycholinguistic}} Applications},
  author = {Levelt, Willem JM},
  year = {1974},
  series = {{{JANUA LINGUARUM}}},
  volume = {192},
  publisher = {Mouton},
  address = {The Hague},
  date-added = {2019-06-11 14:51:49 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@incollection{levin.b:2005,
  title = {Argument Realization: {{Research}} Surveys in Linguistics},
  booktitle = {Argument Realization: {{Research}} Surveys in Linguistics},
  author = {Levin, Beth and Rappaport Hovav, Malka},
  year = {2005},
  publisher = {Cambridge University Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:20 -0400},
  readinglist = {Thesis}
}

@inproceedings{levshina.n:2017,
  title = {Communicative Efficiency and Syntactic Predictability: {{A}} Cross-Linguistic Study Based on the {{Universal Dependencies}} Corpora},
  booktitle = {Proceedings of the {{NoDaLiDa}} 2017 Workshop on Universal Dependencies, 22 May, Gothenburg Sweden},
  author = {Levshina, Natalia},
  year = {2017},
  number = {135},
  pages = {72--78},
  date-added = {2020-04-05 12:14:22 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  organization = {Link{\"o}ping University Electronic Press},
  project = {syntactic embedding},
  keywords = {dependency structures,information theory,random forests}
}

@inproceedings{levy.o:2014,
  title = {Neural Word Embedding as Implicit Matrix Factorization},
  booktitle = {Advances in Neural Information Processing Systems 27 ({{NIPS}} 2014)},
  author = {Levy, Omer and Goldberg, Yoav},
  editor = {Ghahramani, Zoubin and Welling, Max and Cortes, Corinna and Lawrence, Neil D. and Weinberger, Kilian Q.},
  year = {2014},
  month = dec,
  pages = {2177--2185},
  address = {Montr{\'e}al, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyG14.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{levy.o:2014dependency,
  title = {Dependency-Based Word Embeddings},
  booktitle = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: {{Short}} Papers)},
  author = {Levy, Omer and Goldberg, Yoav},
  year = {2014},
  pages = {302--308},
  publisher = {Association for Computational Linguistics},
  address = {Baltimore, Maryland},
  doi = {10.3115/v1/P14-2050},
  bdsk-url-2 = {https://doi.org/10.3115/v1/P14-2050}
}

@phdthesis{levy.r:2005phd,
  title = {Probabilistic Models of Word Order and Syntactic Discontinuity},
  author = {Levy, Roger},
  year = {2005},
  abstract = {This thesis takes up the problem of syntactic comprehension, or parsing---how an agent (human or machine) with knowledge of a specific language goes about inferring the hierarchical structural relationships underlying a surface string in the language. I take the position that probabilistic models of combining evidential information are cognitively plausible and practically useful for syntactic comprehension. In particular, the thesis applies probabilistic methods in investigating the relationship between word order and psycholinguistic models of comprehension; and in the practical problems of accuracy and efficiency in parsing sentences with syntactic discontinuity. On the psychological side, the thesis proposes a theory of expectation-based processing difficulty as a consequence of probabilistic syntactic disambiguation: the ease of processing a word during comprehension is determined primarily by the degree to which that word is expected. I identify a class of syntactic phenomena, associated primarily with verb-final clause order, where the predictions of expectation-based processing diverge most sharply from more established locality-based theories of processing difficulty. Using existing probabilistic parsing algorithms and syntactically annotated data sources, I show that the expectation-based theory matches a range of established experimental psycholinguistic results better than locality-based theories. The comparison of probabilistic- and locality-driven processing theories is a crucial area of psycholinguistic research due to its implications for the relationship between linguistic production and comprehension, and more generally for theories of modularity in cognitive science. The thesis also takes up the problem of probabilistic models for discontinuous constituency, when phrases do not consist of continuous substrings of a sentence. Discontinuity poses a computational challenge in parsing, because it expands the set of possible substructures in a sentence beyond the bound, quadratic in sentence length, on the set of possible continuous constituents. For discontinuous constituency, I investigate the problem of accuracy employing discriminative classifiers organized on principles of syntactic theory and used to introduce discontinuous relationships into otherwise strictly context-free phrase structure trees; and the problem of efficiency in joint inference over both continuous and discontinuous structures, using probabilistic instantiations of mildly context-sensitive grammatical formalisms and factorizing grammatical generalizations into probabilistic components of dominance and linear order.},
  date-added = {2021-09-18 22:16:45 -0400},
  date-modified = {2022-04-04 13:25:27 -0400},
  isbn = {978-0-542-28638-4},
  school = {Stanford University},
  keywords = {Applied sciences,Cognitive psychology,Cognitive therapy,Computer science,Language,Linguistics,literature and linguistics,Natural language processing,Parsing,Probabilistic,Psychology,Syntactic discontinuity,Word order},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2005phd Probabilistic models of word order and s.pdf}
}

@inproceedings{levy.r:2006,
  title = {Speakers Optimize Information Density through Syntactic Reduction},
  booktitle = {Proceedings of the Twentieth Annual Conference on Neural Information Processing Systems},
  author = {Levy, Roger and Jaeger, T. Florian},
  editor = {Sch{\"o}lkopf, Bernhard and Platt, John C. and Hofmann, Thomas},
  year = {2006},
  pages = {849--856},
  publisher = {MIT Press},
  address = {Vancouver, British Columbia, Canada},
  biburl = {https://dblp.org/rec/conf/nips/LevyJ06.bib},
  keywords = {uniform information density},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2006 Speakers optimize information density th.pdf}
}

@article{levy.r:2008,
  title = {Expectation-Based Syntactic Comprehension},
  author = {Levy, Roger},
  year = {2008},
  journal = {Cognition},
  volume = {106},
  number = {3},
  pages = {1126--1177},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2007.05.006},
  abstract = {This paper investigates the role of resource allocation as a source of processing difficulty in human sentence comprehension. The paper proposes a simple information-theoretic characterization of processing difficulty as the work incurred by resource reallocation during parallel, incremental, probabilistic disambiguation in sentence comprehension, and demonstrates its equivalence to the theory of Hale [Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In Proceedings of NAACL (Vol. 2, pp. 159--166)], in which the difficulty of a word is proportional to its surprisal (its negative log-probability) in the context within which it appears. This proposal subsumes and clarifies findings that high-constraint contexts can facilitate lexical processing, and connects these findings to well-known models of parallel constraint-based comprehension. In addition, the theory leads to a number of specific predictions about the role of expectation in syntactic comprehension, including the reversal of locality-based difficulty patterns in syntactically constrained contexts, and conditions under which increased ambiguity facilitates processing. The paper examines a range of established results bearing on these predictions, and shows that they are largely consistent with the surprisal theory.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2007.05.006},
  date-added = {2021-01-14 13:02:24 -0500},
  date-modified = {2021-03-09 22:53:26 -0500},
  keywords = {Frequency,Information theory,Parsing,Prediction,processing,Sentence processing,surprisal,Syntactic complexity,Syntax,Word order},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2008 Expectation-based syntactic comprehensio.pdf}
}

@inproceedings{levy.r:2008noisy,
  title = {A Noisy-Channel Model of Human Sentence Comprehension under Uncertain Input},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Levy, Roger},
  year = {2008},
  month = oct,
  pages = {234--243},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  date-added = {2022-04-11 23:17:10 -0400},
  date-modified = {2022-04-11 23:17:30 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2008noisy A noisy-channel model of human sentence.pdf}
}

@inproceedings{levy.r:2008particle,
  title = {Modeling the Effects of Memory on Human Online Sentence Processing with Particle Filters},
  booktitle = {Proceedings of the Twenty-Second Annual {{Conference}} on {{Neural Information Processing Systems}}},
  author = {Levy, Roger and Reali, Florencia and Griffiths, Thomas L.},
  editor = {Koller, Daphne and Schuurmans, Dale and Bengio, Yoshua and Bottou, L{\'e}on},
  year = {2008},
  month = dec,
  pages = {937--944},
  publisher = {Curran Associates, Inc.},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/LevyRG08.bib},
  date-modified = {2022-05-12 19:43:45 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2008particle Modeling the effects of memory on human.pdf}
}

@article{levy.r:2009pnas,
  title = {Eye Movement Evidence That Readers Maintain and Act on Uncertainty about Past Linguistic Input},
  author = {Levy, Roger and Bicknell, Klinton and Slattery, Tim and Rayner, Keith},
  year = {2009},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {106},
  number = {50},
  eprint = {https://www.pnas.org/doi/pdf/10.1073/pnas.0907664106},
  pages = {21086--21090},
  doi = {10.1073/pnas.0907664106},
  abstract = {In prevailing approaches to human sentence comprehension, the outcome of the word recognition process is assumed to be a categorical representation with no residual uncertainty. Yet perception is inevitably uncertain, and a system making optimal use of available information might retain this uncertainty and interactively recruit grammatical analysis and subsequent perceptual input to help resolve it. To test for the possibility of such an interaction, we tracked readers' eye movements as they read sentences constructed to vary in (i) whether an early word had near neighbors of a different grammatical category, and (ii) how strongly another word further downstream cohered grammatically with these potential near neighbors. Eye movements indicated that readers maintain uncertain beliefs about previously read word identities, revise these beliefs on the basis of relative grammatical consistency with subsequent input, and use these changing beliefs to guide saccadic behavior in ways consistent with principles of rational probabilistic inference.},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.0907664106},
  date-added = {2022-04-27 22:17:38 -0400},
  date-modified = {2022-04-27 22:18:16 -0400},
  keywords = {memory,noisy channel coding}
}

@inproceedings{levy.r:2011,
  title = {Integrating Surprisal and Uncertain-Input Models in Online Sentence Comprehension: Formal Techniques and Empirical Results},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Levy, Roger},
  year = {2011},
  month = jun,
  pages = {1055--1065},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  date-added = {2022-04-27 08:47:55 -0400},
  date-modified = {2022-04-27 08:49:23 -0400},
  keywords = {noisy channel coding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2011 Integrating surprisal and uncertain-inpu.pdf}
}

@article{levy.r:2012,
  title = {The Processing of Extraposed Structures in {{English}}},
  author = {Levy, Roger and Fedorenko, Evelina and Breen, Mara and Gibson, Edward},
  year = {2012},
  month = jan,
  journal = {Cognition},
  volume = {122},
  number = {1},
  pages = {12--36},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2011.07.012},
  urldate = {2022-10-24},
  abstract = {In most languages, most of the syntactic dependency relations found in any given sentence are projective: the word--word dependencies in the sentence do not cross each other. Some syntactic dependency relations, however, are non-projective: some of their word--word dependencies cross each other. Non-projective dependencies are both rarer and more computationally complex than projective dependencies; hence, it is of natural interest to investigate whether there are any processing costs specific to non-projective dependencies, and whether factors known to influence processing of projective dependencies also affect non-projective dependency processing. We report three self-paced reading studies, together with corpus and sentence completion studies, investigating the comprehension difficulty associated with the non-projective dependencies created by the extraposition of relative clauses in English. We find that extraposition over either verbs or prepositional phrases creates comprehension difficulty, and that this difficulty is consistent with probabilistic syntactic expectations estimated from corpora. Furthermore, we find that manipulating the expectation that a given noun will have a postmodifying relative clause can modulate and even neutralize the difficulty associated with extraposition. Our experiments rule out accounts based purely on derivational complexity and/or dependency locality in terms of linear positioning. Our results demonstrate that comprehenders maintain probabilistic syntactic expectations that persist beyond projective-dependency structures, and suggest that it may be possible to explain observed patterns of comprehension difficulty associated with extraposition entirely through probabilistic expectations.},
  langid = {english},
  keywords = {Frequency,Memory and language,Parsing,Prediction,Self-paced reading,Sentence comprehension,surprisal,surprisal theory,Syntactic complexity,Word order}
}

@incollection{levy.r:2013,
  title = {Memory and Surprisal in Human Sentence Comprehension},
  booktitle = {Sentence Processing},
  author = {Levy, Roger},
  editor = {{van Gompel}, Roger P. G.},
  year = {2013},
  pages = {78--114},
  publisher = {Psychology Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-05-03 14:37:38 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2013 Memory and surprisal in human sentence c.pdf}
}

@article{levy.r:2013jml,
  title = {The Syntactic Complexity of {{Russian}} Relative Clauses},
  author = {Levy, Roger and Fedorenko, Evelina and Gibson, Edward},
  year = {2013},
  month = nov,
  journal = {Journal of Memory and Language},
  volume = {69},
  number = {4},
  pages = {461--495},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2012.10.005},
  urldate = {2023-03-09},
  abstract = {Although syntactic complexity has been investigated across dozens of studies, the available data still greatly underdetermine relevant theories of processing difficulty. Memory-based and expectation-based theories make opposite predictions regarding fine-grained time course of processing difficulty in syntactically constrained contexts, and each class of theory receives support from results on some constructions in some languages. Here we report four self-paced reading experiments on the online comprehension of Russian relative clauses together with related corpus studies, taking advantage of Russian's flexible word order to disentangle predictions of competing theories. We find support for key predictions of memory-based theories in reading times at RC verbs, and for key predictions of expectation-based theories in processing difficulty at RC-initial accusative noun phrase (NP) objects, which corpus data suggest should be highly unexpected. These results suggest that a complete theory of syntactic complexity must integrate insights from both expectation-based and memory-based theories.},
  langid = {english},
  keywords = {Expectation-based processing,Memory limitations in language processing,Parsing,Russian,Sentence comprehension,Syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2013jml The syntactic complexity of Russian rela.pdf}
}

@unpublished{levy.r:2013ms,
  type = {Unpublished Manuscript},
  title = {Why Grammar Is Probabilistic},
  author = {Levy, Roger},
  year = {2013},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2013ms Why grammar is probabilistic.pdf}
}

@article{levy.r:2013opinion,
  title = {Surprisal, the {{PDC}}, and the Primary Locus of Processing Difficulty in Relative Clauses},
  author = {Levy, Roger and Gibson, Edward},
  year = {2013},
  journal = {Frontiers in Psychology},
  volume = {4},
  issn = {1664-1078},
  urldate = {2023-03-08},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2013opinion Surprisal, the PDC, and the primary locu.pdf}
}

@inproceedings{levy.r:2018cogsci,
  title = {Communicative Efficiency, Uniform Information Density, and the Rational Speech Act Theory},
  booktitle = {Proceedings of the 40th Annual Meeting of the Cognitive Science Society},
  author = {Levy, Roger},
  editor = {Kalish, Chuck and Martina Rau, Jerry Zhu and Rogers, Timothy},
  year = {2018},
  month = jul,
  pages = {684--689},
  address = {Madison, Wisconsin, USA},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2018cogsci Communicative efficiency, uniform inform.pdf}
}

@unpublished{levy.r:2020ms,
  type = {Unpublished Manuscript},
  title = {How {{Structural Commitments Magnify Surprisal}}},
  author = {Levy, Roger},
  year = {2020},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/levy.r2020ms How Structural Commitments Magnify Surpr.pdf}
}

@article{lew.a:2020cogsci,
  title = {Leveraging {{Unstructured Statistical Knowledge}} in a {{Probabilistic Language}} of {{Thought}}},
  author = {Lew, Alexander K. and Tessler, Michael Henry and Mansinghka, Vikash K. and Tenenbaum, Joshua B.},
  year = {2020},
  month = jan,
  journal = {Proceedings of the Annual Conference of the Cognitive Science Society},
  urldate = {2023-08-04},
  abstract = {One hallmark of human reasoning is that we can bring to bear a diverse web of common-sense knowledge in any situation. The vastness of our knowledge poses a challenge for the practical implementation of reasoning systems as well as for our cognitive theories -- how do people represent their common-sense knowledge? On the one hand, our best models of sophisticated reasoning are top-down, making use primarily of symbolically-encoded knowledge. On the other, much of our understanding of the statistical properties of our environment may arise in a bottom-up fashion, for example through asso- ciationist learning mechanisms. Indeed, recent advances in AI have enabled the development of billion-parameter language models that can scour for patterns in gigabytes of text from the web, picking up a surprising amount of common-sense knowledge along the way---but they fail to learn the structure of coherent reasoning. We propose combining these approaches, by embedding language-model-backed primitives into a state- of-the-art probabilistic programming language (PPL). On two open-ended reasoning tasks, we show that our PPL models with neural knowledge components characterize the distribution of human responses more accurately than the neural language models alone, raising interesting questions about how people might use language as an interface to common-sense knowledge, and suggesting that building probabilistic models with neural language-model components may be a promising approach for more human-like AI.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lew.a2020cogsci Leveraging Unstructured Statistical Know.pdf}
}

@inproceedings{lew.a:2022RAVI,
  title = {Recursive {{Monte Carlo}} and Variational Inference with Auxiliary Variables},
  booktitle = {Proceedings of the {{Thirty-Eighth Conference}} on {{Uncertainty}} in {{Artificial Intelligence}}},
  author = {Lew, Alexander K. and {Cusumano-Towner}, Marco and Mansinghka, Vikash K.},
  year = {2022},
  month = aug,
  pages = {1096--1106},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-06-06},
  abstract = {A key design constraint when implementing Monte Carlo and variational inference algorithms is that it must be possible to cheaply and exactly evaluate the marginal densities of proposal distributions and variational families. This takes many interesting proposals off the table, such as those based on involved simulations or stochastic optimization. This paper broadens the design space, by presenting a framework for applying Monte Carlo and variational inference algorithms when proposal densities cannot be exactly evaluated. Our framework, recursive auxiliary-variable inference (RAVI), instead approximates the necessary densities using meta-inference: an additional layer of Monte Carlo or variational inference, that targets the proposal, rather than the model. RAVI generalizes and unifies several existing methods for inference with expressive approximating families, which we show correspond to specific choices of meta-inference algorithm, and provides new theory for analyzing their bias and variance. We illustrate RAVI's design framework and theorems by using them to analyze and improve upon Salimans et al.'s Markov Chain Variational Inference, and to design a novel sampler for Dirichlet process mixtures, achieving state-of-the-art results on a standard benchmark dataset from astronomy and on a challenging datacleaning task with Medicare hospital data.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lew.a2022RAVI Recursive Monte Carlo and variational in 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/lew.a2022RAVI Recursive Monte Carlo and variational in.pdf}
}

@misc{lew.a:2023LLaMPPL,
  title = {Sequential {{Monte Carlo}} Steering of Large Language Models Using Probabilistic Programs},
  author = {Lew, Alexander K. and {Zhi-Xuan}, Tan and Grand, Gabriel and Mansinghka, Vikash K.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.03081},
  eprint = {2306.03081},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.03081},
  urldate = {2023-06-08},
  abstract = {Even after fine-tuning and reinforcement learning, large language models (LLMs) can be difficult, if not impossible, to control reliably with prompts alone. We propose a new inference-time approach to enforcing syntactic and semantic constraints on the outputs of LLMs, called sequential Monte Carlo (SMC) steering. The key idea is to specify language generation tasks as posterior inference problems in a class of discrete probabilistic sequence models, and replace standard decoding with sequential Monte Carlo inference. For a computational cost similar to that of beam search, SMC can steer LLMs to solve diverse tasks, including infilling, generation under syntactic constraints, and prompt intersection. To facilitate experimentation with SMC steering, we present a probabilistic programming library, LLaMPPL (https://github.com/probcomp/LLaMPPL), for concisely specifying new generation tasks as language model probabilistic programs, and automating steering of LLaMA-family Transformers.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Programming Languages,Statistics - Computation},
  annotation = {note: Presented at ICML 2023 Workshop: Sampling and Optimization in Discrete Space},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lew.a2023LLaMPPL Sequential Monte Carlo steering of large 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/lew.a2023LLaMPPL Sequential Monte Carlo steering of large.pdf}
}

@misc{lew.a:2023SMCP3,
  title = {{{SMCP}}{\textsuperscript{3}}: {{Sequential Monte Carlo}} with {{Probabilistic Program Proposals}}},
  shorttitle = {Smcp3},
  author = {Lew, Alexander K. and Matheos, George and Ghavamizadeh, Matin and Gothoskar, Nishad and Russell, Stuart and Mansinghka, Vikash K.},
  year = {2023},
  abstract = {There is a widespread need for sound, flexible frameworks for Monte Carlo inference. This paper introduces SMCP3, a sequential Monte Carlo framework that broadens the class of strategies practitioners can employ to update particles from iteration to iteration, relative to existing frameworks like resample-move SMC (Gilks \& Berzuini, 2001) and SMC samplers (Del Moral et al., 2006). In SMCP3, proposal kernels can be general probabilistic programs, which differ from traditional proposal densities in that they may sample many auxiliary variables, and may apply deterministic post-processing to calculate a proposed update. We have implemented our framework in the Gen probabilistic programming platform: given probabilistic programs that specify target distributions, forward kernels, and reverse kernels, our implementation fully automates the sound computation of incremental importance weights. To illustrate the effectiveness of SMCP3 algorithms, we apply our framework in two domains. First, we use it for online state-estimation, using proposal programs based on Langevin ascent to reduce the bias in log marginal likelihood estimates relative to resample-move SMC with Langevin rejuvenation. Second, we demonstrate an SMCP3 algorithm that yields more robust online clustering in Dirichlet process mixture models than strong SMC baselines.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lew.a2023SMCP3 SMCP3 Sequential Monte Carlo 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/lew.a2023SMCP3 SMCP3 Sequential Monte Carlo.pdf}
}

@article{lewandowski.d:2009LKJ,
  title = {Generating Random Correlation Matrices Based on Vines and Extended Onion Method},
  author = {Lewandowski, Daniel and Kurowicka, Dorota and Joe, Harry},
  year = {2009},
  month = oct,
  journal = {Journal of Multivariate Analysis},
  volume = {100},
  number = {9},
  pages = {1989--2001},
  issn = {0047-259X},
  doi = {10.1016/j.jmva.2009.04.008},
  urldate = {2024-05-21},
  abstract = {We extend and improve two existing methods of generating random correlation matrices, the onion method of Ghosh and Henderson [S. Ghosh, S.G. Henderson, Behavior of the norta method for correlated random vector generation as the dimension increases, ACM Transactions on Modeling and Computer Simulation (TOMACS) 13 (3) (2003) 276--294] and the recently proposed method of Joe [H. Joe, Generating random correlation matrices based on partial correlations, Journal of Multivariate Analysis 97 (2006) 2177--2189] based on partial correlations. The latter is based on the so-called D-vine. We extend the methodology to any regular vine and study the relationship between the multiple correlation and partial correlations on a regular vine. We explain the onion method in terms of elliptical distributions and extend it to allow generating random correlation matrices from the same joint distribution as the vine method. The methods are compared in terms of time necessary to generate 5000 random correlation matrices of given dimensions.},
  keywords = {Correlation matrix,Dependence vines,Onion method,Partial correlation}
}

@article{lewis.m:2014,
  title = {Combined Distributional and Logical Semantics},
  author = {Lewis, Mike and Steedman, Mark},
  year = {2013},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {1},
  pages = {179--192},
  doi = {10.1162/tacl_a_00219},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lewis.m2014 Combined distributional and logical sema.pdf}
}

@inproceedings{lewis.m:2019bart,
  title = {{{BART}}: {{Denoising}} Sequence-to-Sequence Pre-Training for Natural Language Generation, Translation, and Comprehension},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Veselin and Zettlemoyer, Luke},
  year = {2020},
  pages = {7871--7880},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.703},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.703}
}

@article{lewis.r:2005,
  title = {An Activation-Based Model of Sentence Processing as Skilled Memory Retrieval},
  author = {Lewis, Richard L. and Vasishth, Shravan},
  year = {2005},
  journal = {Cognitive Science},
  volume = {29},
  number = {3},
  pages = {375--419},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog0000_25},
  urldate = {2022-07-15},
  abstract = {We present a detailed process theory of the moment-by-moment working-memory retrievals and associated control structure that subserve sentence comprehension. The theory is derived from the application of independently motivated principles of memory and cognitive skill to the specialized task of sentence parsing. The resulting theory construes sentence processing as a series of skilled associative memory retrievals modulated by similarity-based interference and fluctuating activation. The cognitive principles are formalized in computational form in the Adaptive Control of Thought--Rational (ACT--R) architecture, and our process model is realized in ACT--R. We present the results of 6 sets of simulations: 5 simulation sets provide quantitative accounts of the effects of length and structural interference on both unambiguous and garden-path structures. A final simulation set provides a graded taxonomy of double center embeddings ranging from relatively easy to extremely difficult. The explanation of center-embedding difficulty is a novel one that derives from the model' complete reliance on discriminating retrieval cues in the absence of an explicit representation of serial order information. All fits were obtained with only 1 free scaling parameter fixed across the simulations; all other parameters were ACT--R defaults. The modeling results support the hypothesis that fluctuating activation and similarity-based interference are the key factors shaping working memory in sentence processing. We contrast the theory and empirical predictions with several related accounts of sentence-processing complexity.},
  langid = {english},
  keywords = {ACT-R,Activation,Cognitive architectures,Cognitive modeling,Decay,Interference,Parsing,Sentence processing,Syntax,Working memory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lewis.r2005 An activation-based model of sentence pr.pdf}
}

@article{lewis.r:2006,
  title = {Computational Principles of Working Memory in Sentence Comprehension},
  author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
  year = {2006},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {10},
  number = {10},
  pages = {447--454},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2006.08.007},
  urldate = {2022-09-08},
  abstract = {Understanding a sentence requires a working memory of the partial products of comprehension, so that linguistic relations between temporally distal parts of the sentence can be rapidly computed. We describe an emerging theoretical framework for this working memory system that incorporates several independently motivated principles of memory: a sharply limited attentional focus, rapid retrieval of item (but not order) information subject to interference from similar items, and activation decay (forgetting over time). A computational model embodying these principles provides an explanation of the functional capacities and severe limitations of human processing, as well as accounts of reading times. The broad implication is that the detailed nature of crosslinguistic sentence processing emerges from the interaction of general principles of human memory with the specialized task of language comprehension.},
  langid = {english},
  keywords = {ACT-R,comprehension,memory,sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lewis.r2006 Computational principles of working memo.pdf}
}

@article{lewis.r:2014,
  title = {Computational Rationality: {{Linking}} Mechanism and Behavior through Bounded Utility Maximization},
  author = {Lewis, Richard L. and Howes, Andrew and Singh, Satinder},
  year = {2014},
  journal = {Topics in cognitive science},
  volume = {6},
  number = {2},
  pages = {279--311},
  publisher = {Wiley Online Library},
  doi = {10.1111/tops.12086},
  isbn = {1756-8757}
}

@article{lewis.s:2015,
  title = {Aligning {{Grammatical Theories}} and {{Language Processing Models}}},
  author = {Lewis, Shevaun and Phillips, Colin},
  year = {2015},
  month = feb,
  journal = {Journal of Psycholinguistic Research},
  volume = {44},
  number = {1},
  pages = {27--46},
  issn = {1573-6555},
  doi = {10.1007/s10936-014-9329-z},
  urldate = {2023-08-01},
  abstract = {We address two important questions about the relationship between theoretical linguistics and psycholinguistics. First, do grammatical theories and language processing models describe separate cognitive systems, or are they accounts of different aspects of the same system? We argue that most evidence is consistent with the one-system view. Second, how should we relate grammatical theories and language processing models to each other?},
  langid = {english},
  keywords = {Abstraction,Cognitive architecture of language,grammatical illusions,Grammatical theories,Parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lewis.s2015 Aligning Grammatical Theories and Langua.pdf}
}

@inproceedings{li.b:2020headsup,
  title = {Heads-up! {{Unsupervised}} Constituency Parsing via Self-Attention Heads},
  booktitle = {Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing},
  author = {Li, Bowen and Kim, Taeuk and Amplayo, Reinald Kim and Keller, Frank},
  year = {2020},
  pages = {409--424},
  publisher = {Association for Computational Linguistics},
  address = {Suzhou, China}
}

@phdthesis{li.b:2022PhD,
  title = {Integrating Linguistic Theory and Neural Language Models},
  author = {Li, Bai},
  year = {2022},
  month = jul,
  eprint = {2207.09643},
  primaryclass = {cs},
  address = {Toronto},
  urldate = {2022-07-22},
  abstract = {Transformer-based language models have recently achieved remarkable results in many natural language tasks. However, performance on leaderboards is generally achieved by leveraging massive amounts of training data, and rarely by encoding explicit linguistic knowledge into neural models. This has led many to question the relevance of linguistics for modern natural language processing. In this dissertation, I present several case studies to illustrate how theoretical linguistics and neural language models are still relevant to each other. First, language models are useful to linguists by providing an objective tool to measure semantic distance, which is difficult to do using traditional methods. On the other hand, linguistic theory contributes to language modelling research by providing frameworks and sources of data to probe our language models for specific aspects of language understanding. This thesis contributes three studies that explore different aspects of the syntax-semantics interface in language models. In the first part of my thesis, I apply language models to the problem of word class flexibility. Using mBERT as a source of semantic distance measurements, I present evidence in favour of analyzing word class flexibility as a directional process. In the second part of my thesis, I propose a method to measure surprisal at intermediate layers of language models. My experiments show that sentences containing morphosyntactic anomalies trigger surprisals earlier in language models than semantic and commonsense anomalies. Finally, in the third part of my thesis, I adapt several psycholinguistic studies to show that language models contain knowledge of argument structure constructions. In summary, my thesis develops new connections between natural language processing, linguistic theory, and psycholinguistics to provide fresh perspectives for the interpretation of language models.},
  archiveprefix = {arXiv},
  school = {University of Toronto},
  keywords = {Computer Science - Computation and Language}
}

@misc{li.j:2016a,
  title = {Mutual Information and Diverse Decoding Improve Neural Machine Translation},
  author = {Li, Jiwei and Jurafsky, Dan},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1601.00372},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1601.00372},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:37:17 -0400},
  date-modified = {2022-05-15 15:40:00 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.j2016a Mutual information and diverse decoding.pdf}
}

@misc{li.j:2016b,
  title = {A Simple, Fast Diverse Decoding Algorithm for Neural Generation},
  author = {Li, Jiwei and Monroe, Will and Jurafsky, Dan},
  year = {2016},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1611.08562},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1611.08562},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-05-15 15:39:25 -0400},
  date-modified = {2022-05-15 15:40:09 -0400},
  keywords = {beam search,diversity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.j2016b A simple, fast diverse decoding algorith.pdf}
}

@misc{li.j:2024cogsci,
  title = {An Information-Theoretic Model of Shallow and Deep Language Comprehension},
  author = {Li, Jiaxuan and Futrell, Richard},
  year = {2024},
  month = may,
  number = {arXiv:2405.08223},
  eprint = {2405.08223},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.08223},
  urldate = {2024-06-16},
  abstract = {A large body of work in psycholinguistics has focused on the idea that online language comprehension can be shallow or `good enough': given constraints on time or available computation, comprehenders may form interpretations of their input that are plausible but inaccurate. However, this idea has not yet been linked with formal theories of computation under resource constraints. Here we use information theory to formulate a model of language comprehension as an optimal trade-off between accuracy and processing depth, formalized as bits of information extracted from the input, which increases with processing time. The model provides a measure of processing effort as the change in processing depth, which we link to EEG signals and reading times. We validate our theory against a large-scale dataset of garden path sentence reading times, and EEG experiments featuring N400, P600 and biphasic ERP effects. By quantifying the timecourse of language processing as it proceeds from shallow to deep, our model provides a unified framework to explain behavioral and neural signatures of language comprehension.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.j2024cogsci An information-theoretic model of shallo.pdf}
}

@article{li.m:2004,
  title = {The Similarity Metric},
  author = {Li, M. and Chen, X. and Li, X. and Ma, B. and Vitanyi, P.M.B.},
  year = {2004},
  month = dec,
  journal = {IEEE Transactions on Information Theory},
  volume = {50},
  number = {12},
  pages = {3250--3264},
  issn = {0018-9448},
  doi = {10.1109/TIT.2004.838101},
  urldate = {2023-01-19},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.m2004 The similarity metric.pdf}
}

@book{li.m:2008,
  title = {An Introduction to {{Kolmogorov}} Complexity and Its Applications},
  author = {Li, Ming and Vit{\'a}nyi, Paul and others},
  year = {2008},
  volume = {3},
  publisher = {Springer},
  date-added = {2019-09-13 08:17:22 -0400},
  date-modified = {2019-09-13 08:17:36 -0400},
  project = {information-entropy},
  keywords = {kolmogorov complexity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.m2008 An introduction to Kolmogorov complexity.pdf}
}

@inproceedings{li.x:2019,
  title = {Specializing Word Embeddings (for Parsing) by Information Bottleneck},
  booktitle = {Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing ({{EMNLP-IJCNLP}})},
  author = {Li, Xiang Lisa and Eisner, Jason},
  year = {2019},
  pages = {2744--2754},
  publisher = {Association for Computational Linguistics},
  address = {Hong Kong, China},
  doi = {10.18653/v1/D19-1276}
}

@misc{li.x:2022,
  title = {Diffusion-{{LM Improves Controllable Text Generation}}},
  author = {Li, Xiang Lisa and Thickstun, John and Gulrajani, Ishaan and Liang, Percy and Hashimoto, Tatsunori B.},
  year = {2022},
  month = may,
  number = {arXiv:2205.14217},
  eprint = {2205.14217},
  primaryclass = {cs},
  institution = {arXiv},
  doi = {10.48550/arXiv.2205.14217},
  urldate = {2022-06-13},
  abstract = {Controlling the behavior of language models (LMs) without re-training is a major open problem in natural language generation. While recent works have demonstrated successes on controlling simple sentence attributes (e.g., sentiment), there has been little progress on complex, fine-grained controls (e.g., syntactic structure). To address this challenge, we develop a new non-autoregressive language model based on continuous diffusions that we call Diffusion-LM. Building upon the recent successes of diffusion models in continuous domains, Diffusion-LM iteratively denoises a sequence of Gaussian vectors into word vectors, yielding a sequence of intermediate latent variables. The continuous, hierarchical nature of these intermediate variables enables a simple gradient-based algorithm to perform complex, controllable generation tasks. We demonstrate successful control of Diffusion-LM for six challenging fine-grained control tasks, significantly outperforming prior work.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/li.x2022 Diffusion-LM Improves Controllable Text.pdf}
}

@inproceedings{liang.d:2018,
  title = {Variational Autoencoders for Collaborative Filtering},
  booktitle = {Proceedings of the 2018 {{World Wide Web Conference}}},
  author = {Liang, Dawen and Krishnan, Rahul G. and Hoffman, Matthew D. and Jebara, Tony},
  year = {2018},
  month = apr,
  series = {{{WWW}} '18},
  pages = {689--698},
  publisher = {International World Wide Web Conferences Steering Committee},
  address = {Republic and Canton of Geneva, CHE},
  doi = {10.1145/3178876.3186150},
  urldate = {2022-11-29},
  abstract = {We extend variational autoencoders (VAEs) to collaborative filtering for implicit feedback. This non-linear probabilistic model enables us to go beyond the limited modeling capacity of linear factor models which still largely dominate collaborative filtering research.We introduce a generative model with multinomial likelihood and use Bayesian inference for parameter estimation. Despite widespread use in language modeling and economics, the multinomial likelihood receives less attention in the recommender systems literature. We introduce a different regularization parameter for the learning objective, which proves to be crucial for achieving competitive performance. Remarkably, there is an efficient way to tune the parameter using annealing. The resulting model and learning algorithm has information-theoretic connections to maximum entropy discrimination and the information bottleneck principle. Empirically, we show that the proposed approach significantly outperforms several state-of-the-art baselines, including two recently-proposed neural network approaches, on several real-world datasets. We also provide extended experiments comparing the multinomial likelihood with other commonly used likelihood functions in the latent factor collaborative filtering literature and show favorable results. Finally, we identify the pros and cons of employing a principled Bayesian inference approach and characterize settings where it provides the most significant improvements.},
  isbn = {978-1-4503-5639-8},
  keywords = {bayesian models,collaborative filtering,implicit feedback,recommender systems,variational autoencoder},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/liang.d2018 Variational autoencoders for collaborati.pdf}
}

@article{liberti.l:2016,
  title = {Six Mathematical Gems from the History of Distance Geometry},
  author = {Liberti, Leo and Lavor, Carlile},
  year = {2016},
  journal = {International Transactions in Operational Research},
  volume = {23},
  number = {5},
  pages = {897--920},
  publisher = {Wiley Online Library},
  date-added = {2019-06-11 11:26:58 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {geometry}
}

@article{lieder.f:2020,
  title = {Resource-Rational Analysis: {{Understanding}} Human Cognition as the Optimal Use of Limited Computational Resources},
  shorttitle = {Resource-Rational Analysis},
  author = {Lieder, Falk and Griffiths, Thomas L.},
  year = {2020},
  journal = {Behavioral and Brain Sciences},
  volume = {43},
  pages = {e1},
  issn = {0140-525X, 1469-1825},
  doi = {10.1017/S0140525X1900061X},
  urldate = {2022-11-28},
  abstract = {Modeling human cognition is challenging because there are infinitely many mechanisms that can generate any given observation. Some researchers address this by constraining the hypothesis space through assumptions about what the human mind can and cannot do, while others constrain it through principles of rationality and adaptation. Recent work in economics, psychology, neuroscience, and linguistics has begun to integrate both approaches by augmenting rational models with cognitive constraints, incorporating rational principles into cognitive architectures, and applying optimality principles to understanding neural representations. We identify the rational use of limited resources as a unifying principle underlying these diverse approaches, expressing it in a new cognitive modeling paradigm called resource-rational analysis. The integration of rational principles with realistic cognitive constraints makes resource-rational analysis a promising framework for reverse-engineering cognitive mechanisms and representations. It has already shed new light on the debate about human rationality and can be leveraged to revisit classic questions of cognitive psychology within a principled computational framework. We demonstrate that resource-rational models can reconcile the mind's most impressive cognitive skills with people's ostensive irrationality. Resource-rational analysis also provides a new way to connect psychological theory more deeply with artificial intelligence, economics, neuroscience, and linguistics.},
  langid = {english},
  pmid = {30714890},
  keywords = {Artificial Intelligence,bounded rationality,Cognition,cognitive biases,cognitive mechanisms,cognitive modeling,Decision Making,Humans,Models Theoretical,Problem Solving,Psychological Theory,representations,resource rationality,Thinking},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lieder.f2020 Resource-rational analysis Understandin.pdf}
}

@inproceedings{lin.c:2018,
  title = {Neural Particle Smoothing for Sampling from Conditional Sequence Models},
  booktitle = {Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Lin, Chu-Cheng and Eisner, Jason},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/n18-1085},
  keywords = {parsing,sampling}
}

@article{lindley.d:1956,
  title = {On a Measure of the Information Provided by an Experiment},
  author = {Lindley, D. V.},
  year = {1956},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {27},
  number = {4},
  pages = {986--1005},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177728069},
  urldate = {2024-05-14},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lindley.d1956 On a measure of the information provided.pdf}
}

@inproceedings{linzen.t:2014,
  title = {Investigating the Role of Entropy in Sentence Processing},
  booktitle = {Proceedings of the {{Fifth Workshop}} on {{Cognitive Modeling}} and {{Computational Linguistics}}},
  author = {Linzen, Tal and Jaeger, Florian},
  editor = {Demberg, Vera and O'Donnell, Timothy},
  year = {2014},
  month = jun,
  pages = {10--18},
  publisher = {Association for Computational Linguistics},
  address = {Baltimore, Maryland, USA},
  doi = {10.3115/v1/W14-2002},
  urldate = {2024-07-27},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/linzen.t2014 Investigating the role of entropy in sen.pdf}
}

@article{linzen.t:2015,
  title = {Uncertainty and Expectation in Sentence Processing: {{Evidence}} from Subcategorization Distributions},
  author = {Linzen, Tal and Jaeger, T. Florian},
  year = {2015},
  journal = {Cognitive Science},
  volume = {40},
  number = {6},
  pages = {1382--1411},
  publisher = {Wiley},
  doi = {10.1111/cogs.12274},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12274},
  date-added = {2021-03-18 10:32:01 -0400},
  date-modified = {2021-03-18 10:37:45 -0400},
  keywords = {expectation,processing}
}

@article{linzen.t:2016,
  title = {Assessing the Ability of {{LSTMs}} to Learn Syntax-Sensitive Dependencies},
  author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
  year = {2016},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {4},
  pages = {521--535},
  doi = {10.1162/tacl_a_00115},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0115},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/linzen.t2016 Assessing the ability of LSTMs to learn.pdf}
}

@article{linzen.t:2018,
  title = {What Can Linguistics and Deep Learning Contribute to Each Other?},
  author = {Linzen, Tal},
  year = {2018},
  journal = {arXiv preprint arXiv:1809.04179},
  eprint = {1809.04179},
  archiveprefix = {arXiv},
  date-added = {2019-06-13 08:03:15 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {recurrent neural networks}
}

@article{lipman.b:1995,
  title = {Information {{Processing}} and {{Bounded Rationality}}: {{A Survey}}},
  shorttitle = {Information {{Processing}} and {{Bounded Rationality}}},
  author = {Lipman, Barton L.},
  year = {1995},
  journal = {The Canadian Journal of Economics / Revue canadienne d'Economique},
  volume = {28},
  number = {1},
  eprint = {136022},
  eprinttype = {jstor},
  pages = {42--67},
  publisher = {[Wiley, Canadian Economics Association]},
  issn = {0008-4085},
  doi = {10.2307/136022},
  urldate = {2022-06-14},
  abstract = {This paper surveys recent attempts to formulate a plausible and tractable model of bounded rationality. I focus in particular on models that view bounded rationality as stemming from limited information processing. I discuss partitional models (such as computability, automata, perceptrons, and optimal networks), non-partitional models, and axiomatic approaches. /// Transformation de l'information et rationalit{\'e} limit{\'e}e: une revue de la litt{\'e}rature. Ce m{\'e}moire examine certaines tentatives r{\'e}centes pour formuler un mod{\`e}le plausible et utilisable de la rationalit{\'e} limit{\'e}e. L'auteur s'attache en particulier aux mod{\`e}les qui pr{\'e}sentent la rationalit{\'e} limit{\'e}e comme un ph{\'e}nom{\`e}ne {\'e}manant de la limitation dans la capacit{\'e} {\`a} transformer l'information. L'auteur discute les mod{\`e}les qu'on appelle `partitionnels' (computabilit{\'e}, automates, perceptrons, r{\'e}seaux optimaux), les mod{\`e}les `non partitionnels' ainsi que les approches axiomatiques.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lipman.b1995 Information Processing and Bounded Ratio.pdf}
}

@article{liu.j:1998,
  title = {Rejection Control and Sequential Importance Sampling},
  author = {Liu, Jun S. and Chen, Rong and Wong, Wing Hung},
  year = {1998},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {93},
  number = {443},
  pages = {1022--1031},
  publisher = {Informa UK Limited},
  doi = {10.1080/01621459.1998.10473764},
  bdsk-url-2 = {https://doi.org/10.1080/01621459.1998.10473764},
  date-added = {2022-05-05 09:40:36 -0400},
  date-modified = {2022-05-05 09:42:57 -0400},
  keywords = {importance sampling,rejection controlled sequential importance sampling,sequential importance sampling,sequential monte carlo}
}

@book{liu.j:2004,
  title = {Monte {{Carlo}} Strategies in Scientific Computing},
  author = {Liu, Jun S.},
  year = {2004},
  series = {Springer {{Series}} in {{Statistics}}},
  publisher = {Springer},
  address = {New York, NY},
  doi = {10.1007/978-0-387-76371-2},
  urldate = {2022-12-07},
  isbn = {978-0-387-76369-9 978-0-387-76371-2},
  keywords = {convergence of random variables,Excel,Markov chain,Markov Chains,mathematical statistics,modeling,Monte Carlo Method,optimization,Potential,Probability theory,Random variable,Scientific Computing,statistics},
  file = {/Users/j/Zotfiles/liu.j2004 Monte Carlo strategies in scientific com.pdf}
}

@article{liu.j:2017,
  title = {In-Order Transition-Based Constituent Parsing},
  author = {Liu, Jiangming and Zhang, Yue},
  year = {2017},
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {413--424},
  doi = {10.1162/tacl_a_00070},
  bdsk-url-2 = {https://doi.org/10.1162/tacl\textsubscript{a}{$_0$}0070},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/liu.j2017 In-order transition-based constituent pa.pdf}
}

@inproceedings{liu.p:2023,
  title = {Generating {{Wikipedia}} by Summarizing Long Sequences},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Liu, Peter J. and Saleh, Mohammad and Pot, Etienne and Goodrich, Ben and Sepassi, Ryan and Kaiser, Lukasz and Shazeer, Noam},
  year = {2023},
  month = may,
  urldate = {2023-05-25},
  abstract = {We show that generating English Wikipedia articles can be approached as a multi- document summarization of source documents. We use extractive summarization to coarsely identify salient information and a neural abstractive model to generate the article. For the abstractive model, we introduce a decoder-only architecture that can scalably attend to very long sequences, much longer than typical encoder- decoder architectures used in sequence transduction. We show that this model can generate fluent, coherent multi-sentence paragraphs and even whole Wikipedia articles. When given reference documents, we show it can extract relevant factual information as reflected in perplexity, ROUGE scores and human evaluations.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/liu.p2023 Generating Wikipedia by summarizing long.pdf}
}

@misc{liu.q:2020,
  title = {A Survey on Contextual Embeddings},
  author = {Liu, Qi and Kusner, Matt J. and Blunsom, Phil},
  year = {2020},
  eprint = {2003.07278},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {word embeddings}
}

@inproceedings{liu.z:2021,
  title = {Morphological {{Segmentation}} for {{Seneca}}},
  booktitle = {Proceedings of the {{First Workshop}} on {{Natural Language Processing}} for {{Indigenous Languages}} of the {{Americas}}},
  author = {Liu, Zoey and Jimerson, Robert and Prud'hommeaux, Emily},
  year = {2021},
  month = jun,
  pages = {90--101},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.americasnlp-1.10},
  urldate = {2022-06-06},
  abstract = {This study takes up the task of low-resource morphological segmentation for Seneca, a critically endangered and morphologically complex Native American language primarily spoken in what is now New York State and Ontario. The labeled data in our experiments comes from two sources: one digitized from a publicly available grammar book and the other collected from informal sources. We treat these two sources as distinct domains and investigate different evaluation designs for model selection. The first design abides by standard practices and evaluate models with the in-domain development set, while the second one carries out evaluation using a development domain, or the out-of-domain development set. Across a series of monolingual and crosslinguistic training settings, our results demonstrate the utility of neural encoder-decoder architecture when coupled with multi-task learning.},
  keywords = {computational revitalization,iroquoian,morphology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/liu.z2021 Morphological Segmentation for Seneca.pdf}
}

@techreport{llamateam:2024Llama3,
  title = {The {{Llama}} 3 Herd of Models},
  author = {{Llama team}},
  year = {2024},
  month = jul,
  institution = {AI@Meta},
  urldate = {2024-07-25}
}

@article{lo.s:2015,
  title = {To Transform or Not to Transform: Using Generalized Linear Mixed Models to Analyse Reaction Time Data},
  author = {Lo, Steson and Andrews, Sally},
  year = {2015},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {Frontiers Media SA},
  doi = {10.3389/fpsyg.2015.01171},
  bdsk-url-2 = {https://doi.org/10.3389/fpsyg.2015.01171},
  date-added = {2022-02-23 22:30:34 -0500},
  date-modified = {2022-02-23 22:30:36 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lo.s2015 To transform or not to transform using.DOCX;/Users/j/Dropbox (MIT)/Zotfiles/lo.s2015 To transform or not to transform using.pdf}
}

@article{logacev.p:2016,
  title = {Understanding Underspecification: {{A}} Comparison of Two Computational Implementations},
  shorttitle = {Understanding Underspecification},
  author = {Loga{\v c}ev, Pavel and Vasishth, Shravan},
  year = {2016},
  month = may,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {69},
  number = {5},
  pages = {996--1012},
  publisher = {SAGE Publications},
  issn = {1747-0218},
  doi = {10.1080/17470218.2015.1134602},
  urldate = {2023-08-01},
  abstract = {Swets et al. (2008. Underspecification of syntactic ambiguities: Evidence from self-paced reading. Memory and Cognition, 36(1), 201--216) presented evidence that the so-called ambiguity advantage [Traxler et al. (1998). Adjunct attachment is not a form of lexical ambiguity resolution. Journal of Memory and Language, 39(4), 558--592], which has been explained in terms of the Unrestricted Race Model, can equally well be explained by assuming underspecification in ambiguous conditions driven by task-demands. Specifically, if comprehension questions require that ambiguities be resolved, the parser tends to make an attachment: when questions are about superficial aspects of the target sentence, readers tend to pursue an underspecification strategy. It is reasonable to assume that individual differences in strategy will play a significant role in the application of such strategies, so that studying average behaviour may not be informative. In order to study the predictions of the good-enough processing theory, we implemented two versions of underspecification: the partial specification model (PSM), which is an implementation of the Swets et al. proposal, and a more parsimonious version, the non-specification model (NSM). We evaluate the relative fit of these two kinds of underspecification to Swets et al.'s data; as a baseline, we also fitted three models that assume no underspecification. We find that a model without underspecification provides a somewhat better fit than both underspecification models, while the NSM model provides a better fit than the PSM. We interpret the results as lack of unambiguous evidence in favour of underspecification; however, given that there is considerable existing evidence for good-enough processing in the literature, it is reasonable to assume that some underspecification might occur. Under this assumption, the results can be interpreted as tentative evidence for NSM over PSM. More generally, our work provides a method for choosing between models of real-time processes in sentence comprehension that make qualitative predictions about the relationship between several dependent variables. We believe that sentence processing research will greatly benefit from a wider use of such methods.},
  langid = {english},
  keywords = {underspecification}
}

@article{lomashvili.l:2011,
  title = {Phases and Templates in {{Georgian}} Agreement},
  author = {Lomashvili, Leila and Harley, Heidi},
  year = {2011},
  journal = {Studia Linguistica},
  volume = {65},
  number = {3},
  pages = {233--267},
  publisher = {Wiley Online Library},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:50 -0400},
  project = {Icelandic gluttony},
  keywords = {phase theory,phi features}
}

@misc{lou.p:2018,
  title = {Disfluency Detection Using a Noisy Channel Model and a Deep Neural Language Model},
  author = {Lou, Paria Jamshid and Johnson, Mark},
  year = {2018},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1808.09091},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1808.09091},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-04-27 10:29:36 -0400},
  date-modified = {2022-04-27 10:29:37 -0400},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lou.p2018 Disfluency detection using a noisy chann.pdf}
}

@incollection{lounsbury.f:1954,
  title = {Transitional Probability, Linguistic Structure, and Systems of Habit-Family Hierarchies},
  booktitle = {Psycholinguistics},
  author = {Lounsbury, Floyd G},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {93--101},
  publisher = {Waverly Press Baltimore},
  chapter = {5.1},
  date-added = {2022-04-14 23:38:02 -0400},
  date-modified = {2022-04-14 23:51:56 -0400},
  keywords = {entropy reduction}
}

@article{lowder.m:2018,
  title = {Lexical Predictability during Natural Reading: {{Effects}} of Surprisal and Entropy Reduction},
  shorttitle = {Lexical Predictability during Natural Reading},
  author = {Lowder, Matthew W. and Choi, Wonil and Ferreira, Fernanda and Henderson, John M.},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1166--1183},
  issn = {1551-6709},
  doi = {10.1111/cogs.12597},
  urldate = {2022-10-13},
  abstract = {What are the effects of word-by-word predictability on sentence processing times during the natural reading of a text? Although information complexity metrics such as surprisal and entropy reduction have been useful in addressing this question, these metrics tend to be estimated using computational language models, which require some degree of commitment to a particular theory of language processing. Taking a different approach, this study implemented a large-scale cumulative cloze task to collect word-by-word predictability data for 40 passages and compute surprisal and entropy reduction values in a theory-neutral manner. A separate group of participants read the same texts while their eye movements were recorded. Results showed that increases in surprisal and entropy reduction were both associated with increases in reading times. Furthermore, these effects did not depend on the global difficulty of the text. The findings suggest that surprisal and entropy reduction independently contribute to variation in reading times, as these metrics seem to capture different aspects of lexical predictability.},
  langid = {english},
  keywords = {entropy reduction,Entropy reduction,Eyetracking,Prediction,Sentence processing,Surprisal,surprisal theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lowder.m2018 Lexical predictability during natural re.pdf}
}

@article{luce.r:2003,
  title = {Whatever Happened to Information Theory in Psychology?},
  author = {Luce, R. Duncan},
  year = {2003},
  month = jun,
  journal = {Review of General Psychology},
  volume = {7},
  number = {2},
  pages = {183--188},
  publisher = {SAGE Publications Inc},
  issn = {1089-2680},
  doi = {10.1037/1089-2680.7.2.183},
  urldate = {2024-05-03},
  abstract = {Although Shannon's information theory is alive and well in a number of fields, after an initial fad in psychology during the 1950s and 1960s it no longer is much of a factor, beyond the word bit, in psychological theory. The author discusses what seems to him (and others) to be the root causes of an actual incompatibility between information theory and the psychological phenomena to which it has been applied.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/luce.r2003 Whatever happened to information theory.pdf}
}

@article{ludecke.d:2018,
  title = {{{{\textbf{ggeffects}}}}: Tidy Data Frames of Marginal Effects from Regression Models},
  shorttitle = {Ggeffects},
  author = {L{\"u}decke, Daniel},
  year = {2018},
  month = jun,
  journal = {Journal of Open Source Software},
  volume = {3},
  number = {26},
  pages = {772},
  issn = {2475-9066},
  doi = {10.21105/joss.00772},
  urldate = {2024-01-24}
}

@inproceedings{lueckmann.j:2021,
  title = {Benchmarking Simulation-Based Inference},
  booktitle = {The 24th International Conference on Artificial Intelligence and Statistics, {{AISTATS}} 2021, April 13-15, 2021, Virtual Event},
  author = {Lueckmann, Jan-Matthis and Boelts, Jan and Greenberg, David S. and Gon{\c c}alves, Pedro J. and Macke, Jakob H.},
  editor = {Banerjee, Arindam and Fukumizu, Kenji},
  year = {2021},
  series = {Proceedings of Machine Learning Research},
  volume = {130},
  pages = {343--351},
  publisher = {PMLR},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/aistats/LueckmannBGGM21.bib},
  timestamp = {Wed, 14 Apr 2021 01:00:00 +0200}
}

@misc{lugosch.l:2020,
  title = {Surprisal-Triggered Conditional Computation with Neural Networks},
  author = {Lugosch, Loren and Nowrouzezahrai, Derek and Meyer, Brett H.},
  year = {2020},
  month = jun,
  number = {arXiv:2006.01659},
  eprint = {2006.01659},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-03-19},
  abstract = {Autoregressive neural network models have been used successfully for sequence generation, feature extraction, and hypothesis scoring. This paper presents yet another use for these models: allocating more computation to more difficult inputs. In our model, an autoregressive model is used both to extract features and to predict observations in a stream of input observations. The surprisal of the input, measured as the negative log-likelihood of the current observation according to the autoregressive model, is used as a measure of input difficulty. This in turn determines whether a small, fast network, or a big, slow network, is used. Experiments on two speech recognition tasks show that our model can match the performance of a baseline in which the big network is always used with 15\% fewer FLOPs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning}
}

@article{luke.s:2017,
  title = {The {{Provo Corpus}}: {{A}} Large Eye-Tracking Corpus with Predictability Norms},
  author = {Luke, Steven G. and Christianson, Kiel},
  year = {2017},
  month = may,
  volume = {50},
  number = {2},
  pages = {826--833},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.3758/s13428-017-0908-4},
  bdsk-url-2 = {https://doi.org/10.3758/s13428-017-0908-4},
  date-added = {2021-10-19 00:07:37 -0400},
  date-modified = {2021-10-19 00:07:38 -0400}
}

@inproceedings{luong.t:2015,
  title = {Evaluating Models of Computation and Storage in Human Sentence Processing},
  booktitle = {Proceedings of the Sixth Workshop on Cognitive Aspects of Computational Language Learning},
  author = {Luong, Thang and O'Donnell, Timothy and Goodman, Noah},
  year = {2015},
  month = sep,
  pages = {14--21},
  publisher = {Association for Computational Linguistics},
  address = {Lisbon, Portugal},
  doi = {10.18653/v1/W15-2403},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W15-2403},
  date-added = {2022-05-02 11:30:20 -0400},
  date-modified = {2022-05-17 08:07:37 -0400},
  keywords = {fragment grammars,incrementality,parsing}
}

@article{lupker.s:2008,
  title = {Transposed-Letter Effects: {{Consonants}}, Vowels and Letter Frequency},
  shorttitle = {Transposed-Letter Effects},
  author = {Lupker, Stephen J. and Perea, Manuel and Davis, Colin J.},
  year = {2008},
  month = jan,
  journal = {Language and Cognitive Processes},
  volume = {23},
  number = {1},
  pages = {93--116},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/01690960701579714},
  urldate = {2023-12-13},
  abstract = {There is now considerable evidence (e.g., Perea \& Lupker, 2003a, 2003b) that transposed-letter nonword primes (e.g., jugde for JUDGE) are more effective primes than replacement-letter nonword primes (e.g., jupte for JUDGE). Recently, Perea and Lupker (2004) demonstrated that, in Spanish, this transposed-letter prime advantage exists only when the transposed letters are consonants (C-C transpositions) and not when they are vowels (V-V transpositions). This vowel-consonant difference causes problems even for models that can successfully explain transposed-letter effects (e.g., SOLAR, Davis, 1999). In Experiment 1 in the present paper, we demonstrated a parallel result in a language with a different syllabic structure (English) in both a masked priming experiment and an unprimed lexical decision task in which the transposed letter strings (e.g., ADACEMY, ACEDAMY) were used as the nonwords. Results in Experiment 2 suggest that at least part of the reason for the vowel-consonant difference is because of the higher letter frequencies of the vowels. Possible alternative interpretations of the vowel-consonant difference are discussed.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/lupker.s2008 Transposed-letter effects Consonants, v.pdf}
}

@book{lurie.j:2009,
  title = {Higher Topos Theory (Preprint)},
  author = {Lurie, Jacob},
  year = {2009},
  publisher = {Princeton University Press},
  date-added = {2019-08-24 09:21:19 -0400},
  date-modified = {2019-08-24 09:23:18 -0400},
  keywords = {category theory,topos theory}
}

@article{mackay.d:1992,
  title = {Information-Based Objective Functions for Active Data Selection},
  author = {MacKay, David J. C.},
  year = {1992},
  month = jul,
  journal = {Neural Computation},
  volume = {4},
  number = {4},
  pages = {590--604},
  issn = {0899-7667},
  doi = {10.1162/neco.1992.4.4.590},
  urldate = {2024-05-15},
  abstract = {Learning can be made more efficient if we can actively select particularly salient data points. Within a Bayesian learning framework, objective functions are discussed that measure the expected informativeness of candidate measurements. Three alternative specifications of what we want to gain information about lead to three different criteria for data selection. All these criteria depend on the assumption that the hypothesis space is correct, which may prove to be their main weakness.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mackay.d1992 Information-based objective functions fo.pdf}
}

@book{mackay.d:2003,
  title = {Information Theory, Inference and Learning Algorithms},
  author = {MacKay, David J. C.},
  year = {2003},
  publisher = {Cambridge university press},
  date-added = {2020-02-16 21:05:41 -0500},
  date-modified = {2020-04-29 12:55:23 -0400},
  project = {information-entropy},
  keywords = {information theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mackay.d2003 Information theory, inference and learni.pdf}
}

@inproceedings{madureira.b:2020,
  title = {Incremental Processing in the Age of Non-Incremental Encoders: {{An}} Empirical Assessment of Bidirectional Models for Incremental {{NLU}}},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Madureira, Brielen and Schlangen, David},
  year = {2020},
  pages = {357--374},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.26},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.26}
}

@article{maehara.h:2013,
  title = {Euclidean Embeddings of Finite Metric Spaces},
  author = {Maehara, Hiroshi},
  year = {2013},
  journal = {Discrete Mathematics},
  volume = {313},
  number = {23},
  pages = {2848--2856},
  publisher = {Elsevier},
  date-added = {2019-06-13 07:52:44 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {euclidean space,geometry}
}

@inproceedings{magerman.d:1990,
  title = {Parsing a Natural Language Using Mutual Information Statistics.},
  booktitle = {{{AAAI}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1990},
  volume = {90},
  pages = {984--989},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:27:23 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,word association}
}

@inproceedings{magerman.d:1991,
  title = {{\emph{P}}earl: A Probabilistic Chart Parser},
  shorttitle = {{\emph{P}}earl},
  booktitle = {Proceedings of the Fifth Conference on {{European}} Chapter of the {{Association}} for {{Computational Linguistics}}},
  author = {Magerman, David M. and Marcus, Mitchell P.},
  year = {1991},
  month = apr,
  series = {{{EACL}} '91},
  pages = {15--20},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  doi = {10.3115/977180.977184},
  urldate = {2022-06-13},
  abstract = {This paper describes a natural language parsing algorithm for unrestricted text which uses a probability-based scoring function to select the "best" parse of a sentence. The parser, Pearl, is a time-asynchronous bottom-up chart parser with Earley-type top-down prediction which pursues the highest-scoring theory in the chart, where the score of a theory represents the extent to which the context of the sentence predicts that interpretation. This parser differs from previous attempts at stochastic parsers in that it uses a richer form of conditional probabilities based on context to predict likelihood. Pearl also provides a framework for incorporating the results of previous work in part-of-speech assignment, unknown word models, and other probabilistic models of linguistic features into one parsing tool, interleaving these techniques instead of using the traditional pipeline architecture. In preliminary tests, Pearl has been successful at resolving part-of-speech and word (in speech processing) ambiguity, determining categories for unknown words, and selecting correct parses first using a very loosely fitting covering grammar.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/magerman.d1991 Pearl a probabilistic chart pars.pdf}
}

@article{makkeh.a:2021,
  title = {Introducing a Differentiable Measure of Pointwise Shared Information},
  author = {Makkeh, Abdullah and Gutknecht, Aaron J. and Wibral, Michael},
  year = {2021},
  month = mar,
  journal = {Physical Review E},
  volume = {103},
  number = {3},
  publisher = {American Physical Society (APS)},
  doi = {10.1103/physreve.103.032149},
  bdsk-url-2 = {https://doi.org/10.1103/physreve.103.032149},
  date-added = {2022-04-18 11:15:53 -0400},
  date-modified = {2022-04-18 11:16:04 -0400},
  keywords = {partial information decomposition}
}

@book{malchukov.a:2012,
  title = {The Oxford Handbook of Case},
  author = {Malchukov, Andrej L. and Spencer, Andrew},
  year = {2012},
  publisher = {Oxford University Press},
  date-added = {2020-02-03 16:08:35 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  isbn = {978-0-19-920647-6},
  project = {Icelandic gluttony},
  keywords = {case}
}

@article{malsburg.t:2011,
  title = {What Is the Scanpath Signature of Syntactic Reanalysis?},
  author = {{von der Malsburg}, Titus and Vasishth, Shravan},
  year = {2011},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {65},
  number = {2},
  pages = {109--127},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2011.02.004},
  urldate = {2024-10-10},
  abstract = {Which repair strategy does the language system deploy when it gets garden-pathed, and what can regressive eye movements in reading tell us about reanalysis strategies? Several influential eye-tracking studies on syntactic reanalysis (Frazier and Rayner, 1982, Meseguer et al., 2002, Mitchell et al., 2008) have addressed this question by examining scanpaths, i.e., sequential patterns of eye fixations. However, in the absence of a suitable method for analyzing scanpaths, these studies relied on simplified dependent measures that are arguably ambiguous and hard to interpret. We address the theoretical question of repair strategy by developing a new method that quantifies scanpath similarity. Our method reveals several distinct fixation strategies associated with reanalysis that went undetected in a previously published data set (Meseguer et al., 2002). One prevalent pattern suggests re-parsing of the sentence, a strategy that has been discussed in the literature (Frazier \& Rayner, 1982); however, readers differed tremendously in how they orchestrated the various fixation strategies. Our results suggest that the human parsing system non-deterministically adopts different strategies when confronted with the disambiguating material in garden-path sentences.},
  keywords = {Eye movements,Individual differences,Parsing,Reading,Scanpaths,Syntactic reanalysis}
}

@article{manning.c:2020,
  title = {Emergent Linguistic Structure in Artificial Neural Networks Trained by Self-Supervision},
  author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
  year = {2020},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {117},
  number = {48},
  pages = {30046--30054},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1907367117},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1907367117},
  date-added = {2021-07-16 19:46:55 -0400},
  date-modified = {2021-07-16 19:46:57 -0400}
}

@inproceedings{mansinghka.v:2009,
  title = {Exact and Approximate Sampling by Systematic Stochastic Search},
  booktitle = {Proceedings of the Twelth International Conference on Artificial Intelligence and Statistics},
  author = {Mansinghka, Vikash and Roy, Daniel and Jonas, Eric and Tenenbaum, Joshua},
  editor = {{van Dyk}, David and Welling, Max},
  year = {2009-04-16/2009-04-18},
  series = {Proceedings of Machine Learning Research},
  volume = {5},
  pages = {400--407},
  publisher = {PMLR},
  address = {Hilton Clearwater Beach Resort, Clearwater Beach, Florida USA},
  abstract = {We introduce \textsubscript{a}daptive sequential rejection sampling\textsubscript{,} an algorithm for generating exact samples from high-dimensional, discrete distributions, building on ideas from classical AI search. Just as systematic search algorithms like A* recursively build complete solutions from partial solutions, sequential rejection sampling recursively builds exact samples over high-dimensional spaces from exact samples over lower-dimensional subspaces. Our algorithm recovers widely-used particle filters as an approximate variant without adaptation, and a randomized version of the directed arc consistency algorithm with backtracking when applied to deterministic problems. In this paper, we present the mathematical and algorithmic underpinnings of our approach and measure its behavior on ferromagnetic Isings and other probabilistic graphical models, obtaining exact and approximate samples in a range of situations.},
  date-added = {2022-05-05 09:38:21 -0400},
  date-modified = {2022-05-05 09:39:35 -0400},
  pdf = {http://proceedings.mlr.press/v5/mansinghka09a/mansinghka09a.pdf},
  keywords = {adaptive sequential rejection sampling}
}

@phdthesis{marcken.c:1996,
  title = {Unsupervised Language Acquisition},
  author = {{de Marcken}, Carl},
  year = {1996},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/bib/phd/ndltd/Marcken96},
  date-added = {2020-01-27 11:53:05 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Cambridge, MA, USA},
  keywords = {information theory,unsupervised grammar induction},
  timestamp = {Mon, 08 May 2017 16:29:45 +0200}
}

@incollection{marcken.c:1999,
  title = {On the Unsupervised Induction of Phrase-Structure Grammars},
  booktitle = {Natural Language Processing Using Very Large Corpora},
  author = {{de Marcken}, C.},
  editor = {Armstrong, Susan and Church, Kenneth and Isabelle, Pierre and Manzi, Sandra and Tzoukermann, Evelyne and Yarowsky, David},
  year = {1999},
  pages = {191--208},
  publisher = {Springer Netherlands},
  address = {Dordrecht},
  doi = {10.1007/978-94-017-2390-9_12},
  abstract = {Researchers investigating the acquisition of phrase-structure grammars from raw text have had only mixed success. In particular, unsupervised learning techniques, such as the inside-outside algorithm (Baker, 1979) for estimating the parameters of stochastic context-free grammars (SCFGs), tend to produce grammars that structure text in ways contrary to our linguistic intuitions. One effective way around this problem is to use hand-structured text like the Penn Treebank (Marcus, 1991) to constrain the learner: (Pereira and Schabes, 1992) demonstrate that the inside-outside algorithm can learn grammars effectively given such constraint, and currently the best performing parsers are trained on treebanks (Black et al., 1992; Magerman, 1995).},
  date-added = {2020-01-27 11:50:06 -0500},
  date-modified = {2021-07-16 11:27:47 -0400},
  isbn = {978-94-017-2390-9},
  project = {syntactic embedding},
  keywords = {information theory,unsupervised grammar induction}
}

@article{marcus.g:1999,
  title = {Rule Learning by Seven-Month-Old Infants},
  author = {Marcus, G. F. and Vijayan, S. and Bandi Rao, S. and Vishton, P. M.},
  year = {1999},
  month = jan,
  journal = {Science},
  volume = {283},
  number = {5398},
  pages = {77--80},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.283.5398.77},
  urldate = {2024-05-26},
  abstract = {A fundamental task of language acquisition is to extract abstract algebraic rules. Three experiments show that 7-month-old infants attend longer to sentences with unfamiliar structures than to sentences with familiar structures. The design of the artificial language task used in these experiments ensured that this discrimination could not be performed by counting, by a system that is sensitive only to transitional probabilities, or by a popular class of simple neural network models. Instead, these results suggest that infants can represent, extract, and generalize abstract algebraic rules.}
}

@phdthesis{marcus.m:1978phd,
  title = {A Theory of Syntactic Recognition for Natural Language},
  author = {Marcus, Mitchell P.},
  year = {1978},
  date-added = {2022-03-31 11:14:02 -0400},
  date-modified = {2022-04-26 21:21:13 -0400},
  school = {Massachusetts Institute of Technology},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marcus.m1978phd A theory of syntactic recognition for na.pdf}
}

@book{marcus.m:1980phdbook,
  title = {Theory of Syntactic Recognition for Natural Languages},
  author = {Marcus, Mitchell P.},
  year = {1980},
  publisher = {MIT Press},
  address = {Cambridge, MA, USA},
  date-added = {2022-03-31 11:15:48 -0400},
  date-modified = {2022-04-26 21:21:21 -0400},
  isbn = {0-262-13149-8}
}

@inproceedings{marcus.m:1994,
  title = {The {{Penn Treebank}}: {{Annotating}} Predicate Argument Structure},
  booktitle = {Human {{Language Technology}}: {{Proceedings}} of a {{Workshop}} Held at {{Plainsboro}}, {{New Jersey}}, {{March}} 8-11, 1994},
  author = {Marcus, Mitchell P. and Kim, Grace and Marcinkiewicz, Mary Ann and MacIntyre, Robert and Bies, Ann and Ferguson, Mark and Katz, Karen and Schasberger, Britta},
  year = {1994}
}

@phdthesis{marecek.d:2012,
  title = {Unsupervised Dependency Parsing},
  author = {Mare{\v c}ek, David},
  year = {2012},
  address = {Prague, Czech Republic},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-09-07 16:53:21 -0400},
  project = {syntactic embedding},
  school = {Charles University},
  keywords = {dependency parsing,unsupervised parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marecek.d2012 Unsupervised dependency parsing.pdf}
}

@inproceedings{marecek.d:2018,
  title = {Extracting Syntactic Trees from Transformer Encoder Self-Attentions},
  booktitle = {Proceedings of the 2018 {{EMNLP}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2018},
  month = nov,
  pages = {347--349},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/W18-5444},
  abstract = {This is a work in progress about extracting the sentence tree structures from the encoder's self-attention weights, when translating into another language using the Transformer neural network architecture. We visualize the structures and discuss their characteristics with respect to the existing syntactic theories and annotations.},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W18-5444},
  date-added = {2021-09-08 00:32:46 -0400},
  date-modified = {2021-09-08 00:32:47 -0400}
}

@inproceedings{marecek.d:2019,
  title = {From Balustrades to {{Pierre Vinken}}: Looking for Syntax in Transformer Self-Attentions},
  booktitle = {Proceedings of the 2019 {{ACL}} Workshop {{BlackboxNLP}}: {{Analyzing}} and Interpreting Neural Networks for {{NLP}}},
  author = {Mare{\v c}ek, David and Rosa, Rudolf},
  year = {2019},
  pages = {263--275},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4827},
  bdsk-url-2 = {https://doi.org/10.18653/v1/W19-4827}
}

@article{markov.a:1913,
  title = {{Essai d'une recherche statistique sur le texte du roman "Eug{\`e}ne On{\v e}gin", illustrant la liaison des {\'e}preuves en cha{\^i}ne}},
  author = {Markov, A. A.},
  year = {1913},
  journal = {Bulletin de l'Acad{\'e}mie Imp{\'e}riale des Sciences de St.-P{\'e}tersbourg. VI s{\'e}rie},
  series = {{6}},
  volume = {7},
  number = {3},
  pages = {153--162},
  langid = {russian}
}

@article{marneffe.m:2019,
  title = {Dependency Grammar},
  author = {{de Marneffe}, Marie-Catherine and Nivre, Joakim},
  year = {2019},
  journal = {Annual Review of Linguistics},
  volume = {5},
  number = {1},
  pages = {197--218},
  publisher = {Annual Reviews},
  doi = {10.1146/annurev-linguistics-011718-011842},
  bdsk-url-2 = {https://doi.org/10.1146/annurev-linguistics-011718-011842},
  date-added = {2021-07-16 19:34:18 -0400},
  date-modified = {2021-07-16 19:34:19 -0400}
}

@article{marr.d:1976,
  title = {From Understanding Computation to Understanding Neural Circuitry},
  author = {Marr, D. and Poggio, T.},
  year = {1976},
  month = may,
  urldate = {2022-06-06},
  abstract = {The CNS needs to be understood at four nearly independent levels of description: (1) that at which the nature of computation is expressed; (2) that at which the algorithms that implement a computation are characterized; (3) that at which an algorithm is committed to particular mechanisms; and (4) that at which the mechanisms are realized in hardware. In general, the nature of a computation is determined by the problem to be solved, the mechanisms that are used depend upon the available hardware, and the particular algorithms chosen depend on the problem and on the available mechanisms. Examples are given of theories at each level.},
  langid = {american},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marr.d1976 From understanding computation to unders.pdf}
}

@book{marr.d:1982,
  title = {Vision: A Computational Investigation into the Human Representation and Processing of Visual Information},
  shorttitle = {Vision},
  author = {Marr, David},
  year = {2010},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  collaborator = {Ullman, Shimon and Poggio, Tomaso},
  isbn = {978-0-262-51462-0},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marr.d1982 Vision a computational investigation in.pdf}
}

@article{marsden.e:2018,
  title = {A Methodological Synthesis of Self-Paced Reading in Second Language Research},
  author = {Marsden, Emma and Thompson, Sophie and Plonsky, Luke},
  year = {2018},
  month = sep,
  journal = {Applied Psycholinguistics},
  volume = {39},
  number = {5},
  pages = {861--904},
  issn = {0142-7164, 1469-1817},
  doi = {10.1017/S0142716418000036},
  urldate = {2024-05-26},
  abstract = {Self-paced reading tests (SPRs) are being increasingly adopted by second language (L2) researchers. Using SPR with L2 populations presents specific challenges, and its use is still evolving in L2 research (as well as in first language research, in many respects). Although the topic of several narrative overviews (Keating \& Jegerski, 2015; Roberts, 2016), we do not have a comprehensive picture of its usage in L2 research. Building on the growing body of systematic reviews of research practices in applied linguistics (e.g., Liu \& Brown, 2015; Plonsky, 2013), we report a methodological synthesis of the rationales, study contexts, and methodological decision making in L2 SPR research. Our comprehensive search yielded 74 SPRs used in L2 research. Each instrument was coded along 121 parameters, including: reported rationales and study characteristics, indicating the scope and nature of L2 SPR research agendas; design and analysis features and reporting practices, determining instrument validity and reliability; and materials transparency, affecting reproducibility and systematicity of agendas. Our findings indicate an urgent need to standardize the use and reporting of this technique, requiring empirical investigation to inform methodological decision making. We also identify several areas (e.g., study design, sample demographics, instrument construction, data analysis, and transparency) where SPR research could be improved to enrich our understanding of L2 processing, reading, and learning.},
  langid = {english},
  keywords = {foreign language learning,moving window,open science,research design,research methodology,second language learning,self-paced reading,synthesis,systematic review},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marsden.e2018 A methodological synthesis of self-paced.pdf}
}

@article{marslen-wilson.w:1973,
  title = {Linguistic Structure and Speech Shadowing at Very Short Latencies},
  author = {{Marslen-Wilson}, William D.},
  year = {1973},
  month = aug,
  journal = {Nature},
  volume = {244},
  number = {5417},
  pages = {522--523},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1038/244522a0},
  bdsk-url-2 = {https://doi.org/10.1038/244522a0},
  date-added = {2022-04-14 13:38:15 -0400},
  date-modified = {2022-05-02 14:45:30 -0400},
  keywords = {incrementality},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marslen-wilson.w1973 Linguistic structure and speech shadowin.pdf}
}

@article{marslen-wilson.w:1975,
  title = {Sentence Perception as an Interactive Parallel Process},
  author = {{Marslen-Wilson}, William D.},
  year = {1975},
  month = jul,
  journal = {Science (New York, N.Y.)},
  volume = {189},
  number = {4198},
  pages = {226--228},
  publisher = {American Association for the Advancement of Science (AAAS)},
  doi = {10.1126/science.189.4198.226},
  bdsk-url-2 = {https://doi.org/10.1126/science.189.4198.226},
  date-added = {2022-04-14 13:38:57 -0400},
  date-modified = {2022-05-02 14:45:37 -0400},
  keywords = {incrementality},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marslen-wilson.w1975 Sentence perception as an interactive pa.pdf}
}

@article{martino.l:2017,
  title = {Effective Sample Size for Importance Sampling Based on Discrepancy Measures},
  author = {Martino, Luca and Elvira, V{\'i}ctor and Louzada, Francisco},
  year = {2017},
  month = feb,
  journal = {Signal Processing},
  volume = {131},
  pages = {386--401},
  issn = {0165-1684},
  doi = {10.1016/j.sigpro.2016.08.025},
  urldate = {2022-12-16},
  abstract = {The Effective Sample Size (ESS) is an important measure of efficiency of Monte Carlo methods such as Markov Chain Monte Carlo (MCMC) and Importance Sampling (IS) techniques. In the IS context, an approximation ESS{\textasciicircum} of the theoretical ESS definition is widely applied, involving the inverse of the sum of the squares of the normalized importance weights. This formula, ESS{\textasciicircum}, has become an essential piece within Sequential Monte Carlo (SMC) methods, to assess the convenience of a resampling step. From another perspective, the expression ESS{\textasciicircum} is related to the Euclidean distance between the probability mass described by the normalized weights and the discrete uniform probability mass function (pmf). In this work, we derive other possible ESS functions based on different discrepancy measures between these two pmfs. Several examples are provided involving, for instance, the geometric mean of the weights, the discrete entropy (including the perplexity measure, already proposed in literature) and the Gini coefficient among others. We list five theoretical requirements which a generic ESS function should satisfy, allowing us to classify different ESS measures. We also compare the most promising ones by means of numerical simulations.},
  langid = {english},
  keywords = {Bayesian Inference,Effective Sample Size,Importance Sampling,Particle Filtering,Perplexity,Sequential Monte Carlo},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/martino.l2017 Effective sample size for importance sam.pdf}
}

@incollection{martino.l:2018,
  title = {Adaptive Rejection Sampling Methods},
  booktitle = {Independent {{Random Sampling Methods}}},
  author = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  editor = {Martino, Luca and Luengo, David and M{\'i}guez, Joaqu{\'i}n},
  year = {2018},
  series = {Statistics and {{Computing}}},
  pages = {115--157},
  publisher = {Springer International Publishing},
  address = {Cham},
  doi = {10.1007/978-3-319-72634-2_4},
  urldate = {2022-07-05},
  abstract = {This chapter is devoted to describing the class of the adaptive rejection sampling (ARS) schemes. These (theoretically) universal methods are very efficient samplers that update the proposal density whenever a generated sample is rejected in the RS test. In this way, they can produce i.i.d. samples from the target with an increasing acceptance rate that can converge to 1. As a by-product, these techniques also generate a sequence of proposal pdfs converging to the true shape of the target density. Another advantage of the ARS samplers is that, when they can be applied, the user only has to select a set of initial conditions. After the initialization, they are completely automatic, self-tuning algorithms (i.e., no parameters need to be adjusted by the user) regardless of the specific target density. However, the need to construct a suitable sequence of proposal densities restricts the practical applicability of this methodology. As a consequence, ARS schemes are often tailored to specific classes of target distributions. Indeed, the construction of the proposal is particularly hard in multidimensional spaces. Hence, ARS algorithms are usually designed only for drawing from univariate densities.},
  isbn = {978-3-319-72634-2},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/martino.l2018 Adaptive rejection sampling methods.pdf}
}

@inproceedings{marvin.r:2018,
  title = {Targeted Syntactic Evaluation of Language Models},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Marvin, Rebecca and Linzen, Tal},
  year = {2018},
  pages = {1192--1202},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1151},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1151},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/marvin.r2018 Targeted syntactic evaluation of languag 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/marvin.r2018 Targeted syntactic evaluation of languag.pdf}
}

@article{mazur.b:2008,
  title = {When Is One Thing Equalto Some Other Thing?},
  author = {Mazur, Barry},
  year = {2008},
  journal = {Proof and other dilemmas: Mathematics and philosophy},
  volume = {59},
  pages = {221},
  publisher = {MAA},
  date-added = {2019-08-24 09:29:47 -0400},
  date-modified = {2019-08-24 09:29:57 -0400},
  keywords = {category theory}
}

@inproceedings{mccann.b:2017,
  title = {Learned in Translation: {{Contextualized}} Word Vectors},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, {{CA}}, {{USA}}},
  author = {McCann, Bryan and Bradbury, James and Xiong, Caiming and Socher, Richard},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {6294--6305},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/McCannBXS17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@incollection{mccloskey.j:2017,
  title = {Resumption},
  booktitle = {The {{Wiley Blackwell Companion}} to {{Syntax}}, {{Second Edition}}},
  author = {McCloskey, James},
  year = {2017},
  pages = {1--30},
  publisher = {John Wiley \& Sons, Ltd},
  doi = {10.1002/9781118358733.wbsyncom105},
  urldate = {2023-04-10},
  abstract = {In many languages and in a range of circumstances, a pronominal element may appear in a position in which one might have expected to see a gap bound by a clause-peripheral element (in relative clauses, constituent questions, cleft constructions, and the like). Such elements (often, but not exclusively, personal pronouns) are known as resumptive elements. Since they are formally pronouns but serve many of the core semantic functions associated with movement constructions, the study of resumptive elements raises fundamental questions about the nature of movement, about the nature of anaphoric elements and relationships, and about the nature of the interaction between these two spheres. These questions have been the focus of a great deal of work, beginning especially in the middle 1970s; this chapter surveys that work -- the questions that have been asked, the phenomena that have been discovered, and the current state of thinking about the relevant questions. Consideration of the theoretical questions is organized around the asking of three questions: (i) what are the properties of resumptive elements; (ii) in what ways do they share, or fail to share, properties of movement constructions (island phenomena, reconstruction effects); and (iii) in what ways do they share, or fail to share, properties of anaphoric elements and interactions? In recent years especially, the study of resumption has been enriched by a great deal of experimental work, aimed not only at establishing some basic properties of resumptive elements, but also at better understanding the mechanisms involved in their production and comprehension. The chapter ends with a consideration of that work and its implications. Here, the central question that emerges quickly is that of how theories of competence and theories of performance interact with one another.},
  isbn = {978-1-118-35873-3},
  langid = {english},
  keywords = {anaphora,Chomsky,Noam,production,resumptive pronouns,syntax,theoretical linguistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mccloskey.j2017 Resumption.pdf}
}

@incollection{mcconnell-ginet.s:2000,
  title = {Meaning and Grammar: {{An}} Introduction to Semantics},
  booktitle = {Meaning and Grammar: {{An}} Introduction to Semantics},
  author = {{McConnell-Ginet}, Sally and Chierchia, Gennaro},
  year = {2000},
  publisher = {MIT Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@book{mcdonald.m:1977,
  title = {Iontenwennaweienstahkhwa' {{Mohawk}} Spelling Dictionary},
  author = {McDonald, Mary and Barnes, Ann and Cook, Louise and Herne, Jean and Jacobs, Rita and Jock, Louise and LaFrance, Harriett and Ransom, Elaine and Sinclair, Winnie and Tarbell, Elizabeth},
  editor = {Mithun, Marianne},
  year = {1977},
  month = sep,
  number = {Bulletin 429},
  publisher = {The University of the State of New York, State Education Department},
  address = {Albany, N.Y.},
  date-added = {2022-05-11 11:20:30 -0400},
  date-modified = {2022-05-11 11:26:42 -0400},
  keywords = {kanien'keha},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mcdonald.m1977 Iontenwennaweienstahkhwa' Mohawk spellin.pdf}
}

@inproceedings{mcdonald.r:2005,
  title = {Non-Projective Dependency Parsing Using Spanning Tree Algorithms},
  booktitle = {Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing},
  author = {McDonald, Ryan and Pereira, Fernando and Ribarov, Kiril and Haji{\v c}, Jan},
  year = {2005},
  pages = {523--530},
  publisher = {Association for Computational Linguistics},
  address = {Vancouver, British Columbia, Canada}
}

@inproceedings{mcdonald.r:2005a,
  title = {Online Large-Margin Training of Dependency Parsers},
  booktitle = {Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({{ACL}}'05)},
  author = {McDonald, Ryan and Crammer, Koby and Pereira, Fernando},
  year = {2005},
  pages = {91--98},
  publisher = {Association for Computational Linguistics},
  address = {Ann Arbor, Michigan},
  doi = {10.3115/1219840.1219852},
  bdsk-url-2 = {https://doi.org/10.3115/1219840.1219852}
}

@article{mcdonald.s:2003,
  title = {Low-Level Predictive Inference in Reading: The Influence of Transitional Probabilities on Eye Movements},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = jul,
  journal = {Vision Research},
  volume = {43},
  number = {16},
  pages = {1735--1751},
  publisher = {Elsevier BV},
  doi = {10.1016/s0042-6989(03)00237-2}
}

@article{mcdonald.s:2003a,
  title = {Eye Movements Reveal the On-Line Computation of Lexical Probabilities during Reading},
  author = {McDonald, Scott A. and Shillcock, Richard C.},
  year = {2003},
  month = nov,
  journal = {Psychological Science},
  volume = {14},
  number = {6},
  pages = {648--652},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1046/j.0956-7976.2003.psci_1480.x},
  urldate = {2022-10-13},
  abstract = {Skilled readers are able to derive meaning from a stream of visual input with remarkable efficiency. In this article, we present the first evidence that statistical information latent in the linguistic environment can contribute to an account of reading behavior. In two eye-tracking studies, we demonstrate that the transitional probabilities between words have a measurable influence on fixation durations, and using a simple Bayesian statistical model, we show that lexical probabilities derived by combining transitional probability with the prior probability of a word's occurrence provide the most parsimonious account of the eye movement data. We suggest that the brain is able to draw upon statistical information in order to rapidly estimate the lexical probabilities of upcoming words: a computationally inexpensive mechanism that may underlie proficient reading.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mcdonald.s2003a Eye movements reveal the on-line computa.pdf}
}

@book{mcelreath.r:2020sr2,
  title = {Statistical Rethinking: A {{Bayesian}} Course with Examples in {{R}} and {{Stan}}},
  shorttitle = {Statistical Rethinking},
  author = {McElreath, Richard},
  year = {2020},
  series = {Chapman \& {{Hall}}/{{CRC}} Texts in Statistical Science Series},
  edition = {Second edition},
  publisher = {CRC Press},
  address = {Boca Raton, FL, USA},
  isbn = {978-0-367-13991-9},
  langid = {english}
}

@article{mcfadden.t:2018,
  title = {What the {{EPP}} and Comp-Trace Effects Have in Common: {{Constraining}} Silent Elements at the Edge},
  author = {McFadden, Thomas and Sundaresan, Sandhya},
  year = {2018},
  journal = {Glossa: a journal of general linguistics},
  volume = {3},
  number = {1},
  publisher = {Ubiquity Press},
  date-added = {2020-02-02 08:05:04 -0500},
  date-modified = {2020-02-02 08:05:55 -0500},
  keywords = {EPP}
}

@article{mcgill.w:1954,
  title = {Multivariate Information Transmission},
  author = {McGill, W.},
  year = {1954},
  month = sep,
  journal = {Transactions of the IRE Professional Group on Information Theory},
  volume = {4},
  number = {4},
  pages = {93--111},
  issn = {2168-2704},
  doi = {10.1109/TIT.1954.1057469},
  urldate = {2024-05-03},
  abstract = {A multivariate analysis based on transmitted information is presented. It is shown that sample transmitted information provides a simple method for measuring and testing association in multidimensional contingency tables. Relations with analysis of variance are pointed out, and statistical tests are described.},
  keywords = {Analysis of variance,Communication systems,Contracts,Humans,Information analysis,Laboratories,Multidimensional systems,Organisms,Psychology,Testing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mcgill.w1954 Multivariate information transmission.pdf}
}

@misc{mcguffie.k:2020,
  title = {The Radicalization Risks of {{GPT-3}} and Advanced Neural Language Models},
  author = {McGuffie, Kris and Newhouse, Alex},
  year = {2020},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2009.06807},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2009.06807},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-15 11:01:44 -0400},
  date-modified = {2022-03-15 11:01:46 -0400},
  keywords = {Artificial Intelligence (cs.AI),Computers and Society (cs.CY),FOS: Computer and information sciences},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mcguffie.k2020 The radicalization risks of GPT-3 and ad.pdf}
}

@article{meehl.p:1957,
  title = {When Shall We Use Our Heads Instead of the Formula?},
  author = {Meehl, Paul E.},
  year = {1957},
  journal = {Journal of Counseling Psychology},
  volume = {4},
  number = {4},
  pages = {268--273},
  publisher = {Wm. C. Brown Co.},
  address = {US},
  issn = {1939-2168},
  doi = {10.1037/h0047554},
  abstract = {The statistical vs. clinical prediction issue as applied to daily clinical decisions. The problem of pragmatic decisions, the theoretical derivation of novel patterns, and the relationship of nonfrequentist probability and rational action are considered. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Clinical Judgment (Not Diagnosis),Decision Making,Statistics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/meehl.p1957 When shall we use our heads instead of t.pdf}
}

@article{mehta.p:2019,
  title = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  author = {Mehta, Pankaj and Bukov, Marin and Wang, Ching-Hao and Day, Alexandre G. R. and Richardson, Clint and Fisher, Charles K. and Schwab, David J.},
  year = {2019},
  month = may,
  journal = {Physics Reports},
  series = {A High-Bias, Low-Variance Introduction to {{Machine Learning}} for Physicists},
  volume = {810},
  pages = {1--124},
  issn = {0370-1573},
  doi = {10.1016/j.physrep.2019.03.001},
  urldate = {2022-07-11},
  abstract = {Machine Learning (ML) is one of the most exciting and dynamic areas of modern research and application. The purpose of this review is to provide an introduction to the core concepts and tools of machine learning in a manner easily understood and intuitive to physicists. The review begins by covering fundamental concepts in ML and modern statistics such as the bias--variance tradeoff, overfitting, regularization, generalization, and gradient descent before moving on to more advanced topics in both supervised and unsupervised learning. Topics covered in the review include ensemble models, deep learning and neural networks, clustering and data visualization, energy-based models (including MaxEnt models and Restricted Boltzmann Machines), and variational methods. Throughout, we emphasize the many natural connections between ML and statistical physics. A notable aspect of the review is the use of Python Jupyter notebooks to introduce modern ML/statistical packages to readers using physics-inspired datasets (the Ising Model and Monte-Carlo simulations of supersymmetric decays of proton--proton collisions). We conclude with an extended outlook discussing possible uses of machine learning for furthering our understanding of the physical world as well as open problems in ML where physicists may be able to contribute.},
  langid = {english},
  keywords = {boltzmann machines,energy models},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mehta.p2019 A high-bias, low-variance introduction t.pdf}
}

@inproceedings{meister.c:2020,
  title = {If Beam Search Is the Answer, What Was the Question?},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Meister, Clara and Cotterell, Ryan and Vieira, Tim},
  year = {2020},
  pages = {2173--2185},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.170},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.170},
  date-modified = {2022-03-31 09:40:23 -0400},
  keywords = {beam search,parsing}
}

@article{meister.c:2020tacl,
  title = {Best-First Beam Search},
  author = {Meister, Clara and Vieira, Tim and Cotterell, Ryan},
  year = {2020},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {795--809},
  publisher = {MIT Press - Journals},
  doi = {10.1162/tacl_a_00346},
  date-added = {2022-03-31 09:48:44 -0400},
  date-modified = {2022-03-31 09:55:48 -0400},
  keywords = {beam search,memory,parsing,space-complexity,time-complexity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/meister.c2020tacl Best-first beam search.pdf}
}

@inproceedings{meister.c:2021,
  title = {Revisiting the Uniform Information Density Hypothesis},
  booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
  author = {Meister, Clara and Pimentel, Tiago and Haller, Patrick and J{\"a}ger, Lena and Cotterell, Ryan and Levy, Roger},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.emnlp-main.74},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.emnlp-main.74},
  date-added = {2022-04-15 16:06:48 -0400},
  date-modified = {2022-04-15 16:06:50 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/meister.c2021 Revisiting the uniform information densi.pdf}
}

@inproceedings{meister.c:2022,
  title = {On the Probability--Quality Paradox in Language Generation},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 2: {{Short Papers}})},
  author = {Meister, Clara and Wiher, Gian and Pimentel, Tiago and Cotterell, Ryan},
  year = {2022},
  month = may,
  pages = {36--45},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-short.5},
  urldate = {2023-07-24},
  abstract = {When generating natural language from neural probabilistic models, high probability does not always coincide with high quality: It has often been observed that mode-seeking decoding methods, i.e., those that produce high-probability text under the model, lead to unnatural language. On the other hand, the lower-probability text generated by stochastic methods is perceived as more human-like. In this note, we offer an explanation for this phenomenon by analyzing language generation through an information-theoretic lens. Specifically, we posit that human-like language should contain an amount of information (quantified as negative log-probability) that is close to the entropy of the distribution over natural strings. Further, we posit that language with substantially more (or less) information is undesirable. We provide preliminary empirical evidence in favor of this hypothesis; quality ratings of both human and machine-generated text---covering multiple tasks and common decoding strategies---suggest high-quality text has an information content significantly closer to the entropy than we would expect by chance.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/meister.c2022 On the probability–quality paradox in la.pdf}
}

@article{meister.c:2023,
  title = {Locally {{Typical Sampling}}},
  author = {Meister, Clara and Pimentel, Tiago and Wiher, Gian and Cotterell, Ryan},
  year = {2023},
  month = jan,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {102--121},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00536},
  urldate = {2023-07-24},
  abstract = {Today's probabilistic language generators fall short when it comes to producing coherent and fluent text despite the fact that the underlying models perform well under standard metrics (e.g., perplexity). This discrepancy has puzzled the language generation community for the last few years. In this work, we posit that the abstraction of natural language generation as a discrete stochastic process---which allows for an information-theoretic analysis---can provide new insights into the behavior of probabilistic language generators, for example, why high-probability texts can be dull or repetitive. Humans use language as a means of communicating information, aiming to do so in a simultaneously efficient and error-minimizing manner; in fact, psycholinguistics research suggests humans choose each word in a string with this subconscious goal in mind. We formally define the set of strings that meet this criterion: Those for which each word has an information content close to the expected information content, namely, the conditional entropy of our model. We then propose a simple and efficient procedure for enforcing this criterion when generating from probabilistic models, which we call locally typical sampling. Automatic and human evaluations show that, in comparison to nucleus and top-k sampling, locally typical sampling offers competitive performance (in both abstractive summarization and story generation) in terms of quality while consistently reducing degenerate repetitions.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/meister.c2023 Locally Typical Sampling.pdf}
}

@book{melcuk.i:1988,
  title = {Dependency Syntax : {{Theory}} and Practice},
  author = {Mel'{\v c}uk, Igor A.},
  year = {1988},
  series = {{{SUNY}} Series in Linguistics},
  publisher = {State University of New York Press},
  address = {Albany, N.Y.},
  abstract = {This work presents the first sustained examination of Dependency Syntax. In clear and stimulating analyses Mel'cuk promotes syntactic description in terms of dependency rather than in terms of more familiar phrase-structure. The notions of dependency relations and dependency structure are introduced and substantiated, and the advantages of dependency representation are demonstrated by applying it to a number of popular linguistic problems, e.g. grammatical subject and ergative construction. A wide array of linguistic data is used -- the well-known (Dyirbal), the less known (Lezgian), and the more recent (Alutor). Several "exotic" cases of Russian are discussed to show how dependency can be used to solve difficult technical problems. The book is not only formal and rigorous, but also strongly theory-oriented and data-based. Special attention is paid to linguistic terminology, specifically to its logical consistency. The dependency formalism is presented within the framework of a new semantics-oriented general linguistic theory, Meaning-Text theory.},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-17 10:33:46 -0400},
  isbn = {978-0-88706-450-0},
  keywords = {Dependency Grammar}
}

@article{menne.m:2012,
  title = {An Overview of the Global Historical Climatology Network-Daily Database},
  author = {Menne, Matthew J. and Durre, Imke and Vose, Russell S. and Gleason, Byron E. and Houston, Tamara G.},
  year = {2012},
  month = jul,
  journal = {Journal of Atmospheric and Oceanic Technology},
  volume = {29},
  number = {7},
  pages = {897--910},
  publisher = {American Meteorological Society},
  doi = {10.1175/jtech-d-11-00103.1},
  bdsk-url-2 = {https://doi.org/10.1175/jtech-d-11-00103.1},
  date-added = {2021-12-03 19:02:07 -0500},
  date-modified = {2021-12-03 19:02:09 -0500}
}

@inproceedings{merkx.d:2021,
  title = {Human Sentence Processing: {{Recurrence}} or Attention?},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Merkx, Danny and Frank, Stefan L.},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.cmcl-1.2},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.2},
  date-added = {2021-11-29 10:15:11 -0500},
  date-modified = {2021-11-29 10:15:12 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/merkx.d2021 Human sentence processing Recurrence or.pdf}
}

@techreport{meta:2024Llama3modelcard,
  title = {Llama 3 Model Card},
  author = {{AI@Meta}},
  year = {2024},
  institution = {Meta}
}

@article{meylan.s:2021,
  title = {The Challenges of Large-Scale, Web-Based Language Datasets: {{Word}} Length and Predictability Revisited},
  author = {Meylan, Stephan C. and Griffiths, Thomas L.},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  publisher = {Wiley},
  doi = {10.1111/cogs.12983},
  bdsk-url-2 = {https://doi.org/10.1111/cogs.12983},
  date-added = {2021-07-25 10:58:38 -0400},
  date-modified = {2021-07-25 10:58:39 -0400}
}

@article{michaelov.j:2021,
  title = {Different Kinds of Cognitive Plausibility: Why Are Transformers Better than {{RNNs}} at Predicting {{N400}} Amplitude?},
  shorttitle = {Different Kinds of Cognitive Plausibility},
  author = {Michaelov, James A. and Bardolph, Megan D. and Coulson, Seana and Bergen, Benjamin},
  year = {2021},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {43},
  number = {43},
  urldate = {2023-08-18},
  abstract = {Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation---their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/michaelov.j2021 Different kinds of cognitive plausibilit.pdf}
}

@inproceedings{michaelov.j:2022,
  title = {Collateral Facilitation in Humans and Language Models},
  booktitle = {Proceedings of the 26th {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Michaelov, James and Bergen, Benjamin},
  year = {2022},
  pages = {13--26},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates (Hybrid)},
  doi = {10.18653/v1/2022.conll-1.2},
  urldate = {2024-03-01},
  abstract = {Are the predictions of humans and language models affected by similar things? Research suggests that while comprehending language, humans make predictions about upcoming words, with more predictable words being processed more easily. However, evidence also shows that humans display a similar processing advantage for highly anomalous words when these words are semantically related to the preceding context or to the most probable continuation. Using stimuli from 3 psycholinguistic experiments, we find that this is also almost always also the case for 8 contemporary transformer language models (BERT, ALBERT, RoBERTa, XLM-R, GPT-2, GPT-Neo, GPT-J, and XGLM). We then discuss the implications of this phenomenon for our understanding of both human language comprehension and the predictions made by language models.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/michaelov.j2022 Collateral facilitation in humans and la.pdf}
}

@article{michaelov.j:2023,
  title = {Strong {{Prediction}}: {{Language Model Surprisal Explains Multiple N400 Effects}}},
  shorttitle = {Strong {{Prediction}}},
  author = {Michaelov, James A. and Bardolph, Megan D. and Van Petten, Cyma K. and Bergen, Benjamin K. and Coulson, Seana},
  year = {2023},
  month = jun,
  journal = {Neurobiology of Language},
  pages = {1--29},
  issn = {2641-4368},
  doi = {10.1162/nol_a_00105},
  urldate = {2023-09-08},
  abstract = {Theoretical accounts of the N400 are divided as to whether the amplitude of the N400 response to a stimulus reflects the extent to which the stimulus was predicted, the extent to which the stimulus is semantically similar to its preceding context, or both. We use state-ofthe-art machine learning tools to investigate which of these three accounts is best supported by the evidence. GPT-3, a neural language model trained to compute the conditional probability of any word based on the words that precede it, was used to operationalize contextual predictability. In particular, we used an information-theoretic construct known as surprisal (the negative logarithm of the conditional probability). Contextual semantic similarity was operationalized by using two high-quality co-occurrence-derived vector-based meaning representations for words: GloVe and fastText. The cosine between the vector representation of the sentence frame and final word was used to derive contextual cosine similarity estimates. A series of regression models were constructed, where these variables, along with cloze probability and plausibility ratings, were used to predict single trial N400 amplitudes recorded from healthy adults as they read sentences whose final word varied in its predictability, plausibility, and semantic relationship to the likeliest sentence completion. Statistical model comparison indicated GPT-3 surprisal provided the best account of N400 amplitude and suggested that apparently disparate N400 effects of expectancy, plausibility, and contextual semantic similarity can be reduced to variation in the predictability of words. The results are argued to support predictive coding in the human language network.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/michaelov.j2023 Strong Prediction Language Model Surpri.pdf}
}

@article{michaelov.j:2024,
  title = {On the {{Mathematical Relationship Between Contextual Probability}} and {{N400 Amplitude}}},
  author = {Michaelov, James A. and Bergen, Benjamin K.},
  year = {2024},
  month = jun,
  journal = {Open Mind},
  volume = {8},
  pages = {859--897},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00150},
  urldate = {2024-07-20},
  abstract = {Accounts of human language comprehension propose different mathematical relationships between the contextual probability of a word and how difficult it is to process, including linear, logarithmic, and super-logarithmic ones. However, the empirical evidence favoring any of these over the others is mixed, appearing to vary depending on the index of processing difficulty used and the approach taken to calculate contextual probability. To help disentangle these results, we focus on the mathematical relationship between corpus-derived contextual probability and the N400, a neural index of processing difficulty. Specifically, we use 37 contemporary transformer language models to calculate the contextual probability of stimuli from 6 experimental studies of the N400, and test whether N400 amplitude is best predicted by a linear, logarithmic, super-logarithmic, or sub-logarithmic transformation of the probabilities calculated using these language models, as well as combinations of these transformed metrics. We replicate the finding that on some datasets, a combination of linearly and logarithmically-transformed probability can predict N400 amplitude better than either metric alone. In addition, we find that overall, the best single predictor of N400 amplitude is sub-logarithmically-transformed probability, which for almost all language models and datasets explains all the variance in N400 amplitude otherwise explained by the linear and logarithmic transformations. This is a novel finding that is not predicted by any current theoretical accounts, and thus one that we argue is likely to play an important role in increasing our understanding of how the statistical regularities of language impact language comprehension.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/michaelov.j2024 On the Mathematical Relationship Between.pdf}
}

@article{michel.j:2024,
  title = {Distributions for Compositionally Differentiating Parametric Discontinuities},
  author = {Michel, Jesse and Mu, Kevin and Yang, Xuanda and Bangaru, Sai Praveen and Collins, Elias Rojas and Bernstein, Gilbert and {Ragan-Kelley}, Jonathan and Carbin, Michael and Li, Tzu-Mao},
  year = {2024},
  month = apr,
  journal = {Proceedings of the ACM on Programming Languages},
  volume = {8},
  number = {OOPSLA1},
  pages = {126:893--126:922},
  doi = {10.1145/3649843},
  urldate = {2024-05-09},
  abstract = {Computations in physical simulation, computer graphics, and probabilistic inference often require the differentiation of discontinuous processes due to contact, occlusion, and changes at a point in time. Popular differentiable programming languages, such as PyTorch and JAX, ignore discontinuities during differentiation. This is incorrect for parametric discontinuities---conditionals containing at least one real-valued parameter and at least one variable of integration. We introduce Potto, the first differentiable first-order programming language to soundly differentiate parametric discontinuities. We present a denotational semantics for programs and program derivatives and show the two accord. We describe the implementation of Potto, which enables separate compilation of programs. Our prototype implementation overcomes previous compile-time bottlenecks achieving an 88.1x and 441.2x speed up in compile time and a 2.5x and 7.9x speed up in runtime, respectively, on two increasingly large image stylization benchmarks. We showcase Potto by implementing a prototype differentiable renderer with separately compiled shaders.},
  keywords = {Denotational Semantics,Differentiable Programming,Differentiable Rendering,Distribution Theory,Probabilistic Programming},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/michel.j2024 Distributions for compositionally differ.pdf}
}

@book{michelson.k:2016,
  title = {Iroquoian {{Languages}}},
  author = {Michelson, Karin},
  year = {2016},
  month = aug,
  publisher = {Oxford University Press},
  doi = {10.1093/acrefore/9780199384655.013.47},
  urldate = {2022-05-30},
  abstract = {The Iroquoian languages are spoken today in New York State, Ontario, Quebec, Wisconsin, North Carolina, and Oklahoma. The languages share a relatively small segment inventory, a challenging accentual system, polysynthetic morphology, a complex system of pronominal affixes, an unusual kinship terminology, and a syntax that functions almost exclusively to combine the meaning of two expressions. Some of the languages have been documented since contact with Europeans in the 16th century. There exists substantial scholarly linguistic work on most of the languages, and solid teaching materials continue to be developed.},
  isbn = {978-0-19-938465-5},
  langid = {english},
  keywords = {iroquoian}
}

@inproceedings{mikolov.t:2013,
  title = {Distributed Representations of Words and Phrases and Their Compositionality},
  booktitle = {Advances in {{Neural Information Processing Systems}} 26: 27th Annual Conference on Neural Information Processing Systems 2013. {{Proceedings}} of a Meeting Held {{December}} 5-8, 2013, {{Lake Tahoe}}, {{Nevada}}, {{United States}}},
  author = {Mikolov, Tom{\'a}s and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
  editor = {Burges, Christopher J. C. and Bottou, L{\'e}on and Ghahramani, Zoubin and Weinberger, Kilian Q.},
  year = {2013},
  pages = {3111--3119},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/MikolovSCCD13.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{mikolov.t:2013a,
  title = {Linguistic Regularities in Continuous Space Word Representations},
  booktitle = {Proceedings of the 2013 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Mikolov, Tom{\'a}s and Yih, Wen-tau and Zweig, Geoffrey},
  year = {2013},
  pages = {746--751},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, Georgia}
}

@article{miller.g:1957,
  title = {The Magical Number Seven, plus or Minus Two: {{Some}} Limits on Our Capacity for Processing Information.},
  shorttitle = {The Magical Number Seven, plus or Minus Two},
  author = {Miller, George A.},
  year = {1957},
  month = feb,
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {81},
  publisher = {US: American Psychological Association},
  issn = {1939-1471},
  doi = {10.1037/h0043158},
  urldate = {2022-09-25},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/miller.g1957 The magical number seven, plus or minus.pdf}
}

@incollection{miller.g:1963,
  title = {Finitary Models of Language Users},
  booktitle = {Handbook of Mathematical Psychology},
  author = {Miller, George A. and Chomsky, Noam},
  editor = {Luce, D.},
  year = {1963},
  pages = {2--419},
  publisher = {John Wiley \& Sons.},
  date-added = {2022-03-31 11:48:29 -0400},
  date-modified = {2022-03-31 11:48:31 -0400}
}

@inproceedings{milward.d:1995,
  title = {Incremental Interpretation of Categorial Grammar},
  booktitle = {Seventh Conference of the {{European}} Chapter of the Association for Computational Linguistics},
  author = {Milward, David},
  year = {1995},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland}
}

@inproceedings{min.s:2022,
  title = {Noisy Channel Language Model Prompting for Few-Shot Text Classification},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Min, Sewon and Lewis, Mike and Hajishirzi, Hannaneh and Zettlemoyer, Luke},
  year = {2022},
  month = may,
  pages = {5316--5330},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.365},
  urldate = {2022-10-25},
  abstract = {We introduce a noisy channel approach for language model prompting in few-shot text classification. Instead of computing the likelihood of the label given the input (referred as direct models), channel models compute the conditional probability of the input given the label, and are thereby required to explain every word in the input. We use channel models for recently proposed few-shot learning methods with no or very limited updates to the language model parameters, via either in-context demonstration or prompt tuning. Our experiments show that, for both methods, channel models significantly outperform their direct counterparts, which we attribute to their stability, i.e., lower variance and higher worst-case accuracy. We also present extensive ablations that provide recommendations for when to use channel prompt tuning instead of other competitive models (e.g., direct head tuning): channel prompt tuning is preferred when the number of training examples is small, labels in the training data are imbalanced, or generalization to unseen labels is required.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/min.s2022 Noisy channel language model prompting f.pdf}
}

@inproceedings{minka.t:2001,
  title = {Expectation Propagation for Approximate {{Bayesian}} Inference},
  booktitle = {Proceedings of the {{Seventeenth}} Conference on {{Uncertainty}} in Artificial Intelligence},
  author = {Minka, Thomas Peter},
  year = {2001},
  month = aug,
  series = {{{UAI}}'01},
  pages = {362--369},
  publisher = {Morgan Kaufmann Publishers Inc., San Francisco, CA},
  address = {Seattle, WA},
  urldate = {2023-03-30},
  abstract = {This paper presents a new deterministic approximation technique in Bayesian networks. This method, "Expectation Propagation," unifies two previous techniques: assumed-density filtering, an extension of the Kalman filter, and loopy belief propagation, an extension of belief propagation in Bayesian networks. Loopy belief propagation, because it propagates exact belief states, is useful for a limited class of belief networks, such as those which are purely discrete. Expectation Propagation approximates the belief states by only retaining expectations, such as mean and varitmce, and iterates until these expectations are consistent throughout the network. This makes it applicable to hybrid networks with discrete and continuous nodes. Experiments with Gaussian mixture models show Expectation Propagation to be donvincingly better than methods with similar computational cost: Laplace's method, variational Bayes, and Monte Carlo. Expectation Propagation also provides an efficient algorithm for training Bayes point machine classifiers.},
  isbn = {978-1-55860-800-9},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/minka.t2001 Expectation propagation for approximate.pdf}
}

@phdthesis{minka.t:2001phd,
  title = {A Family of Algorithms for Approximate {{Bayesian}} Inference},
  author = {Minka, Thomas Peter},
  year = {2001},
  address = {Cambridge, MA},
  urldate = {2023-03-30},
  abstract = {Thesis (Ph.D.)--Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science, 2001.},
  copyright = {M.I.T. theses are protected by copyright. They may be viewed from this source for any purpose, but reproduction or distribution in any format is prohibited without written permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {expectation propogation},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/minka.t2001phd A family of algorithms for approximate B.pdf}
}

@incollection{mitchell.d:1984,
  title = {An Evaluation of Subject-Paced Reading Tasks and Other Methods for Investigating Immediate Processes in Reading                      1},
  booktitle = {New {{Methods}} in {{Reading Comprehension Research}}},
  author = {Mitchell, Don C.},
  year = {1984},
  publisher = {Routledge},
  abstract = {Over the last 5-6 years my colleagues and I have made use of three main experimental techniques for investigating immediate processing. In chronological order these were the Rapid Sequential Visual Presentation (RSVP) task, the Subject-Paced Reading Task and various types of priming tasks.},
  isbn = {978-0-429-50537-9},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mitchell.d1984 An evaluation of subject-paced reading t.pdf}
}

@inproceedings{mitchell.j:2010,
  title = {Syntactic and Semantic Factors in Processing Difficulty: {{An}} Integrated Measure},
  booktitle = {Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics},
  author = {Mitchell, Jeff and Lapata, Mirella and Demberg, Vera and Keller, Frank},
  year = {2010},
  pages = {196--206},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mitchell.j2010 Syntactic and semantic factors in proces.pdf}
}

@incollection{mithun.m:2014,
  title = {Syntactic and Prosodic Structures: {{Segmentation}}, Integration, and in Between},
  booktitle = {Spoken {{Corpora}} and {{Linguistic Studies}}},
  author = {Mithun, Marianne},
  editor = {Raso, Tommaso and Mello, Heliana},
  year = {2014},
  series = {Studies in {{Corpus Linguistics}}},
  volume = {61},
  pages = {297--330},
  publisher = {John Benjamins Publishing Company},
  abstract = {In this paper the focus is on syntactic and prosodic structures in a language that is typologically quite different from the majority languages of Europe and Asia. Mohawk, a language of the Iroquoian family, is indigenous to northeastern North America. Examples cited here are drawn from unscripted conversations. Though much of the grammatical structure of Mohawk differs substantially from that of European languages, many of the devices exploited by speakers to shape the flow of information converge.},
  langid = {english},
  keywords = {iroquoian},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mithun.m2014 Syntactic and prosodic structures Segme.pdf}
}

@incollection{mithun.m:2020,
  title = {Discourse Particle Position and Information Structure},
  booktitle = {Information-{{Structural Perspectives}} on {{Discourse Particles}}},
  author = {Mithun, Marianne},
  editor = {Modicom, Pierre-Yves and Dupl{\^a}tre, Olivier},
  year = {2020},
  month = mar,
  series = {Studies in {{Language Companion Series}}},
  number = {213},
  pages = {27--46},
  publisher = {John Benjamins Publishing Company},
  doi = {10.1075/slcs.213.01mit},
  urldate = {2022-05-30},
  abstract = {Discourse markers differ cross-linguistically not only in their functions but also in their positions within the sentence. Some are sentence-initial, some are sentence-final, and some occur in what has been termed the `middle-field'. But many appear simply in second position in the sentence. In many cases the positions of the markers can be explained in terms of the source constructions from which they emerged. Here one likely pathway of development is traced in Mohawk, indigenous to North America, illustrated with a pervasive marker of discourse coherence. Patterns in the modern language suggest that it and others emerged from marked information structures, which, over time, evolved into basic clause structures via familiar mechanisms of grammaticalization.},
  langid = {english},
  keywords = {iroquoian}
}

@article{mollica.f:2017,
  title = {How Data Drive Early Word Learning: A Cross-Linguistic Waiting Time Analysis},
  author = {Mollica, Francis and Piantadosi, Steven T.},
  year = {2017},
  month = sep,
  journal = {Open Mind},
  volume = {1},
  number = {2},
  pages = {67--77},
  issn = {2470-2986},
  doi = {10.1162/OPMI_a_00006},
  urldate = {2022-09-28},
  abstract = {The extent to which word learning is delayed by maturation as opposed to accumulating data is a longstanding question in language acquisition. Further, the precise way in which data influence learning on a large scale is unknown---experimental results reveal that children can rapidly learn words from single instances as well as by aggregating ambiguous information across multiple situations. We analyze Wordbank, a large cross-linguistic dataset of word acquisition norms, using a statistical waiting time model to quantify the role of data in early language learning, building off Hidaka (2013). We find that the model both fits and accurately predicts the shape of children's growth curves. Further analyses of model parameters suggest a primarily data-driven account of early word learning. The parameters of the model directly characterize both the amount of data required and the rate at which informative data occurs. With high statistical certainty, words require on the order of {$\sim$} 10 learning instances, which occur on average once every two months. Our method is extremely simple, statistically principled, and broadly applicable to modeling data-driven learning effects in development.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/mollica.f2017 How data drive early word learning a cr.pdf}
}

@article{montague.r:1970,
  title = {Universal Grammar},
  author = {Montague, Richard},
  year = {1970},
  journal = {Theoria: a Swedish journal of philosophy and psychology},
  volume = {36},
  number = {3},
  pages = {373--398},
  doi = {10.1111/j.1755-2567.1970.tb00434.x},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@phdthesis{montalbetti.m:1984phd,
  title = {After Binding : On the Interpretation of Pronouns},
  shorttitle = {After Binding},
  author = {Montalbetti, Mario M.},
  year = {1984},
  urldate = {2023-08-01},
  abstract = {This thesis is a study of the interpretation of pronouns, in particular, of the interpretive differences between overt and empty pronouns in certain configurations involving binding phenomena. We have captured these differences by means of a constraint which we have called the Overt Pronoun Constraint (Ope) and which is operative at the level of Logical Form. Informally, the ope states that overt pronouns that are in contrastive distribution with empty ones cannot link to formal variables (where by formal variable we roughly mean WH and QR traces). Some theoretically interesting consequences follow from the ope. For one thing, it shows that the lexical realization (or not) of a pronoun carries with it important interpretive consequences hence arguing for the view that the so called Null Subject Parameter has relevant LF properties. Indeed, if overt pronouns Cof the type nlentioned) cannot link to formal variables then they cannot be interpreted as bound variables. However, there are certain configurations in which overt pronouns can act as bound variables, and these configurations involve the presence of an extra bound pronoun which serves as a gate for binding. We will show that these cases present us with empirical evidence in favor of a Linking theory of binding (as outlined in Higginbotham 1983). Furthermore we use the ope as a diagnostic for both the existence and nature of certain controversial empty categories that occur in constructions such as clitic constructions, restructuring constructions, empty operator binding constructions, etc. The case of sloppy identity is also analyzed in terms of the ope. Although our analysis is based on the behavior of Spanish pronouns, we extend it to cover the behavior of pronouns in other Romance languages (Italian, Portuguese, Catalan) as well as in languages like Japanese and Chinese. The hope is thus parametrized to account for the subtle differences which underlie the striking similarities between the languages studied.},
  copyright = {M.I.T. theses are protected by  copyright. They may be viewed from this source for any purpose, but  reproduction or distribution in any format is prohibited without written  permission. See provided URL for inquiries about permission.},
  langid = {english},
  school = {Massachusetts Institute of Technology},
  keywords = {comparative illusions}
}

@incollection{moortgat.m:1997,
  title = {Categorial Type Logics},
  booktitle = {Handbook of Logic and Language},
  author = {Moortgat, M.},
  year = {1997},
  pages = {93--177},
  publisher = {Elsevier},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:01 -0400}
}

@incollection{muller.h:2020,
  title = {Negative Polarity Illusions},
  booktitle = {The {{Oxford Handbook}} of {{Negation}}},
  author = {Muller, Hanna and Phillips, Colin},
  editor = {D{\'e}prez, Viviane and Espinal, M. Teresa},
  year = {2020},
  month = mar,
  pages = {656--676},
  publisher = {Oxford University Press},
  doi = {10.1093/oxfordhb/9780198830528.013.42},
  urldate = {2023-02-22},
  abstract = {Although decades of research have illuminated the licensing requirements, both syntactic and semantic, of negative polarity items, the matter of how these licensing requirements are satisfied in real time, as a sentence is being processed, remains an ill-understood problem. Grammatical illusions---cases where native speakers, as they comprehend an ungrammatical sentence, experience a fleeting perception of acceptability---offer a window into online computations like NPI licensing. This chapter reviews the findings on negative polarity illusions, their parallels (and, in some cases, the lack of parallels) with other grammaticality illusions, and the implications of this line of research for understanding the incremental processing of negative sentences as well as negative polarity phenomena more broadly.},
  isbn = {978-0-19-883052-8},
  keywords = {grammaticality illusions,negative polarity illusions},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/muller.h2020 Negative polarity illusions.pdf}
}

@article{murray.w:2004,
  title = {Serial Mechanisms in Lexical Access: The Rank Hypothesis.},
  author = {Murray, Wayne S and Forster, Kenneth I},
  year = {2004},
  journal = {Psychological Review},
  volume = {111},
  number = {3},
  pages = {721},
  publisher = {American Psychological Association},
  doi = {10.1037/0033-295X.111.3.721},
  date-added = {2021-02-16 16:15:20 -0500},
  date-modified = {2021-02-16 16:16:45 -0500},
  keywords = {frequency effects,psycholinguistics,rank hypothesis,word access},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/murray.w2004 Serial mechanisms in lexical access the.pdf}
}

@article{nadas.a:1984,
  title = {Estimation of Probabilities in the Language Model of the {{IBM}} Speech Recognition System},
  author = {N{\'a}das, Arthur},
  year = {1984},
  month = aug,
  journal = {IEEE Transactions on Acoustics, Speech, and Signal Processing},
  volume = {32},
  number = {4},
  pages = {859--861},
  issn = {0096-3518},
  doi = {10.1109/TASSP.1984.1164378},
  urldate = {2024-08-02},
  abstract = {The language model probabilities are estimated by an empirical Bayes approach in which a prior distribution for the unknown probabilities is itself estimated through a novel choice of data. The predictive power of the model thus fitted is compared by means of its experimental perplexity [1] to the model as fitted by the Jelinek-Mercer deleted estimator and as fitted by the Turing-Good formulas for probabilities of unseen or rarely seen events.},
  keywords = {Bayesian methods,Cities and towns,Natural languages,Power system modeling,Predictive models,Probability,Smoothing methods,Speech recognition,Vocabulary}
}

@misc{naesseth.c:2017VSMC,
  title = {Variational Sequential {{Monte Carlo}}},
  author = {Naesseth, Christian A. and Linderman, Scott W. and Ranganath, Rajesh and Blei, David M.},
  year = {2017},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1705.11140},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.1705.11140},
  date-added = {2022-05-03 21:09:24 -0400},
  date-modified = {2022-05-05 09:15:25 -0400},
  keywords = {sequential monte carlo,variational inference,variational sequential monte carlo},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/naesseth.c2017VSMC Variational sequential Monte Carlo.pdf}
}

@article{naesseth.c:2019,
  title = {Elements of {{Sequential Monte Carlo}}},
  author = {Naesseth, Christian A. and Lindsten, Fredrik and Sch{\"o}n, Thomas B.},
  year = {2019},
  journal = {Foundations and Trends{\textregistered} in Machine Learning},
  volume = {12},
  number = {3},
  pages = {307--392},
  publisher = {Now Publishers},
  issn = {1935-8237},
  doi = {10.1561/2200000074},
  keywords = {Bayesian learning,Learning and statistical methods,Sampling},
  annotation = {note: Accessed at DOI: 10.48550/arXiv.1903.04797},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/naesseth.c2019 Elements of Sequential Monte Carlo.pdf}
}

@misc{nair.s:2023arxiv,
  title = {Words, {{Subwords}}, and {{Morphemes}}: {{What Really Matters}} in the {{Surprisal-Reading Time Relationship}}?},
  shorttitle = {Words, {{Subwords}}, and {{Morphemes}}},
  author = {Nair, Sathvik and Resnik, Philip},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17774},
  eprint = {2310.17774},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {An important assumption that comes with using LLMs on psycholinguistic data has gone unverified. LLM-based predictions are based on subword tokenization, not decomposition of words into morphemes. Does that matter? We carefully test this by comparing surprisal estimates using orthographic, morphological, and BPE tokenization against reading time data. Our results replicate previous findings and provide evidence that in the aggregate, predictions using BPE tokenization do not suffer relative to morphological and orthographic segmentation. However, a finer-grained analysis points to potential issues with relying on BPE-based tokenization, as well as providing promising results involving morphologically-aware surprisal estimates and suggesting a new method for evaluating morphological prediction.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,surprisal,tokenization},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nair.s2023arxiv Words, Subwords, and Morphemes What Rea.pdf}
}

@article{nalborczyk.l:2019,
  title = {An Introduction to {{Bayesian}} Multilevel Models Using {{{\textbf{brms}}}}: A Case Study of Gender Effects on Vowel Variability in Standard Indonesian},
  shorttitle = {An Introduction to Bayesian Multilevel Models Using Brms},
  author = {Nalborczyk, Ladislas and Batailler, C{\'e}dric and L{\oe}venbruck, H{\'e}l{\`e}ne and Vilain, Anne and B{\"u}rkner, Paul-Christian},
  year = {2019},
  month = may,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {62},
  number = {5},
  pages = {1225--1242},
  publisher = {American Speech-Language-Hearing Association},
  doi = {10.1044/2018_JSLHR-S-18-0006},
  urldate = {2024-05-21},
  abstract = {Purpose  Bayesian multilevel models are increasingly used to overcome the limitations of frequentist approaches in the analysis of complex structured data. This tutorial introduces Bayesian multilevel modeling for the specific analysis of speech data, using the brms package developed in R. Method  In this tutorial, we provide a practical introduction to Bayesian multilevel modeling by reanalyzing a phonetic data set containing formant (F1 and F2) values for 5 vowels of standard Indonesian (ISO 639-3:ind), as spoken by 8 speakers (4 females and 4 males), with several repetitions of each vowel. Results  We first give an introductory overview of the Bayesian framework and multilevel modeling. We then show how Bayesian multilevel models can be fitted using the probabilistic programming language Stan and the R package brms, which provides an intuitive formula syntax. Conclusions  Through this tutorial, we demonstrate some of the advantages of the Bayesian framework for statistical modeling and provide a detailed case study, with complete source code for full reproducibility of the analyses (https://osf.io/dpzcb/). Supplemental Material  https://doi.org/10.23641/asha.7973822},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nalborczyk.l2019 An introduction to Bayesian multilevel m.pdf}
}

@inproceedings{narayanan.s:1998,
  title = {Bayesian Models of Human Sentence Processing},
  booktitle = {Procedings of Twentieth Annual Conference of the Cognitive Science Society: {{University}} of Wisconsin-Madison},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  editor = {Gernsbacher, Morton Ann and Derry, Sharon J.},
  year = {1998},
  pages = {752--757},
  publisher = {Lawrence Erlbaum Associates},
  address = {Mahwah, NJ},
  date-added = {2021-03-09 22:52:27 -0500},
  date-modified = {2021-03-09 22:52:27 -0500},
  keywords = {bayesian,processing}
}

@inproceedings{narayanan.s:2001,
  title = {A {{Bayesian}} Model Predicts Human Parse Preference and Reading Times in Sentence Processing},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  year = {2001},
  volume = {14},
  publisher = {MIT Press},
  urldate = {2022-06-28},
  abstract = {Narayanan and Jurafsky (1998) proposed that human language compre- hension can be modeled by treating human comprehenders as Bayesian reasoners, and modeling the comprehension process with Bayesian de- cision trees. In this paper we extend the Narayanan and Jurafsky model to make further predictions about reading time given the probability of difference parses or interpretations, and test the model against reading time data from a psycholinguistic experiment.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/narayanan.s2001 A Bayesian model predicts human parse pr.pdf}
}

@unpublished{narayanan.s:2004,
  type = {Unpublished Manuscript},
  title = {A {{Bayesian}} Model of Human Sentence Processing},
  author = {Narayanan, Srini and Jurafsky, Daniel},
  year = {2004},
  month = nov,
  file = {/Users/j/Dropbox (MIT)/Zotfiles/narayanan.s2004 A Bayesian model of human sentence proce.pdf}
}

@inproceedings{naseem.t:2012,
  title = {Selective Sharing for Multilingual Dependency Parsing},
  booktitle = {Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Naseem, Tahira and Barzilay, Regina and Globerson, Amir},
  year = {2012},
  month = jul,
  pages = {629--637},
  publisher = {Association for Computational Linguistics},
  address = {Jeju Island, Korea},
  date-added = {2022-04-04 12:41:51 -0400},
  date-modified = {2022-04-04 12:41:52 -0400}
}

@article{natrevphys:2023,
  title = {How to Edit Anthropomorphic Language about Artificial Intelligence},
  year = {2023},
  month = may,
  journal = {Nature Reviews Physics},
  volume = {5},
  number = {5},
  pages = {263--263},
  publisher = {Nature Publishing Group},
  issn = {2522-5820},
  doi = {10.1038/s42254-023-00584-1},
  urldate = {2023-07-30},
  abstract = {Advances in artificial intelligence (AI) and the hype they are generating have raised concerns about how scientists should talk about AI~systems. Here is how we will approach the editing of such language to ensure clarity, accuracy and avoid misinterpretation and anthropomorphism.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {anthropomorphism,Fluid dynamics}
}

@article{neal.r:2003,
  title = {Slice Sampling},
  author = {Neal, Radford M.},
  year = {2003},
  month = jun,
  journal = {The Annals of Statistics},
  volume = {31},
  number = {3},
  pages = {705--767},
  publisher = {Institute of Mathematical Statistics},
  issn = {0090-5364, 2168-8966},
  doi = {10.1214/aos/1056562461},
  urldate = {2022-06-10},
  abstract = {Markov chain sampling methods that adapt to characteristics of the distribution being sampled can be constructed using the principle that one can ample from a distribution by sampling uniformly from the region under the plot of its density function. A Markov chain that converges to this uniform distribution can be constructed by alternating uniform sampling in the vertical direction with uniform sampling from the horizontal "slice" defined by the current vertical position, or more generally, with some update that leaves the uniform distribution over this slice invariant. Such "slice sampling" methods are easily implemented for univariate distributions, and can be used to sample from a multivariate distribution by updating each variable in turn. This approach is often easier to implement than Gibbs sampling and more efficient than simple Metropolis updates, due to the ability of slice sampling to adaptively choose the magnitude of changes made. It is therefore attractive for routine and automated use. Slice sampling methods that update all variables simultaneously are also possible. These methods can adaptively choose the magnitudes of changes made to each variable, based on the local properties of the density function. More ambitiously, such methods could potentially adapt to the dependencies between variables by constructing local quadratic approximations. Another approach is to improve sampling efficiency by suppressing random walks. This can be done for univariate slice sampling by "overrelaxation," and for multivariate slice sampling by "reflection" from the edges of the slice.},
  keywords = {65C05,65C60,Adaptive methods,auxiliary variables,dynamical methods,Gibbs sampling,Markov chain Monte Carlo,Metropolis algorithm,overrelaxation,rejection sampling,sampling,slice sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/neal.r2003 Slice sampling.pdf}
}

@article{nevins.a:2011,
  title = {Multiple Agree with Clitics: {{Person}} Complementarity vs. Omnivorous Number},
  author = {Nevins, Andrew},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {939--971},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:39:19 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,omnivorous agree}
}

@inproceedings{newell.a:1959,
  title = {Report on a General Problem Solving Program},
  booktitle = {Proceedings of the {{International Conference}} on {{Information Processing}}},
  author = {Newell, Allen and Shaw, John C. and Simon, Herbert A.},
  year = {1959},
  pages = {256--264},
  publisher = {Pittsburgh, PA},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/newell.a1959 Report on a general problem solving prog.pdf}
}

@book{newell.a:1972,
  title = {Human Problem Solving},
  author = {Newell, Allen and Simon, Herbert Alexander},
  year = {1972},
  volume = {104},
  publisher = {Prentice-hall Englewood Cliffs, NJ}
}

@incollection{newell.a:1973,
  title = {Production Systems: Models of Control Structures},
  shorttitle = {Production Systems},
  booktitle = {Visual {{Information Processing}}},
  author = {Newell, Allen},
  editor = {Chase, William G.},
  year = {1973},
  month = jan,
  pages = {463--526},
  publisher = {Academic Press},
  doi = {10.1016/B978-0-12-170150-5.50016-0},
  urldate = {2022-07-15},
  abstract = {This chapter discusses production systems and the way in which they operate. A production system is a scheme for specifying an information processing system. It consists of a set of productions, each production consisting of a condition and an action. It has also a collection of data structures: expressions that encode the information upon which the production system works---on which the actions operate and on which the conditions can be determined to be true or false. The chapter discusses the possibility of having a theory of the control structure of human information processing. Gains seem possible in many forms such as completeness of the microtheories of how various miniscule experimental tasks are performed, the ability to pose meaningfully the problem of what method a subject is using, the ability to suggest new mechanisms for accomplishing a task, and the facilitation of comparing behavior on diverse tasks. The chapter presents a theory of the control structure.},
  isbn = {978-0-12-170150-5},
  langid = {english}
}

@incollection{newell.a:1981,
  title = {Mechanisms of Skill Acquisition and the Law of Practice},
  shorttitle = {Mechanisms of Skill Acquisition and the Law of Practice},
  booktitle = {Cognitive {{Skills}} and {{Their Acquisition}}},
  author = {Newell, Allen and Paul, Rosenbloom},
  editor = {Anderson, John R.},
  year = {1981},
  publisher = {Psychology Press},
  doi = {10.4324/9780203728178-6},
  abstract = {Practice makes perfect. Correcting the overstatement of a maxim: Almost always, practice brings improvement, and more practice brings more improvement. We all expect improvement with practice to be ubiquitous, though obviously limits exist both in scope and extent. Take only the experimental laboratory: We do not expect people to perform an experimental task correctly without at least some practice; and we design all our psychology experiments with one eye to the confounding influence of practice effects.},
  isbn = {978-0-203-72817-8}
}

@article{newell.a:1981address,
  title = {The Knowledge Level: Presidential Address},
  author = {Newell, Allen},
  year = {1981},
  month = sep,
  journal = {AI Magazine},
  volume = {2},
  number = {2},
  pages = {1},
  doi = {10.1609/aimag.v2i2.99},
  urldate = {2024-05-15},
  abstract = {This is the first presidential address of AAAI, the American Association for Artificial Intelligence. In the grand scheme of history of artificial intelligence (AI), this is surely a minor event. The field this scientific society represents has been thriving for quite some time. No doubt the society itself will make solid contributions to the health of our field. But it is too much to expect a presidential address to have a major impact. So what is the role of the presidential address and what is the significance of the first one? I believe its role is to set a tone, to provide an emphasis. I think the role of the first address is to take a stand about what that tone and emphasis should be-set expectations for future addresses and to communicate to my fellow presidents. Only two foci are really possible for a presidential address: the state of the society or the state of the science. I believe the latter to be correct focus. AAAI itself, its nature and its relationship to the larger society that surrounds it, are surely important. However, our main business is to help AI become a science -- albeit a science with a strong engineering flavor. Thus, though a president's address cannot be narrow or highly technical, it can certainly address a substantive issue. That is what I propose to do.},
  chapter = {Articles},
  annotation = {Also published as: Arificial intelligence, 18(1), 87--127, 1982},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/newell.a1981address The knowledge level presidential addres.pdf}
}

@book{newell.a:1994,
  title = {Unified Theories of Cognition},
  author = {Newell, Allen},
  year = {1994},
  publisher = {Harvard University Press},
  abstract = {Psychology is now ready for unified theories of cognition--so says Allen Newell, a leading investigator in computer science and cognitive psychology. Not everyone will agree on a single set of mechanisms that will explain the full range of human cognition, but such theories are within reach and we should strive to articulate them.In this book, Newell makes the case for unified theories by setting forth a candidate. After reviewing the foundational concepts of cognitive science--knowledge, representation, computation, symbols, architecture, intelligence, and search--Newell introduces Soar, an architecture for general cognition. A pioneer system in artificial intelligence, Soar is the first problem solver to create its own subgoals and learn continuously from its own experience.Newell shows how Soar's ability to operate within the real-time constraints of intelligent behavior, such as immediate-response and item-recognition tasks, illustrates important characteristics of the human cognitive structure. Throughout, Soar remains an exemplar: we know only enough to work toward a fully developed theory of cognition, but Soar's success so far establishes the viability of the enterprise.Given its integrative approach, Unified Theories of Cognition will be of tremendous interest to researchers in a variety of fields, including cognitive science, artificial intelligence, psychology, and computer science. This exploration of the nature of mind, one of the great problems of philosophy, should also transcend disciplines and attract a large scientific audience.},
  googlebooks = {1lbY14DmV2cC},
  isbn = {978-0-674-92101-6},
  langid = {english},
  keywords = {Psychology / General}
}

@article{newton.m:1994,
  title = {Approximate {{Bayesian Inference}} with the {{Weighted Likelihood Bootstrap}}},
  author = {Newton, Michael A. and Raftery, Adrian E.},
  year = {1994},
  journal = {Journal of the Royal Statistical Society: Series B (Methodological)},
  volume = {56},
  number = {1},
  pages = {3--26},
  issn = {2517-6161},
  doi = {10.1111/j.2517-6161.1994.tb01956.x},
  urldate = {2024-06-25},
  abstract = {We introduce the weighted likelihood bootstrap (WLB) as a way to simulate approximately from a posterior distribution. This method is often easy to implement, requiring only an algorithm for calculating the maximum likelihood estimator, such as iteratively reweighted least squares. In the generic weighting scheme, the WLB is first order correct under quite general conditions. Inaccuracies can be removed by using the WLB as a source of samples in the sampling-importance resampling (SIR) algorithm, which also allows incorporation of particular prior information. The SIR-adjusted WLB can be a competitive alternative to other integration methods in certain models. Asymptotic expansions elucidate the second-order properties of the WLB, which is a generalization of Rubin's Bayesian bootstrap. The calculation of approximate Bayes factors for model comparison is also considered. We note that, given a sample simulated from the posterior distribution, the required marginal likelihood may be simulation consistently estimated by the harmonic mean of the associated likelihood values; a modification of this estimator that avoids instability is also noted. These methods provide simple ways of calculating approximate Bayes factors and posterior model probabilities for a very wide class of models.},
  copyright = {{\copyright} 1994 Royal Statistical Society},
  langid = {english},
  keywords = {bayes factor,bayesian inference,dirichlet weights,monte carlo methods}
}

@inproceedings{nguyen.l:2012,
  title = {Accurate Unbounded Dependency Recovery Using Generalized Categorial Grammars},
  booktitle = {Proceedings of {{COLING}} 2012},
  author = {Nguyen, Luan and {van Schijndel}, Marten and Schuler, William},
  year = {2012},
  pages = {2125--2140},
  publisher = {The COLING 2012 Organizing Committee},
  address = {Mumbai, India}
}

@article{nicenboim.b:2016,
  title = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed up: Working Memory and Locality Effects},
  shorttitle = {When High-Capacity Readers Slow down and Low-Capacity Readers Speed Up},
  author = {Nicenboim, Bruno and Loga{\v c}ev, Pavel and Gattei, Carolina and Vasishth, Shravan},
  year = {2016},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00280},
  urldate = {2022-08-13},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nicenboim.b2016 When high-capacity readers slow down and.pdf}
}

@article{nicenboim.b:2018,
  title = {Models of Retrieval in Sentence Comprehension: {{A}} Computational Evaluation Using {{Bayesian}} Hierarchical Modeling},
  shorttitle = {Models of Retrieval in Sentence Comprehension},
  author = {Nicenboim, Bruno and Vasishth, Shravan},
  year = {2018},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {99},
  pages = {1--34},
  issn = {0749596X},
  doi = {10.1016/j.jml.2017.08.004},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nicenboim.b2018 Models of retrieval in sentence comprehe.pdf}
}

@book{nicenboim.b:2024bayescogsci,
  title = {An Introduction to {{Bayesian}} Data Analysis for Cognitive Science},
  author = {Nicenboim, Bruno and Schad, Daniel and Vasishth, Shravan},
  year = {2024},
  month = mar,
  publisher = {Bookdown},
  urldate = {2024-05-21},
  abstract = {An introduction to Bayesian data analysis for Cognitive Science.}
}

@inproceedings{nichol.a:2021,
  title = {Improved Denoising Diffusion Probabilistic Models},
  booktitle = {Proceedings of the 38th {{International Conference}} on {{Machine Learning}}},
  author = {Nichol, Alexander Quinn and Dhariwal, Prafulla},
  year = {2021},
  month = jul,
  pages = {8162--8171},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2022-07-07},
  abstract = {Denoising diffusion probabilistic models (DDPM) are a class of generative models which have recently been shown to produce excellent samples. We show that with a few simple modifications, DDPMs can also achieve competitive log-likelihoods while maintaining high sample quality. Additionally, we find that learning variances of the reverse diffusion process allows sampling with an order of magnitude fewer forward passes with a negligible difference in sample quality, which is important for the practical deployment of these models. We additionally use precision and recall to compare how well DDPMs and GANs cover the target distribution. Finally, we show that the sample quality and likelihood of these models scale smoothly with model capacity and training compute, making them easily scalable. We release our code and pre-trained models at https://github.com/openai/improved-diffusion.},
  langid = {english},
  keywords = {diffusion processes},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nichol.a2021 Improved denoising diffusion probabilist 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/nichol.a2021 Improved denoising diffusion probabilist.pdf}
}

@article{nicklin.c:2020,
  title = {Outliers in {{L2}} Research in Applied Linguistics: A Synthesis and Data Re-Analysis},
  shorttitle = {Outliers in {{L2}} Research in Applied Linguistics},
  author = {Nicklin, Christopher and Plonsky, Luke},
  year = {2020},
  month = mar,
  journal = {Annual Review of Applied Linguistics},
  volume = {40},
  pages = {26--55},
  issn = {0267-1905, 1471-6356},
  doi = {10.1017/S0267190520000057},
  urldate = {2024-05-26},
  abstract = {Data from self-paced reading (SPR) tasks are routinely checked for statistical outliers (Marsden, Thompson, \& Plonsky, 2018). Such data points can be handled in a variety of ways (e.g., trimming, data transformation), each of which may influence study results in a different manner. This two-phase study sought, first, to systematically review outlier handling techniques found in studies that involve SPR and, second, to re-analyze raw data from SPR tasks to understand the impact of those techniques. Toward these ends, in Phase I, a sample of 104 studies that employed SPR tasks was collected and coded for different outlier treatments. As found in Marsden et al. (2018), wide variability was observed across the sample in terms of selection of time and standard deviation (SD)-based boundaries for determining what constitutes a legitimate reading time (RT). In Phase II, the raw data from the SPR studies in Phase I were requested from the authors. Nineteen usable datasets were obtained and re-analyzed using data transformations, SD boundaries, trimming, and winsorizing, in order to test their relative effectiveness for normalizing SPR reaction time data. The results suggested that, in the vast majority of cases, logarithmic transformation circumvented the need for SD boundaries, which blindly eliminate or alter potentially legitimate data. The results also indicated that choice of SD boundary had little influence on the data and revealed no meaningful difference between trimming and winsorizing, implying that blindly removing data from SPR analyses might be unnecessary. Suggestions are provided for future research involving SPR data and the handling of outliers in second language (L2) research more generally.},
  langid = {english}
}

@article{nivre.j:2008,
  title = {Algorithms for {{Deterministic Incremental Dependency Parsing}}},
  author = {Nivre, Joakim},
  year = {2008},
  month = dec,
  journal = {Computational Linguistics},
  volume = {34},
  number = {4},
  pages = {513--553},
  issn = {0891-2017},
  doi = {10.1162/coli.07-056-R1-07-027},
  urldate = {2022-06-19},
  abstract = {Parsing algorithms that process the input from left to right and construct a single derivation have often been considered inadequate for natural language parsing because of the massive ambiguity typically found in natural language grammars. Nevertheless, it has been shown that such algorithms, combined with treebank-induced classifiers, can be used to build highly accurate disambiguating parsers, in particular for dependency-based syntactic representations. In this article, we first present a general framework for describing and analyzing algorithms for deterministic incremental dependency parsing, formalized as transition systems. We then describe and analyze two families of such algorithms: stack-based and list-based algorithms. In the former family, which is restricted to projective dependency structures, we describe an arc-eager and an arc-standard variant; in the latter family, we present a projective and a non-projective variant. For each of the four algorithms, we give proofs of correctness and complexity. In addition, we perform an experimental evaluation of all algorithms in combination with SVM classifiers for predicting the next parsing action, using data from thirteen languages. We show that all four algorithms give competitive accuracy, although the non-projective list-based algorithm generally outperforms the projective algorithms for languages with a non-negligible proportion of non-projective constructions. However, the projective algorithms often produce comparable results when combined with the technique known as pseudo-projective parsing. The linear time complexity of the stack-based algorithms gives them an advantage with respect to efficiency both in learning and in parsing, but the projective list-based algorithm turns out to be equally efficient in practice. Moreover, when the projective algorithms are used to implement pseudo-projective parsing, they sometimes become less efficient in parsing (but not in learning) than the non-projective list-based algorithm. Although most of the algorithms have been partially described in the literature before, this is the first comprehensive analysis and evaluation of the algorithms within a unified framework.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/nivre.j2008 Algorithms for Deterministic Incremental.pdf}
}

@inproceedings{nivre.j:2016universal-dependencies,
  title = {Universal {{Dependencies}} v1: {{A}} Multilingual Treebank Collection},
  booktitle = {Proceedings of the Tenth International Conference on Language Resources and Evaluation ({{LREC}}'16)},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Goldberg, Yoav and Haji{\v c}, Jan and Manning, Christopher D. and McDonald, Ryan and Petrov, Slav and Pyysalo, Sampo and Silveira, Natalia and Tsarfaty, Reut and Zeman, Daniel},
  year = {2016},
  pages = {1659--1666},
  publisher = {European Language Resources Association (ELRA)},
  address = {Portoro{\v z}, Slovenia}
}

@misc{nivre.j:2017,
  title = {Universal Dependencies 2.0 -- {{CoNLL}} 2017 Shared Task Development and Test Data},
  author = {Nivre, Joakim and Agi{\'c}, {\v Z}eljko and Ahrenberg, Lars and Antonsen, Lene and Aranzabe, Maria Jesus and Asahara, Masayuki and Ateyah, Luma and Attia, Mohammed and Atutxa, Aitziber and Badmaeva, Elena and Ballesteros, Miguel and Banerjee, Esha and Bank, Sebastian and Bauer, John and Bengoetxea, Kepa and Bhat, Riyaz Ahmad and Bick, Eckhard and Bosco, Cristina and Bouma, Gosse and Bowman, Sam and Burchardt, Aljoscha and Candito, Marie and Caron, Gauthier and Cebiro{\u g}lu Eryi{\u g}it, G{\"u}l{\c s}en and Celano, Giuseppe G. A. and Cetin, Savas and Chalub, Fabricio and Choi, Jinho and Cho, Yongseok and Cinkov{\'a}, Silvie and {\c C}{\"o}ltekin, {\c C}a{\u g}r{\i} and Connor, Miriam and {de Marneffe}, Marie-Catherine and {de Paiva}, Valeria and {Diaz de Ilarraza}, Arantza and Dobrovoljc, Kaja and Dozat, Timothy and Droganova, Kira and Eli, Marhaba and Elkahky, Ali and Erjavec, Toma{\v z} and Farkas, Rich{\'a}rd and Fernandez Alcalde, Hector and Foster, Jennifer and Freitas, Cl{\'a}udia and Gajdo{\v s}ov{\'a}, Katar{\'i}na and Galbraith, Daniel and Garcia, Marcos and Ginter, Filip and Goenaga, Iakes and Gojenola, Koldo and G{\"o}k{\i}rmak, Memduh and Goldberg, Yoav and G{\'o}mez Guinovart, Xavier and Gonz{\'a}les Saavedra, Berta and Grioni, Matias and Gr{\=u}ztis, Normunds and Guillaume, Bruno and Habash, Nizar and Haji{\v c}, Jan and {Haji{\v c} jr.}, Jan and H{\`a} M{\~y}, Linh and Harris, Kim and Haug, Dag and Hladk{\'a}, Barbora and Hlav{\'a}{\v c}ov{\'a}, Jaroslava and Hohle, Petter and Ion, Radu and Irimia, Elena and Johannsen, Anders and J{\o}rgensen, Fredrik and Ka{\c s}{\i}kara, H{\"u}ner and Kanayama, Hiroshi and Kanerva, Jenna and Kayadelen, Tolga and Kettnerov{\'a}, V{\'a}clava and Kirchner, Jesse and Kotsyba, Natalia and Krek, Simon and Kwak, Sookyoung and Laippala, Veronika and Lambertino, Lorenzo and Lando, Tatiana and L{\^e} Hồng, Phương and Lenci, Alessandro and Lertpradit, Saran and Leung, Herman and Li, Cheuk Ying and Li, Josie and Ljube{\v s}i{\'c}, Nikola and Loginova, Olga and Lyashevskaya, Olga and Lynn, Teresa and Macketanz, Vivien and Makazhanov, Aibek and Mandl, Michael and Manning, Christopher and Manurung, Ruli and M{\u a}r{\u a}nduc, C{\u a}t{\u a}lina and Mare{\v c}ek, David and Marheinecke, Katrin and Mart{\'i}nez Alonso, H{\'e}ctor and Martins, Andr{\'e} and Ma{\v s}ek, Jan and Matsumoto, Yuji and McDonald, Ryan and Mendon{\c c}a, Gustavo and Missil{\"a}, Anna and Mititelu, Verginica and Miyao, Yusuke and Montemagni, Simonetta and More, Amir and Moreno Romero, Laura and Mori, Shunsuke and Moskalevskyi, Bohdan and Muischnek, Kadri and Mustafina, Nina and M{\"u}{\"u}risep, Kaili and Nainwani, Pinkey and Nedoluzhko, Anna and Nguyễn Th{\d i}, Lương and Nguyễn Th{\d i} Minh, Huyền and Nikolaev, Vitaly and Nitisaroj, Rattima and Nurmi, Hanna and Ojala, Stina and Osenova, Petya and {\O}vrelid, Lilja and Pascual, Elena and Passarotti, Marco and Perez, Cenel-Augusto and Perrier, Guy and Petrov, Slav and Piitulainen, Jussi and Pitler, Emily and Plank, Barbara and Popel, Martin and Pretkalni{\c n}a, Lauma and Prokopidis, Prokopis and Puolakainen, Tiina and Pyysalo, Sampo and Rademaker, Alexandre and Real, Livy and Reddy, Siva and Rehm, Georg and Rinaldi, Larissa and Rituma, Laura and Rosa, Rudolf and Rovati, Davide and Saleh, Shadi and Sanguinetti, Manuela and Saulte, Baiba and Sawanakunanon, Yanin and Schuster, Sebastian and Seddah, Djam{\'e} and Seeker, Wolfgang and Seraji, Mojgan and Shakurova, Lena and Shen, Mo and Shimada, Atsuko and Shohibussirri, Muh and Silveira, Natalia and Simi, Maria and Simionescu, Radu and Simk{\'o}, Katalin and {\v S}imkov{\'a}, M{\'a}ria and Simov, Kiril and Smith, Aaron and Stella, Antonio and Strnadov{\'a}, Jana and Suhr, Alane and Sulubacak, Umut and Sz{\'a}nt{\'o}, Zsolt and Taji, Dima and Tanaka, Takaaki and Trosterud, Trond and Trukhina, Anna and Tsarfaty, Reut and Tyers, Francis and Uematsu, Sumire and Ure{\v s}ov{\'a}, Zde{\v n}ka and Uria, Larraitz and Uszkoreit, Hans and {van Noord}, Gertjan and Varga, Viktor and Vincze, Veronika and Washington, Jonathan North and Yu, Zhuoran and {\v Z}abokrtsk{\'y}, Zden{\v e}k and Zeman, Daniel and Zhu, Hanzhi},
  year = {2017},
  copyright = {Licence Universal Dependencies v2.0},
  date-added = {2021-04-30 13:00:05 -0400},
  date-modified = {2021-04-30 13:02:24 -0400},
  keywords = {dependency parsing,dependency structures,universal dependencies}
}

@inproceedings{nivre.j:2020,
  title = {Universal {{Dependencies}} v2: {{An}} Evergrowing Multilingual Treebank Collection},
  booktitle = {Proceedings of the 12th Language Resources and Evaluation Conference},
  author = {Nivre, Joakim and {de Marneffe}, Marie-Catherine and Ginter, Filip and Haji{\v c}, Jan and Manning, Christopher D. and Pyysalo, Sampo and Schuster, Sebastian and Tyers, Francis and Zeman, Daniel},
  year = {2020},
  pages = {4034--4043},
  publisher = {European Language Resources Association},
  address = {Marseille, France},
  isbn = {979-10-95546-34-4},
  langid = {english}
}

@article{norris.d:2006,
  title = {The {{Bayesian}} Reader: {{Explaining}} Word Recognition as an Optimal {{Bayesian}} Decision Process},
  shorttitle = {The {{Bayesian}} Reader},
  author = {Norris, Dennis},
  year = {2006},
  journal = {Psychological Review},
  volume = {113},
  number = {2},
  pages = {327--357},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.113.2.327},
  abstract = {This article presents a theory of visual word recognition that assumes that, in the tasks of word identification, lexical decision, and semantic categorization, human readers behave as optimal Bayesian decision makers. This leads to the development of a computational model of word recognition, the Bayesian reader. The Bayesian reader successfully simulates some of the most significant data on human reading. The model accounts for the nature of the function relating word frequency to reaction time and identification threshold, the effects of neighborhood density and its interaction with frequency, and the variation in the pattern of neighborhood density effects seen in different experimental tasks. Both the general behavior of the model and the way the model predicts different patterns of results in different tasks follow entirely from the assumption that human readers approximate optimal Bayesian decision makers. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Decision Making,Lexical Decision,Models,Reading,Semantics,Statistical Probability,Word Recognition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/norris.d2006 The Bayesian reader Explaining word rec.pdf}
}

@article{norris.d:2009,
  title = {Putting It All Together: {{A}} Unified Account of Word Recognition and Reaction-Time Distributions},
  shorttitle = {Putting It All Together},
  author = {Norris, Dennis},
  year = {2009},
  journal = {Psychological Review},
  volume = {116},
  number = {1},
  pages = {207--219},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/a0014259},
  abstract = {R. Ratcliff, P. Gomez, and G. McKoon (2004) suggested much of what goes on in lexical decision is attributable to decision processes and may not be particularly informative about word recognition. They proposed that lexical decision should be characterized by a decision process, taking the form of a drift-diffusion model (R. Ratcliff, 1978), that operates on the output of lexical model. The present article argues that the distinction between perception and decision making is unnecessary and that it is possible to give a unified account of both lexical processing and decision making. This claim is supported by formal arguments and reinforced by simulations showing how the Bayesian Reader model (D. Norris, 2006) can be extended to fit the data on reaction time distributions collected by Ratcliff, Gomez, and McKoon simply by adding extra sources of noise. The Bayesian Reader gives an integrated explanation of both word recognition and decision making, using fewer parameters than the diffusion model. It can be thought of as a Bayesian diffusion model, which subsumes Ratcliff's drift-diffusion model as a special case. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Lexical Decision,Models,Reaction Time,Word Recognition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/norris.d2009 Putting it all together A unified accou.pdf}
}

@article{oaksford.m:1996,
  title = {Rational Explanation of the Selection Task},
  author = {Oaksford, Mike and Chater, Nick},
  year = {1996},
  month = apr,
  journal = {Psychological Review},
  volume = {103},
  number = {2},
  pages = {381--391},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/0033-295X.103.2.381},
  urldate = {2024-05-15},
  langid = {english}
}

@book{oaksford.m:1998,
  title = {Rational Models of Cognition},
  editor = {Oaksford, Mike and Chater, Nick},
  year = {1998},
  publisher = {Oxford University Press},
  address = {Oxford, England},
  isbn = {0-19-852415-3}
}

@phdthesis{oconnor.e:2015phd,
  title = {Comparative Illusions at the Syntax-Semantics Interface},
  author = {O'Connor, Ellen},
  year = {2015},
  urldate = {2023-08-01},
  abstract = {Psycholinguistic research has focused much attention on the factors that influence structural ambiguity resolution, under the assumption that meaning is derived from a selected syntactic representation in a systematic, compositional way. Problematically, however, researchers have increasingly observed examples suggesting that perceptions of sentence acceptability and meaning are not always straightforwardly constrained by logical semantics. Most English speakers, for example, initially accept the sentence More people have been to Berlin than I have until asked to explain more clearly what it means, at which point its meaninglessness becomes obvious. Meanwhile, the sentence No head injury is too trivial to ignore is overwhelmingly perceived to mean exactly the opposite of its implausible grammar-based meaning, an error that is only readily detected with extended conscious effort. The goal of this thesis is uncover what these ``semantic illusions'' tell us about semantic processing by identifying the locus of nonveridical processing. In spite of appearances I argue that it is impossible to explain perceptions of and reactions to these illusion sentences without referencing properties that influence their logical form; this suggests, contrary to existing proposals, that the illusion is generated by online computations associated with, not external to, the grammar, and that nonveridical perceptions are induced by processing mechanisms responsible for navigating the logical form of sentences that contain probable speech errors and/or those that fall at the outer boundaries of computational tractability. Because the source of these illusions is not well understood, a more general goal of this work is to establish their fundamental properties, including the nature of their percept(s) and the properties that modulate that percept, to pave the way for future research.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  langid = {english},
  school = {University of Southern California},
  keywords = {(UMI)AAI11016211,Language,literature and linguistics}
}

@techreport{odonnell.t:2009,
  type = {Technical Report},
  title = {Fragment {{Grammars}}: {{Exploring Computation}} and {{Reuse}} in {{Language}}},
  shorttitle = {Fragment {{Grammars}}},
  author = {O'Donnell, Timothy J. and Tenenbaum, Joshua B. and Goodman, Noah D.},
  year = {2009},
  month = mar,
  number = {MIT-CSAIL-TR-2009-013},
  institution = {{MIT Computer Science and Artificial Intelligence Laboratory}},
  urldate = {2022-06-15},
  abstract = {Language relies on a division of labor between stored units and structure building operations which combine the stored units into larger structures. This division of labor leads to a tradeoff: more structure-building means less need to store while more storage means less need to compute structure. We develop a hierarchical Bayesian model called fragment grammar to explore the optimum balance between structure-building and reuse. The model is developed in the context of stochastic functional programming (SFP) and in particular using a probabilistic variant of Lisp known as the Church programming language (Goodman, Mansinghka, Roy, Bonawitz, \& Tenenbaum, 2008). We show how to formalize several probabilistic models of language structure using Church, and how fragment grammar generalizes one of them---adaptor grammars (Johnson, Griffiths, \& Goldwater, 2007). We conclude with experimental data with adults and preliminary evaluations of the model on natural language corpus data.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/odonnell.t2009 Fragment Grammars Exploring Computation.pdf}
}

@article{odonnell.t:2011cogsci,
  title = {Productivity and {{Reuse}} in {{Language}}},
  author = {O'Donnell, Timothy and Snedeker, Jesse and Tenenbaum, Joshua and Goodman, Noah},
  year = {2011},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {33},
  number = {33},
  urldate = {2022-06-15},
  abstract = {Author(s): O'Donnell, Timothy; Snedeker, Jesse; Tenenbaum, Joshua; Goodman, Noah},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/odonnell.t2011cogsci Productivity and Reuse in Language.pdf}
}

@phdthesis{odonnell.t:2011phd,
  title = {Productivity and {{Reuse}} in {{Language}}},
  author = {O'Donnell, Timothy J.},
  year = {2011},
  month = may,
  abstract = {A much-celebrated aspect of language is the way in which it allows us to make "infinite use of finite means" (von Humboldt, 1836). This property is made possible because language is fundamentally a productive computational system: Novel expressions can be composed from a large inventory of stored, reusable parts. For any given language, however, there are many more potential ways of forming novel expressions than can actually be used in practice. For example, English contains suffixes that are highly productive (e.g., -ness ; Lady-Gagaesqueness, pine-scentedness), but also contains suffixes which can only be reused in specific, existing words (e.g., -th; truth, width, warmth). How are such differences in productivity and reusability represented? How can the child acquire this system of knowledge? This thesis presents a formal model of productivity and reuse which treats the problem as a structure-by-structure inference in a Bayesian framework. The model---Fragment Grammars, a generalization of Adaptor Grammars (Johnson et al., 2007a)---is built around two proposals. The first is that anything that can be computed can be stored. The specific computational mechanism by which this is accomplished, stochastic memoization, is inherited from Adaptor Grammars (Goodman et al., 2008; Johnson et al., 2007a). The second proposal is that any stored item can include subparts which must be computed productively. This is made possible by the computational mechanism of stochastically lazy evaluation, introduced in the thesis. Throughout the thesis, Fragment Grammars are systematically compared to four other probabilistic models of productivity and reuse which formalize historical proposals from the linguistics and psycholinguistics literatures. The five models are evaluated on two very different sub-systems of English morphology: the English past tense, which is characterized by a sharp dichotomy in productivity between regular (i.e., +ed) and irregular (e.g., sing/sang) forms, and English derivational morphology, which is characterized by a graded cline from very productive (e.g., -ness) to very unproductive (e.g., -th). The thesis examines many aspects of these two domains including: performance on past-tense inflection, past-tense processing phenomena, developmental overregularization, the productivity of derivational suffixes, ordering restrictions on derivational suffixes, and base-driven selectional restrictions on suffix combinations.},
  langid = {english},
  school = {Harvard University},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/odonnell.t2011phd Productivity and Reuse in Language.pdf}
}

@book{odonnell.t:2015phdbook,
  title = {Productivity and {{Reuse}} in {{Language}}: {{A Theory}} of {{Linguistic Computation}} and {{Storage}}},
  author = {O'Donnell, Timothy J.},
  year = {2015},
  month = aug,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9780262028844.001.0001},
  urldate = {2022-06-16},
  abstract = {A proposal for a formal model, Fragment Grammars, that treats productivity and reuse as the target of inference in a probabilistic framework.Language allows us to express and comprehend an unbounded number of thoughts. This fundamental and much-celebrated property is made possible by a division of labor between a large inventory of stored items (e.g., affixes, words, idioms) and a computational system that productively combines these stored units on the fly to create a potentially unlimited array of new expressions. A language learner must discover a language's productive, reusable units and determine which computational processes can give rise to new expressions. But how does the learner differentiate between the reusable, generalizable units (for example, the affix -ness, as in coolness, orderliness, cheapness) and apparent units that do not actually generalize in practice (for example, -th, as in warmth but not coolth)? In this book, Timothy O'Donnell proposes a formal computational model, Fragment Grammars, to answer these questions. This model treats productivity and reuse as the target of inference in a probabilistic framework, asking how an optimal agent can make use of the distribution of forms in the linguistic input to learn the distribution of productive word-formation processes and reusable units in a given language.O'Donnell compares this model to a number of other theoretical and mathematical models, applying them to the English past tense and English derivational morphology, and showing that Fragment Grammars unifies a number of superficially distinct empirical phenomena in these domains and justifies certain seemingly ad hoc assumptions in earlier theories.},
  isbn = {978-0-262-32680-3}
}

@inproceedings{oh.b:2021,
  title = {Surprisal Estimators for Human Reading Times Need Character Models},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2021},
  month = aug,
  pages = {3746--3757},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.290},
  urldate = {2023-05-02},
  abstract = {While the use of character models has been popular in NLP applications, it has not been explored much in the context of psycholinguistic modeling. This paper presents a character model that can be applied to a structural parser-based processing model to calculate word generation probabilities. Experimental results show that surprisal estimates from a structural processing model using this character model deliver substantially better fits to self-paced reading, eye-tracking, and fMRI data than those from large-scale language models trained on much more data. This may suggest that the proposed processing model provides a more humanlike account of sentence processing, which assumes a larger role of morphology, phonotactics, and orthographic complexity than was previously thought.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/oh.b2021 Surprisal estimators for human reading t.pdf}
}

@article{oh.b:2022,
  title = {Comparison of Structural Parsers and Neural Language Models as Surprisal Estimators},
  author = {Oh, Byung-Doh and Clark, Christian and Schuler, William},
  year = {2022},
  journal = {Frontiers in Artificial Intelligence},
  volume = {5},
  issn = {2624-8212},
  urldate = {2023-05-02},
  abstract = {Expectation-based theories of sentence processing posit that processing difficulty is determined by predictability in context. While predictability quantified via surprisal has gained empirical support, this representation-agnostic measure leaves open the question of how to best approximate the human comprehender's latent probability model. This article first describes an incremental left-corner parser that incorporates information about common linguistic abstractions such as syntactic categories, predicate-argument structure, and morphological rules as a computational-level model of sentence processing. The article then evaluates a variety of structural parsers and deep neural language models as cognitive models of sentence processing by comparing the predictive power of their surprisal estimates on self-paced reading, eye-tracking, and fMRI data collected during real-time language processing. The results show that surprisal estimates from the proposed left-corner processing model deliver comparable and often superior fits to self-paced reading and eye-tracking data when compared to those from neural language models trained on much more data. This may suggest that the strong linguistic generalizations made by the proposed processing model may help predict humanlike processing costs that manifest in latency-based measures, even when the amount of training data is limited. Additionally, experiments using Transformer-based language models sharing the same primary architecture and training data show a surprising negative correlation between parameter count and fit to self-paced reading and eye-tracking data. These findings suggest that large-scale neural language models are making weaker generalizations based on patterns of lexical items rather than stronger, more humanlike generalizations based on linguistic structure.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/oh.b2022 Comparison of structural parsers and neu.pdf}
}

@misc{oh.b:2023arxiv,
  title = {Transformer-Based {{LM}} Surprisal Predicts Human Reading Times Best with about Two Billion Training Tokens},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  month = apr,
  number = {arXiv:2304.11389},
  eprint = {2304.11389},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2304.11389},
  urldate = {2023-05-15},
  abstract = {Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a 'tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Zotfiles/oh.b:2023arxiv Transformer-based LM surprisal predicts.pdf}
}

@inproceedings{oh.b:2023emnlpfindings,
  title = {Transformer-Based Language Model Surprisal Predicts Human Reading Times Best with about Two Billion Training Tokens},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Oh, Byung-Doh and Schuler, William},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {1915--1921},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.128},
  urldate = {2024-05-16},
  abstract = {Recent psycholinguistic studies have drawn conflicting conclusions about the relationship between the quality of a language model and the ability of its surprisal estimates to predict human reading times, which has been speculated to be due to the large gap in both the amount of training data and model capacity across studies. The current work aims to consolidate these findings by evaluating surprisal estimates from Transformer-based language model variants that vary systematically in the amount of training data and model capacity on their ability to predict human reading times. The results show that surprisal estimates from most variants with contemporary model capacities provide the best fit after seeing about two billion training tokens, after which they begin to diverge from humanlike expectations. Additionally, newly-trained smaller model variants reveal a `tipping point' at convergence, after which the decrease in language model perplexity begins to result in poorer fits to human reading times. These results suggest that the massive amount of training data is mainly responsible for the poorer fit achieved by surprisal from larger pre-trained language models, and that a certain degree of model capacity is necessary for Transformer-based language models to capture humanlike expectations.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/oh.b2023emnlpfindings Transformer-based language model surpris.pdf}
}

@article{oh.b:2023tacl,
  title = {Why Does Surprisal from Larger Transformer-Based Language Models Provide a Poorer Fit to Human Reading Times?},
  author = {Oh, Byung-Doh and Schuler, William},
  year = {2023},
  month = mar,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {336--350},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00548},
  urldate = {2023-04-30},
  abstract = {This work presents a linguistic analysis into why larger Transformer-based pre-trained language models with more parameters and lower perplexity nonetheless yield surprisal estimates that are less predictive of human reading times. First, regression analyses show a strictly monotonic, positive log-linear relationship between perplexity and fit to reading times for the more recently released five GPT-Neo variants and eight OPT variants on two separate datasets, replicating earlier results limited to just GPT-2 (Oh et al., 2022). Subsequently, analysis of residual errors reveals a systematic deviation of the larger variants, such as underpredicting reading times of named entities and making compensatory overpredictions for reading times of function words such as modals and conjunctions. These results suggest that the propensity of larger Transformer-based models to `memorize' sequences during training makes their surprisal estimates diverge from humanlike expectations, which warrants caution in using pre-trained language models to study human language processing.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/oh.b2023tacl Why does surprisal from larger transform.pdf}
}

@misc{oh.b:2024arxiv,
  title = {Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times},
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02255},
  eprint = {2402.02255},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Zotfiles/oh.b:2024arxiv Frequency explains the inverse correlati.pdf}
}

@inproceedings{oh.b:2024eacl,
  title = {Frequency Explains the Inverse Correlation of Large Language Models' Size, Training Data Amount, and Surprisal's Fit to Reading Times},
  booktitle = {Proceedings of the 18th {{Conference}} of the {{European Chapter}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  editor = {Graham, Yvette and Purver, Matthew},
  year = {2024},
  month = mar,
  pages = {2644--2663},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  urldate = {2024-05-16},
  abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/oh.b2024eacl Frequency explains the inverse correlati.pdf}
}

@book{ontario:2011,
  title = {Native {{Languages}}: {{A Support Document}} for the {{Teaching}} of {{Language Patterns}}: {{Oneida}}, {{Cayuga}}, and {{Mohawk}}},
  author = {{Ontario Ministry of Education}},
  year = {2011},
  series = {The {{Ontario Curriculum}}: {{Grades}} 1 to 12},
  publisher = {Ontario: Queen's Printer for Ontario},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ontario2011 Native Languages A Support Document for.pdf}
}

@misc{oord.a:2018,
  title = {Representation Learning with Contrastive Predictive Coding},
  author = {{van den Oord}, Aaron and Li, Yazhe and Vinyals, Oriol},
  year = {2018},
  eprint = {1807.03748},
  primaryclass = {cs.LG},
  archiveprefix = {arXiv},
  date-added = {2019-07-05 11:07:24 -0400},
  date-modified = {2019-07-05 11:08:29 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,representation learning}
}

@misc{openai:2023GPT4-post,
  title = {{{GPT-4}}},
  author = {OpenAI},
  year = {2023},
  month = mar,
  urldate = {2023-03-15},
  abstract = {We've created GPT-4, the latest milestone in OpenAI's effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.},
  howpublished = {https://openai.com/research/gpt-4},
  langid = {american}
}

@techreport{openai:2023GPT4techreport,
  type = {Technical Report},
  title = {{{GPT-4}} Technical Report},
  author = {OpenAI},
  year = {2023},
  month = mar,
  institution = {OpenAI},
  urldate = {2023-03-15},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/openai2023GPT4techreport GPT-4 technical report.pdf}
}

@incollection{opper.m:2015,
  title = {Expectation Propagation},
  booktitle = {Statistical {{Physics}}, {{Optimization}}, {{Inference}}, and {{Message-Passing Algorithms}}: {{Lecture Notes}} of the {{Les Houches School}} of {{Physics}}: {{Special Issue}}, {{October}} 2013},
  author = {Opper, Manfred},
  editor = {Krzakala, Florent and {Ricci-Tersenghi}, Federico and Zdeborova, Lenka and Zecchina, Riccardo and Tramel, Eric W. and Cugliandolo, Leticia F.},
  year = {2015},
  month = dec,
  pages = {263--292},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780198743736.003.0009},
  urldate = {2024-05-09},
  abstract = {Variational inference is a powerful concept that underlies many iterative approximation algorithms: expectation propagation, mean-field methods, belief propagation, and TAP equations can all be perceived in terms of this unifying framework. This chapter introduces the archetypal example of expectation propagation; after following its original derivation, we describe some of its properties, introduce some examples, and establish connections with the other approximation methods. The Gibbs free energy and its relation to these approximations, as well as double-loop algorithms for its minimization, are briefly discussed. Corrections by expansion about expectation propagation are then explained and, finally, some advanced inference topics and applications, such as recommender systems, Gaussian regression models and continuous-time stochastic dynamics, are explored.},
  isbn = {978-0-19-874373-6},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/opper.m2015 Expectation propagation.pdf}
}

@article{ortega.p:2013,
  title = {Thermodynamics as a Theory of Decision-Making with Information-Processing Costs},
  author = {Ortega, Pedro A. and Braun, Daniel A.},
  year = {2013},
  month = may,
  journal = {Proceedings of the Royal Society A: Mathematical, Physical and Engineering Sciences},
  volume = {469},
  number = {2153},
  pages = {20120683},
  publisher = {Royal Society},
  doi = {10.1098/rspa.2012.0683},
  urldate = {2022-06-09},
  abstract = {Perfectly rational decision-makers maximize expected utility, but crucially ignore the resource costs incurred when determining optimal actions. Here, we propose a thermodynamically inspired formalization of bounded rational decision-making where information processing is modelled as state changes in thermodynamic systems that can be quantified by differences in free energy. By optimizing a free energy, bounded rational decision-makers trade off expected utility gains and information-processing costs measured by the relative entropy. As a result, the bounded rational decision-making problem can be rephrased in terms of well-known variational principles from statistical physics. In the limit when computational costs are ignored, the maximum expected utility principle is recovered. We discuss links to existing decision-making frameworks and applications to human decision-making experiments that are at odds with expected utility theory. Since most of the mathematical machinery can be borrowed from statistical physics, the main contribution is to re-interpret the formalism of thermodynamic free-energy differences in terms of bounded rational decision-making and to discuss its relationship to human decision-making experiments.},
  keywords = {bounded rationality,decision-making,information processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ortega.p2013 Thermodynamics as a theory of decision-m.pdf}
}

@article{orth.w:2021,
  title = {Negative Polarity Item ({{NPI}}) Illusion Is a Quantification Phenomenon.},
  author = {Orth, Wesley and Yoshida, Masaya and Sloggett, Shayne},
  year = {2021},
  month = jun,
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {47},
  number = {6},
  pages = {906--947},
  issn = {1939-1285, 0278-7393},
  doi = {10.1037/xlm0000957},
  urldate = {2023-08-03},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/orth.w2021 Negative polarity item (NPI) illusion is.pdf}
}

@article{owsley.c:2011,
  title = {Aging and Vision},
  author = {Owsley, Cynthia},
  year = {2011},
  month = jul,
  journal = {Vision Research},
  series = {Vision {{Research}} 50th {{Anniversary Issue}}: {{Part}} 2},
  volume = {51},
  number = {13},
  pages = {1610--1622},
  issn = {0042-6989},
  doi = {10.1016/j.visres.2010.10.020},
  urldate = {2024-03-19},
  abstract = {Given the increasing size of the older adult population in many countries, there is a pressing need to identify the nature of aging-related vision impairments, their underlying mechanisms, and how they impact older adults' performance of everyday visual tasks. The results of this research can then be used to develop and evaluate interventions to slow or reverse aging-related declines in vision, thereby improving quality of life. Here we summarize salient developments in research on aging and vision over the past 25years, focusing on spatial contrast sensitivity, vision under low luminance, temporal sensitivity and motion perception, and visual processing speed.},
  keywords = {aging,Aging,vision,Vision,Vision impairment},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/owsley.c2011 Aging and vision.pdf}
}

@article{oxford.w:2019,
  title = {Inverse Marking and Multiple Agree in Algonquin},
  author = {Oxford, Will},
  year = {2019},
  journal = {Natural Language \& Linguistic Theory},
  volume = {37},
  number = {3},
  pages = {955--996},
  doi = {10.1007/s11049-018-9428-x},
  abstract = {This paper shows that inverse marking and portmanteau agreement are in complementary distribution in Algonquin: inverse marking is possible only in contexts where portmanteau agreement is not. This correlation holds despite intralanguage variation in both phenomena. The paper proposes that the two phenomena pattern together because both are determined by the outcome of the Agree operation on Infl. When Infl enters a Multiple Agree relation with both arguments, the realization of portmanteau agreement morphology is possible. When Infl agrees only with the object, it duplicates the result of an earlier object agreement operation on Voice. The presence of identical features on Infl and Voice triggers an impoverishment operation that deletes the features of Voice, resulting in its spellout as an underspecified elsewhere form---which is the exponent that we know descriptively as the inverse marker. This analysis explains why inverse marking and portmanteau agreement never co-occur in Algonquin: the two phenomena are determined by alternative outcomes of the Agree operation on Infl. The analysis also enables a simple account of the intralanguage variation in the patterning of the two phenomena, which is shown to follow from variation in the specification of the probe on Infl.},
  da = {2019/08/01},
  date-added = {2020-06-16 10:51:08 -0400},
  date-modified = {2020-06-16 10:52:50 -0400},
  isbn = {1573-0859},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects}
}

@article{paape.d:2020,
  title = {Quadruplex {{Negatio Invertit}}? {{The On-Line Processing}} of {{Depth Charge Sentences}}},
  shorttitle = {Quadruplex {{Negatio Invertit}}?},
  author = {Paape, Dario and Vasishth, Shravan and {von der Malsburg}, Titus},
  year = {2020},
  month = nov,
  journal = {Journal of Semantics},
  volume = {37},
  number = {4},
  pages = {509--555},
  issn = {0167-5133},
  doi = {10.1093/jos/ffaa009},
  urldate = {2023-08-01},
  abstract = {So-called ``depth charge'' sentences (No head injury is too trivial to be ignored) are interpreted by the vast majority of speakers to mean the opposite of what their compositional semantics would dictate. The semantic inversion that is observed for sentences of this type is the strongest and most persistent linguistic illusion known to the field ( Wason \&amp; Reich, 1979). However, it has recently been argued that the preferred interpretation arises not because of a prevailing failure of the processing system, but rather because the non-compositional meaning is grammaticalized in the form of a stored construction ( Cook \&amp; Stevenson, 2010; Fortuin, 2014). In a series of five experiments, we investigate whether the depth charge effect is better explained by processing failure due to memory overload (the overloading hypothesis) or by the existence of an underlying grammaticalized construction with two available meanings (the ambiguity hypothesis). To our knowledge, our experiments are the first to explore the on-line processing profile of depth charge sentences. Overall, the data are consistent with specific variants of the ambiguity and overloading hypotheses while providing evidence against other variants. As an extension of the overloading hypothesis, we suggest two heuristic processes that may ultimately yield the incorrect reading when compositional processing is suspended for strategic reasons.},
  keywords = {depth-charge illusions,grammatical illusions},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/paape.d2020 Quadruplex Negatio Invertit The On-Line.pdf}
}

@article{paape.d:2021,
  title = {Does {{Local Coherence Lead}} to {{Targeted Regressions}} and {{Illusions}} of {{Grammaticality}}?},
  author = {Paape, Dario and Vasishth, Shravan and Engbert, Ralf},
  year = {2021},
  month = jul,
  journal = {Open Mind},
  volume = {5},
  pages = {42--58},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00041},
  urldate = {2024-03-06},
  abstract = {Local coherence effects arise when the human sentence processor is temporarily misled by a locally grammatical but globally ungrammatical analysis (The coach smiled at the player tossed a frisbee by the opposing team). It has been suggested that such effects occur either because sentence processing occurs in a bottom-up, self-organized manner rather than under constant grammatical supervision, or because local coherence can disrupt processing due to readers maintaining uncertainty about previous input. We report the results of an eye-tracking study in which subjects read German grammatical and ungrammatical sentences that either contained a locally coherent substring or not and gave binary grammaticality judgments. In our data, local coherence affected on-line processing immediately at the point of the manipulation. There was, however, no indication that local coherence led to illusions of grammaticality (a prediction of self-organization), and only weak, inconclusive support for local coherence leading to targeted regressions to critical context words (a prediction of the uncertain-input approach). We discuss implications for self-organized and noisy-channel models of local coherence.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/paape.d2021 Does Local Coherence Lead to Targeted Re.pdf}
}

@phdthesis{paille.m:2022phd,
  title = {Strengthening {{Predicates}}},
  author = {Paill{\'e}, Mathieu},
  year = {2022},
  month = aug,
  address = {Montr{\'e}al, Canada},
  urldate = {2024-04-30},
  abstract = {Sentences in natural language are routinely interpreted as stronger than would be expected from the lexical meanings of the overt lexical items alone. This has led to the postulation of exhaustification (strengthening) mechanisms in pragmatics and semantics. Such exhaustivity effects have largely been discussed for logical vocabulary, focused expressions, and predicates forming entailment scales with other predicates. Relying on recent work on additive particles, I argue that exhaustivity is at play in a significantly broader array of meanings than previously appreciated: all predicates are exhaustified, in all sentences. That is, the intuited meanings of predicates in sentences are stronger than their lexical--conceptual meanings. I focus on 'taxonomic' predicates, which do not form entailment scales with other predicates. I make this case first and foremost based on apparently banal contradictions like This comedy is a tragedy or The white flag is green. While these contradictions are intuitively due to the meanings of the predicates, the interaction of these predicates with additive particles (This comedy is also a tragedy) and conjunction (This play is both a comedy and a tragedy) is argued to show that the predicates are underlyingly consistent. As such, the contradiction observed in the basic case must result from exhaustification.},
  langid = {canadian},
  school = {McGill University},
  keywords = {additive particles,alternatives,exhaustivity,homogeneity,predicates,semantics}
}

@inproceedings{paiva-alves.e:1996,
  title = {The Selection of the Most Probable Dependency Structure in {{Japanese}} Using Mutual Information},
  booktitle = {34th Annual Meeting of the Association for Computational Linguistics},
  author = {{de Paiva Alves}, Eduardo},
  year = {1996},
  pages = {372--374},
  publisher = {Association for Computational Linguistics},
  address = {Santa Cruz, California, USA},
  doi = {10.3115/981863.981919},
  bdsk-url-2 = {https://doi.org/10.3115/981863.981919}
}

@inproceedings{pal.c:2006,
  title = {Sparse Forward-Backward Using Minimum Divergence Beams for Fast Training of Conditional Random Fields},
  booktitle = {{{IEEE}} International Conference on Acoustics Speed and Signal Processing Proceedings},
  author = {Pal, C. and Sutton, C. and McCallum, A.},
  year = {2006},
  volume = {V},
  pages = {581--584},
  publisher = {IEEE},
  doi = {10.1109/icassp.2006.1661342},
  bdsk-url-2 = {https://doi.org/10.1109/icassp.2006.1661342},
  date-added = {2022-03-25 11:41:07 -0400},
  date-modified = {2022-03-25 11:45:04 -0400}
}

@book{palermo.d:1964,
  title = {Word Association Norms: {{Grade}} School through College},
  author = {Palermo, David and Jenkins, James},
  year = {1964},
  publisher = {U. Minnesota Press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@misc{park.k:2024arxiv,
  title = {Grammar-{{Aligned Decoding}}},
  author = {Park, Kanghee and Wang, Jiayu and {Berg-Kirkpatrick}, Taylor and Polikarpova, Nadia and D'Antoni, Loris},
  year = {2024},
  month = may,
  number = {arXiv:2405.21047},
  eprint = {2405.21047},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.21047},
  urldate = {2024-06-25},
  abstract = {Large Language Models (LLMs) struggle with reliably generating highly structured outputs, such as program code, mathematical formulas, or well-formed markup. Constrained decoding approaches mitigate this problem by greedily restricting what tokens an LLM can output at each step to guarantee that the output matches a given constraint. Specifically, in grammar-constrained decoding (GCD), the LLM's output must follow a given grammar. In this paper we demonstrate that GCD techniques (and in general constrained decoding techniques) can distort the LLM's distribution, leading to outputs that are grammatical but appear with likelihoods that are not proportional to the ones given by the LLM, and so ultimately are low-quality. We call the problem of aligning sampling with a grammar constraint, grammar-aligned decoding (GAD), and propose adaptive sampling with approximate expected futures (ASAp), a decoding algorithm that guarantees the output to be grammatical while provably producing outputs that match the conditional probability of the LLM's distribution conditioned on the given grammar constraint. Our algorithm uses prior sample outputs to soundly overapproximate the future grammaticality of different output prefixes. Our evaluation on code generation and structured NLP tasks shows how ASAp often produces outputs with higher likelihood (according to the LLM's distribution) than existing GCD techniques, while still enforcing the desired grammatical constraints.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/park.k2024arxiv Grammar-Aligned Decoding.pdf}
}

@inproceedings{park.y:2009,
  title = {Minimal-Length Linearizations for Mildly Context-Sensitive Dependency Trees},
  booktitle = {Proceedings of Human Language Technologies: {{The}} 2009 Annual Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2009},
  pages = {335--343},
  publisher = {Association for Computational Linguistics},
  address = {Boulder, Colorado}
}

@inproceedings{park.y:2011,
  title = {Automated Whole Sentence Grammar Correction Using a Noisy Channel Model},
  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {Park, Y. Albert and Levy, Roger},
  year = {2011},
  month = jun,
  pages = {934--944},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  date-added = {2022-04-11 23:09:22 -0400},
  date-modified = {2022-04-11 23:09:27 -0400}
}

@article{parker.d:2016,
  title = {Negative Polarity Illusions and the Format of Hierarchical Encodings in Memory},
  author = {Parker, Dan and Phillips, Colin},
  year = {2016},
  month = dec,
  journal = {Cognition},
  volume = {157},
  pages = {321--339},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2016.08.016},
  urldate = {2023-02-22},
  abstract = {Linguistic illusions have provided valuable insights into how we mentally navigate complex representations in memory during language comprehension. Two notable cases involve illusory licensing of agreement and negative polarity items (NPIs), where comprehenders fleetingly accept sentences with unlicensed agreement or an unlicensed NPI, but judge those same sentences as unacceptable after more reflection. Existing accounts have argued that illusions are a consequence of faulty memory access processes, and make the additional assumption that the encoding of the sentence remains fixed over time. This paper challenges the predictions made by these accounts, which assume that illusions should generalize to a broader set of structural environments and a wider range of syntactic and semantic phenomena. We show across seven reading-time and acceptability judgment experiments that NPI illusions can be reliably switched ``on'' and ``off'', depending on the amount of time from when the potential licensor is processed until the NPI is encountered. But we also find that the same profile does not extend to agreement illusions. This contrast suggests that the mechanisms responsible for switching the NPI illusion on and off are not shared across all illusions. We argue that the contrast reflects changes over time in the encoding of the semantic/pragmatic representations that can license NPIs. Just as optical illusions have been informative about the visual system, selective linguistic illusions are informative not only about the nature of the access mechanisms, but also about the nature of the encoding mechanisms.},
  langid = {english},
  keywords = {Agreement,Binding,Linguistic illusions,Memory,Negative polarity,Representation,Sentence processing}
}

@article{partee.b:1990,
  title = {Mathematical Methods in Linguistics},
  author = {Manaster Ramer, Alexis},
  year = {1992},
  journal = {Computational Linguistics},
  volume = {18},
  number = {1}
}

@inproceedings{paskin.m:2001,
  title = {Grammatical Bigrams},
  booktitle = {Advances in Neural Information Processing Systems 14 ({{NIPS}} 2001)},
  author = {Paskin, Mark A.},
  editor = {Dietterich, Thomas G. and Becker, Suzanna and Ghahramani, Zoubin},
  year = {2001},
  month = dec,
  pages = {91--97},
  publisher = {MIT Press},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/Paskin01.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{pennington.j:2014,
  title = {{{GloVe}}: {{Global Vectors}} for {{Word Representation}}},
  shorttitle = {{{GloVe}}},
  booktitle = {Proceedings of the 2014 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}} ({{EMNLP}})},
  author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
  year = {2014},
  month = oct,
  pages = {1532--1543},
  publisher = {Association for Computational Linguistics},
  address = {Doha, Qatar},
  doi = {10.3115/v1/D14-1162},
  urldate = {2023-10-29},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pennington.j2014 GloVe Global Vectors for Word Represent.pdf}
}

@phdthesis{pentangelo.j:2020phd,
  title = {360{$^\circ$} {{Video}} and {{Language Documentation}}: {{Towards}} a {{Corpus}} of {{Kanien}}'k{\'e}ha ({{Mohawk}})},
  shorttitle = {360{$^\circ$} {{Video}} and {{Language Documentation}}},
  author = {Pentangelo, Joseph and link will open in a new window {Link to external site}, this},
  year = {2020},
  address = {United States -- New York},
  urldate = {2022-05-31},
  abstract = {Robust documentation is a major goal of documentary linguistics. Recognizing spoken language as a multimodal phenomenon, researchers working in this field broadly agree that video is an improvement over audio-only recording. At the same time, video is limited by the format's frame, which permits only a relatively small portion of the visual field to be recorded at any given time. This results in much data being lost, as the documenter must decide where to aim their camera, necessarily leaving out more than they record. In this dissertation, I apply 360º video to language documentation for the first time. 360º video, which is one variety of virtual reality, improves upon traditional video by drastically expanding the frame, recording in all directions surrounding the camera. In this way, a maximum of visual data is recorded, and there is no need for the camera to be redirected as participants take turns speaking or move around the space. I recorded over 10 hours of 360º video with ambisonic audio, containing mostly naturalistic conversation in the Akwesasne variety of Kanien'k{\'e}ha (Mohawk), an endangered Northern Iroquoian language spoken in New York State, Ontario, and Quebec. Most of the existing documentation of Kanien'k{\'e}ha outside of this corpus is formal or non-naturalistic. The resulting corpus thus serves a dual purpose: it is both a demonstration of the capabilities of 360º video for language documentation, and a contribution to the documentation of Kanien'k{\'e}ha. This dissertation includes a brief grammatical description of Kanien'k{\'e}ha phonology and morphology, a discussion of the interplay between technology and language documentation throughout North American history, an exploration of the significance of 360º video to documentary linguistics, a brief analysis of gesture and intonation in the present corpus, and an assessment of the suitability of ambisonic audio for linguistic analysis. Directions for potential future research are indicated throughout.},
  copyright = {Database copyright ProQuest LLC; ProQuest does not claim copyright in the individual underlying works.},
  isbn = {9798678110015},
  langid = {english},
  school = {City University of New York},
  keywords = {Akwesasne,Documentary linguistics,Iroquoian,Language documentation,Virtual reality},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pentangelo.j2020 360° Video and Language Documentation T.pdf}
}

@article{perea.m:2003,
  title = {Does Jugde Activate {{COURT}}? {{Transposed-letter}} Similarity Effects in Masked Associative Priming},
  shorttitle = {Does Jugde Activate {{COURT}}?},
  author = {Perea, Manuel and Lupker, Stephen J.},
  year = {2003},
  month = sep,
  journal = {Memory \& Cognition},
  volume = {31},
  number = {6},
  pages = {829--841},
  issn = {1532-5946},
  doi = {10.3758/BF03196438},
  urldate = {2023-10-29},
  abstract = {Transposed-letter (TL) nonwords (e.g.,jugde) can be easily misperceived as words, a fact that is somewhat inconsistent with the letter-position-coding schemes employed by most current models of visual word recognition. To examine this issue further, we conducted four masked semantic/associative priming experiments, using a lexical decision task. In Experiment 1, the related primes could be words, TL-internal nonwords, or replacement-letter (RL) nonwords (e.g.,judge, jugde, orjudpe, respectively; the target would be COURT). Relative to an unrelated condition, masked TL-internal primes produced a significant semantic/associative priming effect, an effect that was only slightly smaller than the priming effect for word primes. No effect, however, was observed for RL-nonword primes. In Experiment 2, the TL-nonword primes were created by switching the two final letters of the primes (e.g.,judeg). The results again showed a semantic/associative priming effect for word primes, but not for TL-final nonword primes or for RL-nonword primes. Experiment 3 replicated the associative/semantic priming effect for TL-internal nonword primes, with, again, no effect for TL-final nonword primes. Finally, Experiment 4 again failed to yield a priming effect for TL-final nonword primes. The implications of these results for the choice of a letter-position-coding scheme in visual word recognition models are discussed.},
  langid = {english},
  keywords = {Lexical Decision,Lexical Decision Task,priming,Priming Effect,transposed letter effects,Visual Word Recognition,Word Target}
}

@article{pereira.f:2000,
  title = {Formal Grammar and Information Theory: {{Together}} Again?},
  author = {Pereira, Fernando C. N.},
  year = {2000},
  journal = {Philosophical Transactions of the Royal Society: Mathematical, Physical and Engineering Sciences},
  volume = {358},
  number = {1769},
  pages = {1239--1253},
  doi = {10.1098/rsta.2000.0583},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding},
  keywords = {information theory},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pereira.f2000 Formal grammar and information theory T.pdf}
}

@article{perfors.a:2011,
  title = {A Tutorial Introduction to {{Bayesian}} Models of Cognitive Development},
  author = {Perfors, Amy and Tenenbaum, Joshua B. and Griffiths, Thomas L. and Xu, Fei},
  year = {2011},
  month = sep,
  journal = {Cognition},
  series = {Probabilistic Models of Cognitive Development},
  volume = {120},
  number = {3},
  pages = {302--321},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2010.11.015},
  urldate = {2024-05-03},
  abstract = {We present an introduction to Bayesian inference as it is used in probabilistic models of cognitive development. Our goal is to provide an intuitive and accessible guide to the what, the how, and the why of the Bayesian approach: what sorts of problems and data the framework is most relevant for, and how and why it may be useful for developmentalists. We emphasize a qualitative understanding of Bayesian inference, but also include information about additional resources for those interested in the cognitive science applications, mathematical foundations, or machine learning details in more depth. In addition, we discuss some important interpretation issues that often arise when evaluating Bayesian models in cognitive science.},
  keywords = {Bayesian models,Cognitive development},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/perfors.a2011 A tutorial introduction to Bayesian mode.pdf}
}

@article{perfors2011learnability,
  title = {The Learnability of Abstract Syntactic Principles},
  author = {Perfors, Amy and Tenenbaum, Joshua B and Regier, Terry},
  year = {2011},
  journal = {Cognition},
  volume = {118},
  number = {3},
  pages = {306--338},
  publisher = {Elsevier},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{peters.m:2018,
  title = {Deep Contextualized Word Representations},
  booktitle = {Proceedings of the 2018 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long Papers)},
  author = {Peters, Matthew and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
  year = {2018},
  pages = {2227--2237},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1202},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N18-1202}
}

@inproceedings{peters.m:2018a,
  title = {Dissecting Contextual Word Embeddings: {{Architecture}} and Representation},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Peters, Matthew and Neumann, Mark and Zettlemoyer, Luke and Yih, Wen-tau},
  year = {2018},
  pages = {1499--1509},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1179},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1179}
}

@inproceedings{petrov.s:2007,
  title = {Discriminative Log-Linear Grammars with Latent Variables},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Petrov, Slav and Klein, Dan},
  year = {2007},
  volume = {20},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-10-16},
  abstract = {We demonstrate that log-linear grammars with latent variables can be practically trained using discriminative methods. Central to efficient discriminative training is a hierarchical pruning procedure which allows feature expectations to be effi- ciently approximated in a gradient-based procedure. We compare L1 and L2 reg- ularization and show that L1 regularization is superior, requiring fewer iterations to converge, and yielding sparser solutions. On full-scale treebank parsing exper- iments, the discriminative latent models outperform both the comparable genera- tive latent models as well as the discriminative non-latent baselines.},
  keywords = {pruning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/petrov.s2007 Discriminative log-linear grammars with.pdf}
}

@incollection{phillips.c:2011,
  title = {Grammatical Illusions and Selective Fallibility in Real-Time Language Comprehension},
  shorttitle = {Chapter 5},
  booktitle = {Experiments at the {{Interfaces}}},
  author = {Phillips, Colin and Wagers, Matthew W. and Lau, Ellen F.},
  year = {2011},
  month = jan,
  series = {Syntax and {{Semantics}}},
  volume = {37},
  pages = {147--180},
  publisher = {Brill},
  doi = {10.1163/9781780523750_006},
  urldate = {2024-05-26},
  isbn = {978-1-78052-375-0},
  langid = {english},
  keywords = {Languages and Linguistics,Morphology & Syntax,Psycholinguistics & Language and Cognition,Semantics},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/phillips.c2011 Grammatical illusions and selective fall.pdf}
}

@article{piantadosi.s:2011,
  title = {Word Lengths Are Optimized for Efficient Communication},
  author = {Piantadosi, Steven T. and Tily, Harry and Gibson, Edward},
  year = {2011},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {108},
  number = {9},
  pages = {3526--3529},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1012551108},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1012551108},
  date-added = {2021-07-25 11:06:49 -0400},
  date-modified = {2021-07-25 11:06:50 -0400}
}

@article{piantadosi.s:2014,
  title = {Zipf's Word Frequency Law in Natural Language: {{A}} Critical Review and Future Directions},
  shorttitle = {Zipf's Word Frequency Law in Natural Language},
  author = {Piantadosi, Steven T.},
  year = {2014},
  month = oct,
  journal = {Psychonomic bulletin \& review},
  volume = {21},
  number = {5},
  pages = {1112--1130},
  issn = {1069-9384},
  doi = {10.3758/s13423-014-0585-6},
  urldate = {2022-09-27},
  abstract = {The frequency distribution of words has been a key object of study in statistical linguistics for the past 70 years. This distribution approximately follows a simple mathematical form known as Zipf ' s law. This article first shows that human language has a highly complex, reliable structure in the frequency distribution over and above this classic law, although prior data visualization methods have obscured this fact. A number of empirical phenomena related to word frequencies are then reviewed. These facts are chosen to be informative about the mechanisms giving rise to Zipf's law and are then used to evaluate many of the theoretical explanations of Zipf's law in language. No prior account straightforwardly explains all the basic facts or is supported with independent evaluation of its underlying assumptions. To make progress at understanding why language obeys Zipf's law, studies must seek evidence beyond the law itself, testing assumptions and evaluating novel predictions with new, independent data.},
  pmcid = {PMC4176592},
  pmid = {24664880},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/piantadosi.s2014 Zipf’s word frequency law in natural lan.pdf}
}

@article{pickering.m:2007,
  title = {Do People Use Language Production to Make Predictions during Comprehension?},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2007},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {11},
  number = {3},
  pages = {105--110},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2006.12.002},
  urldate = {2023-07-29},
  langid = {english},
  pmid = {17254833},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pickering.m2007 Do people use language production to mak.pdf}
}

@article{pickering.m:2013,
  title = {An Integrated Theory of Language Production and Comprehension},
  author = {Pickering, Martin J. and Garrod, Simon},
  year = {2013},
  journal = {Behavioral and Brain Sciences},
  volume = {36},
  number = {4},
  pages = {329--347},
  publisher = {Cambridge University Press},
  doi = {10.1017/S0140525X12001495},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pickering.m2013 An integrated theory of language product.pdf}
}

@article{pickering.m:2018predicting,
  title = {Predicting While Comprehending Language: {{A}} Theory and Review.},
  author = {Pickering, Martin J and Gambi, Chiara},
  year = {2018},
  journal = {Psychological Bulletin},
  volume = {144},
  number = {10},
  pages = {1002},
  publisher = {American Psychological Association},
  doi = {10.1037/bul0000158},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@inproceedings{piktus.a:2019,
  title = {Misspelling {{Oblivious Word Embeddings}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long}} and {{Short Papers}})},
  author = {Piktus, Aleksandra and Edizel, Necati Bora and Bojanowski, Piotr and Grave, Edouard and Ferreira, Rui and Silvestri, Fabrizio},
  year = {2019},
  month = jun,
  pages = {3226--3234},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1326},
  urldate = {2023-08-25},
  abstract = {In this paper we present a method to learn word embeddings that are resilient to misspellings. Existing word embeddings have limited applicability to malformed texts, which contain a non-negligible amount of out-of-vocabulary words. We propose a method combining FastText with subwords and a supervised task of learning misspelling patterns. In our method, misspellings of each word are embedded close to their correct variants. We train these embeddings on a new dataset we are releasing publicly. Finally, we experimentally show the advantages of this approach on both intrinsic and extrinsic NLP tasks using public test sets.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/piktus.a2019 Misspelling Oblivious Word Embeddings.pdf}
}

@article{pimentel.t:2023tacl,
  title = {On the Effect of Anticipation on Reading Times},
  author = {Pimentel, Tiago and Meister, Clara and Wilcox, Ethan G. and Levy, Roger P. and Cotterell, Ryan},
  year = {2023},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1624--1642},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00603},
  urldate = {2024-05-26},
  abstract = {Abstract             Over the past two decades, numerous studies have demonstrated how less-predictable (i.e., higher surprisal) words take more time to read. In general, these studies have implicitly assumed the reading process is purely responsive: Readers observe a new word and allocate time to process it as required. We argue that prior results are also compatible with a reading process that is at least partially anticipatory: Readers could make predictions about a future word and allocate time to process it based on their expectation. In this work, we operationalize this anticipation as a word's contextual entropy. We assess the effect of anticipation on reading by comparing how well surprisal and contextual entropy predict reading times on four naturalistic reading datasets: two self-paced and two eye-tracking. Experimentally, across datasets and analyses, we find substantial evidence for effects of contextual entropy over surprisal on a word's reading time (RT): In fact, entropy is sometimes better than surprisal in predicting a word's RT. Spillover effects, however, are generally not captured by entropy, but only by surprisal. Further, we hypothesize four cognitive mechanisms through which contextual entropy could impact RTs---three of which we are able to design experiments to analyze. Overall, our results support a view of reading that is not just responsive, but also anticipatory.1},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pimentel.t2023tacl On the effect of anticipation on reading.pdf}
}

@phdthesis{pimentel.t:2024phd,
  title = {On the {{Optimality}} of the {{Lexicon}}},
  author = {Pimentel Martins Da Silva, Tiago},
  year = {2024},
  month = apr,
  urldate = {2024-04-29},
  abstract = {The principle of least effort posits that a pressure towards communicative efficiency shapes natural languages. In this thesis, we investigate the existence, the nature, and the impact of such a pressure in natural languages' lexicons. We investigate the existence of this pressure by (i) estimating what optimal word lengths would be using coding theory, (ii) proposing pressure-free baselines and estimating their word lengths, and then (iii) comparing natural lexicons to both these optimal and pressure-free artificial lexicons. We investigate the nature of this pressure by comparing multiple ways in which communicative efficiency can be operationalised; we formalise it as either a pressure to shorten utterances, or a pressure to keep information rates as close as possible to an unknown communication channel capacity. Finally, we study the impact of this pressure on cross-linguistic differences in word lengths and on the ratio of homophones in natural languages. Overall, our results support a Zipfian view of communicative efficiency, in which lexicons are pressured towards having utterances that are as short as possible. Our results, however, also highlight the existence of competing constraints and pressures in how lexicons are structured: (i) a language's phonotactic complexity seems to bottleneck the extent to which economy of expression can optimise a lexicon, and (ii) a pressure for clarity seems to keep the ratio of homophones in a language close to chance.},
  langid = {english},
  school = {University of Cambridge},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pimentel.t2024phd On the Optimality of the Lexicon.pdf}
}

@inproceedings{pine.a:2022,
  title = {Requirements and {{Motivations}} of {{Low-Resource Speech Synthesis}} for {{Language Revitalization}}},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Pine, Aidan and Wells, Dan and Brinklow, Nathan and Littell, Patrick and Richmond, Korin},
  year = {2022},
  month = may,
  pages = {7346--7359},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.507},
  urldate = {2022-06-04},
  abstract = {This paper describes the motivation and development of speech synthesis systems for the purposes of language revitalization. By building speech synthesis systems for three Indigenous languages spoken in Canada, Kanien'k{\'e}ha, Gitksan \& SEN{\'C}O{\textTstroke}EN, we re-evaluate the question of how much data is required to build low-resource speech synthesis systems featuring state-of-the-art neural models. For example, preliminary results with English data show that a FastSpeech2 model trained with 1 hour of training data can produce speech with comparable naturalness to a Tacotron2 model trained with 10 hours of data. Finally, we motivate future research in evaluation and classroom integration in the field of speech synthesis for language revitalization.},
  keywords = {computational revitalization,iroquoian},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pine.a2022 Requirements and Motivations of Low-Reso.pdf}
}

@incollection{polinsky.m:2013,
  title = {Resumption in {{English}}},
  booktitle = {Experimental {{Syntax}} and {{Island Effects}}},
  author = {Polinsky, Maria and Eby Clemens, Lauren and Milton Morgan, Adam and Xiang, Ming and Heestand, Dustin},
  editor = {Sprouse, Jon and Hornstein, Norbert},
  year = {2013},
  pages = {341--359},
  publisher = {Cambridge University Press},
  address = {Cambridge},
  doi = {10.1017/CBO9781139035309.017},
  urldate = {2023-04-05},
  isbn = {978-1-107-00870-0},
  keywords = {resumptive pronouns},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/polinsky.m2013 Resumption in English.pdf}
}

@book{polyanskiy.y:2024,
  title = {Information Theory: From Coding to Learning},
  author = {Polyanskiy, Yury and Wu, Yihong},
  year = {2024},
  month = nov,
  edition = {1},
  isbn = {978-1-108-83290-8},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/polyanskiy.y2023 Information theory from coding to learn.pdf}
}

@article{poole.e:2016,
  title = {Deconstructing Subjecthood},
  author = {Poole, Ethan},
  year = {2016},
  journal = {Ms., UMass Amherst.},
  date-added = {2019-06-14 09:32:06 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,subject positions}
}

@inproceedings{poppels.t:2016,
  title = {Structure-Sensitive {{Noise Inference}}: {{Comprehenders Expect Exchange Errors}}},
  booktitle = {Proceedings of the 38th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Poppels, Till and Levy, Roger},
  year = {2016},
  pages = {378--383},
  publisher = {Cognitive Science Society},
  address = {Austin, TX},
  abstract = {Previous research has found that comprehenders are willing to adopt non-literal interpretations of sentences whose literal reading is unlikely. Several studies found evidence that comprehenders decide whether a given utterance should be taken at face value in accordance with principles of Bayesian rationality, by weighing the prior probability of potential interpretations against the degree to which they are (in)consistent with the literal form of the utterance. While all of these results are consistent with string-edit noise models, many error processes are known to be sensitive to the underlying linguistic structure of the intended utterance. Here, we explore the case of exchange errors and provide experimental evidence that comprehenders' noise model is structure-sensitive. Our results add further support to the noisy-channel theory of language comprehension, extend the set of known noise operations to include positional exchanges, and show that comprehenders' noise models are well-adapted to structure-sensitive sources of signal corruption.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/poppels.t2016 Structure-sensitive Noise Inference Com.pdf}
}

@article{post.e:1943,
  title = {Formal Reductions of the General Combinatorial Decision Problem},
  author = {Post, Emil L.},
  year = {1943},
  journal = {American Journal of Mathematics},
  volume = {65},
  number = {2},
  eprint = {2371809},
  eprinttype = {jstor},
  pages = {197--215},
  publisher = {Johns Hopkins University Press},
  issn = {0002-9327},
  doi = {10.2307/2371809},
  urldate = {2022-07-15},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/post.e1943 Formal reductions of the general combina.pdf}
}

@article{post.m:2013,
  title = {Bayesian Tree Substitution Grammars as a Usage-Based Approach},
  author = {Post, Matt and Gildea, Daniel},
  year = {2013},
  journal = {Language and Speech},
  volume = {56},
  number = {3},
  pages = {291--308},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:27 -0400},
  keywords = {Tree Substitution Grammar}
}

@misc{prasad.g:2019readingMVRR,
  title = {Rapid Syntactic Adaptation in Self-Paced Reading: Detectable, but Requires Many Participants.},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  publisher = {Center for Open Science},
  doi = {10.31234/osf.io/9ptg4},
  bdsk-url-2 = {https://doi.org/10.31234/osf.io/9ptg4},
  date-added = {2021-03-18 11:13:22 -0400},
  date-modified = {2021-03-18 17:40:38 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing,reading time,self-paced reading}
}

@misc{prasad.g:2019readingNPS-NPZ,
  title = {How Much Harder Are Hard Garden-Path Sentences than Easy Ones?},
  author = {Prasad, Grusha and Linzen, Tal},
  year = {2019},
  journal = {CogSci},
  date-added = {2021-03-18 11:20:31 -0400},
  date-modified = {2021-03-18 17:38:35 -0400},
  howpublished = {OSF preprint},
  keywords = {processing,reading time,self-paced reading}
}

@article{preminger.o:2011,
  title = {Asymmetries between Person and Number in Syntax: A Commentary on {{Baker}}'s {{SCOPA}}},
  author = {Preminger, Omer},
  year = {2011},
  journal = {Natural Language \& Linguistic Theory},
  volume = {29},
  number = {4},
  pages = {917--937},
  publisher = {Springer},
  date-added = {2020-02-25 21:43:01 -0500},
  date-modified = {2020-02-26 09:11:06 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement,phi features}
}

@book{preminger.o:2014,
  title = {Agreement and Its Failures},
  author = {Preminger, Omer},
  year = {2014},
  volume = {68},
  publisher = {MIT Press},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:08:07 -0400},
  project = {Icelandic gluttony}
}

@article{prim.r:1957,
  title = {Shortest Connection Networks and Some Generalizations},
  author = {Prim, R. C.},
  year = {1957},
  journal = {The Bell System Technical Journal},
  volume = {36},
  number = {6},
  pages = {1389--1401},
  doi = {10.1002/j.1538-7305.1957.tb01515.x},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{prince.e:1990,
  title = {Syntax and Discourse: {{A}} Look at Resumptive Pronouns},
  booktitle = {Proceedings of the Sixteenth Annual Meeting of the {{Berkeley}} Linguistics Society},
  author = {Prince, Ellen F},
  year = {1990},
  volume = {16},
  pages = {482--497},
  isbn = {2377-1666},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/prince.e1990 Syntax and discourse A look at resumpti.pdf}
}

@article{priva.u:2020,
  title = {The Causal Structure of Lenition: A Case for the Causal Precedence of Durational Shortening},
  author = {Priva, Uriel Cohen and Gleason, Emily},
  year = {2020},
  journal = {Language},
  volume = {96},
  number = {2},
  pages = {413--448},
  publisher = {Project Muse},
  doi = {10.1353/lan.2020.0025},
  bdsk-url-2 = {https://doi.org/10.1353/lan.2020.0025},
  date-added = {2022-05-10 10:31:58 -0400},
  date-modified = {2022-05-10 10:32:14 -0400},
  keywords = {causality,lenition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/priva.u2020 The causal structure of lenition a case.pdf}
}

@inproceedings{przepiorkowski.a:2018arguments-adjuncts-ud,
  title = {Arguments and Adjuncts in {{Universal Dependencies}}},
  booktitle = {Proceedings of the 27th International Conference on Computational Linguistics},
  author = {Przepi{\'o}rkowski, Adam and Patejuk, Agnieszka},
  year = {2018},
  pages = {3837--3852},
  publisher = {Association for Computational Linguistics},
  address = {Santa Fe, New Mexico, USA}
}

@article{pustejovsky.j:1991,
  title = {The Generative Lexicon},
  author = {Pustejovsky, James},
  year = {1991},
  journal = {Computational Linguistics},
  volume = {17},
  number = {4},
  pages = {409--441},
  urldate = {2023-12-20},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pustejovsky.j1991 The generative lexicon.pdf}
}

@book{pustejovsky.j:2002book,
  title = {The Generative Lexicon},
  author = {Pustejovsky, James},
  year = {2002},
  edition = {1. MIT Press paperback ed., 4. print},
  publisher = {MIT Press},
  address = {Cambridge, Mass.},
  isbn = {978-0-262-16158-9 978-0-262-66140-9},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pustejovsky.j2002book The generative lexicon.pdf}
}

@book{pylyshyn.z:1984,
  title = {Computation and Cognition: Toward a Foundation for Cognitive Science},
  shorttitle = {Computation and Cognition},
  author = {Pylyshyn, Zenon W.},
  year = {1984},
  publisher = {MIT Press},
  abstract = {This systematic investigation of computation and mental phenomena by a noted psychologist and computer scientist argues that cognition is a form of computation, that the semantic contents of mental states are encoded in the same general way as computer representations are encoded.},
  isbn = {978-0-262-16098-8},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/pylyshyn.z1984 Computation and cognition toward a foun.pdf}
}

@inproceedings{qian.p:2022,
  title = {Flexible Generation from Fragmentary Linguistic Input},
  booktitle = {Proceedings of the 60th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Qian, Peng and Levy, Roger},
  year = {2022},
  month = may,
  pages = {8176--8196},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.acl-long.563},
  urldate = {2023-02-09},
  abstract = {The dominant paradigm for high-performance models in novel NLP tasks today is direct specialization for the task via training from scratch or fine-tuning large pre-trained models. But does direct specialization capture how humans approach novel language tasks? We hypothesize that human performance is better characterized by flexible inference through composition of basic computational motifs available to the human language user. To test this hypothesis, we formulate a set of novel fragmentary text completion tasks, and compare the behavior of three direct-specialization models against a new model we introduce, GibbsComplete, which composes two basic computational motifs central to contemporary models: masked and autoregressive word prediction. We conduct three types of evaluation: human judgments of completion quality, satisfaction of syntactic constraints imposed by the input fragment, and similarity to human behavior in the structural statistics of the completions. With no task-specific parameter tuning, GibbsComplete performs comparably to direct-specialization models in the first two evaluations, and outperforms all direct-specialization models in the third evaluation. These results support our hypothesis that human behavior in novel language tasks and environments may be better characterized by flexible composition of basic computational motifs rather than by direct specialization.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/qian.p2022 Flexible generation from fragmentary lin.pdf}
}

@misc{qian.p:2023psyarxiv,
  title = {Comprehenders' Error Correction Mechanisms Are Finely Calibrated to Language Production Statistics},
  author = {Qian, Peng and Levy, Roger Philip},
  year = {2023},
  month = sep,
  number = {e3v5b},
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/e3v5b},
  urldate = {2023-09-30},
  abstract = {An essential feature of human communication is robustness to errors: even when speakers and writers make mistakes, listeners and readers can usually determine the intended meaning. A leading hypothesis is that comprehenders achieve robust error correction by internally deploying a "noisy-channel" model of the language production process. Crucially, to best achieve error robustness, a comprehender's noisy-channel model should be finely calibrated to the environmental statistics of language production, incorporating not only prior expectations about communicative intents but also structure-sensitive likelihoods of different types of speaker error. This environmentally-calibrated noisy-channel hypothesis has been difficult to test, because the rates of most types of language production errors are hard to estimate. Here we test this hypothesis by experimentally investigating comprehenders' interpretations of subject-verb agreement errors, for which context-contingent language production error rates are feasible to estimate. In a novel free-form error correction task,  participants edited sentences with subject-verb agreement mismatches to indicate what they think was intended. Bayesian analysis of the resulting data shows that comprehenders' interpretations reflect both item-specific prior expectations and structure-sensitive error likelihoods that closely correspond to error rates in language production. These results provide quantitative evidence that humans closely track prior statistics over linguistic forms and deploy a noisy-channel model finely calibrated to statistics of the linguistic environment to achieve robust language understanding.},
  langid = {american},
  keywords = {Bayesian models of cognition,Cognitive Psychology,Language,language comprehension,noisy-channel language processing,psycholinguistics,rational adaptation,Social and Behavioral Sciences},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/qian.p2023psyarxiv Comprehenders’ error correction mechanis.pdf}
}

@manual{r.coreteam:2021,
  type = {Manual},
  title = {R: {{A}} Language and Environment for Statistical Computing},
  author = {{R Core Team}},
  year = {2021},
  address = {Vienna, Austria},
  organization = {R Foundation for Statistical Computing}
}

@article{rabagliati.h:2016,
  title = {Learning to Predict or Predicting to Learn?},
  author = {Rabagliati, Hugh and Gambi, Chiara and Pickering, J},
  year = {2016},
  journal = {Language, Cognition and Neuroscience},
  volume = {31},
  number = {1},
  pages = {94--105},
  doi = {10.1080/23273798.2015.1077979},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@misc{rabe.m:2023arxiv,
  title = {{{SEAM}}: An Integrated Activation-Coupled Model of Sentence Processing and Eye Movements in Reading},
  shorttitle = {Seam},
  author = {Rabe, Maximilian M. and Paape, Dario and Mertzen, Daniela and Vasishth, Shravan and Engbert, Ralf},
  year = {2023},
  month = mar,
  number = {arXiv:2303.05221},
  eprint = {2303.05221},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-06-05},
  abstract = {Models of eye-movement control during reading, developed largely within psychology, usually focus on visual, attentional, and motor processes but neglect post-lexical language processing; by contrast, models of sentence comprehension processes, developed largely within psycholinguistics, generally focus only on post-lexical language processes. We present a model that combines these two research threads, by integrating eye-movement control and sentence processing. Developing such an integrated model is extremely challenging and computationally demanding, but such an integration is an important step toward complete mathematical models of natural language comprehension in reading. We combine the SWIFT model of eye-movement control (Engbert et al., Psychological Review, 112, 2005, pp. 777-813) with key components of the Lewis and Vasishth sentence processing model (Lewis and Vasishth, Cognitive Science, 29, 2005, pp. 375-419). This integration becomes possible, for the first time, due in part to recent advances in successful parameter identification in dynamical models, which allows us to investigate profile log-likelihoods for individual model parameters. We present a fully implemented proof-of-concept model demonstrating how such an integrated model can be achieved; our approach includes Bayesian model inference with Markov Chain Monte Carlo (MCMC) sampling as a key computational tool. The integrated model, SEAM, can successfully reproduce eye movement patterns that arise due to similarity-based interference in reading. To our knowledge, this is the first-ever integration of a complete process model of eye-movement control with linguistic dependency completion processes in sentence comprehension. In future work, this proof of concept model will need to be evaluated using a comprehensive set of benchmark data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,cue-based retrieval,eye-tracking,Quantitative Biology - Neurons and Cognition,sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rabe.m2023 SEAM an integrated activation-coupled m.pdf}
}

@book{rabinovich.m:2012,
  title = {Principles of Brain Dynamics: Global State Interactions},
  editor = {Rabinovich, Mikhail I. and Friston, Karl J. and Varona, Pablo},
  year = {2012},
  month = jul,
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/9108.001.0001},
  urldate = {2022-07-08},
  abstract = {Experimental and theoretical approaches to global brain dynamics that draw on the latest research in the field.The consideration of time or dynamics is fundamental for all aspects of mental activity---perception, cognition, and emotion---because the main feature of brain activity is the continuous change of the underlying brain states even in a constant environment. The application of nonlinear dynamics to the study of brain activity began to flourish in the 1990s when combined with empirical observations from modern morphological and physiological observations. This book offers perspectives on brain dynamics that draw on the latest advances in research in the field. It includes contributions from both theoreticians and experimentalists, offering an eclectic treatment of fundamental issues.Topics addressed range from experimental and computational approaches to transient brain dynamics to the free-energy principle as a global brain theory. The book concludes with a short but rigorous guide to modern nonlinear dynamics and their application to neural dynamics.},
  isbn = {978-0-262-30558-7}
}

@misc{radford.a:2018GPT,
  title = {Improving Language Understanding by Generative Pre-Training},
  author = {Radford, Alec and Narasimhan, Karthik and Salimans, Tim and Sutskever, Ilya},
  year = {2018},
  publisher = {OpenAI},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {generative pre-training,GPT,GPT1,transfer learning}
}

@misc{radford.a:2018GPT-post,
  title = {Improving Language Understanding with Unsupervised Learning},
  shorttitle = {{{GPT}}},
  author = {Radford, Alec},
  year = {2018},
  month = jun,
  journal = {OpenAI},
  urldate = {2023-03-12},
  abstract = {We've obtained state-of-the-art results on a suite of diverse language tasks with a scalable, task-agnostic system, which we're also releasing. Our approach is a combination of two existing ideas:~transformers~and~unsupervised pre-training. These results provide a convincing example that pairing supervised learning methods with unsupervised pre-training works very well; this is an idea that many have explored in the past, and we hope our result motivates further research into applying this idea on larger and more diverse~datasets.},
  howpublished = {https://openai.com/research/language-unsupervised},
  langid = {american}
}

@misc{radford.a:2019GPT2,
  title = {Language Models Are Unsupervised Multitask Learners},
  author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
  year = {2019},
  howpublished = {OpenAI blog},
  project = {syntactic embedding},
  keywords = {GPT,GPT2}
}

@misc{radford.a:2019GPT2-post,
  title = {Better Language Models and Their Implications},
  shorttitle = {{{GPT-2}}},
  author = {Radford, Alec and Wu, Jeffrey and Amodei, Dario and Amodei, Daniella and Clark, Jack and Brundage, Miles and Sutskever, Ilya},
  year = {2019},
  month = feb,
  journal = {OpenAI},
  urldate = {2023-03-12},
  abstract = {We've trained a large-scale unsupervised language model which generates coherent paragraphs of text, achieves state-of-the-art performance on many language modeling benchmarks, and performs rudimentary reading comprehension, machine translation, question answering, and summarization---all without task-specific~training.},
  howpublished = {https://openai.com/research/better-language-models},
  langid = {american}
}

@article{raffel.c:2020T5,
  title = {Exploring the Limits of Transfer Learning with a Unified Text-to-Text {{Transformer}}},
  author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
  year = {2020},
  journal = {Journal of Machine Learning Research},
  volume = {21},
  number = {140},
  pages = {1--67},
  issn = {1533-7928},
  urldate = {2023-05-24},
  abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
  keywords = {unified LM},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/raffel.c2020T5 Exploring the limits of transfer learnin.pdf}
}

@misc{rahimi.h:2023arxiv,
  title = {Contextualized {{Topic Coherence Metrics}}},
  author = {Rahimi, Hamed and Hoover, Jacob Louis and Mimno, David and Naacke, Hubert and Constantin, Camelia and Amann, Bernd},
  year = {2023},
  month = may,
  number = {arXiv:2305.14587},
  eprint = {2305.14587},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-11},
  abstract = {The recent explosion in work on neural topic modeling has been criticized for optimizing automated topic evaluation metrics at the expense of actual meaningful topic identification. But human annotation remains expensive and time-consuming. We propose LLM-based methods inspired by standard human topic evaluations, in a family of metrics called Contextualized Topic Coherence (CTC). We evaluate both a fully automated version as well as a semi-automated CTC that allows human-centered evaluation of coherence while maintaining the efficiency of automated methods. We evaluate CTC relative to five other metrics on six topic models and find that it outperforms automated topic coherence methods, works well on short documents, and is not susceptible to meaningless but high-scoring topics.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Information Retrieval},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rahimi.h2023arxiv Contextualized Topic Coherence Metrics.pdf}
}

@inproceedings{rahimi.h:2024,
  title = {Contextualized {{Topic Coherence Metrics}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EACL}} 2024},
  author = {Rahimi, Hamed and Mimno, David and Hoover, Jacob and Naacke, Hubert and Constantin, Camelia and Amann, Bernd},
  editor = {Graham, Yvette and Purver, Matthew},
  year = {2024},
  month = mar,
  pages = {1760--1773},
  publisher = {Association for Computational Linguistics},
  address = {St. Julian's, Malta},
  urldate = {2024-04-29},
  abstract = {This article proposes a new family of LLM-based topic coherence metrics called Contextualized Topic Coherence (CTC) and inspired by standard human topic evaluation methods. CTC metrics simulate human-centered coherence evaluation while maintaining the efficiency of other automated methods. We compare the performance of our CTC metrics and five other baseline metrics on seven topic models and show that CTC metrics better reflect human judgment, particularly for topics extracted from short text collections by avoiding highly scored topics that are meaningless to humans.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rahimi.h2024 Contextualized Topic Coherence Metrics.pdf}
}

@article{ramgoolam.s:2019,
  title = {Permutation Invariant Gaussian Matrix Models},
  author = {Ramgoolam, Sanjaye},
  year = {2019},
  journal = {Nuclear Physics B},
  pages = {114682},
  publisher = {Elsevier},
  date-added = {2019-08-06 08:51:05 +0300},
  date-modified = {2019-08-06 08:52:06 +0300},
  project = {syntactic embedding},
  keywords = {gaussian matrix models,physics}
}

@article{rasmussen.n:2018,
  title = {Left-Corner Parsing with Distributed Associative Memory Produces Surprisal and Locality Effects},
  author = {Rasmussen, Nathan E. and Schuler, William},
  year = {2018},
  journal = {Cognitive Science},
  volume = {42},
  number = {S4},
  pages = {1009--1042},
  issn = {1551-6709},
  doi = {10.1111/cogs.12511},
  urldate = {2022-06-13},
  abstract = {This article describes a left-corner parser implemented within a cognitively and neurologically motivated distributed model of memory. This parser's approach to syntactic ambiguity points toward a tidy account both of surprisal effects and of locality effects, such as the parsing breakdowns caused by center embedding. The model provides an algorithmic-level (Marr, 1982) account of these breakdowns: The structure of the parser's memory and the nature of incremental parsing produce a smooth degradation of processing accuracy for longer center embeddings, and a steeper degradation when they are nested, in line with recall observations by Miller and Isard (1964) and speed-accuracy trade-off observations by McElree et al. (2003). Modeling results show that this effect is distinct from the effects of ambiguity and exceeds the effect of mere sentence length.},
  langid = {english},
  keywords = {Computational modeling,Computer simulation,Language understanding,Linguistics,Locality,Memory,Sentence processing,Surprisal,Syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rasmussen.n2018 Left-corner parsing with distributed ass.pdf}
}

@misc{rasooli.m:2015arxiv,
  title = {Yara {{Parser}}: {{A Fast}} and {{Accurate Dependency Parser}}},
  shorttitle = {Yara {{Parser}}},
  author = {Rasooli, Mohammad Sadegh and Tetreault, Joel},
  year = {2015},
  month = mar,
  number = {arXiv:1503.06733},
  eprint = {1503.06733},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2022-05-17},
  abstract = {Dependency parsers are among the most crucial tools in natural language processing as they have many important applications in downstream tasks such as information retrieval, machine translation and knowledge acquisition. We introduce the Yara Parser, a fast and accurate open-source dependency parser based on the arc-eager algorithm and beam search. It achieves an unlabeled accuracy of 93.32 on the standard WSJ test set which ranks it among the top dependency parsers. At its fastest, Yara can parse about 4000 sentences per second when in greedy mode (1 beam). When optimizing for accuracy (using 64 beams and Brown cluster features), Yara can parse 45 sentences per second. The parser can be trained on any syntactic dependency treebank and different options are provided in order to make it more flexible and tunable for specific tasks. It is released with the Apache version 2.0 license and can be used for both commercial and academic purposes. The parser can be found at https://github.com/yahoo/YaraParser.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rasooli.m2015 Yara Parser A Fast and Accurate Depende.pdf}
}

@article{ratcliff.r:1978,
  title = {A Theory of Memory Retrieval},
  author = {Ratcliff, Roger},
  year = {1978},
  journal = {Psychological Review},
  volume = {85},
  number = {2},
  pages = {59--108},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/0033-295X.85.2.59},
  abstract = {Develops a theory of memory retrieval and shows that it applies over a range of experimental paradigms. Access to memory traces is viewed in terms of a resonance metaphor. The probe item evokes the search set on the basis of probe--memory item relatedness, just as a ringing tuning fork evokes sympathetic vibrations in other tuning forks. Evidence is accumulated in parallel from each probe--memory item comparison, and each comparison is modeled by a continuous random walk process. In item recognition, the decision process is self-terminating on matching comparisons and exhaustive on nonmatching comparisons. The mathematical model produces predictions about accuracy, mean reaction time, error latency, and reaction time distributions that are in good accord with data from 2 experiments conducted with 6 undergraduates. The theory is applied to 4 item recognition paradigms (Sternberg, prememorized list, study--test, and continuous) and to speed--accuracy paradigms; results are found to provide a basis for comparison of these paradigms. It is noted that neural network models can be interfaced to the retrieval theory with little difficulty and that semantic memory models may benefit from such a retrieval scheme. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Memory,Theories}
}

@article{ratcliff.r:2008,
  title = {The {{Diffusion Decision Model}}: {{Theory}} and {{Data}} for {{Two-Choice Decision Tasks}}},
  shorttitle = {The {{Diffusion Decision Model}}},
  author = {Ratcliff, Roger and McKoon, Gail},
  year = {2008},
  month = apr,
  journal = {Neural Computation},
  volume = {20},
  number = {4},
  pages = {873--922},
  issn = {0899-7667},
  doi = {10.1162/neco.2008.12-06-420},
  urldate = {2024-02-29},
  abstract = {The diffusion decision model allows detailed explanations of behavior in two-choice discrimination tasks. In this article, the model is reviewed to show how it translates behavioral data---accuracy, mean response times, and response time distributions---into components of cognitive processing. Three experiments are used to illustrate experimental manipulations of three components: stimulus difficulty affects the quality of information on which a decision is based; instructions emphasizing either speed or accuracy affect the criterial amounts of information that a subject requires before initiating a response; and the relative proportions of the two stimuli affect biases in drift rate and starting point. The experiments also illustrate the strong constraints that ensure the model is empirically testable and potentially falsifiable. The broad range of applications of the model is also reviewed, including research in the domains of aging and neurophysiology.},
  keywords = {drift diffusion},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ratcliff.r2008 The Diffusion Decision Model Theory and.pdf}
}

@inproceedings{rathi.n:2021,
  title = {An {{Information-Theoretic Characterization}} of {{Morphological Fusion}}},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Rathi, Neil and Hahn, Michael and Futrell, Richard},
  editor = {Moens, Marie-Francine and Huang, Xuanjing and Specia, Lucia and Yih, Scott Wen-tau},
  year = {2021},
  month = nov,
  pages = {10115--10120},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.793},
  urldate = {2024-03-31},
  abstract = {Linguistic typology generally divides synthetic languages into groups based on their morphological fusion. However, this measure has long been thought to be best considered a matter of degree. We present an information-theoretic measure, called informational fusion, to quantify the degree of fusion of a given set of morphological features in a surface form, which naturally provides such a graded scale. Informational fusion is able to encapsulate not only concatenative, but also nonconcatenative morphological systems (e.g. Arabic), abstracting away from any notions of morpheme segmentation. We then show, on a sample of twenty-one languages, that our measure recapitulates the usual linguistic classifications for concatenative systems, and provides new measures for nonconcatenative ones. We also evaluate the long-standing hypotheses that more frequent forms are more fusional, and that paradigm size anticorrelates with degree of fusion. We do not find evidence for the idea that languages have characteristic levels of fusion; rather, the degree of fusion varies across part-of-speech within languages.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rathi.n2021 An Information-Theoretic Characterizatio.pdf}
}

@phdthesis{ravishankar.m:1996,
  title = {Efficient Algorithms for Speech Recognition},
  author = {Ravishankar, Mosur K},
  year = {1996},
  month = may,
  date-added = {2022-03-25 23:17:15 -0400},
  date-modified = {2022-03-25 23:19:35 -0400},
  school = {Carnegie Mellon University, Department of Computer Science}
}

@article{rayner.k:1998,
  title = {Eye Movements in Reading and Information Processing: 20 Years of Research},
  author = {Rayner, Keith},
  year = {1998},
  journal = {Psychological Bulletin},
  volume = {124},
  pages = {372--422},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1455(Electronic),0033-2909(Print)},
  doi = {10.1037/0033-2909.124.3.372},
  abstract = {Recent studies of eye movements in reading and other information processing tasks, such as music reading, typing, visual search, and scene perception, are reviewed. The major emphasis of the review is on reading as a specific example of cognitive processing. Basic topics discussed with respect to reading are (a) the characteristics of eye movements, (b) the perceptual span, (c) integration of information across saccades, (d) eye movement control, and (e) individual differences (including dyslexia). Similar topics are discussed with respect to the other tasks examined. The basic theme of the review is that eye movement data reflect moment-to-moment cognitive processes in the various tasks examined. Theoretical and practical considerations concerning the use of eye movement data are also discussed. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {*Cognitive Processes,*Eye Movements,Reading}
}

@article{redington.m:1997,
  title = {Probabilistic and Distributional Approaches to Language Acquisition},
  author = {Redington, Martin and Chater, Nick},
  year = {1997},
  month = oct,
  journal = {Trends in Cognitive Sciences},
  volume = {1},
  number = {7},
  pages = {273--281},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/S1364-6613(97)01081-4},
  urldate = {2024-05-26},
  langid = {english},
  pmid = {21223923},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/redington.m1997 Probabilistic and distributional approac.pdf}
}

@inproceedings{rehurek.r:2010gensim,
  title = {Software Framework for Topic Modelling with Large Corpora},
  booktitle = {Proceedings of the {{LREC}} 2010 Workshop on New Challenges for {{NLP}} Frameworks},
  author = {{\v R}eh{\r u}{\v r}ek, Radim and Sojka, Petr},
  year = {2010},
  pages = {45--50},
  publisher = {ELRA},
  address = {Valletta, Malta},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2020-06-08 21:52:31 -0400},
  project = {syntactic embedding}
}

@article{reichle.e:2003,
  title = {The {{E-Z Reader}} Model of Eye-Movement Control in Reading: {{Comparisons}} to Other Models},
  author = {Reichle, Erik D. and Rayner, Keith and Pollatsek, Alexander},
  year = {2003},
  journal = {Behavioral and Brain Sciences},
  volume = {26},
  number = {4},
  pages = {445--476},
  publisher = {Cambridge University Press (CUP)},
  doi = {10.1017/s0140525x03000104},
  bdsk-url-2 = {https://doi.org/10.1017/s0140525x03000104},
  date-added = {2021-05-22 14:59:06 -0400},
  date-modified = {2022-05-06 15:37:48 -0400},
  keywords = {eye-tracking,processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/reichle.e2003 The E-Z Reader model of eye-movement con.pdf}
}

@article{reiss.c:2018,
  title = {Substance Free Phonology},
  author = {Reiss, Charles},
  editor = {S. J. Hannahs, A. R. K. Bosch},
  year = {2017},
  journal = {The Routledge handbook of phonological theory},
  pages = {425--452},
  publisher = {Routledge New York},
  date-added = {2019-06-17 08:29:14 -0400},
  date-modified = {2019-06-17 08:38:02 -0400},
  isbn = {9781138025813},
  keywords = {substance free phonology}
}

@inproceedings{renyi.a:1961,
  title = {On Measures of Entropy and Information},
  booktitle = {Proceedings of the Fourth {{Berkeley}} Symposium on Mathematical Statistics and Probability},
  author = {R{\'e}nyi, Alfr{\'e}d},
  editor = {Neyman, Jerzy},
  year = {1961},
  volume = {1},
  pages = {547--561},
  date-added = {2021-10-27 09:20:40 -0400},
  date-modified = {2021-10-27 09:27:42 -0400},
  organization = {University of California Press}
}

@article{rezac.m:2008,
  title = {Phi-Agree and Theta-Related Case},
  author = {Rezac, Milan},
  year = {2008},
  journal = {Phi theory: Phi-features across interfaces and modules},
  pages = {83--129},
  publisher = {Oxford University Press Oxford},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:25:09 -0400},
  project = {Icelandic gluttony},
  keywords = {phi features}
}

@unpublished{rezac.m:2016,
  type = {Ms. {{UMR}} 5478, {{IKER CNRS}}},
  title = {The Ways of Referential Deficiency: {{Impersonal}} {{{\emph{on}}}} and Its Kin},
  shorttitle = {The Ways of Referential Deficiency},
  author = {Rezac, Milan and Jouitteau, M{\'e}lanie},
  year = {2016},
  month = sep,
  doi = {10.5281/ZENODO.5823635},
  urldate = {2024-05-09},
  abstract = {This work is a study of the French impersonal {$<$}em{$>$}on{$<$}/em{$>$} and a theory of the unique "referential deficiency" of impersonals: a range of uses that spans those covered by indefinites and definites; neutrality about content like number; systematic participation in syntactic and semantic dependencies but with unparalleled restrictions like binding of only local, number-neutral anaphora. Current understanding of the syntax and semantics of DPs and properties of French let us study this behavior in depth and extend previous findings, often in unexpected ways. The study reveals a DP with content unique in French but drawing only on options available in UG. It leads to a theory of impersonal {$<$}em{$>$}on{$<$}/em{$>$} as an indefinite DP whose content interacts with certain theories of phi-features, indefinites and definites, and anaphoric dependencies to give an explanatory account of the nature of impersonals. In turn, impersonal {$<$}em{$>$}on{$<$}/em{$>$} contributes to the theories that enter into its analysis: the relationship between syntactic underspecification and semantic neutrality, the nature of indefinites and their relationship to definites (Heim 1991, 2011; Heim 1982, Elbourne 2013), and minimal pronoun anaphora (Kratzer 2009). The study is extended to the grammaticalisation of a distinct 1PL {$<$}em{$>$}on{$<$}/em{$>$}, whose complex properties reflect the colexicalisation of impersonal {$<$}em{$>$}on{$<$}/em{$>$} and a 1PL element. We end the book on the place of impersonals in the landscape of argument coding and explore the expected parameter space through {$<$}em{$>$}on{$<$}/em{$>$}-like impersonals cross-linguistically. The work builds on the analysis of {$<$}em{$>$}on{$<$}/em{$>$} and its kin in Cinque (1988), Chierchia (1995b), Egerland (2003b), Kayne (2010), and explores them in the Principles-and-Parameters approach to syntax and the "situated descriptions" (Elbourne 2013) extension of the syntax-semantics mapping of Heim and Kratzer (1998).},
  copyright = {Creative Commons Attribution 4.0 International, Open Access}
}

@book{riehl.e:2017,
  title = {Category Theory in Context},
  author = {Riehl, Emily},
  year = {2017},
  publisher = {Courier Dover Publications},
  date-added = {2019-08-24 09:26:31 -0400},
  date-modified = {2019-08-24 09:26:50 -0400},
  keywords = {category theory}
}

@article{rigby.r:2005GAMLSS,
  title = {Generalized Additive Models for Location, Scale and Shape},
  author = {Rigby, R. A. and Stasinopoulos, D. M.},
  year = {2005},
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {54},
  number = {3},
  pages = {507--554},
  issn = {1467-9876},
  doi = {10.1111/j.1467-9876.2005.00510.x},
  urldate = {2023-03-01},
  abstract = {Summary. A general class of statistical models for a univariate response variable is presented which we call the generalized additive model for location, scale and shape (GAMLSS). The model assumes independent observations of the response variable y given the parameters, the explanatory variables and the values of the random effects. The distribution for the response variable in the GAMLSS can be selected from a very general family of distributions including highly skew or kurtotic continuous and discrete distributions. The systematic part of the model is expanded to allow modelling not only of the mean (or location) but also of the other parameters of the distribution of y, as parametric and/or additive nonparametric (smooth) functions of explanatory variables and/or random-effects terms. Maximum (penalized) likelihood estimation is used to fit the (non)parametric models. A Newton--Raphson or Fisher scoring algorithm is used to maximize the (penalized) likelihood. The additive terms in the model are fitted by using a backfitting algorithm. Censored data are easily incorporated into the framework. Five data sets from different fields of application are analysed to emphasize the generality of the GAMLSS class of models.},
  langid = {english},
  keywords = {Beta-binomial distribution,Box-Cox transformation,Centile estimation,Cubic smoothing splines,GAMLSS,Generalized linear mixed model,LMS method,Negative binomial distribution,Non-normality,Nonparametric models,Overdispersion,Penalized likelihood,Random effects,Skewness and kurtosis},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rigby.r2005GAMLSS Generalized additive models for location.pdf}
}

@article{ritchie.d:1986,
  title = {Shannon and {{Weaver}}: Unravelling the Paradox of Information},
  author = {Ritchie, David},
  year = {1986},
  month = apr,
  journal = {Communication Research},
  volume = {13},
  number = {2},
  pages = {278--298},
  publisher = {SAGE Publications},
  doi = {10.1177/009365086013002007},
  abstract = {A case is presented for separating Shannon's (1949) paper on information theory from Weaver's introduction, which is shown to contain distortions, as well as proofs by coincidence and homonym. Shannon's mathematical tools and methods are distinguished from his theory, which consists of 23 theorems setting forth the conditions for maximum efficiency in electromechanical signal transmission. Attempts to apply Shannon's theory to our field are reviewed, along with previous critiques, and it is recommended that future uses of Shannon's theory adopt a more methodologically rigorous approach. In particular, it is argued that Shannon's assumptions must be shown to hold before his theorems can be successfully applied.},
  bdsk-url-2 = {https://doi.org/10.1177/009365086013002007},
  date-added = {2022-04-07 12:59:21 -0400},
  date-modified = {2022-04-07 13:01:58 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{roark.b:2001,
  title = {Probabilistic Top-down Parsing and Language Modeling},
  author = {Roark, Brian},
  year = {2001},
  journal = {Computational Linguistics},
  volume = {27},
  number = {2},
  pages = {249--276},
  doi = {10.1162/089120101750300526},
  bdsk-url-2 = {https://doi.org/10.1162/089120101750300526},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/roark.b2001 Probabilistic top-down parsing and langu.pdf}
}

@inproceedings{roark.b:2009,
  title = {Deriving Lexical and Syntactic Expectation-Based Measures for Psycholinguistic Modeling via Incremental Top-down Parsing},
  booktitle = {Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing},
  author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
  year = {2009},
  pages = {324--333},
  publisher = {Association for Computational Linguistics},
  address = {Singapore}
}

@techreport{roark.b:2011techreport,
  type = {Technical Report},
  title = {Expected Surprisal and Entropy},
  author = {Roark, Brian},
  year = {2011},
  number = {CSLU-11-004},
  institution = {{Center for Spoken Language Processing, Oregon Health and Science University}},
  date-added = {2021-06-16 09:42:11 -0400},
  date-modified = {2021-06-16 09:48:54 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/roark.b2011techreport Expected surprisal and entropy.pdf}
}

@misc{robert.c:2010,
  type = {Blog},
  title = {Effective Sample Size},
  author = {Robert, Christian P.},
  year = {2010},
  month = sep,
  journal = {Xi'an's Og},
  urldate = {2022-12-10},
  langid = {english},
  keywords = {effective sample size,importance sampling}
}

@misc{rogers.a:2024arxiv,
  title = {Position: {{Key Claims}} in {{LLM Research Have}} a {{Long Tail}} of {{Footnotes}}},
  shorttitle = {Position},
  author = {Rogers, Anna and Luccioni, Alexandra Sasha},
  year = {2024},
  month = jun,
  number = {arXiv:2308.07120},
  eprint = {2308.07120},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.07120},
  urldate = {2024-06-04},
  abstract = {Much of the recent discourse within the ML community has been centered around Large Language Models (LLMs), their functionality and potential -- yet not only do we not have a working definition of LLMs, but much of this discourse relies on claims and assumptions that are worth re-examining. We contribute a definition of LLMs, critically examine five common claims regarding their properties (including 'emergent properties'), and conclude with suggestions for future research directions and their framing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rogers.a2024arxiv Position Key Claims in LLM Research Hav.pdf}
}

@article{rogers.j:2003,
  title = {Syntactic Structures as Multi-Dimensional Trees},
  author = {Rogers, James},
  year = {2003},
  journal = {Research on Language and Computation},
  volume = {1},
  number = {3-4},
  pages = {265--305},
  publisher = {Springer},
  date-added = {2019-06-15 11:50:06 -0400},
  date-modified = {2019-06-16 13:55:14 -0400},
  project = {syntactic embedding},
  keywords = {automata,control languages}
}

@incollection{rohatgi.v:2015,
  title = {Some Special Distributions},
  booktitle = {An {{Introduction}} to {{Probability}} and {{Statistics}}},
  author = {Rohatgi, Vijay K. and Saleh, A. K. Md. Ehsanes},
  year = {2015},
  pages = {173--244},
  publisher = {John Wiley \& Sons, Ltd},
  urldate = {2022-07-20},
  abstract = {This chapter focuses on some commonly occurring probability distributions and investigates their basic properties. The results of this chapter will be of considerable use in theoretical as well as practical applications. The chapter begins with some univariate and multivariate discrete distributions and proceeds with some continuous models. It then deals with bivariate and multivariate normal distributions and subsequently discusses the exponential family of distributions. Finally, the chapter records briefly some of the several other distributions which are related to these special distributions and their important characteristics.},
  chapter = {5},
  isbn = {978-1-118-79963-5},
  langid = {english},
  keywords = {bivariate normal distribution,continuous distributions,discrete distributions,exponential distribution,multivariate normal distribution},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/rohatgi.v2015 Some special distributions.pdf}
}

@article{ronai.e:2023,
  title = {Memory versus Expectation: Processing Relative Clauses in a Flexible Word Order Language},
  shorttitle = {Memory versus Expectation},
  author = {Ronai, Eszter and Xiang, Ming},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {1},
  pages = {e13227},
  issn = {1551-6709},
  doi = {10.1111/cogs.13227},
  urldate = {2023-03-09},
  abstract = {Memory limitations and probabilistic expectations are two key factors that have been posited to play a role in the incremental processing of natural language. Relative clauses (RCs) have long served as a key proving ground for such theories of language processing. Across three self-paced reading experiments, we test the online comprehension of Hungarian subject- and object-extracted RCs (SRCs and ORCs, respectively). We capitalize on the syntactic properties of Hungarian that allow for a variety of word orders within RCs, which helps us to delineate the processing costs associated with memory demand and violated expectations. Results showed a processing cost at the RC verb for structures that have longer verb-argument distances, despite those structures being more frequent in the corpus. These findings thus support theories that attribute processing difficulty to memory limitations, rather than theories that attribute difficulty to less expected structures.},
  langid = {english},
  keywords = {Language processing,Memory models,object-extracted relative clauses,ORC,Predictive processing,Relative clauses,Syntactic parsing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ronai.e2023 Memory versus expectation processing re.pdf}
}

@misc{rosa.r:2019short,
  title = {Inducing Syntactic Trees from {{BERT}} Representations},
  author = {Rosa, Rudolf and Mare{\v c}ek, David},
  year = {2019},
  eprint = {1906.11511},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-07-16 11:51:58 -0400}
}

@incollection{rosenbloom.p:1987,
  title = {Learning by Chunking: A Production System Model of Practice},
  booktitle = {Production {{System Models}} of {{Learning}} and {{Development}}},
  author = {Rosenbloom, Paul and Newell, Allen},
  editor = {Klahr, David and Langley, Patrick W. and Neches, Robert T.},
  year = {1987},
  month = jan,
  pages = {221--286},
  publisher = {The MIT Press},
  doi = {10.7551/mitpress/5605.003.0007},
  urldate = {2022-09-25},
  isbn = {978-0-262-31596-8}
}

@inproceedings{rosenkrantz.d:1970,
  title = {Deterministic Left Corner Parsing},
  booktitle = {11th Annual Symposium on Switching and Automata Theory (Swat 1970)},
  author = {Rosenkrantz, D. J. and Lewis, P. M.},
  year = {1970},
  month = oct,
  pages = {139--152},
  publisher = {IEEE},
  address = {Los Angeles, California},
  doi = {10.1109/SWAT.1970.5},
  date-added = {2022-03-11 22:35:17 -0500},
  date-modified = {2022-03-11 22:35:18 -0500}
}

@book{rosenthal.j:2006,
  title = {A First Look at Rigorous Probability Theory},
  author = {Rosenthal, Jeffrey S},
  year = {2006},
  month = nov,
  edition = {2},
  publisher = {World Scientific},
  doi = {10.1142/6300},
  bdsk-url-2 = {https://doi.org/10.1142/6300},
  bdsk-url-3 = {http://probability.ca/jeff/grprobbook.html},
  date-added = {2021-10-15 14:24:25 -0400},
  date-modified = {2021-10-15 14:25:40 -0400}
}

@article{rouder.j:2015,
  title = {The Lognormal Race: A Cognitive-Process Model of Choice and Latency with Desirable Psychometric Properties},
  shorttitle = {The Lognormal Race},
  author = {Rouder, Jeffrey N. and Province, Jordan M. and Morey, Richard D. and Gomez, Pablo and Heathcote, Andrew},
  year = {2015},
  month = jun,
  journal = {Psychometrika},
  volume = {80},
  number = {2},
  pages = {491--513},
  issn = {0033-3123, 1860-0980},
  doi = {10.1007/s11336-013-9396-3},
  urldate = {2022-08-13},
  langid = {english}
}

@book{royden.h:2010,
  title = {Real Analysis},
  author = {Royden, H. L. and Fitzpatrick, Patrick},
  year = {2010},
  edition = {4th ed},
  publisher = {Prentice Hall},
  address = {Boston},
  abstract = {Real Analysis, Fourth Edition, covers the basic material that every reader should know in the classical theory of functions of a real variable, measure and integration theory, and some of the more important and elementary topics in general topology and normed linear space theory. This text assumes a general background in mathematics and familiarity with the fundamental concepts of analysis. Classical theory of functions, including the classical Banach spaces; General topology and the theory of general Banach spaces; Abstract treatment of measure and integration. For all readers interested in real analysis},
  isbn = {978-0-13-143747-0},
  langid = {english}
}

@article{ryskin.r:2018,
  title = {Comprehenders Model the Nature of Noise in the Environment},
  author = {Ryskin, Rachel and Futrell, Richard and Kiran, Swathi and Gibson, Edward},
  year = {2018},
  month = dec,
  journal = {Cognition},
  volume = {181},
  pages = {141--150},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2018.08.018},
  urldate = {2022-11-28},
  abstract = {In everyday communication, speakers make errors and produce language in a noisy environment. Recent work suggests that comprehenders possess cognitive mechanisms for dealing with noise in the linguistic signal: a noisy-channel model. A key parameter of these models is the noise model: the comprehender's implicit model of how noise affects utterances before they are perceived. Here we examine this noise model in detail, asking whether comprehension behavior reflects a noise model that is adapted to context. We asked readers to correct sentences if they noticed errors, and manipulated context by including exposure sentences containing obvious deletions (A bystander was rescued by the fireman in the nick time.), insertions, exchanges, mixed errors, or no errors. On test sentences (The bat swung the player.), participants' corrections differed depending on the exposure condition. The results demonstrate that participants model specific types of errors and make inferences about the intentions of the speaker accordingly.},
  langid = {english},
  keywords = {Adaptation,Error correction,Noisy-channel,Rational inference,Sentence comprehension},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ryskin.r2018 Comprehenders model the nature of noise.pdf}
}

@article{ryskin.r:2021,
  title = {An {{ERP}} Index of Real-Time Error Correction within a Noisy-Channel Framework of Human Communication},
  author = {Ryskin, Rachel and Stearns, Laura and Bergen, Leon and Eddy, Marianna and Fedorenko, Evelina and Gibson, Edward},
  year = {2021},
  month = jul,
  journal = {Neuropsychologia},
  volume = {158},
  pages = {107855},
  issn = {0028-3932},
  doi = {10.1016/j.neuropsychologia.2021.107855},
  urldate = {2022-06-24},
  abstract = {Recent evidence suggests that language processing is well-adapted to noise in the input (e.g., spelling or speech errors, misreading or mishearing) and that comprehenders readily correct the input via rational inference over possible intended sentences given probable noise corruptions. In the current study, we probed the processing of noisy linguistic input, asking whether well-studied ERP components may serve as useful indices of this inferential process. In particular, we examined sentences where semantic violations could be attributed to noise---for example, in ``The storyteller could turn any incident into an amusing antidote'', where the implausible word ``antidote'' is orthographically and phonologically close to the intended ``anecdote''. We found that the processing of such sentences---where the probability that the message was corrupted by noise exceeds the probability that it was produced intentionally and perceived accurately---was associated with a reduced (less negative) N400 effect and an increased P600 effect, compared to semantic violations which are unlikely to be attributed to noise (``The storyteller could turn any incident into an amusing hearse''). Further, the magnitudes of these ERP effects were correlated with the probability that the comprehender retrieved a plausible alternative. This work thus adds to the growing body of literature that suggests that many aspects of language processing are optimized for dealing with noise in the input, and opens the door to electrophysiologic investigations of the computations that support the processing of imperfect input.},
  langid = {english},
  keywords = {electroencephalography,event-related potential,N400,noisy-channel,P600},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ryskin.r2021 An ERP index of real-time error correcti.pdf}
}

@book{sag.i:2003,
  title = {Syntactic Theory: {{A}} Formal Introduction},
  author = {Sag, Ivan A. and Wasow, Thomas and Bender, Emily M.},
  year = {2003},
  edition = {2},
  publisher = {CSLI},
  address = {Stanford, CA},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@incollection{sag.i:2012,
  title = {Sign-Based Construction Grammar: {{An}} Informal Synopsis},
  booktitle = {Sign--{{Based}} Construction Grammar},
  author = {Sag, Ivan A.},
  editor = {{Boas, Hans abd Sag}, Ivan A.},
  year = {2012},
  pages = {101--107},
  publisher = {CSLI Publications},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2022-04-04 12:17:05 -0400}
}

@article{sainburg.t:2019,
  title = {Parallels in the Sequential Organization of Birdsong and Human Speech},
  author = {Sainburg, Tim and Theilman, Brad and Thielk, Marvin and Gentner, Timothy Q},
  year = {2019},
  journal = {Nature communications},
  volume = {10},
  publisher = {Nature Publishing Group},
  date-added = {2019-10-01 16:50:44 -0400},
  date-modified = {2019-10-01 16:51:36 -0400},
  keywords = {birdsong,mutual information,structure,syntax}
}

@inproceedings{salimans.t:2015MCVI,
  title = {Markov Chain {{Monte Carlo}} and Variational Inference: Bridging the Gap},
  booktitle = {Proceedings of the 32nd International Conference on Machine Learning},
  author = {Salimans, Tim and Kingma, Diederik and Welling, Max},
  editor = {Bach, Francis and Blei, David},
  year = {2015-07-07/2015-07-09},
  series = {Proceedings of Machine Learning Research},
  volume = {37},
  pages = {1218--1226},
  publisher = {PMLR},
  address = {Lille, France},
  abstract = {Recent advances in stochastic gradient variational inference have made it possible to perform variational Bayesian inference with posterior approximations containing auxiliary random variables. This enables us to explore a new synthesis of variational inference and Monte Carlo methods where we incorporate one or more steps of MCMC into our variational approximation. By doing so we obtain a rich class of inference algorithms bridging the gap between variational methods and MCMC, and offering the best of both worlds: fast posterior approximation through the maximization of an explicit objective, with the option of trading off additional computation for additional accuracy. We describe the theoretical foundations that make this possible and show some promising first results.},
  date-added = {2022-05-05 11:01:25 -0400},
  date-modified = {2022-05-05 11:02:29 -0400},
  pdf = {http://proceedings.mlr.press/v37/salimans15.pdf},
  keywords = {markov chain monte carlo,markov chain variational inference,variational inference}
}

@misc{salle.a:2019,
  title = {Why so down? {{The}} Role of Negative (and Positive) Pointwise Mutual Information in Distributional Semantics},
  author = {Salle, Alexandre and Villavicencio, Aline},
  year = {2019},
  eprint = {1908.06941},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2020-07-13 08:39:45 -0400},
  date-modified = {2020-07-13 08:40:45 -0400},
  project = {syntactic embedding},
  keywords = {mutual information,pmi}
}

@article{salthouse.t:2010,
  title = {Selective Review of Cognitive Aging},
  author = {Salthouse, Timothy A.},
  year = {2010},
  month = sep,
  journal = {Journal of the International Neuropsychological Society},
  volume = {16},
  number = {5},
  pages = {754--760},
  issn = {1469-7661, 1355-6177},
  doi = {10.1017/S1355617710000706},
  urldate = {2024-03-19},
  abstract = {Research concerned with relations between adult age and cognitive functioning is briefly reviewed. The coverage is necessarily selective, and is organized in terms of five major questions. These are what abilities are related to age, how many distinct influences are contributing to the relations between age and cognitive functioning, do the differences between people increase with advancing age, what is responsible for the discrepancies between cross-sectional and longitudinal age comparisons of cognitive functioning, and what methods can be used to identify causes of age-related influences on cognition. Although definitive answers are not yet possible, quite a bit of information relevant to the questions is now available. Moreover, the existing information has implications for the design, analysis, and interpretation of cognitive and neuropsychological research concerned with aging. (JINS, 2010, 16, 754--760.)},
  langid = {english},
  keywords = {cognitive aging,Cognitive aging,Cross-sectional,Longitudinal,Mediators,Moderators,Variability},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/salthouse.t2010 Selective review of cognitive aging.pdf}
}

@article{samson.e:1953,
  title = {Fundamental Natural Concepts of Information Theory},
  author = {Samson, Edward W.},
  year = {1953},
  journal = {ETC: A Review of General Semantics},
  volume = {10},
  number = {4, Summer 1953, special issue on information theory},
  eprint = {42581366},
  eprinttype = {jstor},
  pages = {283--297},
  publisher = {Institute of General Semantics},
  issn = {0014164X, 21689245},
  urldate = {2024-03-25},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/samson.e1953 Fundamental natural concepts of informat.pdf}
}

@inproceedings{sanborn.a:2006cogsci,
  title = {A More Rational Model of Categorization},
  booktitle = {Proceedings of the 28th {{Annual Conference}} of the {{Cognitive Science Society}}},
  author = {Sanborn, Adam and Griffiths, Thomas L. and Navarro, Daniel J.},
  year = {2006},
  publisher = {LAWRENCEE},
  address = {Vancouver, British Columbia, Canada},
  urldate = {2022-10-20},
  abstract = {Adam N. Sanborn, Thomas L. Griffiths, Daniel J. Navarro},
  langid = {english},
  keywords = {particle filtering},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sanborn.a2006cogsci A more rational model of categorization.pdf}
}

@article{sanford.a:2002,
  title = {Depth of Processing in Language Comprehension: Not Noticing the Evidence},
  shorttitle = {Depth of Processing in Language Comprehension},
  author = {Sanford, Anthony J. and Sturt, Patrick},
  year = {2002},
  month = sep,
  journal = {Trends in Cognitive Sciences},
  volume = {6},
  number = {9},
  pages = {382--386},
  issn = {1364-6613},
  doi = {10.1016/S1364-6613(02)01958-7},
  urldate = {2022-06-14},
  abstract = {The study of processes underlying the interpretation of language often produces evidence that they are complete and occur incrementally. However, computational linguistics has shown that interpretations are often effective even if they are underspecified. We present evidence that similar underspecified representations are used by humans during comprehension, drawing on a scattered and varied literature. We also show how linguistic properties of focus, subordination and focalization can control depth of processing, leading to underspecified representations. Modulation of degrees of specification might provide a way forward in the development of models of the processing underlying language understanding.},
  langid = {english},
  keywords = {Language interpretation,Representation,Semantic anomalies,Text-change-blindness,Underspecification}
}

@misc{sanh.v:2019distilbert,
  title = {{{DistilBERT}}, a Distilled Version of {{BERT}}: Smaller, Faster, Cheaper and Lighter},
  author = {Sanh, Victor and Debut, Lysandre and Chaumond, Julien and Wolf, Thomas},
  year = {2019},
  eprint = {1910.01108},
  primaryclass = {cs.CL},
  archiveprefix = {arXiv},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  project = {syntactic embedding}
}

@article{sanz-alonso.d:2018,
  title = {Importance Sampling and Necessary Sample Size: An Information Theory Approach},
  shorttitle = {Importance Sampling and Necessary Sample Size},
  author = {{Sanz-Alonso}, Daniel},
  year = {2018},
  month = jan,
  journal = {SIAM/ASA Journal on Uncertainty Quantification},
  volume = {6},
  number = {2},
  pages = {867--879},
  issn = {2166-2525},
  doi = {10.1137/16M1093549},
  urldate = {2022-12-21},
  langid = {english},
  keywords = {importance sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sanz-alonso.d2018 Importance sampling and necessary sample.pdf}
}

@inproceedings{saphra.n:2018,
  title = {Understanding Learning Dynamics of Language Models with {{SVCCA}}},
  booktitle = {Proceedings of the 2019 Conference of the North {{American}} Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies, Volume 1 (Long and Short Papers)},
  author = {Saphra, Naomi and Lopez, Adam},
  year = {2019},
  pages = {3257--3267},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1329},
  bdsk-url-2 = {https://doi.org/10.18653/v1/N19-1329}
}

@article{sartran.l:2022,
  title = {Transformer {{Grammars}}: {{Augmenting Transformer Language Models}} with {{Syntactic Inductive Biases}} at {{Scale}}},
  shorttitle = {Transformer {{Grammars}}},
  author = {Sartran, Laurent and Barrett, Samuel and Kuncoro, Adhiguna and Stanojevi{\'c}, Milo{\v s} and Blunsom, Phil and Dyer, Chris},
  year = {2022},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {10},
  pages = {1423--1439},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00526},
  urldate = {2023-08-18},
  abstract = {We introduce Transformer Grammars (TGs), a novel class of Transformer language models that combine (i) the expressive power, scalability, and strong performance of Transformers and (ii) recursive syntactic compositions, which here are implemented through a special attention mask and deterministic transformation of the linearized tree. We find that TGs outperform various strong baselines on sentence-level language modeling perplexity, as well as on multiple syntax-sensitive language modeling evaluation metrics. Additionally, we find that the recursive syntactic composition bottleneck which represents each sentence as a single vector harms perplexity on document-level language modeling, providing evidence that a different kind of memory mechanism---one that is independent of composed syntactic representations---plays an important role in current successful models of long text.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sartran.l2022 Transformer Grammars Augmenting Transfo.pdf}
}

@article{sason.i:2016,
  title = {F -{{Divergence Inequalities}}},
  author = {Sason, Igal and Verd{\'u}, Sergio},
  year = {2016},
  month = nov,
  journal = {IEEE Transactions on Information Theory},
  volume = {62},
  number = {11},
  pages = {5973--6006},
  issn = {1557-9654},
  doi = {10.1109/TIT.2016.2603151},
  urldate = {2024-07-29},
  abstract = {This paper develops systematic approaches to obtain f -divergence inequalities, dealing with pairs of probability measures defined on arbitrary alphabets. Functional domination is one such approach, where special emphasis is placed on finding the best possible constant upper bounding a ratio of f -divergences. Another approach used for the derivation of bounds among f -divergences relies on moment inequalities and the logarithmic-convexity property, which results in tight bounds on the relative entropy and Bhattacharyya distance in terms of {$\chi$}2 divergences. A rich variety of bounds are shown to hold under boundedness assumptions on the relative information. Special attention is devoted to the total variation distance and its relation to the relative information and relative entropy, including ``reverse Pinsker inequalities,'' as well as on the E{$\gamma$} divergence, which generalizes the total variation distance. Pinsker's inequality is extended for this type of f -divergence, a result which leads to an inequality linking the relative entropy and relative information spectrum. Integral expressions of the R{\'e}nyi divergence in terms of the relative information spectrum are derived, leading to bounds on the R{\'e}nyi divergence in terms of either the variational distance or relative entropy.},
  keywords = {Conferences,Entropy,f-divergence,Information theory,Integral equations,Joining processes,Pinsker's inequality,Q measurement,Relative entropy,relative information,Renyi divergence,Systematics,total variation distance},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sason.i2016 f -Divergence Inequalities.pdf}
}

@book{savage.l:1972,
  title = {The Foundations of Statistics},
  author = {Savage, Leonard J.},
  year = {1972},
  edition = {2d rev. ed.},
  publisher = {Dover Publications},
  address = {New York},
  isbn = {978-0-486-62349-8},
  langid = {english},
  keywords = {Statistics}
}

@article{savin.h:1963,
  title = {Word-frequency Effect and Errors in the Perception of Speech},
  author = {Savin, Harris B.},
  year = {1963},
  journal = {The Journal of the Acoustical Society of America},
  volume = {35},
  number = {2},
  pages = {200--206},
  publisher = {Acoustical Society of America},
  isbn = {0001-4966}
}

@inproceedings{savinov.n:2022,
  title = {Step-Unrolled Denoising Autoencoders for Text Generation},
  booktitle = {International Conference on Learning Representations},
  author = {Savinov, Nikolay and Chung, Junyoung and Binkowski, Mikolaj and Elsen, Erich and {van den Oord}, Aaron},
  year = {2022},
  date-added = {2022-05-04 10:51:51 -0400},
  date-modified = {2022-05-04 10:52:30 -0400},
  keywords = {autoencoders,denoising,diffusion processes,language modeling,SUNDAE}
}

@incollection{scha.r:1990,
  title = {Taaltheorie En Taaltechnologie; Competence En Performance.},
  booktitle = {Computertoepassingen in de Neerlandistiek},
  author = {Scha, Remko},
  editor = {{de Kort}, R. and Leerdam, G.L.J.},
  year = {1990},
  pages = {7--22},
  publisher = {Landelijke Vereniging van Neerlandici},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500},
  pass = {1},
  readinglist = {Thesis}
}

@phdthesis{schabes.y:1990phd,
  title = {Mathematical and Computational Aspects of Lexicalized Grammars},
  author = {Schabes, Yves},
  year = {1990},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2022-04-26 21:21:04 -0400},
  project = {syntactic embedding},
  school = {University of Pennsylvania}
}

@misc{schick-poland.k:2021,
  title = {A Partial Information Decomposition for Discrete and Continuous Variables},
  author = {{Schick-Poland}, Kyle and Makkeh, Abdullah and Gutknecht, Aaron J. and Wollstadt, Patricia and Sturm, Anja and Wibral, Michael},
  year = {2021},
  eprint = {2106.12393},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-30 17:30:27 -0400},
  date-modified = {2021-09-30 17:30:29 -0400}
}

@article{schijndel.m:2013,
  title = {A Model of Language Processing as Hierarchic Sequential Prediction},
  author = {{van Schijndel}, Marten and Exley, Andy and Schuler, William},
  year = {2013},
  month = jun,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {522--540},
  publisher = {Wiley},
  doi = {10.1111/tops.12034},
  bdsk-url-2 = {https://doi.org/10.1111/tops.12034},
  date-added = {2021-09-13 21:29:14 -0400},
  date-modified = {2021-09-13 21:29:17 -0400}
}

@inproceedings{schijndel.m:2013NAACL,
  title = {An Analysis of Frequency- and Memory-Based Processing Costs},
  booktitle = {Proceedings of the 2013 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2013},
  month = jun,
  pages = {95--105},
  publisher = {Association for Computational Linguistics},
  address = {Atlanta, Georgia},
  urldate = {2023-03-01},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schijndel.m2013NAACL An analysis of frequency- and memory-bas.pdf}
}

@inproceedings{schijndel.m:2015,
  title = {Hierarchic Syntax Improves Reading Time Prediction},
  booktitle = {Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: {{Human}} Language Technologies},
  author = {{van Schijndel}, Marten and Schuler, William},
  year = {2015},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/v1/n15-1183},
  bdsk-url-2 = {https://doi.org/10.3115/v1/n15-1183},
  date-added = {2021-09-13 21:36:26 -0400},
  date-modified = {2021-09-13 21:36:30 -0400}
}

@inproceedings{schijndel.m:2017,
  title = {Approximations of Predictive Entropy Correlate with Reading Times},
  booktitle = {Proceedings of the 37th Annual Meeting of the {{Cognitive Science Society}}},
  author = {{van Schijndel}, Marten and Schuler, William},
  editor = {Gunzelmann, Glenn and Andrew Howes, Thora Tenbrink and Davelaar, Eddy},
  year = {2017},
  pages = {1266--1271},
  address = {London, United Kingdom},
  date-added = {2022-04-21 09:42:08 -0400},
  date-modified = {2022-04-21 09:44:53 -0400},
  organization = {Cognitive Science Society},
  keywords = {entropy reduction,predictability,predictive entropy}
}

@inproceedings{schijndel.m:2018,
  title = {Modeling Garden Path Effects without Explicit Hierarchical Syntax.},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  editor = {{Rogers} and {Rau} and {Zhu} and {Kalish}},
  year = {2018},
  address = {Madison, Wisconsin},
  date-added = {2021-03-18 10:48:52 -0400},
  date-modified = {2021-03-18 11:56:18 -0400},
  isbn = {978-0-9911967-8-4}
}

@inproceedings{schijndel.m:2018a,
  title = {A Neural Model of Adaptation in Reading},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2018},
  pages = {4704--4710},
  publisher = {Association for Computational Linguistics},
  address = {Brussels, Belgium},
  doi = {10.18653/v1/D18-1499},
  bdsk-url-2 = {https://doi.org/10.18653/v1/D18-1499}
}

@inproceedings{schijndel.m:2019,
  title = {Can Entropy Explain Successor Surprisal Effects in Reading?},
  booktitle = {Proceedings of the Society for Computation in Linguistics ({{SCiL}}), {{NYC}}, January 3--6, 2019},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2019},
  volume = {2},
  pages = {1--7},
  publisher = {University of Massachusetts Amherst},
  doi = {10.7275/QTBB-9D05},
  bdsk-url-2 = {https://doi.org/10.7275/QTBB-9D05},
  date-added = {2022-04-21 09:21:31 -0400},
  date-modified = {2022-04-21 09:39:00 -0400}
}

@misc{schijndel.m:2020psyarxiv,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2020},
  doi = {10.31234/osf.io/sgbqy},
  bdsk-url-1 = {psyarxiv.com/sgbqy},
  date-added = {2021-03-10 11:20:44 -0500},
  date-modified = {2021-03-18 17:20:22 -0400},
  howpublished = {PsyArXiv},
  keywords = {processing}
}

@article{schijndel.m:2021,
  title = {Single-Stage Prediction Models Do Not Explain the Magnitude of Syntactic Disambiguation Difficulty},
  author = {{van Schijndel}, Marten and Linzen, Tal},
  year = {2021},
  journal = {Cognitive Science},
  volume = {45},
  number = {6},
  pages = {e12988},
  issn = {1551-6709},
  doi = {10.1111/cogs.12988},
  urldate = {2022-10-11},
  abstract = {The disambiguation of a syntactically ambiguous sentence in favor of a less preferred parse can lead to slower reading at the disambiguation point. This phenomenon, referred to as a garden-path effect, has motivated models in which readers initially maintain only a subset of the possible parses of the sentence, and subsequently require time-consuming reanalysis to reconstruct a discarded parse. A more recent proposal argues that the garden-path effect can be reduced to surprisal arising in a fully parallel parser: words consistent with the initially dispreferred but ultimately correct parse are simply less predictable than those consistent with the incorrect parse. Since predictability has pervasive effects in reading far beyond garden-path sentences, this account, which dispenses with reanalysis mechanisms, is more parsimonious. Crucially, it predicts a linear effect of surprisal: the garden-path effect is expected to be proportional to the difference in word surprisal between the ultimately correct and ultimately incorrect interpretations. To test this prediction, we used recurrent neural network language models to estimate word-by-word surprisal for three temporarily ambiguous constructions. We then estimated the slowdown attributed to each bit of surprisal from human self-paced reading times, and used that quantity to predict syntactic disambiguation difficulty. Surprisal successfully predicted the existence of garden-path effects, but drastically underpredicted their magnitude, and failed to predict their relative severity across constructions. We conclude that a full explanation of syntactic disambiguation difficulty may require recovery mechanisms beyond predictability.},
  langid = {english},
  keywords = {Garden paths,Information theory,Neural networks,Self-paced reading,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schijndel.m2021 Single-stage prediction models do not ex.pdf}
}

@article{schooler.l:1997,
  title = {The Role of Process in the Rational Analysis of Memory},
  author = {Schooler, Lael J. and Anderson, John R.},
  year = {1997},
  month = apr,
  journal = {Cognitive Psychology},
  volume = {32},
  number = {3},
  pages = {219--250},
  issn = {00100285},
  doi = {10.1006/cogp.1997.0652},
  urldate = {2022-09-27},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schooler.l1997 The role of process in the rational anal.pdf}
}

@article{schotter.e:2014,
  title = {Task Effects Reveal Cognitive Flexibility Responding to Frequency and Predictability: {{Evidence}} from Eye Movements in Reading and Proofreading},
  shorttitle = {Task Effects Reveal Cognitive Flexibility Responding to Frequency and Predictability},
  author = {Schotter, Elizabeth R. and Bicknell, Klinton and Howard, Ian and Levy, Roger and Rayner, Keith},
  year = {2014},
  month = apr,
  journal = {Cognition},
  volume = {131},
  number = {1},
  pages = {1--27},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.11.018},
  urldate = {2023-12-01},
  abstract = {It is well-known that word frequency and predictability affect processing time. These effects change magnitude across tasks, but studies testing this use tasks with different response types (e.g., lexical decision, naming, and fixation time during reading; Schilling, Rayner, \& Chumbley, 1998), preventing direct comparison. Recently, Kaakinen and Hy{\"o}n{\"a} (2010) overcame this problem, comparing fixation times in reading for comprehension and proofreading, showing that the frequency effect was larger in proofreading than in reading. This result could be explained by readers exhibiting substantial cognitive flexibility, and qualitatively changing how they process words in the proofreading task in a way that magnifies effects of word frequency. Alternatively, readers may not change word processing so dramatically, and instead may perform more careful identification generally, increasing the magnitude of many word processing effects (e.g., both frequency and predictability). We tested these possibilities with two experiments: subjects read for comprehension and then proofread for spelling errors (letter transpositions) that produce nonwords (e.g., trcak for track as in Kaakinen \& Hy{\"o}n{\"a}) or that produce real but unintended words (e.g., trial for trail) to compare how the task changes these effects. Replicating Kaakinen and Hy{\"o}n{\"a}, frequency effects increased during proofreading. However, predictability effects only increased when integration with the sentence context was necessary to detect errors (i.e., when spelling errors produced words that were inappropriate in the sentence; trial for trail). The results suggest that readers adopt sophisticated word processing strategies to accommodate task demands.},
  keywords = {Eye movements,Frequency,Predictability,Reading,Task effects,typos},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schotter.e2014 Task effects reveal cognitive flexibilit.pdf}
}

@article{schrimpf.m:2021,
  title = {The Neural Architecture of Language: {{Integrative}} Modeling Converges on Predictive Processing},
  shorttitle = {The Neural Architecture of Language},
  author = {Schrimpf, Martin and Blank, Idan Asher and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2021},
  month = nov,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {118},
  number = {45},
  pages = {e2105646118},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2105646118},
  urldate = {2023-05-02},
  abstract = {The neuroscience of perception has recently been revolutionized with an integrative modeling approach in which computation, brain function, and behavior are linked across many datasets and many computational models. By revealing trends across models, this approach yields novel insights into cognitive and neural mechanisms in the target domain. We here present a systematic study taking this approach to higher-level cognition: human language processing, our species' signature cognitive skill. We find that the most powerful ``transformer'' models predict nearly 100\% of explainable variance in neural responses to sentences and generalize across different datasets and imaging modalities (functional MRI and electrocorticography). Models' neural fits (``brain score'') and fits to behavioral responses are both strongly correlated with model accuracy on the next-word prediction task (but not other language tasks). Model architecture appears to substantially contribute to neural fit. These results provide computationally explicit evidence that predictive processing fundamentally shapes the language comprehension mechanisms in the human brain.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schrimpf.m2021 The neural architecture of language Int.pdf}
}

@book{schuchardt.h:1972,
  title = {Schuchardt, the {{Neogrammarians}}, and the Transformational Theory of Phonological Change: Four Essays},
  shorttitle = {Schuchardt, the Neogrammarians, and the Transformational Theory of Phonological Change},
  author = {Schuchardt, Hugo Ernst Mario},
  translator = {Vennemann, Theo and Wilbur, Terence H.},
  year = {1972},
  series = {Linguistische {{Forschungen}}, {{Bd}}. 26},
  publisher = {Athen{\"a}um Verlag},
  address = {Frankfurt/M.},
  isbn = {978-3-7610-4826-9},
  lccn = {P123 .S36 1972},
  keywords = {Linguistic change}
}

@inproceedings{schuler.w:2008,
  title = {Toward a Psycholinguistically-Motivated Model of Language Processing},
  booktitle = {Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008)},
  author = {Schuler, William and AbdelRahman, Samir and Miller, Tim and Schwartz, Lane},
  year = {2008},
  month = aug,
  pages = {785--792},
  publisher = {Coling 2008 Organizing Committee},
  address = {Manchester, UK},
  date-added = {2022-05-02 11:58:18 -0400},
  date-modified = {2022-05-02 11:58:33 -0400},
  keywords = {incrementality,parsing,parsing algorithm}
}

@article{schutze.c:2003,
  title = {Syncretism and Double Agreement with {{Icelandic}} Nominative Objects},
  author = {Sch{\"u}tze, Carson T},
  year = {2003},
  bdsk-url-1 = {escholarship.org/uc/item/8vn9b04q},
  date-added = {2020-03-05 10:17:01 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {syncretism}
}

@article{schwartz.m:2008,
  title = {The Importance of Stupidity in Scientific Research},
  author = {Schwartz, Martin A.},
  year = {2008},
  month = jun,
  journal = {Journal of Cell Science},
  volume = {121},
  number = {11},
  pages = {1771},
  issn = {0021-9533},
  doi = {10.1242/jcs.033340},
  urldate = {2022-06-16},
  keywords = {scientific method,stupidity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/schwartz.m2008 The importance of stupidity in scientifi.pdf}
}

@misc{schwartz.r:2019,
  title = {Green {{AI}}},
  author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
  year = {2019},
  eprint = {1907.10597},
  primaryclass = {cs.CY},
  archiveprefix = {arXiv},
  date-added = {2019-09-30 11:48:17 -0400},
  date-modified = {2019-09-30 11:50:22 -0400},
  keywords = {energy,environmental impact}
}

@article{searle.s:1980,
  title = {Population Marginal Means in the Linear Model: An Alternative to Least Squares Means},
  shorttitle = {Population Marginal Means in the Linear Model},
  author = {Searle, S. R. and Speed, F. M. and Milliken, G. A.},
  year = {1980},
  month = nov,
  journal = {The American Statistician},
  volume = {34},
  number = {4},
  pages = {216--221},
  publisher = {Taylor \& Francis},
  issn = {0003-1305},
  doi = {10.1080/00031305.1980.10483031},
  urldate = {2024-05-28},
  abstract = {The parameter concept in the term least squares mean is defined and given the more meaningful name population marginal mean; and its estimation is discussed.},
  keywords = {Covariance,Empty cells,Estimable function,Estimated marginal mean,Least squares mean,Population marginal mean,Unequal subclass numbers},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/searle.s1980 Population marginal means in the linear.pdf}
}

@article{sebastiani.p:2000,
  title = {Maximum Entropy Sampling and Optimal {{Bayesian}} Experimental Design},
  author = {Sebastiani, Paola and Wynn, Henry P.},
  year = {2000},
  journal = {Journal of the Royal Statistical Society. Series B (Statistical Methodology)},
  volume = {62},
  number = {1},
  eprint = {2680683},
  eprinttype = {jstor},
  pages = {145--157},
  publisher = {[Royal Statistical Society, Wiley]},
  issn = {13697412, 14679868},
  abstract = {When Shannon entropy is used as a criterion in the optimal design of experiments, advantage can be taken of the classical identity representing the joint entropy of parameters and observations as the sum of the marginal entropy of the observations and the preposterior conditional entropy of the parameters. Following previous work in which this idea was used in spatial sampling, the method is applied to standard parameterized Bayesian optimal experimental design. Under suitable conditions, which include non-linear as well as linear regression models, it is shown in a few steps that maximizing the marginal entropy of the sample is equivalent to minimizing the pre-posterior entropy, the usual Bayesian criterion, thus avoiding the use of conditional distributions. It is shown using this marginal formulation that under normality assumptions every standard model which has a two-point prior distribution on the parameters gives an optimal design supported on a single point. Other results include a new asymptotic formula which applies as the error variance is large and bounds on support size.},
  date-added = {2021-09-15 10:25:32 -0400},
  date-modified = {2021-09-15 10:25:34 -0400}
}

@article{seidenberg.m:1999,
  title = {Do Infants Learn Grammar with Algebra or Statistics?},
  author = {Seidenberg, Mark S. and Elman, Jeff L.},
  year = {1999},
  month = apr,
  journal = {Science},
  volume = {284},
  number = {5413},
  pages = {433--433},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.284.5413.433f},
  urldate = {2024-05-26},
  langid = {english}
}

@inproceedings{sennrich.r:2016,
  title = {Neural Machine Translation of Rare Words with Subword Units},
  booktitle = {Proceedings of the 54th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
  year = {2016},
  month = aug,
  pages = {1715--1725},
  publisher = {Association for Computational Linguistics},
  address = {Berlin, Germany},
  doi = {10.18653/v1/P16-1162},
  urldate = {2023-03-03},
  keywords = {BPE,byte-pair encoding,machine translation,neural machine translation,NMT,subword units},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sennrich.r2016 Neural machine translation of rare words.pdf}
}

@inproceedings{shain.c:2018,
  title = {Deep Syntactic Annotations for Broad-Coverage Psycholinguistic Modeling},
  booktitle = {Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({{LREC}} 2018)},
  author = {Shain, Cory and {van Schijndel}, Marten},
  editor = {Devereux, Barry and Shutova, Ekaterina and Huang, Chu-Ren},
  year = {2018},
  pages = {33--37},
  publisher = {European Language Resources Association (ELRA)},
  address = {Paris, France},
  isbn = {979-10-95546-08-5},
  langid = {english}
}

@inproceedings{shain.c:2018CDR,
  title = {Deconvolutional Time Series Regression: {{A}} Technique for Modeling Temporally Diffuse Effects},
  booktitle = {Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing},
  author = {Shain, Cory and Schuler, William},
  year = {2018},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/d18-1288},
  bdsk-url-2 = {https://doi.org/10.18653/v1/d18-1288},
  date-added = {2021-09-18 22:25:03 -0400},
  date-modified = {2021-09-18 22:25:04 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2018CDR Deconvolutional time series regression.pdf}
}

@article{shain.c:2021,
  title = {Continuous-Time Deconvolutional Regression for Psycholinguistic Modeling},
  author = {Shain, Cory and Schuler, William},
  year = {2021},
  month = oct,
  journal = {Cognition},
  volume = {215},
  pages = {104735},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2021.104735},
  urldate = {2022-10-26},
  abstract = {The influence of stimuli in psycholinguistic experiments diffuses across time because the human response to language is not instantaneous. The linear models typically used to analyze psycholinguistic data are unable to account for this phenomenon due to strong temporal independence assumptions, while existing deconvolutional methods for estimating diffuse temporal structure model time discretely and therefore cannot be directly applied to natural language stimuli where events (words) have variable duration. In light of evidence that continuous-time deconvolutional regression (CDR) can address these issues (Shain \& Schuler, 2018), this article motivates the use of CDR for many experimental settings, exposits some of its mathematical properties, and empirically evaluates the influence of various experimental confounds (noise, multicollinearity, and impulse response misspecification), hyperparameter settings, and response types (behavioral and fMRI). Results show that CDR (1) yields highly consistent estimates across a variety of hyperparameter configurations, (2) faithfully recovers the data-generating model on synthetic data, even under adverse training conditions, and (3) outperforms widely-used statistical approaches when applied to naturalistic reading and fMRI data. In addition, procedures for testing scientific hypotheses using CDR are defined and demonstrated, and empirically-motivated best-practices for CDR modeling are proposed. Results support the use of CDR for analyzing psycholinguistic time series, especially in a naturalistic experimental paradigm.},
  langid = {english},
  file = {/Users/j/Zotfiles/shain.c2021 Continuous-time deconvolutional regressi.pdf}
}

@inproceedings{shain.c:2021CDRNN,
  title = {{{CDRNN}}: Discovering Complex Dynamics in Human Language Processing},
  shorttitle = {Cdrnn},
  booktitle = {Proceedings of the 59th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} and the 11th {{International Joint Conference}} on {{Natural Language Processing}} ({{Volume}} 1: {{Long Papers}})},
  author = {Shain, Cory},
  year = {2021},
  month = aug,
  pages = {3718--3734},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2021.acl-long.288},
  urldate = {2022-10-26},
  abstract = {The human mind is a dynamical system, yet many analysis techniques used to study it are limited in their ability to capture the complex dynamics that may characterize mental processes. This study proposes the continuous-time deconvolutional regressive neural network (CDRNN), a deep neural extension of continuous-time deconvolutional regression (Shain \& Schuler, 2021) that jointly captures time-varying, non-linear, and delayed influences of predictors (e.g. word surprisal) on the response (e.g. reading time). Despite this flexibility, CDRNN is interpretable and able to illuminate patterns in human cognition that are otherwise difficult to study. Behavioral and fMRI experiments reveal detailed and plausible estimates of human language processing dynamics that generalize better than CDR and other baselines, supporting a potential role for CDRNN in studying human language processing.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2021CDRNN CDRNN discovering complex dynamics in h.pdf}
}

@misc{shain.c:2022CDRNNdetails,
  title = {A Deep Learning Approach to Analyzing Continuous-Time Systems},
  author = {Shain, Cory and Schuler, William},
  year = {2022},
  month = sep,
  number = {arXiv:2209.12128},
  eprint = {2209.12128},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2209.12128},
  urldate = {2022-10-26},
  abstract = {Scientists often use observational time series data to study complex natural processes, from climate change to civil conflict to brain activity. But regression analyses of these data often assume simplistic dynamics. Recent advances in deep learning have yielded startling improvements to the performance of models of complex processes, from speech comprehension to nuclear physics to competitive gaming. But deep learning is generally not used for scientific analysis. Here, we bridge this gap by showing that deep learning can be used, not just to imitate, but to analyze complex processes, providing flexible function approximation while preserving interpretability. Our approach -- the continuous-time deconvolutional regressive neural network (CDRNN) -- relaxes standard simplifying assumptions (e.g., linearity, stationarity, and homoscedasticity) that are implausible for many natural systems and may critically affect the interpretation of data. We evaluate CDRNNs on incremental human language processing, a domain with complex continuous dynamics. We demonstrate dramatic improvements to predictive likelihood in behavioral and neuroimaging data, and we show that CDRNNs enable flexible discovery of novel patterns in exploratory analyses, provide robust control of possible confounds in confirmatory analyses, and open up research questions that are otherwise hard to study using observational data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning,Statistics - Methodology},
  file = {/Users/j/Zotfiles/shain.c2022CDRNNdetails A deep learning approach to analyzing co.pdf}
}

@unpublished{shain.c:2022draft,
  type = {Draft},
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability in Reading},
  author = {Shain, Cory and Meister, Clara and Pimental, Tiago and Levy, Roger P. and Cotterell, Ryan},
  year = {2022},
  copyright = {Confidential},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2022draft Large-scale evidence for logarithmic eff.pdf}
}

@misc{shain.c:2022preprint,
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time},
  author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger Philip},
  year = {2022},
  month = nov,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/4hyna},
  urldate = {2023-01-26},
  abstract = {Words which are less predictable in context are harder to process, but theories of language processing diverge in how they explain this fact. Representational theories emphasize the demands of assembling sentence interpretations in memory; these theories predict a linear effect of predictability on processing cost. Inferential theories emphasize the demands of updating a probability distribution over possible sentence interpretations; these theories predict either a logarithmic or a superlogarithmic effect of predictability on processing cost, depending on whether they posit pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here we revisit this question at scale: we analyze six reading datasets, estimate next-word probabilities with diverse language models, and predict reading times with advanced nonlinear regression methods. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.},
  langid = {american},
  keywords = {Cognitive Psychology,Language,Naturalistic,Nonlinear regression,Prediction,Psycholinguistics,Sentence processing,Social and Behavioral Sciences,Surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2022preprint Large-scale evidence for logarithmic eff.pdf}
}

@article{shain.c:2023psyarxiv,
  title = {Word {{Frequency}} and {{Predictability Dissociate}} in {{Naturalistic Reading}}},
  author = {Shain, Cory},
  year = {2023},
  month = jul,
  publisher = {OSF},
  doi = {10.31234/osf.io/9zdfw},
  urldate = {2024-03-01},
  abstract = {Many studies of human language processing have shown that readers slow down at less frequent or less predictable words, but there is debate about whether frequency and predictability effects reflect separable cognitive phenomena: are cognitive operations that retrieve words from the mental lexicon based on sensory cues distinct from those that predict upcoming words based on context? Previous evidence for a frequency-predictability dissociation is mostly based on small samples (both for estimating predictability and frequency and for testing their effects on human behavior), artificial materials (e.g., isolated constructed sentences), and implausible modeling assumptions (discrete-time dynamics, linearity, additivity, constant variance, and invariance over time), which raises the question: do frequency and predictability dissociate in ordinary language comprehension, such as story reading? This study leverages recent progress in open data and computational modeling to address this question at scale. A large collection of naturalistic reading data (six datasets, \&gt;2.2M datapoints) is analyzed using nonlinear continuous-time regression, and frequency and predictability are estimated using statistical language models trained on more data than is currently typical in psycholinguistics. Despite the use of naturalistic data, strong predictability estimates, and flexible regression models, results converge with earlier experimental studies in supporting dissociable and additive frequency and predictability effects.},
  langid = {american},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2023psyarxiv Word Frequency and Predictability Dissoc.pdf}
}

@article{shain.c:2024OPMI,
  title = {Word {{Frequency}} and {{Predictability Dissociate}} in {{Naturalistic Reading}}},
  author = {Shain, Cory},
  year = {2024},
  month = mar,
  journal = {Open Mind},
  volume = {8},
  pages = {177--201},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00119},
  urldate = {2024-03-13},
  abstract = {Abstract             Many studies of human language processing have shown that readers slow down at less frequent or less predictable words, but there is debate about whether frequency and predictability effects reflect separable cognitive phenomena: are cognitive operations that retrieve words from the mental lexicon based on sensory cues distinct from those that predict upcoming words based on context? Previous evidence for a frequency-predictability dissociation is mostly based on small samples (both for estimating predictability and frequency and for testing their effects on human behavior), artificial materials (e.g., isolated constructed sentences), and implausible modeling assumptions (discrete-time dynamics, linearity, additivity, constant variance, and invariance over time), which raises the question: do frequency and predictability dissociate in ordinary language comprehension, such as story reading? This study leverages recent progress in open data and computational modeling to address this question at scale. A large collection of naturalistic reading data (six datasets, \&gt;2.2 M datapoints) is analyzed using nonlinear continuous-time regression, and frequency and predictability are estimated using statistical language models trained on more data than is currently typical in psycholinguistics. Despite the use of naturalistic data, strong predictability estimates, and flexible regression models, results converge with earlier experimental studies in supporting dissociable and additive frequency and predictability effects.},
  langid = {english}
}

@article{shain.c:2024PNAS,
  title = {Large-Scale Evidence for Logarithmic Effects of Word Predictability on Reading Time},
  author = {Shain, Cory and Meister, Clara and Pimentel, Tiago and Cotterell, Ryan and Levy, Roger},
  year = {2024},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {10},
  pages = {e2307876121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2307876121},
  urldate = {2024-03-01},
  abstract = {During real-time language comprehension, our minds rapidly decode complex meanings from sequences of words. The difficulty of doing so is known to be related to words' contextual predictability, but what cognitive processes do these predictability effects reflect? In one view, predictability effects reflect facilitation due to anticipatory processing of words that are predictable from context. This view predicts a linear effect of predictability on processing demand. In another view, predictability effects reflect the costs of probabilistic inference over sentence interpretations. This view predicts either a logarithmic or a superlogarithmic effect of predictability on processing demand, depending on whether it assumes pressures toward a uniform distribution of information over time. The empirical record is currently mixed. Here, we revisit this question at scale: We analyze six reading datasets, estimate next-word probabilities with diverse statistical language models, and model reading times using recent advances in nonlinear regression. Results support a logarithmic effect of word predictability on processing difficulty, which favors probabilistic inference as a key component of human language processing.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shain.c2024PNAS Large-scale evidence for logarithmic eff 2.pdf;/Users/j/Dropbox (MIT)/Zotfiles/shain.c2024PNAS Large-scale evidence for logarithmic eff.pdf}
}

@article{shannon.c:1948,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  month = jun,
  journal = {Bell System Technical Journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1002/j.1538-7305.1948.tb01338.x},
  bdsk-url-2 = {https://doi.org/10.1002/j.1538-7305.1948.tb01338.x},
  date-added = {2022-04-07 13:03:48 -0400},
  date-modified = {2022-05-04 18:57:35 -0400},
  keywords = {communication theory,information theory,mutual information}
}

@article{shannon.c:1948reprint,
  title = {A Mathematical Theory of Communication},
  author = {Shannon, Claude E.},
  year = {1948},
  journal = {The Bell system technical journal},
  volume = {27},
  number = {3},
  pages = {379--423, 623--656},
  publisher = {Nokia Bell Labs},
  project = {information-entropy},
  keywords = {asymptotic equipartition property,information theory}
}

@article{shannon.c:1951,
  title = {Prediction and Entropy of Printed {{English}}},
  author = {Shannon, Claude E.},
  year = {1951},
  journal = {Bell System Technical Journal},
  volume = {30},
  number = {1},
  pages = {50--64},
  issn = {1538-7305},
  doi = {10.1002/j.1538-7305.1951.tb01366.x},
  urldate = {2024-05-26},
  abstract = {A new method of estimating the entropy and redundancy of a language is described. This method exploits the knowledge of the language statistics possessed by those who speak the language, and depends on experimental results in prediction of the next letter when the preceding text is known. Results of experiments in prediction are given, and some properties of an ideal predictor are developed.},
  copyright = {{\copyright} 1951 The Bell System Technical Journal},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shannon.c1951 Prediction and entropy of printed Englis.pdf}
}

@misc{shazeer.n:2020arxiv,
  title = {{{GLU}} Variants Improve {{Transformer}}},
  author = {Shazeer, Noam},
  year = {2020},
  month = feb,
  number = {arXiv:2002.05202},
  eprint = {2002.05202},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-27},
  abstract = {Gated Linear Units (arXiv:1612.08083) consist of the component-wise product of two linear projections, one of which is first passed through a sigmoid function. Variations on GLU are possible, using different nonlinear (or even linear) functions in place of sigmoid. We test these variants in the feed-forward sublayers of the Transformer (arXiv:1706.03762) sequence-to-sequence model, and find that some of them yield quality improvements over the typically-used ReLU or GELU activations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shazeer.n2020 GLU variants improve Transformer.pdf}
}

@techreport{sheldon.d:2013,
  type = {Unpublished Report},
  title = {Discrete Adaptive Rejection Sampling},
  author = {Sheldon, Daniel R.},
  year = {2013},
  number = {UM-CS-2013-012},
  institution = {{College of Information and Computer Sciences, University of Massachusetts Amherst}},
  abstract = {Adaptive rejection sampling (ARS) is an algorithm by Gilks and Wild for drawing samples from a continuous log-concave probability distribution with only black-box access to a function that computes the (unnormalized) density function. The ideas extend in a straightforward way to discrete log-concave distributions, but some details of the extension and its implementation can be tricky. This report provides the details of a discrete ARS algorithm. A companion implementation in C, with a MATLAB interface, accompanies the report. 1},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sheldon.d2013 Discrete adaptive rejection sampling.pdf}
}

@inproceedings{shen.t:2020,
  title = {Blank Language Models},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Shen, Tianxiao and Quach, Victor and Barzilay, Regina and Jaakkola, Tommi},
  year = {2020},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2020.emnlp-main.420},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.420},
  date-added = {2022-04-05 19:37:55 -0400},
  date-modified = {2022-04-05 19:38:07 -0400}
}

@inproceedings{shen.y:2017,
  title = {Neural Language Modeling by Jointly Learning Syntax and Lexicon},
  booktitle = {6th International Conference on Learning Representations, {{ICLR}} 2018, Vancouver, {{BC}}, Canada, April 30 - May 3, 2018, Conference Track Proceedings},
  author = {Shen, Yikang and Lin, Zhouhan and Huang, Chin-Wei and Courville, Aaron C.},
  year = {2018},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenLHC18.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{shen.y:2018,
  title = {Ordered Neurons: {{Integrating}} Tree Structures into Recurrent Neural Networks},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Shen, Yikang and Tan, Shawn and Sordoni, Alessandro and Courville, Aaron C.},
  year = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/ShenTSC19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shen.y2018 Ordered neurons Integrating tree struct.pdf}
}

@article{shepard.r:1987,
  title = {Toward a Universal Law of Generalization for Psychological Science},
  author = {Shepard, Roger N.},
  year = {1987},
  month = sep,
  journal = {Science},
  volume = {237},
  number = {4820},
  pages = {1317--1323},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.3629243},
  urldate = {2022-10-11},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shepard.r1987 Toward a universal law of generalization.pdf}
}

@book{shields.p:1996,
  title = {The Ergodic Theory of Discrete Sample Paths},
  author = {Shields, Paul C},
  year = {1996},
  volume = {13},
  publisher = {American Mathematical Soc.},
  date-added = {2020-08-08 19:47:15 -0400},
  date-modified = {2020-08-08 19:49:28 -0400},
  project = {information-compositionality},
  keywords = {entropy,information theory}
}

@article{shmueli.g:2010,
  title = {To Explain or to Predict?},
  author = {Shmueli, Galit},
  year = {2010},
  month = aug,
  journal = {Statistical Science},
  volume = {25},
  number = {3},
  issn = {0883-4237},
  doi = {10.1214/10-STS330},
  urldate = {2022-09-26},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/shmueli.g2010 To explain or to predict.pdf}
}

@article{sigurdsson.h:1996,
  title = {Icelandic Finite Verb Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {1996},
  journal = {Working papers in Scandinavian syntax},
  volume = {57},
  pages = {1--46},
  publisher = {Department of Scandinavian Languages},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,syncretism},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sigurdsson.h1996 Icelandic finite verb agreement.pdf}
}

@incollection{sigurdsson.h:2000,
  title = {The Locus of Case and Agreement},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann},
  year = {2000},
  series = {Working Papers in {{Scandinavian}} Syntax},
  volume = {65},
  publisher = {Department of Scandinavian Languages, Lund University},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2021-03-12 11:36:31 -0500},
  howpublished = {Working paper},
  project = {Icelandic gluttony},
  keywords = {agreement,subject positions}
}

@article{sigurdsson.h:2008,
  title = {Icelandic Dative Intervention: {{Person}} and Number Are Separate Probes},
  author = {Sigur{\dh}sson, Halld{\'o}r {\'A}rmann and Holmberg, Anders},
  year = {2008},
  journal = {Agreement restrictions},
  pages = {251--280},
  publisher = {Mouton de Gruyter Berlin},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:26:40 -0400},
  project = {Icelandic gluttony},
  keywords = {hierarchy effects,split probe}
}

@article{simon.h:1955,
  title = {A {{Behavioral Model}} of {{Rational Choice}}},
  author = {Simon, Herbert A.},
  year = {1955},
  month = feb,
  journal = {The Quarterly Journal of Economics},
  volume = {69},
  number = {1},
  pages = {99--118},
  issn = {0033-5533},
  doi = {10.2307/1884852},
  urldate = {2022-06-13},
  abstract = {Introduction, 99. --- I. Some general features of rational choice, 100.--- II. The essential simplifications, 103. --- III. Existence and uniqueness of solutions, 111. --- IV. Further comments on dynamics, 113. --- V. Conclusion, 114. --- Appendix, 115.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/simon.h1955 A Behavioral Model of Rational Choice.pdf}
}

@article{simon.h:1956,
  title = {Rational Choice and the Structure of the Environment},
  author = {Simon, H. A.},
  year = {1956},
  journal = {Psychological Review},
  volume = {63},
  number = {2},
  pages = {129--138},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-1471},
  doi = {10.1037/h0042769},
  abstract = {"In this paper I have attempted to identify some of the structural characteristics that are typical of the 'psychological' environments of organisms. We have seen that an organism in an environment with these characteristics requires only very simple perceptual and choice mechanisms to satisfy its several needs and to assure a high probability of its survival over extended periods of time. In particular, no 'utility function' needs to be postulated for the organism, nor does it require any elaborate procedure for calculating marginal rates of substitution among different wants." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Choice Behavior,Decision Making},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/simon.h1956 Rational choice and the structure of the.pdf}
}

@article{simon.h:1990,
  title = {Invariants of Human Behavior},
  author = {Simon, Herbert A.},
  year = {1990},
  month = feb,
  journal = {Annual Review of Psychology},
  volume = {41},
  number = {Volume 41, 1990},
  pages = {1--20},
  publisher = {Annual Reviews},
  issn = {0066-4308, 1545-2085},
  doi = {10.1146/annurev.ps.41.020190.000245},
  urldate = {2024-05-03},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/simon.h1990 Invariants of human behavior.pdf}
}

@article{slaats.s:2024psyarxiv,
  title = {What's Surprising about Surprisal},
  author = {Slaats, Sophie and Martin, Andrea E.},
  year = {2024},
  month = feb,
  publisher = {OSF},
  doi = {10.31234/osf.io/7pvau},
  urldate = {2024-02-10},
  abstract = {In the computational and experimental psycholinguistic literature, the mechanisms behind syntactic structure building (e.g., combining words into phrases and sentences) are the subject of considerable debate.  Much experimental work has shown that surprisal is a good predictor of human behavioral and neural data. These findings have led some authors to model language comprehension in a purely probabilistic way. In this paper, we use simulation to exemplify why surprisal works so well to model human data, and to illustrate why exclusive reliance on it can be problematic for the development of a theory of language comprehension in general, and for a theory of composition in particular. Rather than arguing for the importance of structural or distributional information to the exclusion or exhaustion of the other, we argue more emphasis should be placed on how the brain leverages both types of information (viz., statistical and structured). We propose that distributional information is an important cue to the structure in the message, but is not a substitute for the structure itself - neither computationally, formally, nor conceptually. Surprisal and other distributional metrics must play a key role as theoretical objects in any explanatory mechanistic theory of language processing, but that role remains in the service of the brain's goal of constructing structured meaning from sensory input.},
  langid = {american},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/slaats.s2024psyarxiv What's surprising about surprisal.pdf}
}

@misc{smith.d:2023chiralmono,
  title = {A Chiral Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = {2023},
  month = may,
  number = {arXiv:2305.17743},
  eprint = {2305.17743},
  primaryclass = {cs, math},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.17743},
  urldate = {2023-06-03},
  abstract = {The recently discovered "hat" aperiodic monotile mixes unreflected and reflected tiles in every tiling it admits, leaving open the question of whether a single shape can tile aperiodically using translations and rotations alone. We show that a close relative of the hat -- the equilateral member of the continuum to which it belongs -- is a weakly chiral aperiodic monotile: it admits only non-periodic tilings if we forbid reflections by fiat. Furthermore, by modifying this polygon's edges we obtain a family of shapes called Spectres that are strictly chiral aperiodic monotiles: they admit only chiral non-periodic tilings based on a hierarchical substitution system.},
  archiveprefix = {arXiv},
  keywords = {05B45 52C20 (Primary) 05B50 (Secondary),Computer Science - Discrete Mathematics,F.2.2,G.2.1,Mathematics - Combinatorics,Mathematics - Metric Geometry}
}

@misc{smith.d:2023mono,
  title = {An Aperiodic Monotile},
  author = {Smith, David and Myers, Joseph Samuel and Kaplan, Craig S. and {Goodman-Strauss}, Chaim},
  year = {2023},
  number = {2303.10798 [math.CO]},
  eprint = {2303.10798},
  primaryclass = {math.CO},
  publisher = {arXiv},
  abstract = {A longstanding open problem asks for an aperiodic monotile, also known as an "einstein": a shape that admits tilings of the plane, but never periodic tilings. We answer this problem for topological disk tiles by exhibiting a continuum of combinatorially equivalent aperiodic polygons. We first show that a representative example, the "hat" polykite, can form clusters called "metatiles", for which substitution rules can be defined. Because the metatiles admit tilings of the plane, so too does the hat. We then prove that generic members of our continuum of polygons are aperiodic, through a new kind of geometric incommensurability argument. Separately, we give a combinatorial, computer-assisted proof that the hat must form hierarchical -- and hence aperiodic -- tilings.},
  archiveprefix = {arXiv}
}

@inproceedings{smith.n:2008cogsci,
  title = {Optimal Processing Times in Reading: A Formal Model and Empirical Investigation},
  booktitle = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = jul,
  volume = {30},
  pages = {570--576},
  address = {Washington, DC, USA},
  date-added = {2021-05-31 12:59:02 -0400},
  date-modified = {2021-12-14 20:34:04 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/smith.n2008cogsci Optimal processing times in reading a f.pdf}
}

@inproceedings{smith.n:2008csdl,
  title = {Probabilistic Prediction and the Continuity of Language Comprehension},
  booktitle = {9th Conference on Conceptual Structure, Discourse, and Language ({{CSDL9}})},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2008},
  month = oct,
  abstract = {It is well known that humans comprehend language in an incremental fashion, and that while doing so they use diverse contextual clues to make predictions about upcoming words (e.g. Tanenhaus et al., 1995). One way to model such predictions quantitatively is to use conditional probability, with P(continuation{\textbar}context) denoting the fraction of the time that some continuation is expected to occur in a given context (Hale, 2001). For cognitive linguistics, such probabilities have special appeal. Generally, we seek theories that can faithfully describe the messy world of language and thought, and that are precise enough to make sharp, testable predictions; in practice we rarely achieve both goals simultaneously. Conditional probability provides a (partial) framework for representing linguistic knowledge that is subject to precise mathematics, but at the same time - unlike traditional formalisms - is amenable to incremental learning and gradient representations, and generalizes naturally to all levels of linguistic and extra-linguistic structure. Understanding the details of prediction in online language comprehension, therefore, may eventually pay considerable theoretical dividends. To this end, consider one behavioral correlate of predictability: words which are more predictable are processed faster (e.g. Ehrlich and Rayner, 1981). This effect is well known, but there is currently no agreement on why it occurs; and, since previous studies (Rayner and Well, 1996) have used exclusively factorial comparisons, we know the effect's direction but have little insight into its functional form. To address these issues, we first present a novel theory of linguistic processing time that draws inspiration from two sources: the literature on motor control, and on construction grammar (Fillmore, 1988; Kay and Fillmore, 1999). The model consists of an optimal control system (Todorov, 2004) that, rather than controlling muscles, controls the allocation of preparatory resources in the linguistic processing system. Its challenge is to manage the trade-off between conserving resources and processing quickly; for efficiency, it preferentially allocates resources to more probable continuations, which results in a speedup for predictable words. Next, following the principles of construction grammar, we constrain the model by requiring that it have no preferred scale - we assume that processing proceeds continuously and simultaneously at all levels from phoneme to clause, using similar mechanisms. This turns out to make a strong prediction: processing time for an item should be proportional to the logarithm of that item's probability-in-context. Second, we test this prediction by analyzing the relation between probability and fixation time in the Dundee eye-movement corpus (Kennedy et al., 2003), approximating probability using a computational language model. As compared to previous studies, this approach has the advantage of both improved ecological validity and vastly greater statistical power, allowing the examination of curve shape. We find that after controlling for confounds, probability does have a logarithmic effect on standard reading-time measures, and this effect is substantial and systematic over several orders of magnitude. This result invalidates a number of competing theories which predict different curve shapes, confirms the prediction of our model, and thus provides supporting evidence for constructional accounts of language processing.},
  date-added = {2020-06-08 21:52:31 -0400},
  date-modified = {2021-03-16 17:32:08 -0400},
  project = {syntactic embedding}
}

@inproceedings{smith.n:2011cloze,
  title = {Cloze but No Cigar: {{The}} Complex Relationship between Cloze, Corpus, and Subjective Probabilities in Language Processing},
  booktitle = {Proceedings of the 33rd Annual Meeting of the Cognitive Science Society},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2011},
  date-added = {2021-03-16 14:39:39 -0400},
  date-modified = {2022-03-16 10:06:04 -0400}
}

@article{smith.n:2013,
  title = {The Effect of Word Predictability on Reading Time Is Logarithmic},
  author = {Smith, Nathaniel J. and Levy, Roger},
  year = {2013},
  journal = {Cognition},
  volume = {128},
  number = {3},
  pages = {302--319},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2013.02.013},
  abstract = {It is well known that real-time human language processing is highly incremental and context-driven, and that the strength of a comprehender's expectation for each word encountered is a key determinant of the difficulty of integrating that word into the preceding context. In reading, this differential difficulty is largely manifested in the amount of time taken to read each word. While numerous studies over the past thirty years have shown expectation-based effects on reading times driven by lexical, syntactic, semantic, pragmatic, and other information sources, there has been little progress in establishing the quantitative relationship between expectation (or prediction) and reading times. Here, by combining a state-of-the-art computational language model, two large behavioral data-sets, and non-parametric statistical techniques, we establish for the first time the quantitative form of this relationship, finding that it is logarithmic over six orders of magnitude in estimated predictability. This result is problematic for a number of established models of eye movement control in reading, but lends partial support to an optimal perceptual discrimination account of word recognition. We also present a novel model in which language processing is highly incremental well below the level of the individual word, and show that it predicts both the shape and time-course of this effect. At a more general level, this result provides challenges for both anticipatory processing and semantic integration accounts of lexical predictability effects. And finally, this result provides evidence that comprehenders are highly sensitive to relative differences in predictability -- even for differences between highly unpredictable words -- and thus helps bring theoretical unity to our understanding of the role of prediction at multiple levels of linguistic structure in real-time language comprehension.},
  bdsk-url-2 = {https://doi.org/10.1016/j.cognition.2013.02.013},
  date-added = {2021-03-09 22:52:29 -0500},
  date-modified = {2021-11-14 23:57:40 -0500},
  keywords = {Expectation,Information theory,Probabilistic models of cognition,Psycholinguistics,Reading,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/smith.n2013 The effect of word predictability on rea.pdf}
}

@article{smith.p:1969,
  title = {Coding Strategies in Language},
  author = {Smith, Philip Twitchell},
  year = {1969},
  journal = {Information and Control},
  volume = {14},
  number = {1},
  pages = {72--97},
  issn = {0019-9958},
  doi = {10.1016/S0019-9958(69)90033-3},
  abstract = {The problem of selecting a code to transmit four messages over the binary symmetric channel is studied in relation to two types of channel noise (``substitution≓ error and ``deletion≓ error) and to two types of decoding strategy (maximum hit and minimum error). It is shown that mean Hamming distance is a good general guide to coding efficiency, except in the case of a minimum error strategy with a deletion error channel, where coding efficiency is critically dependent on noise level. An experiment in which subjects selected codes in an artificial language suggests that the process of recall from memory is similar to the process of transmitting over a deletion error channel with a minimum error strategy. A similar interpretation can be placed on the analysis of consonant systems in English, French, German and Welsh, where the sets of consonants of a given class in a given environment are considered as codes whose alphabet is the phonological distinctive feature system of Halle (1958)},
  bdsk-url-2 = {https://doi.org/10.1016/S0019-9958(69)90033-3},
  date-added = {2022-04-27 13:10:31 -0400},
  date-modified = {2022-04-27 13:11:36 -0400},
  keywords = {artificial language,hamming distance,noisy channel coding,phonology}
}

@inproceedings{snyder.b:2009,
  title = {Unsupervised Multilingual Grammar Induction},
  booktitle = {Proceedings of the Joint Conference of the 47th Annual Meeting of the {{ACL}} and the 4th International Joint Conference on Natural Language Processing of the {{AFNLP}}: {{Volume}} 1 - Volume 1},
  author = {Snyder, Benjamin and Naseem, Tahira and Barzilay, Regina},
  year = {2009},
  series = {{{ACL}} '09},
  pages = {73--81},
  publisher = {Association for Computational Linguistics},
  address = {USA},
  abstract = {We investigate the task of unsupervised constituency parsing from bilingual parallel corpora. Our goal is to use bilingual cues to learn improved parsing models for each language and to evaluate these models on held-out monolingual test data. We formulate a generative Bayesian model which seeks to explain the observed parallel data through a combination of bilingual and monolingual parameters. To this end, we adapt a formalism known as unordered tree alignment to our probabilistic setting. Using this formalism, our model loosely binds parallel trees while allowing language-specific syntactic structure. We perform inference under this model using Markov Chain Monte Carlo and dynamic programming. Applying this model to three parallel corpora (Korean-English, Urdu-English, and Chinese-English) we find substantial performance gains over the CCM model, a strong monolingual baseline. On average, across a variety of testing scenarios, our model achieves an 8.8 absolute gain in F-measure.},
  date-added = {2022-04-04 12:46:23 -0400},
  date-modified = {2022-04-04 12:47:09 -0400},
  isbn = {978-1-932432-45-9}
}

@inproceedings{socolof.m:2022,
  title = {Characterizing Idioms: Conventionality and Contingency},
  booktitle = {Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: {{Long}} Papers)},
  author = {Socolof, Michaela and Cheung, Jackie and Wagner, Michael and O'Donnell, Timothy},
  year = {2022},
  month = may,
  pages = {4024--4037},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  abstract = {Idioms are unlike most phrases in two important ways. First, words in an idiom have non-canonical meanings. Second, the non-canonical meanings of words in an idiom are contingent on the presence of other words in the idiom. Linguistic theories differ on whether these properties depend on one another, as well as whether special theoretical machinery is needed to accommodate idioms. We define two measures that correspond to the properties above, and we show that idioms fall at the expected intersection of the two dimensions, but that the dimensions themselves are not correlated. Our results suggest that introducing special machinery to handle idioms may not be warranted.},
  date-added = {2022-05-17 08:04:25 -0400},
  date-modified = {2022-05-17 08:05:15 -0400}
}

@inproceedings{socolof.m:2022coling,
  title = {Measuring Morphological Fusion Using Partial Information Decomposition},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Computational Linguistics}}},
  author = {Socolof, Michaela and Hoover, Jacob Louis and Futrell, Richard and Sordoni, Alessandro and O'Donnell, Timothy J.},
  year = {2022},
  month = oct,
  pages = {44--54},
  publisher = {International Committee on Computational Linguistics},
  address = {Gyeongju, Republic of Korea},
  abstract = {Morphological systems across languages vary when it comes to the relation between form and meaning. In some languages, a single meaning feature corresponds to a single morpheme, whereas in other languages, multiple meaning features are bundled together into one morpheme. The two types of languages have been called agglutinative and fusional, respectively, but this distinction does not capture the graded nature of the phenomenon. We provide a mathematically precise way of characterizing morphological systems using partial information decomposition, a framework for decomposing mutual information into three components: unique, redundant, and synergistic information. We show that highly fusional languages are characterized by high levels of synergy.},
  copyright = {All rights reserved},
  openaccess = {true},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/socolof.m2022coling Measuring morphological fusion using par.pdf}
}

@phdthesis{socolof.m:2024phd,
  title = {Partial Compositionality},
  author = {Socolof, Michaela},
  year = {2024},
  address = {Montr{\'e}al, Canada},
  langid = {english},
  school = {McGill University}
}

@misc{sohl-dickstein.j:2015arxiv,
  title = {Deep {{Unsupervised Learning}} Using {{Nonequilibrium Thermodynamics}}},
  author = {{Sohl-Dickstein}, Jascha and Weiss, Eric A. and Maheswaranathan, Niru and Ganguli, Surya},
  year = {2015},
  month = nov,
  number = {arXiv:1503.03585},
  eprint = {1503.03585},
  primaryclass = {cond-mat, q-bio, stat},
  publisher = {arXiv},
  urldate = {2022-05-17},
  abstract = {A central problem in machine learning involves modeling complex data-sets using highly flexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both flexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly flexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks,diffusion processes,Quantitative Biology - Neurons and Cognition,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sohl-dickstein.j2015 Deep Unsupervised Learning using Nonequi.pdf}
}

@book{sonderegger.m:2023rmld,
  title = {Regression Modeling for Linguistic Data},
  author = {Sonderegger, Morgan},
  year = {2023},
  month = jun,
  publisher = {The MIT Press},
  address = {Cambridge, Massachusetts},
  isbn = {978-0-262-37503-0},
  langid = {english}
}

@inproceedings{song.y:2019,
  title = {Generative Modeling by Estimating Gradients of the Data Distribution},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Song, Yang and Ermon, Stefano},
  year = {2019},
  volume = {32},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-07-07},
  abstract = {We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples  comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/song.y2019 Generative modeling by estimating gradie.pdf}
}

@inproceedings{song.y:2022,
  title = {Score-Based Generative Modeling through Stochastic Differential Equations},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Song, Yang and {Sohl-Dickstein}, Jascha and Kingma, Diederik P. and Kumar, Abhishek and Ermon, Stefano and Poole, Ben},
  year = {2022},
  month = feb,
  urldate = {2022-07-11},
  abstract = {Creating noise from data is easy; creating data from noise is generative modeling. We present a stochastic differential equation (SDE) that smoothly transforms a complex data distribution to a...},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/song.y2022 Score-based generative modeling through.pdf}
}

@article{soskuthy.m:2021,
  title = {Evaluating Generalised Additive Mixed Modelling Strategies for Dynamic Speech Analysis},
  author = {S{\'o}skuthy, M{\'a}rton},
  year = {2021},
  month = jan,
  journal = {Journal of Phonetics},
  volume = {84},
  pages = {101017},
  publisher = {Elsevier BV},
  doi = {10.1016/j.wocn.2020.101017},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2020.101017},
  date-added = {2022-03-04 16:06:14 -0500},
  date-modified = {2022-03-04 16:06:18 -0500}
}

@inproceedings{stabler.e:1997,
  title = {Derivational Minimalism},
  booktitle = {Logical Aspects of Computational Linguistics},
  author = {Stabler, Edward},
  editor = {Retor{\'e}, Christian},
  year = {1997},
  pages = {68--95},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0052152},
  abstract = {A basic idea of the transformational tradition is that constituents move. More recently, there has been a trend towards the view that all features are lexical features. And in recent ``minimalist'' grammars, structure building operations are assumed to be feature driven. A simple grammar formalism with these properties is presented here and briefly explored. Grammars in this formalism can define languages that are not in the ``mildly context sensitive'' class defined by Vijay-Shanker and Weir (1994).},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  isbn = {978-3-540-69631-5},
  project = {syntactic embedding}
}

@incollection{stabler.e:1997a,
  title = {Derivational Minimalism},
  booktitle = {Logical {{Aspects}} of {{Computational Linguistics}}},
  author = {Stabler, Edward},
  editor = {Carbonell, Jaime G. and Siekmann, J{\"o}rg and Goos, G. and Hartmanis, J. and {van Leeuwen}, J. and Retor{\'e}, Christian},
  year = {1997},
  volume = {1328},
  pages = {68--95},
  publisher = {Springer Berlin Heidelberg},
  address = {Berlin, Heidelberg},
  doi = {10.1007/BFb0052152},
  urldate = {2022-09-30},
  isbn = {978-3-540-63700-4 978-3-540-69631-5}
}

@article{stabler.e:2013,
  title = {Two Models of Minimalist, Incremental Syntactic Analysis},
  author = {Stabler, Edward P.},
  year = {2013},
  month = jul,
  journal = {Topics in Cognitive Science},
  volume = {5},
  number = {3},
  pages = {611--633},
  issn = {17568757},
  doi = {10.1111/tops.12031},
  urldate = {2022-09-30},
  langid = {english}
}

@article{stanley.d:1999,
  title = {A {{Multiplicative Calculus}}},
  author = {Stanley, Dick},
  year = {1999},
  month = jan,
  journal = {PRIMUS},
  volume = {9},
  number = {4},
  pages = {310--326},
  publisher = {Taylor \& Francis},
  issn = {1051-1970},
  doi = {10.1080/10511979908965937},
  urldate = {2023-04-25},
  abstract = {A new type of calculus called ``multiplicative calculus'' is developed and some basic theorems about derivatives, integrals, and infinite products are proved within this calculus. Multiplicative calculus is based on a multiplicative mechanism in the same sense that the usual calculus is based on an additive mechanism. One consequence is that, just as the usual derivative of a linear function is constant, the multiplicative derivative of an exponential function is constant (and just as the usual derivative of a constant function is 0, the multiplicative derivative of a constant function is 1). Similarly, just as two functions that have a constant difference have the same usual derivative, two functions that have a constant ratio have the same multiplicative derivative. Finally, just as many functions have infinite series representations based on the usual derivative, the same functions have infinite product representations based on the multiplicative derivative. These in turn are derived from exponential approximations to functions that are analogous to the linear approximations of the usual calculus. Multiplicative calculus is a useful supplement to the usual calculus in that it is tailored to situations involving exponential functions in the same sense that the usual calculus is tailored to situations involving linear functions.},
  keywords = {additive,Calculus,differential calculus,infinite products,infinite series,integral calculus,multiplicative}
}

@inproceedings{stanojevic.m:2021,
  title = {Modeling Incremental Language Comprehension in the Brain with Combinatory Categorial Grammar},
  booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
  author = {Stanojevi{\'c}, Milo{\v s} and Bhattasali, Shohini and Dunagan, Donald and Campanelli, Luca and Steedman, Mark and Brennan, Jonathan and Hale, John},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.cmcl-1.3},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.cmcl-1.3},
  date-added = {2022-04-14 13:33:35 -0400},
  date-modified = {2022-04-14 13:33:47 -0400}
}

@article{staub.a:2010,
  title = {Eye Movements and Processing Difficulty in Object Relative Clauses},
  author = {Staub, Adrian},
  year = {2010},
  month = jul,
  journal = {Cognition},
  volume = {116},
  number = {1},
  pages = {71--86},
  issn = {1873-7838},
  doi = {10.1016/j.cognition.2010.04.002},
  abstract = {It is well known that sentences containing object-extracted relative clauses (e.g., The reporter that the senator attacked admitted the error) are more difficult to comprehend than sentences containing subject-extracted relative clauses (e.g., The reporter that attacked the senator admitted the error). Two major accounts of this phenomenon make different predictions about where, in the course of incremental processing of an object relative, difficulty should first appear. An account emphasizing memory processes (Gibson, 1998; Grodner \& Gibson, 2005) predicts difficulty at the relative clause verb, while an account emphasizing experience-based expectations (Hale, 2001; Levy, 2008) predicts earlier difficulty, at the relative clause subject. Two eye movement experiments tested these predictions. Regressive saccades were much more likely from the subject noun phrase of an object relative than from the same noun phrase occurring within a subject relative (Experiment 1) or within a verbal complement clause (Experiment 2). This effect was further amplified when the relative pronoun that was omitted. However, reading time was also inflated on the object relative clause verb in both experiments. These results suggest that the violation of expectations and the difficulty of memory retrieval both contribute to the difficulty of object relative clauses, but that these two sources of difficulty have qualitatively distinct behavioral consequences in normal reading.},
  langid = {english},
  pmid = {20427040},
  keywords = {Eye Movements,Female,Fixation Ocular,Humans,Male,Psycholinguistics,Psychomotor Performance,Reading,Semantics,Young Adult}
}

@article{staub.a:2011,
  title = {The Effect of Lexical Predictability on Distributions of Eye Fixation Durations},
  author = {Staub, Adrian},
  year = {2011},
  month = apr,
  journal = {Psychonomic Bulletin \& Review},
  volume = {18},
  number = {2},
  pages = {371--376},
  issn = {1531-5320},
  doi = {10.3758/s13423-010-0046-9},
  urldate = {2022-10-26},
  abstract = {A word's predictability in context has a well-established effect on fixation durations in reading. To investigate how this effect is manifested in distributional terms, an experiment was carried out in which subjects read each of 50 target words twice, once in a high-predictability context and once in a low-predictability context. The ex-Gaussian distribution was fit to each subject's first-fixation durations and single-fixation durations. For both measures, the {$\mu$} parameter increased when a word was unpredictable, while the {$\tau$} parameter was not significantly affected, indicating that a predictability manipulation shifts the distribution of fixation durations but does not affect the degree of skew. Vincentile plots showed that the mean ex-Gaussian parameters described the typical distribution shapes extremely well. These results suggest that the predictability and frequency effects are functionally distinct, since a frequency manipulation has been shown to influence both {$\mu$} and {$\tau$}. The results may also be seen as consistent with the finding from single-word recognition paradigms that semantic priming affects only {$\mu$}.},
  langid = {english},
  keywords = {Distributional analysis,ex-gaussian,Eye movements in reading,Visual word recognition},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/staub.a2011 The effect of lexical predictability on.pdf}
}

@article{staub.a:2015,
  title = {The Effect of Lexical Predictability on Eye Movements in Reading: {{Critical}} Review and Theoretical Interpretation},
  author = {Staub, Adrian},
  year = {2015},
  journal = {Language and Linguistics Compass},
  volume = {9},
  number = {8},
  pages = {311--327},
  publisher = {Wiley},
  doi = {10.1111/lnc3.12151},
  bdsk-url-2 = {https://doi.org/10.1111/lnc3.12151},
  date-added = {2021-05-22 15:55:42 -0400},
  date-modified = {2021-05-22 15:55:54 -0400},
  keywords = {predictability,processing,review}
}

@article{staub.a:2015a,
  title = {The Influence of Cloze Probability and Item Constraint on Cloze Task Response Time},
  author = {Staub, Adrian and Grant, Margaret and Astheimer, Lori and Cohen, Andrew},
  year = {2015},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {82},
  pages = {1--17},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2015.02.004},
  urldate = {2022-09-07},
  abstract = {In research on the role of lexical predictability in language comprehension, predictability is generally defined as the probability that a word is provided as a sentence continuation in the cloze task (Taylor, 1953), in which subjects are asked to guess the next word of a sentence. The present experiments investigate the process by which subjects generate a cloze response, by measuring the latency to initiate a response in a version of the task in which subjects produce a spoken continuation to a visually presented sentence fragment. Higher probability responses were produced faster than lower probability responses. The latency to produce a response was also influenced by item constraint: A response at a given level of probability was issued faster when the context was more constraining, i.e., a single response was elicited with high probability. We show that these patterns are naturally produced by an activation-based race model in which potential responses independently race towards a response threshold. Implications for the interpretation of cloze probability as a measure of lexical predictability are discussed.},
  langid = {english},
  keywords = {Cloze task,Language processing,Prediction,Response time}
}

@article{staub.a:2024cogpsych,
  title = {Perceptual Inference Corrects Function Word Errors in Reading: {{Errors}} That Are Not Noticed Do Not Disrupt Eye Movements},
  shorttitle = {Perceptual Inference Corrects Function Word Errors in Reading},
  author = {Staub, Adrian and McMurray, Harper and Wickett, Anthony},
  year = {2024},
  month = nov,
  journal = {Cognitive Psychology},
  volume = {154},
  pages = {101691},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2024.101691},
  urldate = {2024-09-23},
  abstract = {Both everyday experience and laboratory research demonstrate that readers often fail to notice errors such as an omitted or repeated function word. This phenomenon challenges central tenets of reading and sentence processing models, according to which each word is lexically processed and incrementally integrated into a syntactic representation. One solution would propose that apparent failure to notice such errors reflects post-perceptual inference; the reader does initially perceive the error, but then unconsciously 'corrects' the perceived string. Such a post-perceptual account predicts that when readers fail to explicitly notice an error, the error will nevertheless disrupt reading, at least fleetingly. We present a large-scale eyetracking experiment investigating whether disruption is detectable in the eye movement record when readers fail to notice an omitted or repeated two-letter function word in naturalistic sentences. Readers failed to notice both omission and repetition errors over 36\% of the time. In an analysis that included all trials, both omission and repetition resulted in pronounced eye movement disruption, compared to reading of grammatical control sentences. But in an analysis including only trials on which readers failed to notice the errors, neither type of error disrupted eye movements on any measure. Indeed, there was evidence in some measures that reading was relatively fast on the trials on which errors were missed. It does not appear that when an error is not consciously noticed, it is initially perceived, and then later corrected; rather, linguistic knowledge influences what the reader perceives.},
  keywords = {Perceptual inference,Reading,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/staub.a2024cogpsych Perceptual inference corrects function w.pdf}
}

@article{staub.a:2024review,
  title = {Predictability in {{Language Comprehension}}: {{Prospects}} and {{Problems}} for {{Surprisal}}},
  shorttitle = {Predictability in {{Language Comprehension}}},
  author = {Staub, Adrian},
  year = {2024},
  month = jul,
  journal = {Annual Review of Linguistics},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-011724-121517},
  urldate = {2024-07-25},
  abstract = {Surprisal theory proposes that a word's predictability influences processing difficulty because each word requires the comprehender to update a probability distribution over possible sentences. This article first considers the theory's detailed predictions regarding the effects of predictability on reading time and N400 amplitude. Two rather unintuitive predictions appear to be correct based on the current evidence: There is no specific cost when an unpredictable word is encountered in a context where another word is predictable, and the function relating predictability to processing difficulty is logarithmic, not linear. Next, the article addresses the viability of the claim, also associated with Surprisal, that conditional probability is the ``causal bottleneck'' mediating all effects on incremental processing difficulty. This claim fares less well as conditional probability does not account for the difficulty associated with encountering a low-frequency word or the difficulty associated with garden path disambiguation. Surprisal provides a compelling account of predictability effects but does not provide a complete account of incremental processing difficulty.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/staub.a2024 Predictability in Language Comprehension.pdf}
}

@misc{steedman.m:2000,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {MIT Press},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2020-05-05 13:00:02 -0400},
  project = {syntactic embedding}
}

@book{steedman.m:2000a,
  title = {The Syntactic Process},
  author = {Steedman, Mark},
  year = {2000},
  publisher = {The MIT press},
  date-added = {2021-02-05 12:03:58 -0500},
  date-modified = {2021-02-05 12:03:58 -0500}
}

@inproceedings{steinhardt.j:2014,
  title = {Filtering with Abstract Particles},
  booktitle = {Proceedings of the 31st International Conference on Machine Learning},
  author = {Steinhardt, Jacob and Liang, Percy},
  editor = {Xing, Eric P. and Jebara, Tony},
  year = {2014-06-22/2014-06-24},
  series = {Proceedings of Machine Learning Research},
  volume = {32},
  pages = {727--735},
  publisher = {PMLR},
  address = {Bejing, China},
  abstract = {Using particles, beam search and sequential Monte Carlo can approximate distributions in an extremely flexible manner. However, they can suffer from sparsity and inadequate coverage on large state spaces. We present a new filtering method that addresses this issue by using ``abstract particles'' that each represent an entire region of the state space. These abstract particles are combined into a hierarchical decomposition, yielding a representation that is both compact and flexible. Empirically, our method outperforms beam search and sequential Monte Carlo on both a text reconstruction task and a multiple object tracking task.},
  date-added = {2022-03-25 12:02:29 -0400},
  date-modified = {2022-03-25 12:02:31 -0400},
  pdf = {http://proceedings.mlr.press/v32/steinhardt14.pdf}
}

@book{stevens.e:2020,
  title = {Deep Learning with {{PyTorch}}},
  author = {Stevens, Eli and Antiga, Luca and Viehmann, Thomas},
  year = {2020},
  publisher = {Manning Publications Company},
  date-added = {2021-08-02 19:51:35 -0400},
  date-modified = {2021-08-02 19:52:08 -0400},
  isbn = {978-1-61729-526-3}
}

@inproceedings{stolcke.a:1994,
  title = {Inducing Probabilistic Grammars by {{Bayesian}} Model Merging},
  booktitle = {Grammatical {{Inference}} and {{Applications}}},
  author = {Stolcke, Andreas and Omohundro, Stephen},
  editor = {Carrasco, Rafael C. and Oncina, Jose},
  year = {1994},
  series = {Lecture {{Notes}} in {{Computer Science}}},
  pages = {106--118},
  publisher = {Springer},
  address = {Berlin, Heidelberg},
  doi = {10.1007/3-540-58473-0_141},
  abstract = {We describe a framework for inducing probabilistic grammars from corpora of positive samples. First, samples are incorporated by adding ad-hoc rules to a working grammar; subsequently, elements of the model (such as states or nonterminals) are merged to achieve generalization and a more compact representation. The choice of what to merge and when to stop is governed by the Bayesian posterior probability of the grammar given the data, which formalizes a trade-off between a close fit to the data and a default preference for simpler models (`Occam's Razor'). The general scheme is illustrated using three types of probabilistic grammars: Hidden Markov models, class-based n-grams, and stochastic context-free grammars.},
  isbn = {978-3-540-48985-6},
  langid = {english},
  keywords = {Bayesian Posterior Probability,Beam Search,Hide Markov Model,Merging Algorithm,Relative Clause},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/stolcke.a1994 Inducing probabilistic grammars by Bayes.pdf}
}

@article{stolcke.a:1995,
  title = {An Efficient Probabilistic Context-Free Parsing Algorithm That Computes Prefix Probabilities},
  author = {Stolcke, Andreas},
  year = {1995},
  journal = {Computational Linguistics},
  volume = {21},
  number = {2},
  pages = {165--201},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/stolcke.a1995 An efficient probabilistic context-free.pdf}
}

@article{stone.m:1960,
  title = {Models for Choice-Reaction Time},
  author = {Stone, Mervyn},
  year = {1960},
  month = sep,
  journal = {Psychometrika},
  volume = {25},
  number = {3},
  pages = {251--260},
  issn = {1860-0980},
  doi = {10.1007/BF02289729},
  urldate = {2022-07-04},
  abstract = {In the two-choice situation, the Wald sequential probability ratio decision procedure is applied to relate the mean and variance of the decision times, for each alternative separately, to the error rates and the ratio of the frequencies of presentation of the alternatives. For situations involving more than two choices, a fixed sample decision procedure (selection of the alternative with highest likelihood) is examined, and the relation is found between the decision time (or size of sample), the error rate, and the number of alternatives.},
  langid = {english},
  keywords = {Decision Procedure,Error Rate,High Likelihood,Public Policy,Statistical Theory}
}

@inproceedings{strubell.e:2019,
  title = {Energy and Policy Considerations for Deep Learning in {{NLP}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
  year = {2019},
  pages = {3645--3650},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1355},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1355}
}

@inproceedings{sundermeyer.m:2012LSTMsForLM,
  title = {{{LSTM}} Neural Networks for Language Modeling},
  booktitle = {Interspeech 2012},
  author = {Sundermeyer, Martin and Schl{\"u}ter, Ralf and Ney, Hermann},
  year = {2012},
  month = sep,
  pages = {194--197},
  publisher = {ISCA},
  doi = {10.21437/Interspeech.2012-65},
  urldate = {2024-05-18},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/sundermeyer.m2012LSTMsForLM LSTM neural networks for language modeli.pdf}
}

@article{svenonius.p:2002,
  title = {Icelandic Case and the Structure of Events},
  author = {Svenonius, Peter},
  year = {2002},
  journal = {The Journal of Comparative Germanic Linguistics},
  volume = {5},
  number = {1-3},
  pages = {197--225},
  publisher = {Springer},
  date-added = {2020-03-06 14:58:07 -0800},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {Icelandic gluttony}
}

@article{swets.b:2008,
  title = {Underspecification of Syntactic Ambiguities: {{Evidence}} from Self-Paced Reading},
  shorttitle = {Underspecification of Syntactic Ambiguities},
  author = {Swets, Benjamin and Desmet, Timothy and Clifton, Charles and Ferreira, Fernanda},
  year = {2008},
  month = jan,
  journal = {Memory \& Cognition},
  volume = {36},
  number = {1},
  pages = {201--216},
  issn = {1532-5946},
  doi = {10.3758/MC.36.1.201},
  urldate = {2023-08-01},
  abstract = {Syntactically ambiguous sentences are sometimes read faster than disambiguated strings. Models of parsing have explained this tendency by appealing either to a race in the construction of alternative structures or to reanalysis. However, it is also possible that readers of ambiguous sentences save time by strategically underspecifying interpretations of ambiguous attachments. In a self-paced reading study, participants viewed sentences with relative clauses that could attach to one of two sites. Type of question was also manipulated between participants in order to test whether goals can influence reading/parsing strategies. The experiment revealed an ambiguity advantage in reading times, but only when participants expected superficial comprehension questions. When participants expected queries about relative clause interpretation, disambiguating regions were inspected with more care, and the ambiguity advantage was attenuated. However, even when participants expected relative clause queries, question-answering times suggested underspecified representations of ambiguous relative clause attachments. The results support the construal and ``good-enough'' models of parsing.},
  langid = {english},
  keywords = {Question Type,Race Model,Reading Time,Relative Clause,Sentence Type,underspecification},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/swets.bdesmet.t2008 Underspecification of syntactic ambiguit.pdf}
}

@article{szewczyk.j:2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  year = {2022},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2021.104311},
  urldate = {2024-03-01},
  abstract = {Stimuli are easier to process when context makes them predictable, but does context-based facilitation arise from preactivation of a limited set of relatively probable upcoming stimuli (with facilitation then linearly related to probability) or, instead, because the system maintains and updates a probability distribution across all items (with facilitation logarithmically related to probability)? We measured the N400, an index of semantic access, to words of varying probability, including unpredictable words. Word predictability was measured using both cloze probabilities and a state-of-the-art machine learning language model (GPT-2). We reanalyzed five datasets (n~=~138) to demonstrate and then replicate that context-based facilitation on the N400 is graded, even among unpredictable words. Furthermore, we established that the relationship between word predictability and context-based facilitation combines linear and logarithmic functions. We argue that this composite function reveals properties of the mapping between words and semantic features and how feature- and word-related information is activated on-line.},
  keywords = {Context-based facilitation,GPT-2,N400,Semantic access},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/szewczyk.j2022 Context-based facilitation of semantic a.pdf}
}

@article{tabor.w:2004,
  title = {Evidence for Self-Organized Sentence Processing: {{Digging-in}} Effects.},
  author = {Tabor, Whitney and Hutchins, Sean},
  year = {2004},
  journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
  volume = {30},
  number = {2},
  pages = {431--450},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/0278-7393.30.2.431},
  bdsk-url-2 = {https://doi.org/10.1037/0278-7393.30.2.431},
  date-added = {2021-04-11 18:58:19 -0400},
  date-modified = {2021-04-11 18:58:36 -0400},
  keywords = {diggin in effect,reading time}
}

@inproceedings{takabatake.k:2004,
  title = {Information Geometry of {{Gibbs}} Sampler},
  booktitle = {Proc. of {{WSEAS}} Int. {{Conf}}. on Neural Networks and Applications ({{NNA}})},
  author = {Takabatake, Kazuya},
  year = {2004},
  date-added = {2021-03-11 16:56:36 -0500},
  date-modified = {2021-03-11 17:08:06 -0500},
  keywords = {information theory,sampling},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/takabatake.k2004 Information geometry of Gibbs sampler.pdf}
}

@article{tanenhaus.m:1995,
  title = {Integration of Visual and Linguistic Information in Spoken Language Comprehension},
  author = {Tanenhaus, Michael K. and {Spivey-Knowlton}, Michael J. and Eberhard, Kathleen M. and Sedivy, Julie C.},
  year = {1995},
  month = jun,
  journal = {Science (New York, N.Y.)},
  volume = {268},
  number = {5217},
  pages = {1632--1634},
  publisher = {American Association for the Advancement of Science (AAAS)},
  doi = {10.1126/science.7777863},
  abstract = {Psycholinguists have commonly assumed that as a spoken linguistic message unfolds over time, it is initially structured by a syntactic processing module that is encapsulated from information provided by other perceptual and cognitive systems. To test the effects of relevant visual context on the rapid mental processes that accompany spoken language comprehension, eye movements were recorded with a head-mounted eye-tracking system while subjects followed instructions to manipulate real objects. Visual context influenced spoken word recognition and mediated syntactic processing, even during the earliest moments of language processing.},
  bdsk-url-2 = {https://doi.org/10.1126/science.7777863},
  date-added = {2022-04-20 13:14:59 -0400},
  date-modified = {2022-05-02 14:45:52 -0400},
  keywords = {incrementality}
}

@incollection{taraldsen.k:1995,
  title = {On Agreement and Nominative Objects in {{Icelandic}}},
  booktitle = {Studies in Comparative {{Germanic}} Syntax},
  author = {Taraldsen, Knut Tarald},
  year = {1995},
  pages = {307--327},
  publisher = {Springer},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-15 16:17:31 -0400},
  project = {Icelandic gluttony},
  keywords = {agreement,split probe}
}

@article{tax.t:2017,
  title = {The Partial Information Decomposition of Generative Neural Network Models},
  author = {Tax, Tycho M.S. and Mediano, Pedro A.M. and Shanahan, Murray},
  year = {2017},
  journal = {Entropy. An International and Interdisciplinary Journal of Entropy and Information Studies},
  volume = {19},
  number = {9},
  issn = {1099-4300},
  doi = {10.3390/e19090474},
  abstract = {In this work we study the distributed representations learnt by generative neural network models. In particular, we investigate the properties of redundant and synergistic information that groups of hidden neurons contain about the target variable. To this end, we use an emerging branch of information theory called partial information decomposition (PID) and track the informational properties of the neurons through training. We find two differentiated phases during the training process: a first short phase in which the neurons learn redundant information about the target, and a second phase in which neurons start specialising and each of them learns unique information about the target. We also find that in smaller networks individual neurons learn more specific information about certain features of the input, suggesting that learning pressure can encourage disentangled representations.},
  article-number = {474},
  bdsk-url-2 = {https://doi.org/10.3390/e19090474},
  date-added = {2022-05-14 10:28:04 -0400},
  date-modified = {2022-05-14 10:28:21 -0400},
  keywords = {neural networks,partial information decomposition}
}

@misc{tay.y:2022,
  title = {Transformer Memory as a Differentiable Search Index},
  author = {Tay, Yi and Tran, Vinh Q. and Dehghani, Mostafa and Ni, Jianmo and Bahri, Dara and Mehta, Harsh and Qin, Zhen and Hui, Kai and Zhao, Zhe and Gupta, Jai and Schuster, Tal and Cohen, William W. and Metzler, Donald},
  year = {2022},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.2202.06991},
  bdsk-url-2 = {https://doi.org/10.48550/ARXIV.2202.06991},
  copyright = {arXiv.org perpetual, non-exclusive license},
  date-added = {2022-03-31 11:01:29 -0400},
  date-modified = {2022-03-31 11:02:20 -0400},
  keywords = {transformer},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/tay.y2022 Transformer memory as a differentiable s.pdf}
}

@misc{tay.y:2022UL2R,
  title = {Transcending Scaling Laws with 0.1\% Extra Compute},
  author = {Tay, Yi and Wei, Jason and Chung, Hyung Won and Tran, Vinh Q. and So, David R. and Shakeri, Siamak and Garcia, Xavier and Zheng, Huaixiu Steven and Rao, Jinfeng and Chowdhery, Aakanksha and Zhou, Denny and Metzler, Donald and Petrov, Slav and Houlsby, Neil and Le, Quoc V. and Dehghani, Mostafa},
  year = {2022},
  month = nov,
  number = {arXiv:2210.11399},
  eprint = {2210.11399},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-31},
  abstract = {Scaling language models improves performance but comes with significant computational costs. This paper proposes UL2R, a method that substantially improves existing language models and their scaling curves with a relatively tiny amount of extra compute. The key idea is to continue training a state-of-the-art large language model (e.g., PaLM) on a few more steps with UL2's mixture-of-denoiser objective. We show that, with almost negligible extra computational costs and no new sources of data, we are able to substantially improve the scaling properties of large language models on downstream metrics. In this paper, we continue training PaLM with UL2R, introducing a new set of models at 8B, 62B, and 540B scale which we call U-PaLM. Impressively, at 540B scale, we show an approximately 2x computational savings rate where U-PaLM achieves the same performance as the final PaLM 540B model at around half its computational budget (i.e., saving \${\textbackslash}sim\$4.4 million TPUv4 hours). We further show that this improved scaling curve leads to 'emergent abilities' on challenging BIG-Bench tasks -- for instance, U-PaLM does much better than PaLM on some tasks or demonstrates better quality at much smaller scale (62B as opposed to 540B). Overall, we show that U-PaLM outperforms PaLM on many few-shot setups, i.e., English NLP tasks (e.g., commonsense reasoning, question answering), reasoning tasks with chain-of-thought (e.g., GSM8K), multilingual tasks (MGSM, TydiQA), MMLU and challenging BIG-Bench tasks. Finally, we provide qualitative examples showing the new capabilities of U-PaLM for single and multi-span infilling.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/tay.y2022UL2R Transcending scaling laws with 0.1% extr.pdf}
}

@misc{tay.y:2023UL2,
  title = {{{UL2}}: {{Unifying}} Language Learning Paradigms},
  shorttitle = {Ul2},
  author = {Tay, Yi and Dehghani, Mostafa and Tran, Vinh Q. and Garcia, Xavier and Wei, Jason and Wang, Xuezhi and Chung, Hyung Won and Shakeri, Siamak and Bahri, Dara and Schuster, Tal and Zheng, Huaixiu Steven and Zhou, Denny and Houlsby, Neil and Metzler, Donald},
  year = {2023},
  month = feb,
  number = {arXiv:2205.05131},
  eprint = {2205.05131},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-24},
  abstract = {Existing pre-trained models are generally geared towards a particular class of problems. To date, there seems to be still no consensus on what the right architecture and pre-training setup should be. This paper presents a unified framework for pre-training models that are universally effective across datasets and setups. We begin by disentangling architectural archetypes with pre-training objectives -- two concepts that are commonly conflated. Next, we present a generalized \& unified perspective for self-supervision in NLP and show how different pre-training objectives can be cast as one another and how interpolating between different objectives can be effective. We then propose Mixture-of-Denoisers (MoD), a pre-training objective that combines diverse pre-training paradigms together. We furthermore introduce a notion of mode switching, wherein downstream fine-tuning is associated with specific pre-training schemes. We conduct extensive ablative experiments to compare multiple pre-training objectives and find that our method pushes the Pareto-frontier by outperforming T5 \& GPT-like models across multiple diverse setups. By scaling our model up to 20B parameters, we achieve SOTA performance on 50 well-established supervised finetuning based NLP tasks. Our model also achieve strong results at in-context learning, outperforming 175B GPT-3 on zero-shot SuperGLUE and tripling the performance of T5-XXL on one-shot summarization. On 0-shot MMLU, UL2 20B outperforms T0 and T5 models. UL2 20B also works well with chain-of-thought prompting and reasoning, making it an appealing choice for research into reasoning at a small to medium scale of 20B parameters. Finally, we apply FLAN instruction tuning to the UL2 20B model, achieving MMLU and Big-Bench scores competitive to FLAN-PaLM 62B. We release Flax-based T5X checkpoints for the UL2 20B \& Flan-UL2 20B.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,unified LM},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/tay.y2023UL2 UL2 Unifying language learning paradigm.pdf}
}

@article{taylor.w:1953,
  title = {`{{Cloze}} Procedure': {{A}} New Tool for Measuring Readability},
  author = {Taylor, Wilson L.},
  year = {1953},
  journal = {Journalism Quarterly},
  volume = {30},
  number = {4},
  pages = {415--433},
  publisher = {SAGE Publications},
  doi = {10.1177/107769905303000401},
  bdsk-url-2 = {https://doi.org/10.1177/107769905303000401},
  date-added = {2021-03-18 10:41:54 -0400},
  date-modified = {2021-03-18 11:25:18 -0400},
  keywords = {cloze,processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/taylor.w1953 ‘Cloze procedure’ A new tool for measur.pdf}
}

@inproceedings{teh.y:2006,
  title = {A Hierarchical {{Bayesian}} Language Model Based on {{Pitman-Yor}} Processes},
  booktitle = {Proceedings of the 21st {{International Conference}} on {{Computational Linguistics}} and the 44th Annual Meeting of the {{ACL}} - {{ACL}} 06},
  author = {Teh, Yee Whye},
  year = {2006},
  publisher = {Association for Computational Linguistics},
  doi = {10.3115/1220175.1220299},
  bdsk-url-2 = {https://doi.org/10.3115/1220175.1220299},
  date-added = {2022-04-25 21:37:17 -0400},
  date-modified = {2022-04-25 21:38:48 -0400},
  keywords = {bayesian,HMM,Natural language processing,Pitman-Yor processes}
}

@techreport{teh.y:2006techreport,
  type = {Technical Report},
  title = {A {{Bayesian}} Interpretation of Interpolated {{Kneser-Ney}}},
  author = {Teh, Yee Whye},
  year = {2006},
  number = {TRA2/06},
  institution = {School of Computing, National University of Singapore},
  abstract = {Interpolated Kneser-Ney is one of the best smoothing methods for n-gram language models. Previous explanations for its superiority have been based on intuitive and empirical justifications of specific properties of the method. We propose a novel interpretation of interpolated Kneser-Ney as approximate inference in a hierarchical Bayesian model consisting of Pitman-Yor processes. As opposed to past explanations, our interpretation can recover exactly the formulation of interpolated Kneser-Ney, and performs better than interpolated Kneser-Ney when a better inference procedure is used.},
  date-added = {2022-04-26 10:12:38 -0400},
  date-modified = {2022-04-26 10:16:03 -0400},
  keywords = {bayesian,Dirichlet processes,hierarchical clustering,language modeling,Pitman-Yor processes},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/teh.y2006techreport A Bayesian interpretation of interpolate.pdf}
}

@article{tenenbaum.j:2001,
  title = {Generalization, Similarity, and {{Bayesian}} Inference},
  author = {Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2001},
  month = aug,
  journal = {Behavioral and Brain Sciences},
  volume = {24},
  number = {4},
  pages = {629--640},
  publisher = {Cambridge University Press},
  issn = {1469-1825, 0140-525X},
  doi = {10.1017/S0140525X01000061},
  urldate = {2022-10-11},
  abstract = {Shepard has argued that a universal law should govern generalization across different domains of perception and cognition, as well as across organisms from different species or even different planets. Starting with some basic assumptions about natural kinds, he derived an exponential decay function as the form of the universal generalization gradient, which accords strikingly well with a wide range of empirical data. However, his original formulation applied only to the ideal case of generalization from a single encountered stimulus to a single novel stimulus, and for stimuli that can be represented as points in a continuous metric psychological space. Here we recast Shepard's theory in a more general Bayesian framework and show how this naturally extends his approach to the more realistic situation of generalizing from multiple consequential stimuli with arbitrary representational structure. Our framework also subsumes a version of Tversky's set-theoretic model of similarity, which is conventionally thought of as the primary alternative to Shepard's continuous metric space model of similarity and generalization. This unification allows us not only to draw deep parallels between the set-theoretic and spatial approaches, but also to significantly advance the explanatory power of set-theoretic models.},
  langid = {english},
  keywords = {additive clustering,Bayesian inference,categorization,concept learning,contrast model,features,generalization,psychological space,similarity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/tenenbaum.j2001 Generalization, similarity, and Bayesian.pdf}
}

@inproceedings{tenney.i:2019,
  title = {What Do You Learn from Context? {{Probing}} for Sentence Structure in Contextualized Word Representations},
  booktitle = {7th International Conference on Learning Representations, {{ICLR}} 2019, New Orleans, {{LA}}, {{USA}}, May 6-9, 2019},
  author = {Tenney, Ian and Xia, Patrick and Chen, Berlin and Wang, Alex and Poliak, Adam and McCoy, R. Thomas and Kim, Najoung and Durme, Benjamin Van and Bowman, Samuel R. and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  publisher = {OpenReview.net},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/iclr/TenneyXCWPMKDBD19.bib},
  timestamp = {Thu, 25 Jul 2019 01:00:00 +0200}
}

@inproceedings{tenney.t:2019,
  title = {{{BERT}} Rediscovers the Classical {{NLP}} Pipeline},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Tenney, Ian and Das, Dipanjan and Pavlick, Ellie},
  year = {2019},
  pages = {4593--4601},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1452},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1452}
}

@inproceedings{terra.e:2003,
  title = {Frequency Estimates for Statistical Word Similarity Measures},
  booktitle = {Proceedings of the 2003 Human Language Technology Conference of the North {{American}} Chapter of the Association for Computational Linguistics},
  author = {Terra, Egidio L. and Clarke, Charles L. A.},
  year = {2003},
  pages = {244--251}
}

@book{tesniere.l:1959,
  title = {{\'E}lements de Syntaxe Structurale. {{Pr{\'e}f}}. de Jean Fourquet},
  author = {Tesni{\`e}re, Lucien},
  year = {1959},
  publisher = {C. Klincksieck},
  date-added = {2021-07-16 19:40:41 -0400},
  date-modified = {2021-07-16 19:40:43 -0400}
}

@book{tesniere.l:2015translation,
  title = {Elements of Structural Syntax},
  author = {Tesni{\`e}re, Lucien},
  translator = {Osborne, Timothy and Kahane, Sylvain},
  year = {2015},
  publisher = {John Benjamins Publishing Company},
  doi = {10.1075/z.185},
  bdsk-url-2 = {https://doi.org/10.1075/z.185},
  date-added = {2021-06-24 10:12:36 -0400},
  date-modified = {2021-06-24 10:13:38 -0400},
  isbn = {978-90-272-6999-7},
  langid = {english}
}

@inproceedings{thai.b:2020,
  title = {Fully {{Convolutional ASR}} for {{Less-Resourced Endangered Languages}}},
  booktitle = {Proceedings of the 1st {{Joint Workshop}} on {{Spoken Language Technologies}} for {{Under-resourced}} Languages ({{SLTU}}) and {{Collaboration}} and {{Computing}} for {{Under-Resourced Languages}} ({{CCURL}})},
  author = {Thai, Bao and Jimerson, Robert and Ptucha, Raymond and Prud'hommeaux, Emily},
  year = {2020},
  month = may,
  pages = {126--130},
  publisher = {European Language Resources association},
  address = {Marseille, France},
  urldate = {2022-06-06},
  abstract = {The application of deep learning to automatic speech recognition (ASR) has yielded dramatic accuracy increases for languages with abundant training data, but languages with limited training resources have yet to see accuracy improvements on this scale. In this paper, we compare a fully convolutional approach for acoustic modelling in ASR with a variety of established acoustic modeling approaches. We evaluate our method on Seneca, a low-resource endangered language spoken in North America. Our method yields word error rates up to 40\% lower than those reported using both standard GMM-HMM approaches and established deep neural methods, with a substantial reduction in training time. These results show particular promise for languages like Seneca that are both endangered and lack extensive documentation.},
  isbn = {979-10-95546-35-1},
  langid = {english},
  keywords = {automatic speech recognition,computational revitalization,iroquoian},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/thai.b2020 Fully Convolutional ASR for Less-Resourc.pdf}
}

@book{thrun.s:2005book,
  title = {Probabilistic Robotics},
  author = {Thrun, Sebastian and Burgard, Wolfram and Fox, Dieter},
  year = {2005},
  publisher = {MIT Press},
  date-added = {2022-05-05 10:19:08 -0400},
  date-modified = {2022-05-05 10:21:11 -0400},
  isbn = {978-0-262-36380-8}
}

@misc{timkey.w:2023arxiv,
  title = {A {{Language Model}} with {{Limited Memory Capacity Captures Interference}} in {{Human Sentence Processing}}},
  author = {Timkey, William and Linzen, Tal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16142},
  eprint = {2310.16142},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-09},
  abstract = {Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's single attention head captures semantic and syntactic interference effects observed in human experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,memory effects,parsing,processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/timkey.w2023arxiv A Language Model with Limited Memory Cap.pdf}
}

@misc{tishby.n:2000,
  title = {The Information Bottleneck Method},
  author = {Tishby, Naftali and Pereira, Fernando C. and Bialek, William},
  year = {2000},
  eprint = {physics/0004057},
  archiveprefix = {arXiv},
  date-added = {2020-07-21 08:44:18 -0400},
  date-modified = {2020-07-21 08:47:35 -0400},
  project = {syntactic embedding},
  keywords = {information bottleneck,information theory,variational inference}
}

@inproceedings{titov.i:2007,
  title = {A Latent Variable Model for Generative Dependency Parsing},
  booktitle = {Proceedings of the Tenth International Conference on Parsing Technologies},
  author = {Titov, Ivan and Henderson, James},
  year = {2007},
  month = jun,
  pages = {144--155},
  publisher = {Association for Computational Linguistics},
  address = {Prague, Czech Republic},
  date-added = {2022-04-26 17:35:02 -0400},
  date-modified = {2022-04-26 17:35:31 -0400},
  keywords = {Dependency Grammar,dependency parsing,generative grammar}
}

@misc{touvron.h:2023Llama,
  title = {{{LLaMA}}: {{Open}} and Efficient Foundation Language Models},
  shorttitle = {{{LLaMA}}},
  author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
  year = {2023},
  month = feb,
  number = {arXiv:2302.13971},
  eprint = {2302.13971},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-01},
  abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language}
}

@misc{touvron.h:2023Llama2,
  title = {Llama 2: {{Open}} Foundation and Fine-Tuned Chat Models},
  shorttitle = {Llama 2},
  author = {Touvron, Hugo and Martin, Louis and Stone, Kevin and Albert, Peter and Almahairi, Amjad and Babaei, Yasmine and Bashlykov, Nikolay and Batra, Soumya and Bhargava, Prajjwal and Bhosale, Shruti and Bikel, Dan and Blecher, Lukas and Ferrer, Cristian Canton and Chen, Moya and Cucurull, Guillem and Esiobu, David and Fernandes, Jude and Fu, Jeremy and Fu, Wenyin and Fuller, Brian and Gao, Cynthia and Goswami, Vedanuj and Goyal, Naman and Hartshorn, Anthony and Hosseini, Saghar and Hou, Rui and Inan, Hakan and Kardas, Marcin and Kerkez, Viktor and Khabsa, Madian and Kloumann, Isabel and Korenev, Artem and Koura, Punit Singh and Lachaux, Marie-Anne and Lavril, Thibaut and Lee, Jenya and Liskovich, Diana and Lu, Yinghai and Mao, Yuning and Martinet, Xavier and Mihaylov, Todor and Mishra, Pushkar and Molybog, Igor and Nie, Yixin and Poulton, Andrew and Reizenstein, Jeremy and Rungta, Rashi and Saladi, Kalyan and Schelten, Alan and Silva, Ruan and Smith, Eric Michael and Subramanian, Ranjan and Tan, Xiaoqing Ellen and Tang, Binh and Taylor, Ross and Williams, Adina and Kuan, Jian Xiang and Xu, Puxin and Yan, Zheng and Zarov, Iliyan and Zhang, Yuchen and Fan, Angela and Kambadur, Melanie and Narang, Sharan and Rodriguez, Aurelien and Stojnic, Robert and Edunov, Sergey and Scialom, Thomas},
  year = {2023},
  month = jul,
  number = {arXiv:2307.09288},
  eprint = {2307.09288},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-01},
  abstract = {In this work, we develop and release Llama 2, a collection of pretrained and fine-tuned large language models (LLMs) ranging in scale from 7 billion to 70 billion parameters. Our fine-tuned LLMs, called Llama 2-Chat, are optimized for dialogue use cases. Our models outperform open-source chat models on most benchmarks we tested, and based on our human evaluations for helpfulness and safety, may be a suitable substitute for closed-source models. We provide a detailed description of our approach to fine-tuning and safety improvements of Llama 2-Chat in order to enable the community to build on our work and contribute to the responsible development of LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@incollection{townsend.j:1974,
  title = {Issues and Models Concerning the Processing of a Finite Number of Inputs},
  booktitle = {Human Information Processing},
  author = {Townsend, James T.},
  year = {1974},
  publisher = {Routledge},
  abstract = {The broadest and most critical arena of investigation and contention with regard to these two matters has always been consciousness itself. Broadbent unveiled a theoretical structure that allowed collation of a substantial body of experimental literature and that was seminal in its influence on later developments. Hybrid models have been of limited current theoretical interest, probably due in part to the difficulty in testing them experimentally. As remarked earlier, the quite special case of seriality versus parallelism is difficult enough to discriminate experimentally. Among such hybrid models are those that represent processing as being serial part of the time and partially parallel within trials. There are many occasions in perceptual and memorial experiments where the information sufficient to make a correct response is embedded in only part of the total stimulus pattern presented to the subject. A finding of independence of total completion times, for instance, although perhaps more intuitively associated with parallel models, can be predicted by serial models.},
  isbn = {978-1-00-317668-8},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/townsend.j1974 Issues and models concerning the process.pdf}
}

@article{townsend.j:1990,
  title = {Serial vs. Parallel Processing: Sometimes They Look like Tweedledum and Tweedledee but They Can (and Should) Be Distinguished},
  shorttitle = {Serial vs. Parallel Processing},
  author = {Townsend, James T.},
  year = {1990},
  month = jan,
  journal = {Psychological science},
  volume = {1},
  number = {1},
  pages = {46--54},
  publisher = {SAGE Publications Inc},
  issn = {0956-7976},
  doi = {10.1111/j.1467-9280.1990.tb00067.x},
  urldate = {2022-06-24},
  abstract = {A number of important models of information processing depend on whether processing is serial or parallel. However, many of the studies purporting to settle the case use weak experimental paradigms or results to draw conclusions. A brief history of the issue is given along with examples from the literature. Then a number of promising methods are presented from a variety of sources with some discussion of their potential. A brief discussion of the topic with regard to overall issues of model testing and applications concludes the paper.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/townsend.j1990 Serial vs. parallel processing sometime.pdf}
}

@article{townsend.j:2004,
  title = {Parallel versus Serial Processing and Individual Differences in High-Speed Search in Human Memory},
  author = {Townsend, James T. and Fifi{\'c}, Mario},
  year = {2004},
  month = aug,
  journal = {Perception \& Psychophysics},
  volume = {66},
  number = {6},
  pages = {953--962},
  issn = {1532-5962},
  doi = {10.3758/BF03194987},
  urldate = {2022-06-24},
  abstract = {Many mental tasks that involve operations on a number of items take place within a few hundred milliseconds. In such tasks, whether the items are processed simultaneously (in parallel) or sequentially (serially) has long been of interest to psychologists. Although certain types of parallel and serial models have been ruled out, it has proven extremely difficult to entirely separate reasonable serial and limitedcapacity parallel models on the basis of typical data. Recent advances in theory-driven methodology now permit strong tests of serial versus parallel processing in such tasks, in ways that bypass the capacity issue and that are distribution and parameter free. We employ new methodologies to assess serial versus parallel processing and find strong evidence for pure serial or pure parallel processing, with some striking apparent differences across individuals and interstimulus conditions.},
  langid = {english},
  keywords = {Memory Search,Parallel Model,Parallel Processing,Serial Processing,Visual Search},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/townsend.j2004 Parallel versus serial processing and in.pdf}
}

@article{traxler.m:2002,
  title = {Processing Subject and Object Relative Clauses: {{Evidence}} from Eye Movements},
  shorttitle = {Processing Subject and Object Relative Clauses},
  author = {Traxler, Matthew J and Morris, Robin K and Seely, Rachel E},
  year = {2002},
  month = jul,
  journal = {Journal of Memory and Language},
  volume = {47},
  number = {1},
  pages = {69--90},
  issn = {0749-596X},
  doi = {10.1006/jmla.2001.2836},
  urldate = {2023-03-09},
  abstract = {Three eye-movement-monitoring experiments investigated processing of sentences containing subject-relative and object-relative clauses. The first experiment showed that sentences containing object-relative clauses were more difficult to process than sentences containing subject-relative clauses during the relative clause and the matrix verb. The second experiment manipulated the plausibility of the sentential subject and the noun within the relative clause as the agent of the action represented by the verb in the relative clause. Readers experienced greater difficulty during processing of sentences containing object-relative clauses than subject-relative clauses. The third experiment manipulated the animacy of the sentential subject and the noun within the relative clause. This experiment demonstrated that the difficulty associated with object-relative clauses was greatly reduced when the sentential subject was inanimate. We interpret the results with respect to theories of syntactic parsing.},
  langid = {english},
  keywords = {parsing,relative clauses,sentence processing,syntax,working memory.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/traxler.m2002 Processing subject and object relative c.pdf}
}

@inproceedings{trevisan.l:2009,
  title = {Regularity, {{Boosting}}, and {{Efficiently Simulating Every High-Entropy Distribution}}},
  booktitle = {2009 24th {{Annual IEEE Conference}} on {{Computational Complexity}}},
  author = {Trevisan, Luca and Tulsiani, Madhur and Vadhan, Salil},
  year = {2009},
  month = jul,
  pages = {126--136},
  issn = {1093-0159},
  doi = {10.1109/CCC.2009.41},
  abstract = {We show that every bounded function g: 0,1n rarr [0,1] admits an efficiently computable "simulator" function h: 0,1n rarr [0,1] such that every fixed polynomial size circuit has approximately the same correlation with g as with h. If g describes (up to scaling) a high min-entropy distribution D, then h can be used to efficiently sample a distribution D' of the same min-entropy that is indistinguishable from D by circuits of fixed polynomial size. We state and prove our result in a more abstract setting, in which we allow arbitrary finite domains instead of 0,1n, and arbitrary families of distinguishers, instead of fixed polynomial size circuits. Our result implies (a) the weak Szemeredi regularity Lemma of Frieze and Kannan (b) a constructive version of the dense model theorem of Green, Tao and Ziegler with better quantitative parameters (polynomial rather than exponential in the distinguishing probability), and (c) the Impagliazzo hardcore set Lemma. It appears to be the general result underlying the known connections between "regularity" results in graph theory, "decomposition" results in additive combinatorics, and the hardcore Lemma in complexity theory. We present two proofs of our result, one in the spirit of Nisan's proof of the hardcore Lemma via duality of linear programming, and one similar to Impagliazzo's "boosting" proof. A third proof by iterative partitioning, which gives the complexity of the sampler to be exponential in the distinguishing probability, is also implicit in the Green-Tao-Ziegler proofs of the dense model theorem.},
  keywords = {additive combinatorics,average-case complexity,boosting,Boosting,Circuit simulation,Combinatorial mathematics,Complexity theory,Computational complexity,Computational modeling,Computer science,Computer simulation,Graph theory,Polynomials,pseudorandomness},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/trevisan.l2009 Regularity, Boosting, and Efficiently Si.pdf}
}

@article{turing.a:1937TuringMachine,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}},
  author = {Turing, A. M.},
  year = {1937},
  journal = {Proceedings of the London Mathematical Society},
  volume = {s2-42},
  number = {1},
  pages = {230--265},
  issn = {1460-244X},
  doi = {10.1112/plms/s2-42.1.230},
  urldate = {2023-05-20},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/turing.a1937TuringMachine On computable numbers, with an applicati.pdf}
}

@article{turing.a:1938TuringMachineCorrection,
  title = {On Computable Numbers, with an Application to the {{Entscheidungsproblem}}. {{A}} Correction},
  author = {Turing, A. M.},
  year = {1938},
  journal = {Proceedings of the London Mathematical Society},
  volume = {s2-43},
  number = {1},
  pages = {544--546},
  issn = {1460-244X},
  doi = {10.1112/plms/s2-43.6.544},
  urldate = {2023-05-20},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/turing.a1938TuringMachineCorrection On computable numbers, with an applicati.pdf}
}

@article{turing.a:1950TuringTest,
  title = {Computing Machinery and Intelligence},
  author = {Turing, A. M.},
  year = {1950},
  month = oct,
  journal = {Mind},
  volume = {LIX},
  number = {236},
  pages = {433--460},
  issn = {0026-4423},
  doi = {10.1093/mind/LIX.236.433},
  urldate = {2023-05-20},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/turing.a1950TuringTest Computing machinery and intelligence.pdf}
}

@article{tversky.a:1971,
  title = {Belief in the Law of Small Numbers.},
  author = {Tversky, Amos and Kahneman, Daniel},
  year = {1971},
  journal = {Psychological Bulletin},
  volume = {76},
  number = {2},
  pages = {105--110},
  publisher = {American Psychological Association (APA)},
  doi = {10.1037/h0031322},
  bdsk-url-2 = {https://doi.org/10.1037/h0031322},
  date-added = {2021-08-08 20:24:01 -0400},
  date-modified = {2021-08-08 20:24:02 -0400}
}

@article{upper.d:1974,
  title = {The {{Unsuccessful Self-Treatment}} of a {{Case}} of ``{{Writer}}'s {{Block}}''1},
  author = {Upper, Dennis},
  year = {1974},
  journal = {Journal of Applied Behavior Analysis},
  volume = {7},
  number = {3},
  pages = {497--497},
  issn = {1938-3703},
  doi = {10.1901/jaba.1974.7-497a},
  urldate = {2022-06-16},
  langid = {english},
  keywords = {humor,writer's block},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/upper.d1974 The Unsuccessful Self-Treatment of a Cas.pdf}
}

@article{ussery.c:2017,
  title = {Dimensions of Variation},
  author = {Ussery, Cherlon},
  year = {2017},
  journal = {Syntactic variation in insular Scandinavian},
  volume = {1},
  pages = {165},
  publisher = {John Benjamins Publishing Company},
  date-added = {2019-06-14 09:19:24 -0400},
  date-modified = {2019-06-17 08:38:48 -0400},
  project = {Icelandic gluttony},
  keywords = {quirky case,syntactic variation}
}

@article{vandemeerendonk.n:2011,
  title = {Monitoring in Language Perception: {{Electrophysiological}} and Hemodynamic Responses to Spelling Violations},
  shorttitle = {Monitoring in Language Perception},
  author = {{van de Meerendonk}, Nan and Indefrey, Peter and Chwilla, Dorothee J. and Kolk, Herman H. J.},
  year = {2011},
  month = feb,
  journal = {NeuroImage},
  volume = {54},
  number = {3},
  pages = {2350--2363},
  issn = {1053-8119},
  doi = {10.1016/j.neuroimage.2010.10.022},
  urldate = {2022-06-24},
  abstract = {The monitoring theory of language perception proposes that competing representations that are caused by strong expectancy violations can trigger a conflict which elicits reprocessing of the input to check for possible processing errors. This monitoring process is thought to be reflected by the P600 component in the EEG. The present study further investigated this monitoring process by comparing syntactic and spelling violations in an EEG and an fMRI experiment. To assess the effect of conflict strength, misspellings were embedded in sentences that were weakly or strongly predictive of a critical word. In support of the monitoring theory, syntactic and spelling violations elicited similarly distributed P600 effects. Furthermore, the P600 effect was larger to misspellings in the strongly compared to the weakly predictive sentences. The fMRI results showed that both syntactic and spelling violations increased activation in the left inferior frontal gyrus (lIFG), while only the misspellings activated additional areas. Conflict strength did not affect the hemodynamic response to spelling violations. These results extend the idea that the lIFG is involved in implementing cognitive control in the presence of representational conflicts in general to the processing of errors in language perception.},
  langid = {english},
  keywords = {Cognitive control,Conflict,Left inferior frontal gyrus,P600,Reprocessing}
}

@article{vandermude.a:1978,
  title = {On the Inference of Stochastic Regular Grammars},
  author = {{Van der Mude}, Antony and Walker, Adrian},
  year = {1978},
  journal = {Information and Control},
  volume = {38},
  number = {3},
  pages = {310--329},
  publisher = {Elsevier},
  doi = {10.1016/S0019-9958(78)90106-7},
  date-added = {2020-05-05 13:00:02 -0400},
  date-modified = {2021-07-16 11:33:31 -0400},
  project = {syntactic embedding},
  keywords = {dependency parsing,mutual information}
}

@article{vandyke.j:2006,
  title = {Retrieval Interference in Sentence Comprehension},
  author = {Van Dyke, Julie A. and McElree, Brian},
  year = {2006},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {55},
  number = {2},
  pages = {157--166},
  issn = {0749596X},
  doi = {10.1016/j.jml.2006.03.007},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vandyke.j2006 Retrieval interference in sentence compr.pdf}
}

@article{vanerven.t:2014,
  title = {R{\'e}nyi Ivergence and {{Kullback-Leibler}} Divergence},
  author = {{van Erven}, Tim and Harremos, Peter},
  year = {2014},
  month = jul,
  journal = {IEEE Transactions on Information Theory},
  volume = {60},
  number = {7},
  pages = {3797--3820},
  issn = {1557-9654},
  doi = {10.1109/TIT.2014.2320500},
  abstract = {R{\'e}nyi divergence is related to R{\'e}nyi entropy much like Kullback-Leibler divergence is related to Shannon's entropy, and comes up in many settings. It was introduced by R{\'e}nyi as a measure of information that satisfies almost the same axioms as Kullback-Leibler divergence, and depends on a parameter that is called its order. In particular, the R{\'e}nyi divergence of order 1 equals the Kullback-Leibler divergence. We review and extend the most important properties of R{\'e}nyi divergence and Kullback-Leibler divergence, including convexity, continuity, limits of {\textbackslash}({\textbackslash}sigma {\textbackslash}) -algebras, and the relation of the special order 0 to the Gaussian dichotomy and contiguity. We also show how to generalize the Pythagorean inequality to orders different from 1, and we extend the known equivalence between channel capacity and minimax redundancy to continuous channel inputs (for all orders) and present several other minimax results.},
  keywords = {alpha-divergence,Bhattacharyya distance,Convergence,Data processing,Entropy,information divergence,Kullback-Leibler divergence,Markov processes,Pythagorean inequality,Q measurement,Renyi divergence},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vanerven.t2014 Rényi ivergence and Kullback-Leibler div.pdf}
}

@article{vani.p:2021,
  title = {Using the Interpolated Maze Task to Assess Incremental Processing in {{English}} Relative Clauses},
  author = {Vani, Pranali and Wilcox, Ethan Gotlieb and Levy, Roger},
  year = {2021},
  journal = {Proceedings of the 43rd Annual Meeting of the Cognitive Science Society},
  urldate = {2023-03-09},
  abstract = {In English, Subject Relative Clauses are processed more quickly than Object Relative Clauses, but open questions remain about where in the clause slowdown occurs. The surprisal theory of incremental processing, under which processing difficulty corresponds to probabilistic expectations about upcoming material, predicts that slowdown should occur immediately on material that disambiguates the subject from object relative clause. However, evidence from eye tracking and self-paced reading studies suggests that slowdown occurs downstream of RC-disambiguating material, on the relative clause verb. These methods, however, suffer from well-known spillover effects which makes their results difficult to interpret. To address these issues, we introduce and deploy a novel variant of the Maze task for reading times (Forster, Guerrera, \&amp; Elliot, 2009), called the Interpolated Maze in two English web-based experiments. In Experiment 1, we find that the locus of reading-time differences between SRCs and ORCs falls on immediate disambiguating definite determiner. Experiment 2 provides a control, showing that ORCs are read more slowly than lexically-matching, non-anomalous material. These results provide new evidence for the locus of processing difficulty in relative clauses and support the surprisal theory of incremental processing.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vani.p2021 Using the interpolated maze task to asse.pdf}
}

@article{vanoostendorp.h:1990,
  title = {Failing to Notice Errors in Sentences},
  author = {{van Oostendorp}, Herre and Kok, Ineke},
  year = {1990},
  month = apr,
  journal = {Language and Cognitive Processes},
  volume = {5},
  number = {2},
  pages = {105--113},
  publisher = {Routledge},
  issn = {0169-0965},
  doi = {10.1080/01690969008402100},
  urldate = {2024-05-28},
  abstract = {The purpose of the present study was to investigate the role of conceptual relatedness of proper names in failure to notice errors in sentences. We hypothesised that sentences with highly related, but inaccurate proper names may lead to less complete processing than sentences with less related proper names, and hence to failure to notice factual inaccuracies. This hypothesis was investigated in two ways: first, by using inaccurate proper names varying in degree of overlap in attributes with target names; and, secondly, by using a paired-associate learning task in order to strengthen relations. The results indicate that greater overlap between proper names and stronger relations lead to more frequent failures to notice errors in sentences.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vanoostendorp.h1990 Failing to notice errors in sentences.pdf}
}

@article{vanoostendorp.h:1990a,
  title = {Moses Beats {{Adam}}: {{A}} Semantic Relatedness Effect on a Semantic Illusion},
  shorttitle = {Moses Beats {{Adam}}},
  author = {Van Oostendorp, Herre and De Mul, Sjaak},
  year = {1990},
  month = jun,
  journal = {Acta Psychologica},
  volume = {74},
  number = {1},
  pages = {35--46},
  issn = {0001-6918},
  doi = {10.1016/0001-6918(90)90033-C},
  urldate = {2024-05-28},
  abstract = {The purpose of the present study is to investigate the role of semantic relatedness in the occurence of semantic illusions like the Moses illusion (first described by Erickson and Mattson 1981). This illusion is investigated by using statements with inaccurate proper names varying in degree of overlap in attributes with target names and by registering judgment times. The results show a clear effect of semantic relatedness. Inaccuracies more often appeared to be left unnoticed in high-related statements than in low-related statements. Furthermore, the results of the corresponding judgment times indicate that judging statements with high-related inaccurate names need the same amount of time as judging statements with low-related names. However, more errors are made in this same amount of time, which indicates that high-related statements are processed less extensively. Finally, to achieve a precise judgment of high-related statements takes more time compared with low-related statements. In other words, more time is needed to unmask a high-related inaccuracy.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vanoostendorp.h1990a Moses beats Adam A semantic relatedness.pdf}
}

@misc{vanos.m:2022amlap,
  type = {Poster},
  title = {Rational Speech Comprehension: Interaction between Predictability, Acoustic Signal, and Noise},
  author = {{van Os}, Marjolein and Kray, Jutta and Demberg, Vera},
  year = {2022},
  month = sep,
  address = {York, England},
  urldate = {2022-09-09},
  annotation = {link-abstract: https://oxford-abstracts.s3.amazonaws.com/2dbcb575-3c17-4635-a1b8-24e1e8b44d3d.pdf\\
link-poster: https://oxford-abstracts.s3.amazonaws.com/0e28630c-2678-413b-bb42-daa834276886.pdf}
}

@article{vanos.m:2022frontiers,
  title = {Rational Speech Comprehension: {{Interaction}} between Predictability, Acoustic Signal, and Noise},
  shorttitle = {Rational Speech Comprehension},
  author = {{van Os}, Marjolein and Kray, Jutta and Demberg, Vera},
  year = {2022},
  month = dec,
  journal = {Frontiers in Psychology},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.914239},
  urldate = {2024-05-05},
  abstract = {{$<$}sec{$><$}title{$>$}Introduction{$<$}/title{$><$}p{$>$}During speech comprehension, multiple sources of information are available to listeners, which are combined to guide the recognition process. Models of speech comprehension posit that when the acoustic speech signal is obscured, listeners rely more on information from other sources. However, these models take into account only word frequency information and local contexts (surrounding syllables), but not sentence-level information. To date, empirical studies investigating predictability effects in noise did not carefully control the tested speech sounds, while the literature investigating the effect of background noise on the recognition of speech sounds does not manipulate sentence predictability. Additionally, studies on the effect of background noise show conflicting results regarding which noise type affects speech comprehension most. We address this in the present experiment.{$<$}/p{$><$}/sec{$><$}sec{$><$}title{$>$}Methods{$<$}/title{$><$}p{$>$}We investigate how listeners combine information from different sources when listening to sentences embedded in background noise. We manipulate top-down predictability, type of noise, and characteristics of the acoustic signal, thus creating conditions which differ in the extent to which a specific speech sound is masked in a way that is grounded in prior work on the confusability of speech sounds in noise. Participants complete an online word recognition experiment.{$<$}/p{$><$}/sec{$><$}sec{$><$}title{$>$}Results and discussion{$<$}/title{$><$}p{$>$}The results show that participants rely more on the provided sentence context when the acoustic signal is harder to process. This is the case even when interactions of the background noise and speech sounds lead to small differences in intelligibility. Listeners probabilistically combine top-down predictions based on context with noisy bottom-up information from the acoustic signal, leading to a trade-off between the different types of information that is dependent on the combination of a specific type of background noise and speech sound.{$<$}/p{$><$}/sec{$>$}},
  langid = {english},
  keywords = {background noise,Mishearing,Noisy channel,predictive context,rational processing,speech comprehension},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vanos.m2022frontiers Rational speech comprehension Interacti.pdf}
}

@article{varda.a:2024aligned,
  title = {Cloze Probability, Predictability Ratings, and Computational Estimates for 205 {{English}} Sentences, Aligned with Existing {{EEG}} and Reading Time Data},
  author = {{de Varda}, Andrea Gregor and Marelli, Marco and Amenta, Simona},
  year = {2024},
  month = aug,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {5},
  pages = {5190--5213},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02261-8},
  urldate = {2024-10-17},
  abstract = {We release a database of cloze probability values, predictability ratings, and computational estimates for a sample of 205 English sentences (1726 words), aligned with previously released word-by-word reading time data (both self-paced reading and eye-movement records; Frank et al., Behavior Research Methods, 45(4), 1182--1190. 2013) and EEG responses (Frank et al., Brain and Language, 140, 1--11. 2015). Our analyses show that predictability ratings are the best predictors of the EEG signal (N400, P600, LAN) self-paced reading times, and eye movement patterns, when spillover effects are taken into account. The computational estimates are particularly effective at explaining variance in the eye-tracking data without spillover. Cloze probability estimates have decent overall psychometric accuracy and are the best predictors of early fixation patterns (first fixation duration). Our results indicate that the choice of the best measurement of word predictability in context critically depends on the processing index being considered.},
  langid = {english},
  keywords = {Cloze probability,Predictability ratings,Prediction,Surprisal estimates},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/varda.a2024aligned Cloze probability, predictability rating.pdf}
}

@article{vasishth.s:2006,
  title = {Argument-Head Distance and Processing Complexity: Explaining Both Locality and Antilocality Effects},
  author = {Vasishth, Shravan and Lewis, Richard L.},
  year = {2006},
  journal = {Language},
  volume = {82},
  number = {4},
  eprint = {4490268},
  eprinttype = {jstor},
  pages = {767--794},
  publisher = {Linguistic Society of America},
  issn = {00978507, 15350665},
  abstract = {Although proximity between arguments and verbs (locality) is a relatively robust determinant of sentence-processing difficulty (Hawkins 1998, 2001, Gibson 2000), increasing argument-verb distance can also facilitate processing (Konieczny 2000). We present two self-paced reading (SPR) experiments involving Hindi that provide further evidence of antilocality, and a third SPR experiment which suggests that similarity-based interference can attenuate this distance-based facilitation. A unified explanation of interference, locality, and antilocality effects is proposed via an independently motivated theory of activation decay and retrieval interference (Anderson et al. 2004).},
  date-added = {2022-03-31 11:51:04 -0400},
  date-modified = {2022-03-31 11:52:05 -0400},
  keywords = {antilocality effects,Dependency locality theory,locality effects,processing,processing complexity,self-paced reading}
}

@inproceedings{vasishth.s:2006icle,
  title = {On the Proper Treatment of Spillover in Real-Time Reading Studies: {{Consequences}} for Psycholinguistic Theories},
  booktitle = {Proceedings of the International Conference on Linguistic Evidence},
  author = {Vasishth, Shravan},
  year = {2006},
  pages = {96--100}
}

@article{vasishth.s:2010,
  title = {Short-Term Forgetting in Sentence Comprehension: Crosslinguistic Evidence from Verb-Final Structures},
  author = {Vasishth, Shravan and Suckow, Katja and Lewis, Richard L. and Kern, Sabine},
  year = {2010},
  month = may,
  journal = {Language and Cognitive Processes},
  volume = {25},
  number = {4},
  pages = {533--567},
  publisher = {Informa UK Limited},
  doi = {10.1080/01690960903310587},
  bdsk-url-2 = {https://doi.org/10.1080/01690960903310587},
  date-added = {2022-04-19 22:48:38 -0400},
  date-modified = {2022-04-19 22:48:39 -0400}
}

@article{vasishth.s:2018,
  title = {The Statistical Significance Filter Leads to Overoptimistic Expectations of Replicability},
  author = {Vasishth, Shravan and Mertzen, Daniela and J{\"a}ger, Lena A. and Gelman, Andrew},
  year = {2018},
  month = dec,
  journal = {Journal of Memory and Language},
  volume = {103},
  pages = {151--175},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2018.07.004},
  urldate = {2022-07-01},
  abstract = {It is well-known in statistics (e.g., Gelman \& Carlin, 2014) that treating a result as publishable just because the p-value is less than 0.05 leads to overoptimistic expectations of replicability. These effects get published, leading to an overconfident belief in replicability. We demonstrate the adverse consequences of this statistical significance filter by conducting seven direct replication attempts (268 participants in total) of a recent paper (Levy \& Keller, 2013). We show that the published claims are so noisy that even non-significant results are fully compatible with them. We also demonstrate the contrast between such small-sample studies and a larger-sample study; the latter generally yields a less noisy estimate but also a smaller effect magnitude, which looks less compelling but is more realistic. We reiterate several suggestions from the methodology literature for improving current practices.},
  langid = {english},
  keywords = {Bayesian data analysis,Expectation,Locality,Parameter estimation,Replicability,Surprisal,Type M error},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vasishth.s2018 The statistical significance filter lead.pdf}
}

@article{vasishth.s:2019,
  title = {Computational Models of Retrieval Processes in Sentence Processing},
  author = {Vasishth, Shravan and Nicenboim, Bruno and Engelmann, Felix and Burchert, Frank},
  year = {2019},
  month = nov,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {11},
  pages = {968--982},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.09.003},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vasishth.s2019 Computational models of retrieval proces.pdf}
}

@book{vasishth.s:2021book,
  title = {Sentence Comprehension as a Cognitive Process: A Computational Approach},
  shorttitle = {Sentence Comprehension as a Cognitive Process},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  month = oct,
  edition = {1},
  publisher = {Cambridge University Press},
  doi = {10.1017/9781316459560},
  urldate = {2022-10-12},
  abstract = {Sentence comprehension - the way we process and understand spoken and written language - is a central and important area of research within psycholinguistics. This book explores the contribution of computational linguistics to the field, showing how computational models of sentence processing can help scientists in their investigation of human cognitive processes. It presents the leading computational model of retrieval processes in sentence processing, the Lewis and Vasishth cue-based retrieval mode, and develops a principled methodology for parameter estimation and model comparison/evaluation using benchmark data, to enable researchers to test their own models of retrieval against the present model. It also provides readers with an overview of the last 20 years of research on the topic of retrieval processes in sentence comprehension, along with source code that allows researchers to extend the model and carry out new research. Comprehensive in its scope, this book is essential reading for researchers in cognitive science.},
  isbn = {978-1-316-45956-0}
}

@incollection{vasishth.s:2021bookch3,
  title = {The Core {{ACT-R-based}} Model of Retrieval Processes},
  booktitle = {Sentence {{Comprehension}} as a {{Cognitive Process}}: {{A Computational Approach}}},
  author = {Vasishth, Shravan and Engelmann, Felix},
  year = {2021},
  pages = {49--70},
  publisher = {Cambridge University Press},
  address = {Cambridge, England},
  doi = {10.1017/9781316459560.008},
  urldate = {2022-10-12},
  abstract = {The core model of sentence processing used in the book is introduced and its empirical coverage relative to the existing reading time data is considered. Here, we also discuss the Approximate Bayesian Computation method for parameter estimation for model evaluation.},
  isbn = {978-1-107-13311-2},
  keywords = {ACT-R,cue-based retrieval,parsing,psycholinguistics,sentence comprehension},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vasishth.s2021bookch3 The core ACT-R-based model of retrieval.pdf}
}

@inproceedings{vaswani.a:2017,
  title = {Attention Is All You Need},
  booktitle = {Advances in Neural Information Processing Systems 30: {{Annual}} Conference on Neural Information Processing Systems 2017, {{December}} 4-9, 2017, {{Long Beach}}, {{CA}}, {{USA}}},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  editor = {Guyon, Isabelle and {von Luxburg}, Ulrike and Bengio, Samy and Wallach, Hanna M. and Fergus, Rob and Vishwanathan, S. V. N. and Garnett, Roman},
  year = {2017},
  pages = {5998--6008},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VaswaniSPUJGKP17.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@article{verdu.s:1998,
  title = {Fifty Years of {{Shannon}} Theory},
  author = {Verdu, S.},
  year = {1998},
  journal = {IEEE Transactions on information theory},
  volume = {44},
  number = {6},
  pages = {2057--2078},
  issn = {1557-9654},
  doi = {10.1109/18.720531},
  abstract = {A brief chronicle is given of the historical development of the central problems in the theory of fundamental limits of data compression and reliable communication.},
  date-added = {2020-08-17 11:37:37 -0400},
  date-modified = {2020-08-17 11:39:25 -0400},
  project = {information-entropy},
  keywords = {channel capacity,data compression,information theory,rate distortion theory,source coding}
}

@misc{vieira.t:2014blog,
  type = {Blog},
  title = {Gumbel-Max Trick and Weighted Reservoir Sampling},
  author = {Vieira, Tim},
  year = {2014},
  month = aug,
  journal = {Graduate Descent},
  urldate = {2022-11-06},
  howpublished = {https://timvieira.github.io/blog/post/2014/08/01/gumbel-max-trick-and-weighted-reservoir-sampling/}
}

@article{vieira.t:2017,
  title = {Learning to Prune: Exploring the Frontier of Fast and Accurate Parsing},
  author = {Vieira, Tim and Eisner, Jason},
  year = {2017},
  month = aug,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {5},
  pages = {263--278},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00060},
  abstract = {Pruning hypotheses during dynamic programming is commonly used to speed up inference in settings such as parsing. Unlike prior work, we train a pruning policy under an objective that measures end-to-end performance: we search for a fast and accurate policy. This poses a difficult machine learning problem, which we tackle with the lols algorithm. lols training must continually compute the effects of changing pruning decisions: we show how to make this efficient in the constituency parsing setting, via dynamic programming and change propagation algorithms. We find that optimizing end-to-end performance in this way leads to a better Pareto frontier---i.e., parsers which are more accurate for a given runtime.},
  keywords = {chart parsing,pruning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vieira.t2017 Learning to prune exploring the frontie.pdf}
}

@inproceedings{vijayakumar.a:2018,
  title = {Diverse Beam Search for Improved Description of Complex Scenes},
  booktitle = {Proceedings of the Thirty-Second {{AAAI}} Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth {{AAAI}} Symposium on Educational Advances in Artificial Intelligence},
  author = {Vijayakumar, Ashwin K. and Cogswell, Michael and Selvaraju, Ramprasaath R. and Sun, Qing and Lee, Stefan and Crandall, David and Batra, Dhruv},
  year = {2018},
  series = {{{AAAI}}'18/{{IAAI}}'18/{{EAAI}}'18},
  publisher = {AAAI Press},
  address = {New Orleans, Louisiana, USA},
  abstract = {A single image captures the appearance and position of multiple entities in a scene as well as their complex interactions. As a consequence, natural language grounded in visual contexts tends to be diverse -- with utterances differing as focus shifts to specific objects, interactions, or levels of detail. Recently, neural sequence models such as RNNs and LSTMs have been employed to produce visually-grounded language. Beam Search, the standard work-horse for decoding sequences from these models, is an approximate inference algorithm that decodes the top-B sequences in a greedy left-to-right fashion. In practice, the resulting sequences are often minor rewordings of a common utterance, failing to capture the multimodal nature of source images. To address this shortcoming, we propose Diverse Beam Search (DBS), a diversity promoting alternative to BS for approximate inference. DBS produces sequences that are significantly different from each other by incorporating diversity constraints within groups of candidate sequences during decoding; moreover, it achieves this with minimal computational or memory overhead. We demonstrate that our method improves both diversity and quality of decoded sequences over existing techniques on two visually-grounded language generation tasks -- image captioning and visual question generation -- particularly on complex scenes containing diverse visual content. We also show similar improvements at language-only machine translation tasks, highlighting the generality of our approach.},
  articleno = {903},
  date-added = {2022-03-25 22:32:37 -0400},
  date-modified = {2022-03-25 22:34:11 -0400},
  isbn = {978-1-57735-800-8}
}

@misc{vilnis.l:2022arxiv,
  title = {Arithmetic Sampling: Parallel Diverse Decoding for Large Language Models},
  shorttitle = {Arithmetic Sampling},
  author = {Vilnis, Luke and Zemlyanskiy, Yury and Murray, Patrick and Passos, Alexandre and Sanghai, Sumit},
  year = {2022},
  month = oct,
  number = {arXiv:2210.15458},
  eprint = {2210.15458},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.15458},
  urldate = {2022-11-07},
  abstract = {Decoding methods for large language models often trade-off between diversity of outputs and parallelism of computation. Methods such as beam search and Gumbel top-k sampling can guarantee a different output for each element of the beam, but are not easy to parallelize. Alternatively, methods such as temperature sampling and its modifications (top-k sampling, nucleus sampling, typical decoding, and others), are embarrassingly parallel, but have no guarantees about duplicate samples. We present a framework for sampling according to an arithmetic code book implicitly defined by a large language model, compatible with common sampling variations, with provable beam diversity under certain conditions, as well as being embarrassingly parallel and providing unbiased and consistent expectations from the original model. We demonstrate the effectiveness of our approach on WMT machine translation, showing substantially reduced variance when estimating expected BLEU score and up to 1 point increased BLEU in oracle experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/vilnis.l2022 Arithmetic sampling parallel diverse de.pdf}
}

@article{vincent.p:2010,
  title = {Stacked Denoising Autoencoders: {{Learning}} Useful Representations in a Deep Network with a Local Denoising Criterion},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  year = {2010},
  month = dec,
  journal = {Journal of Machine Learning Research},
  volume = {11},
  pages = {3371--3408},
  publisher = {JMLR.org},
  issn = {1532-4435},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  issue_date = {3/1/2010}
}

@inproceedings{vinyals.o:2014,
  title = {Grammar as a Foreign Language},
  booktitle = {Advances in Neural Information Processing Systems 28: {{Annual}} Conference on Neural Information Processing Systems 2015, December 7-12, 2015, Montreal, Quebec, Canada},
  author = {Vinyals, Oriol and Kaiser, Lukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey E.},
  editor = {Cortes, Corinna and Lawrence, Neil D. and Lee, Daniel D. and Sugiyama, Masashi and Garnett, Roman},
  year = {2015},
  pages = {2773--2781},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/VinyalsKKPSH15.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@inproceedings{voita.e:2020mdlprobing,
  title = {Information-Theoretic Probing with Minimum Description Length},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ({{EMNLP}})},
  author = {Voita, Elena and Titov, Ivan},
  year = {2020},
  pages = {183--196},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-main.14},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-main.14}
}

@article{vul.e:2014,
  title = {One and Done? {{Optimal}} Decisions from Very Few Samples},
  author = {Vul, Edward and Goodman, Noah and Griffiths, Thomas L. and Tenenbaum, Joshua B.},
  year = {2014},
  journal = {Cognitive Science},
  volume = {38},
  number = {4},
  pages = {599--637},
  publisher = {Wiley},
  doi = {10.1111/cogs.12101},
  date-added = {2021-03-16 23:51:30 -0400},
  date-modified = {2021-03-16 23:51:30 -0400},
  keywords = {bayesian,bounded rationality,inference algorithms,sampling}
}

@article{vulkan.n:2000,
  title = {An Economist's Perspective on Probability Matching},
  author = {Vulkan, Nir},
  year = {2000},
  journal = {Journal of Economic Surveys},
  volume = {14},
  number = {1},
  eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1111/1467-6419.00106},
  pages = {101--118},
  doi = {10.1111/1467-6419.00106},
  abstract = {The experimental phenomenon known as `probability matching' is often offered as evidence in support of adaptive learning models and against the idea that people maximise their expected utility. Recent interest in dynamic-based equilibrium theories means the term re-appears in Economics. However, there seems to be conflicting views on what is actually meant by the term and about the validity of the data. The purpose of this paper is therefore threefold: First, to introduce today's readers to what is meant by probability matching, and in particular to clarify which aspects of this phenomenon challenge the utility-maximisation hypothesis. Second, to familiarise the reader with the different theoretical approaches to behaviour in such circumstances, and to focus on the differences in predictions between these theories in light of recent advances. Third, to provide a comprehensive survey of repeated, binary choice experiments.},
  bdsk-url-2 = {https://doi.org/10.1111/1467-6419.00106},
  date-added = {2021-05-31 13:57:04 -0400},
  date-modified = {2021-05-31 13:57:05 -0400},
  keywords = {Optimisation,Probability matching,Stochastic learning}
}

@article{wagers.m:2009,
  title = {Agreement Attraction in Comprehension: {{Representations}} and Processes},
  shorttitle = {Agreement Attraction in Comprehension},
  author = {Wagers, Matthew W. and Lau, Ellen F. and Phillips, Colin},
  year = {2009},
  month = aug,
  journal = {Journal of Memory and Language},
  volume = {61},
  number = {2},
  pages = {206--237},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2009.04.002},
  urldate = {2023-04-03},
  abstract = {Much work has demonstrated so-called attraction errors in the production of subject--verb agreement (e.g., `The key to the cabinets are on the table', [Bock, J. K., \& Miller, C. A. (1991). Broken agreement. Cognitive Psychology, 23, 45--93]), in which a verb erroneously agrees with an intervening noun. Six self-paced reading experiments examined the online mechanisms underlying the analogous attraction effects that have been shown in comprehension; namely reduced disruption for subject--verb agreement violations when these `attractor' nouns intervene. One class of theories suggests that these effects are rooted in faulty representation of the number of the subject, while another class of theories suggests instead that such effects arise in the process of re-accessing subject number at the verb. Two main findings provide evidence against the first class of theories. First, attraction also occurs in relative clause configurations in which the attractor noun does not intervene between subject and verb and is not in a direct structural relationship with the subject head (e.g., `The drivers who the runner wave to each morning'). Second, we observe a `grammatical asymmetry': attraction effects are limited to ungrammatical sentences, which would be unexpected if the representation of subject number were inherently prone to error. We argue that agreement attraction in comprehension instead reflects a cue-based retrieval mechanism that is subject to retrieval errors. The grammatical asymmetry can be accounted for under one implementation that we propose, or if the mechanism is only called upon when the predicted agreement features fail to be instantiated on the verb.},
  langid = {english},
  keywords = {Agreement,agreement attraction,Comprehension,Prediction,Retrieval,Syntax},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wagers.m2009 Agreement attraction in comprehension R.pdf}
}

@article{wainwright.m:2007,
  title = {Graphical Models, Exponential Families, and Variational Inference},
  author = {Wainwright, Martin J. and Jordan, Michael I.},
  year = {2007},
  journal = {Foundations and Trends in Machine Learning},
  volume = {1},
  number = {1--2},
  pages = {1--305},
  issn = {1935-8237, 1935-8245},
  doi = {10.1561/2200000001},
  urldate = {2024-05-24},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wainwright.m2007 Graphical models, exponential families,.pdf}
}

@article{wald.a:1939,
  title = {Contributions to the {{Theory}} of {{Statistical Estimation}} and {{Testing Hypotheses}}},
  author = {Wald, Abraham},
  year = {1939},
  month = dec,
  journal = {The Annals of Mathematical Statistics},
  volume = {10},
  number = {4},
  pages = {299--326},
  publisher = {Institute of Mathematical Statistics},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177732144},
  urldate = {2022-05-21},
  abstract = {The Annals of Mathematical Statistics},
  keywords = {decision theory}
}

@article{wald.a:1945,
  title = {Sequential {{Tests}} of {{Statistical Hypotheses}}},
  author = {Wald, A.},
  year = {1945},
  month = jun,
  journal = {The Annals of Mathematical Statistics},
  volume = {16},
  number = {2},
  pages = {117--186},
  issn = {0003-4851},
  doi = {10.1214/aoms/1177731118},
  urldate = {2023-11-04},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wald.a1945 Sequential Tests of Statistical Hypothes.pdf}
}

@article{wald.a:1947,
  title = {Foundations of a General Theory of Sequential Decision Functions},
  author = {Wald, Abraham},
  year = {1947},
  journal = {Econometrica},
  volume = {15},
  number = {4},
  eprint = {1905331},
  eprinttype = {jstor},
  pages = {279--313},
  publisher = {[Wiley, Econometric Society]},
  issn = {0012-9682},
  doi = {10.2307/1905331},
  urldate = {2022-07-04}
}

@book{walters.p:1982,
  title = {An Introduction to Ergodic Theory},
  author = {Walters, Peter},
  year = {1982},
  series = {Graduate Texts in Mathematics},
  number = {79},
  publisher = {Springer},
  address = {New York},
  abstract = {This text provides an introduction to ergodic theory suitable for readers knowing basic measure theory. The mathematical prerequisites are summarized in Chapter 0. It is hoped the reader will be ready to tackle research papers after reading the book. The first part of the text is concerned with measure-preserving transformations of probability spaces; recurrence properties, mixing properties, the Birkhoff ergodic theorem, isomorphism and spectral isomorphism, and entropy theory are discussed. Some examples are described and are studied in detail when new properties are presented. The second part of the text focuses on the ergodic theory of continuous transformations of compact metrizable spaces. The family of invariant probability measures for such a transformation is studied and related to properties of the transformation such as topological traitivity, minimality, the size of the non-wandering set, and existence of periodic points. Topological entropy is introduced and related to measure-theoretic entropy. Topological pressure and equilibrium states are discussed, and a proof is given of the variational principle that relates pressure to measure-theoretic entropies. Several examples are studied in detail. The final chapter outlines significant results and some applications of ergodic theory to other branches of mathematics.},
  isbn = {978-0-387-95152-2},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/walters.p1982 An introduction to ergodic theory.djvu}
}

@inproceedings{wang.a:2019,
  title = {{{SuperGLUE}}: {{A}} Stickier Benchmark for General-Purpose Language Understanding Systems},
  booktitle = {Advances in Neural Information Processing Systems 32: {{Annual}} Conference on Neural Information Processing Systems 2019, {{NeurIPS}} 2019, December 8-14, 2019, Vancouver, {{BC}}, Canada},
  author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  pages = {3261--3275},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/WangPNSMHLB19.bib},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100}
}

@misc{wang.b:2021GPT-J-6B,
  title = {{{GPT-J-6B}}: {{A}} 6 Billion Parameter Autoregressive Language Model},
  author = {Wang, Ben and Komatsuzaki, Aran},
  year = {2021},
  month = may,
  date-added = {2021-11-30 10:11:28 -0500},
  date-modified = {2021-12-13 19:43:33 -0500},
  howpublished = {Software}
}

@article{wang.l:2009,
  title = {Semantic Illusion Depends on Information Structure: {{ERP}} Evidence},
  shorttitle = {Semantic Illusion Depends on Information Structure},
  author = {Wang, Lin and Hagoort, Peter and Yang, Yufang},
  year = {2009},
  month = jul,
  journal = {Brain Research},
  volume = {1282},
  pages = {50--56},
  issn = {0006-8993},
  doi = {10.1016/j.brainres.2009.05.069},
  urldate = {2024-05-28},
  abstract = {Next to propositional content, speakers distribute information in their utterances in such a way that listeners can make a distinction between new (focused) and given (non-focused) information. This is referred to as information structure. We measured event-related potentials (ERPs) to explore the role of information structure in semantic processing. Following different questions in wh-question--answer pairs (e.g. What kind of vegetable did Ming buy for cooking today?/Who bought the vegetables for cooking today?), the answer sentences (e.g., Ming bought eggplant/beef to cook today.) contained a critical word, which was either semantically appropriate (eggplant) or inappropriate (beef), and either focus or non-focus. The results showed a full N400 effect only when the critical words were in focus position. In non-focus position a strongly reduced N400 effect was observed, in line with the well-known semantic illusion effect. The results suggest that information structure facilitates semantic processing by devoting more resources to focused information.},
  keywords = {Information structure,N400 effect,Semantic illusion,Semantic integration,Wh-question-answer pair},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wang.l2009 Semantic illusion depends on information.pdf}
}

@misc{wang.t:2022arxiv,
  title = {What Language Model Architecture and Pretraining Objective Work Best for Zero-Shot Generalization?},
  author = {Wang, Thomas and Roberts, Adam and Hesslow, Daniel and Scao, Teven Le and Chung, Hyung Won and Beltagy, Iz and Launay, Julien and Raffel, Colin},
  year = {2022},
  month = apr,
  number = {arXiv:2204.05832},
  eprint = {2204.05832},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2023-05-25},
  abstract = {Large pretrained Transformer language models have been shown to exhibit zero-shot generalization, i.e. they can perform a wide variety of tasks that they were not explicitly trained on. However, the architectures and pretraining objectives used across state-of-the-art models differ significantly, and there has been limited systematic comparison of these factors. In this work, we present a large-scale evaluation of modeling choices and their impact on zero-shot generalization. In particular, we focus on text-to-text models and experiment with three model architectures (causal/non-causal decoder-only and encoder-decoder), trained with two different pretraining objectives (autoregressive and masked language modeling), and evaluated with and without multitask prompted finetuning. We train models with over 5 billion parameters for more than 170 billion tokens, thereby increasing the likelihood that our conclusions will transfer to even larger scales. Our experiments show that causal decoder-only models trained on an autoregressive language modeling objective exhibit the strongest zero-shot generalization after purely unsupervised pretraining. However, models with non-causal visibility on their input trained with a masked language modeling objective followed by multitask finetuning perform the best among our experiments. We therefore consider the adaptation of pretrained models across architectures and objectives. We find that pretrained non-causal decoder models can be adapted into performant generative causal decoder models, using autoregressive language modeling as a downstream task. Furthermore, we find that pretrained causal decoder models can be efficiently adapted into non-causal decoder models, ultimately achieving competitive performance after multitask finetuning. Code and checkpoints are available at https://github.com/bigscience-workshop/architecture-objective.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wang.t2022 What language model architecture and pre.pdf}
}

@article{ward.j:1963,
  title = {Hierarchical Grouping to Optimize an Objective Function},
  author = {Ward, Jr., Joe H.},
  year = {1963},
  journal = {Journal of the American Statistical Association},
  volume = {58},
  number = {301},
  pages = {236--244},
  publisher = {Taylor \& Francis Group},
  bdsk-url-2 = {https://pdfs.semanticscholar.org/0430/b241bdd0b67d37e1143370f8d24fc46d83e9.pdf},
  bdsk-url-3 = {https://doi.org/10.1080/01621459.1963.10500845},
  date-added = {2019-06-15 15:57:19 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  read = {0},
  keywords = {hierarchical clustering}
}

@article{warstadt.a:2019,
  title = {Neural Network Acceptability Judgments},
  author = {Warstadt, Alex and Singh, Amanpreet and Bowman, Samuel R.},
  year = {2019},
  month = nov,
  volume = {7},
  pages = {625--641},
  publisher = {MIT Press - Journals},
  doi = {10.1162/tacl_a_00290},
  date-added = {2021-10-19 00:10:14 -0400},
  date-modified = {2021-10-19 00:10:15 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/warstadt.a2019 Neural network acceptability judgments.pdf}
}

@article{warstadt.a:2020,
  title = {{{BLiMP}}: {{The Benchmark}} of {{Linguistic Minimal Pairs}} for {{English}}},
  shorttitle = {{{BLiMP}}},
  author = {Warstadt, Alex and Parrish, Alicia and Liu, Haokun and Mohananey, Anhad and Peng, Wei and Wang, Sheng-Fu and Bowman, Samuel R.},
  year = {2020},
  month = jul,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {8},
  pages = {377--392},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00321},
  urldate = {2023-04-30},
  abstract = {We introduce The Benchmark of Linguistic Minimal Pairs (BLiMP),1 a challenge set for evaluating the linguistic knowledge of language models (LMs) on major grammatical phenomena in English. BLiMP consists of 67 individual datasets, each containing 1,000 minimal pairs---that is, pairs of minimally different sentences that contrast in grammatical acceptability and isolate specific phenomenon in syntax, morphology, or semantics. We generate the data according to linguist-crafted grammar templates, and human aggregate agreement with the labels is 96.4\%. We evaluate n-gram, LSTM, and Transformer (GPT-2 and Transformer-XL) LMs by observing whether they assign a higher probability to the acceptable sentence in each minimal pair. We find that state-of-the-art models identify morphological contrasts related to agreement reliably, but they struggle with some subtle semantic and syntactic phenomena, such as negative polarity items and extraction islands.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/warstadt.a2020 BLiMP The Benchmark of Linguistic Minim.pdf}
}

@article{wason.p:1979,
  title = {A {{Verbal Illusion}}},
  author = {Wason, Peter C. and Reich, Shuli S.},
  year = {1979},
  month = nov,
  journal = {Quarterly Journal of Experimental Psychology},
  volume = {31},
  number = {4},
  pages = {591--597},
  publisher = {SAGE Publications},
  issn = {0033-555X},
  doi = {10.1080/14640747908400750},
  urldate = {2023-07-27},
  abstract = {The sentence, No head injury is too trivial to be ignored, tends to be systematically misconstrued to mean that one should not ignore head injuries. It is argued that the sentence is anomalous in two ways. It is semantically anomalous because the relation between the adjective and the verb is the same as in the sentence, No sinner is too wicked to be condemned. It is pragmatically anomalous because the relation between the noun and the verb expresses an injunction which is inconsistent with commonly held beliefs. An experiment was conducted to investigate the effect of this pragmatic anomaly on comprehension. Four pragmatic and four non-pragmatic sentences were paraphrased. The prediction was confirmed that the pragmatic sentences were paraphrased more accurately than the non-pragmatic sentences. Some alternative explanations are considered.},
  langid = {english},
  keywords = {depth-charge illusion,grammatical illusions},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wason.p1979 A Verbal Illusion.pdf}
}

@book{watrous.j:2018,
  title = {The Theory of Quantum Information},
  author = {Watrous, John},
  year = {2018},
  publisher = {Cambridge University Press},
  date-added = {2020-02-15 11:52:26 -0500},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {information-entropy},
  keywords = {entropy,information theory,quantum information theory}
}

@misc{weissbart.h:2024biorxiv,
  title = {The Structure and Statistics of Language Jointly Shape Cross-Frequency Neural Dynamics during Spoken Language Comprehension},
  author = {Weissbart, Hugo and Martin, Andrea E.},
  year = {2024},
  month = may,
  publisher = {bioRxiv},
  doi = {10.1101/2023.10.06.561087},
  urldate = {2024-05-25},
  abstract = {Humans excel at extracting structurally-determined meaning from speech despite inherent physical variability. This study explores the brain's ability to predict and understand spoken language robustly. It investigates the relationship between structural and statistical language knowledge in brain dynamics, focusing on phase and amplitude modulation. Using syntactic features from constituent hierarchies and surface statistics from a transformer model as predictors of forward encoding models, we reconstructed cross-frequency neural dynamics from MEG data during audiobook listening. Our findings challenge a strict separation of linguistic structure and statistics in the brain, with both aiding neural signal reconstruction. Syntactic features had a more temporally spread impact, and both word entropy and the number of closing syntactic constituents were linked to the phase-amplitude coupling of neural dynamics, implying a role in temporal prediction and cortical oscillation alignment during speech processing. Our results indicate that structured and statistical information jointly shape neural dynamics during spoken language comprehension and suggest an integration process via a cross-frequency coupling mechanism.},
  archiveprefix = {bioRxiv},
  copyright = {{\copyright} 2024, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
  langid = {english}
}

@article{wellwood.a:2018,
  title = {The {{Anatomy}} of a {{Comparative Illusion}}},
  author = {Wellwood, Alexis and Pancheva, Roumyana and Hacquard, Valentine and Phillips, Colin},
  year = {2018},
  month = aug,
  journal = {Journal of Semantics},
  volume = {35},
  number = {3},
  pages = {543--583},
  issn = {0167-5133},
  doi = {10.1093/jos/ffy014},
  urldate = {2023-08-01},
  abstract = {Comparative constructions like More people have been to Russia than I have are reported to be acceptable and meaningful by native speakers of English; yet, upon closer reflection, they are judged to be incoherent. This mismatch between initial perception and more considered judgment challenges the idea that we perceive sentences veridically, and interpret them fully; it is thus potentially revealing about the relationship between grammar and language processing. This paper presents the results of the first detailed investigation of these so-called `comparative illusions'. We test four hypotheses about their source: a shallow syntactic parser, some type of repair by ellipsis, an incorrectly-resolved lexical ambiguity, or a persistent event comparison interpretation. Two formal acceptability studies show that speakers are most prone to the illusion when the matrix clause supports an event comparison reading. A verbatim recall task tests and finds evidence for such construals in speakers' recollections of the sentences. We suggest that this reflects speakers' entertaining an interpretation that is initially consistent with the sentence, but failing to notice when this interpretation becomes unavailable at the than-clause. In particular, semantic knowledge blinds people to an illicit operator-variable configuration in the syntax. Rather than illustrating processing in the absence of grammatical analysis, comparative illusions thus underscore the importance of syntactic and semantic rules in sentence processing.},
  keywords = {comparative illusions}
}

@article{white.s:2008,
  title = {Eye Movements When Reading Transposed Text: {{The}} Importance of Word-Beginning Letters.},
  shorttitle = {Eye Movements When Reading Transposed Text},
  author = {White, Sarah J. and Johnson, Rebecca L. and Liversedge, Simon P. and Rayner, Keith},
  year = {2008},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {34},
  number = {5},
  pages = {1261--1276},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.34.5.1261},
  urldate = {2024-02-24},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/white.s2008 Eye movements when reading transposed te.pdf}
}

@article{wieling.m:2016,
  title = {Investigating Dialectal Differences Using Articulography},
  author = {Wieling, Martijn and Tomaschek, Fabian and Arnold, Denis and Tiede, Mark and Br{\"o}ker, Franziska and Thiele, Samuel and Wood, Simon N. and Baayen, R. Harald},
  year = {2016},
  month = nov,
  journal = {Journal of Phonetics},
  volume = {59},
  pages = {122--143},
  publisher = {Elsevier BV},
  doi = {10.1016/j.wocn.2016.09.004},
  bdsk-url-2 = {https://doi.org/10.1016/j.wocn.2016.09.004},
  date-added = {2021-12-03 00:24:24 -0500},
  date-modified = {2021-12-03 00:24:40 -0500}
}

@inproceedings{wilcox.e:2020,
  title = {On the Predictive Power of Neural Language Models for Human Real-Time Comprehension Behavior},
  booktitle = {Proceedings of the 42nd Annual Meeting of the {{Cognitive Science Society}}},
  author = {Wilcox, Ethan Gotlieb and Gauthier, Jon and Hu, Jennifer and Qian, Peng and Levy, Roger},
  year = {2020},
  pages = {1707--1713},
  publisher = {Cognitive Science Society},
  address = {Virtual},
  keywords = {sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wilcox.e2020 On the predictive power of neural langua.pdf}
}

@inproceedings{wilcox.e:2021,
  title = {A Targeted Assessment of Incremental Processing in Neural Language Models and Humans},
  booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: {{Long}} Papers)},
  author = {Wilcox, Ethan and Vani, Pranali and Levy, Roger},
  year = {2021},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.acl-long.76},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2021.acl-long.76},
  date-added = {2022-04-15 16:04:48 -0400},
  date-modified = {2022-04-15 16:04:50 -0400}
}

@phdthesis{wilcox.e:2022phd,
  title = {Informative {{Presupposition}} \& {{Accommodation}}},
  author = {Wilcox, Ethan Gotlieb},
  year = {2022},
  month = may,
  urldate = {2023-09-30},
  abstract = {Presuppositions are the parts of meanings of utterances which are backgrounded and strongly committed to by the speaker. They are carried by a diverse range of lexical items called presupposition triggers, which include determiners, particles, open class verbs, and syntactic constructions. For example, the sentence "Lee read War and Peace again" asserts that Lee read War and Peace and presupposes that she has done so previously via the trigger 'again.' Most triggers occur in contexts where their presuppositions are supported (i.e. already entailed) by a local context; however some can also occur in contexts that lack local support, in which case their presuppositions are informative. Informative use of presupposition is typically modeled via an accommodation mechanism (Lewis, 1979) that pre-updates a context prior to utterance interpretation to go along with the presuppositions of a sentence. Understanding when triggers can communicate novel information using accommodation---which I refer to as the "Novelty Problem" for presupposition triggers---is the main goal of this dissertation. The dissertation is arranged into five chapters. Chapter 1 provides a background on presupposition and accommodation, and introduces the notion of Contextual Felicity Constraints, or CFCs (Tonhauser et al., 2013). If a trigger is infelicitous in cases where its presuppositions are not entailed, it is said to be subject to a strong CFC. Chapter 2 measures the CFCs for thirteen English presupposition triggers in two online comprehension studies, making it the largest cross-trigger comparison of CFCs reported in the literature to-date. A ranking of triggers is proposed, from weak-CFC triggers to strong-CFC triggers. Chapter 3 presents a theoretical solution to the novelty problem, which views CFCs as the result of an information-structural discourse clash. The proposal, which I refer to as the Maximalty/Accommodation Clash (or MAC), treats CFCs as arising not from accommodation failure, but from downstream semantic contradictions that result from successful accommodation. Chapter 4 develops this proposal within alternative pragmatic frameworks. Finally, Chapter 5 presents two studies that test the MAC experimentally. Taken together, the results lend support for the perspective that presupposition triggers impose constraints on the context in which they are uttered and that their contextual felicity is modulated by local information structure---the two key ingredients of the MAC approach},
  school = {Harvard University},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wilcox.e2022phd Informative Presupposition & Accommodati.pdf}
}

@misc{wilcox.e:2023arxiv,
  title = {Testing the Predictions of Surprisal Theory in 11 Languages},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
  year = {2023},
  month = jul,
  number = {arXiv:2307.03667},
  eprint = {2307.03667},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.03667},
  urldate = {2023-07-11},
  abstract = {A fundamental result in psycholinguistics is that less predictable words take a longer time to process. One theoretical explanation for this finding is Surprisal Theory (Hale, 2001; Levy, 2008), which quantifies a word's predictability as its surprisal, i.e. its negative log-probability given a context. While evidence supporting the predictions of Surprisal Theory have been replicated widely, most have focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times; (ii) whether expected surprisal, i.e. contextual entropy, is predictive of reading times; (iii) and whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to-date between information theory and incremental language processing across languages.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,surprisal theory}
}

@article{wilcox.e:2023tacl,
  title = {Testing the {{Predictions}} of {{Surprisal Theory}} in 11 {{Languages}}},
  author = {Wilcox, Ethan G. and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan and Levy, Roger P.},
  year = {2023},
  month = dec,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {11},
  pages = {1451--1470},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00612},
  urldate = {2024-03-27},
  abstract = {Surprisal theory posits that less-predictable words should take more time to process, with word predictability quantified as surprisal, i.e., negative log probability in context. While evidence supporting the predictions of surprisal theory has been replicated widely, much of it has focused on a very narrow slice of data: native English speakers reading English texts. Indeed, no comprehensive multilingual analysis exists. We address this gap in the current literature by investigating the relationship between surprisal and reading times in eleven different languages, distributed across five language families. Deriving estimates from language models trained on monolingual and multilingual corpora, we test three predictions associated with surprisal theory: (i) whether surprisal is predictive of reading times, (ii) whether expected surprisal, i.e., contextual entropy, is predictive of reading times, and (iii) whether the linking function between surprisal and reading times is linear. We find that all three predictions are borne out crosslinguistically. By focusing on a more diverse set of languages, we argue that these results offer the most robust link to date between information theory and incremental language processing across languages.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wilcox.e2023tacl Testing the Predictions of Surprisal The.pdf}
}

@article{wilcox.e:2024cognition,
  title = {An Information-Theoretic Analysis of Targeted Regressions during Reading},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},
  year = {2024},
  month = aug,
  journal = {Cognition},
  volume = {249},
  pages = {105765},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2024.105765},
  urldate = {2024-05-26},
  abstract = {Regressions, or backward saccades, are common during reading, accounting for between 5\% and 20\% of all saccades. And yet, relatively little is known about what causes them. We provide an information-theoretic operationalization for two previous qualitative hypotheses about regressions, which we dub reactivation and reanalysis. We argue that these hypotheses make different predictions about the pointwise mutual information or pmi between a regression's source and target. Intuitively, the pmi between two words measures how much more (or less) likely one word is to be present given the other. On one hand, the reactivation hypothesis predicts that regressions occur between words that are associated, implying high positive values of pmi. On the other hand, the reanalysis hypothesis predicts that regressions should occur between words that are not associated with each other, implying negative, low values of pmi. As a second theoretical contribution, we expand on previous theories by considering not only pmi but also expected values of pmi, E[pmi], where the expectation is taken over all possible realizations of the regression's target. The rationale for this is that language processing involves making inferences under uncertainty, and readers may be uncertain about what they have read, especially if a previous word was skipped. To test both theories, we use contemporary language models to estimate pmi-based statistics over word pairs in three corpora of eye tracking data in English, as well as in six languages across three language families (Indo-European, Uralic, and Turkic). Our results are consistent across languages and models tested: Positive values of pmi and E[pmi] consistently help to predict the patterns of regressions during reading, whereas negative values of pmi and E[pmi] do not. Our information-theoretic interpretation increases the predictive scope of both theories and our studies present the first systematic crosslinguistic analysis of regressions in the literature. Our results support the reactivation hypothesis and, more broadly, they expand the number of language processing behaviors that can be linked to information-theoretic principles.},
  keywords = {Eye tracking,Information theory,Language processing,Mutual information,Reading,Regressions},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wilcox.e2024cognition An information-theoretic analysis of tar.pdf}
}

@article{wilcox.e:2024MoTR,
  title = {Mouse {{Tracking}} for {{Reading}} ({{MoTR}}): {{A}} New Naturalistic Incremental Processing Measurement Tool},
  shorttitle = {Mouse {{Tracking}} for {{Reading}} ({{MoTR}})},
  author = {Wilcox, Ethan Gotlieb and Ding, Cui and Sachan, Mrinmaya and J{\"a}ger, Lena Ann},
  year = {2024},
  month = oct,
  journal = {Journal of Memory and Language},
  volume = {138},
  pages = {104534},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2024.104534},
  urldate = {2024-06-04},
  abstract = {We introduce Mouse Tracking for Reading (MoTR) a new incremental processing measurement tool that can be used to collect word-by-word reading times. In a MoTR trial, participants are presented with text, which is blurred, except for a small region around the tip of the mouse. Participants must move the mouse to reveal and read the text. Mouse movement is recorded, and, using a postprocessing pipeline we present, can be analyzed to produce scanpaths as well as word-by-word reading times. We validate MoTR in two suites of experiments. In the first experiment, we collect data for the English-language Provo Corpus (Luke and Christianson, 2018). We analyze scanpaths and show that participants interpolate between two types of strategies for reading during a MoTR trial -- sometimes they fixate on individual words, somewhat akin to eye-tracking, while other times they produce a more constant pass over the text, slowing down in response to processing difficulties. Taking these strategies into account, we show that the word-by-word reading times produced by our data analysis pipeline correlate well with previously collected eye-tracking data for this corpus, and that these correlations are higher than those produced by SPR data, which we also collect for the corpus. Furthermore, we demonstrate that there is a linear relationship between by-word MoTR values and word-level surprisal values, as has been previously shown for eye-tracking data (Smith and Levy, 2013). In the second experiment, we assess whether MoTR can be used to study sentence processing phenomena in targeted psycholinguistics experiments. Using materials from Witzel et al. (2012), we show that MoTR can reveal English speakers' preferences for low attachment during online sentence comprehension. We argue that MoTR presents a compelling tradeoff between multiple experimental considerations: It is cheap to run and can be presented in a browser enabling the collection of data over the internet. It is more naturalistic than some alternative processing measures, allowing participants to skip words and regress to previous sentence regions. Finally, it has good sensitivity, detecting signatures of psycholinguistic processing behaviors from a relatively small number of participants.},
  keywords = {Experimental methods,Eye-tracking,Maze task,Mouse Tracking,Reading times,Self-paced reading},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wilcox.e2024MoTR Mouse Tracking for Reading (MoTR) A new.pdf}
}

@misc{wilcox.e:2024psyarxiv,
  title = {An {{Information-Theoretic Analysis}} of {{Targeted Regressions}} during {{Reading}}},
  author = {Wilcox, Ethan Gotlieb and Pimentel, Tiago and Meister, Clara and Cotterell, Ryan},
  year = {2024},
  month = mar,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/3qf9a},
  urldate = {2024-04-11},
  abstract = {Regressions, or backward saccades, are common during reading, accounting for between 5\% and 20\% of all saccades. And yet, relatively little is known about what causes them. We provide an information-theoretic operationalization for two previous qualitative hypotheses about regressions, which we dub reactivation and reanalysis. We argue that these hypotheses make different predictions about the pointwise mutual information or pmi between a regression's source and target. Intuitively, the pmi between two words measures how much more (or less) likely one word is to be present given the other. On one hand, the reactivation hypothesis predicts that regressions occur between words that are associated, implying high positive values of pmi. On the other hand, the reanalysis hypothesis predicts that regressions should occur between words that are disassociated with each other, implying negative, low values of pmi. As a second theoretical contribution, we expand on previous theories by considering not only pmi but also expected values of pmi, E[pmi], where the expectation is taken over all possible realizations of the regression's target. The rationale for this is that language processing involves making inferences under uncertainty, and readers may be uncertain about what they have read, especially if a previous word was skipped. To test both theories, we use contemporary language models to estimate pmi-based statistics over word pairs in three corpora of eye tracking data in English, as well as in six languages across three language families (Indo-European, Uralic, and Turkic). Our results are consistent across languages and models tested: Positive values of pmi and E[pmi] consistently help to predict the patterns of regressions during reading, whereas negative values of pmi and E[pmi] do not. Our information-theoretic interpretation increases the predictive scope of both theories and our studies present the first systematic crosslinguistic analysis of regressions in the literature. Our results support the reactivation hypothesis and, more broadly, they expand the number of language processing behaviors that can be linked to information-theoretic principles.},
  langid = {american},
  keywords = {Eye-tracking,Information Theory,Language Processing,Mutual Information,Reading,Regressions},
  annotation = {Published as: 10.1016/j.cognition.2024.105765},
  file = {/Users/j/Zotfiles/wilcox.e:2024psyarxiv An Information-Theoretic Analysis of Tar.pdf}
}

@misc{willard.b:2023outlines,
  title = {Efficient {{Guided Generation}} for {{Large Language Models}}},
  author = {Willard, Brandon T. and Louf, R{\'e}mi},
  year = {2023},
  month = aug,
  number = {arXiv:2307.09702},
  eprint = {2307.09702},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2307.09702},
  urldate = {2024-10-10},
  abstract = {In this article we show how the problem of neural text generation can be constructively reformulated in terms of transitions between the states of a finite-state machine. This framework leads to an efficient approach to guiding text generation with regular expressions and context-free grammars by allowing the construction of an index over a language model's vocabulary. The approach is model agnostic, allows one to enforce domain-specific knowledge and constraints, and enables the construction of reliable interfaces by guaranteeing the structure of the generated text. It adds little overhead to the token sequence generation process and significantly outperforms existing solutions. An implementation is provided in the open source Python library Outlines},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/willard.b2023outlines Efficient Guided Generation for Large La.pdf}
}

@misc{williams.p:2010,
  title = {Nonnegative Decomposition of Multivariate Information},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2010},
  eprint = {1004.2515},
  primaryclass = {cs.IT},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:31:54 -0400},
  date-modified = {2021-09-29 21:31:56 -0400}
}

@misc{williams.p:2011,
  title = {Generalized Measures of Information Transfer},
  author = {Williams, Paul L. and Beer, Randall D.},
  year = {2011},
  eprint = {1102.1507},
  primaryclass = {physics.data-an},
  archiveprefix = {arXiv},
  date-added = {2021-09-29 21:29:14 -0400},
  date-modified = {2021-09-29 21:29:15 -0400}
}

@incollection{wilson.k:1954,
  title = {Applications of Entropy Measures to Problems of Sequential Structure},
  booktitle = {Psycholinguistics},
  author = {Wilson, Kellogg and Carroll, John B},
  editor = {Osgood, Charles E. and Sebeok, Thomas A.},
  year = {1954},
  volume = {Psycholinguistics: A survey of theory and research problems},
  pages = {103--110},
  publisher = {Indiana University Press},
  doi = {10.1037/h0063655},
  chapter = {5.3},
  date-added = {2021-05-20 11:54:33 -0400},
  date-modified = {2022-04-14 23:51:44 -0400},
  keywords = {entropy reduction}
}

@book{winter.b:2019sfl,
  title = {Statistics for Linguists: An Introduction Using {{R}}},
  shorttitle = {Statistics for Linguists},
  author = {Winter, Bodo},
  year = {2019},
  month = nov,
  publisher = {Routledge},
  address = {New York},
  doi = {10.4324/9781315165547},
  abstract = {Statistics for Linguists: An Introduction Using R is the first statistics textbook on linear models for linguistics. The book covers simple uses of linear models through generalized models to more advanced approaches, maintaining its focus on conceptual issues and avoiding excessive mathematical details. It contains many applied examples using the R statistical programming environment. Written in an accessible tone and style, this text is the ideal main resource for graduate and advanced undergraduate students of Linguistics statistics courses as well as those in other fields, including Psychology, Cognitive Science, and Data Science.},
  isbn = {978-1-315-16554-7}
}

@article{witzel.n:2012,
  title = {Comparisons of {{Online Reading Paradigms}}: {{Eye Tracking}}, {{Moving-Window}}, and {{Maze}}},
  shorttitle = {Comparisons of {{Online Reading Paradigms}}},
  author = {Witzel, Naoko and Witzel, Jeffrey and Forster, Kenneth},
  year = {2012},
  month = apr,
  journal = {Journal of Psycholinguistic Research},
  volume = {41},
  number = {2},
  pages = {105--128},
  issn = {1573-6555},
  doi = {10.1007/s10936-011-9179-x},
  urldate = {2024-02-22},
  abstract = {This study compares four methodologies used to examine online sentence processing during reading. Specifically, self-paced, non-cumulative, moving-window reading (Just et al. in J Exp Psychol Gen 111:228--238, 1982), eye tracking (see e.g., Rayner in Q J Exp Psychol 62:1457--1506, 2009), and two versions of the maze task (Forster et al. in Behav Res Methods 41:163--171, 2009)---the lexicality maze and the grammaticality maze---were used to investigate the processing of sentences containing temporary structural ambiguities. Of particular interest were (i) whether each task was capable of revealing processing differences on these sentences and (ii) whether these effects were indicated precisely at the predicted word/region. Although there was considerable overlap in the general pattern of results from the four tasks, there were also clear differences among them in terms of the strength and timing of the observed effects. In particular, excepting sentences that tap into clause-closure commitments, both maze task versions provided robust, ``localized'' indications of incremental sentence processing difficulty relative to self-paced reading and eye tracking.},
  langid = {english},
  keywords = {Eye tracking,Maze task,Moving-window reading,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/witzel.n2012 Comparisons of Online Reading Paradigms.pdf}
}

@inproceedings{wolf.t:2020transformers,
  title = {Transformers: {{State-of-the-art}} Natural Language Processing},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: {{System}} Demonstrations},
  author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and {von Platen}, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
  year = {2020},
  pages = {38--45},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.emnlp-demos.6},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.emnlp-demos.6}
}

@article{wong.c:1980,
  title = {An Efficient Method for Weighted Sampling without Replacement},
  author = {Wong, C. K. and Easton, M. C.},
  year = {1980},
  month = feb,
  journal = {SIAM Journal on Computing},
  volume = {9},
  number = {1},
  pages = {111--113},
  issn = {0097-5397, 1095-7111},
  doi = {10.1137/0209009},
  urldate = {2022-07-10},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wong.c1980 An efficient method for weighted samplin.pdf}
}

@misc{wong.l:2023arxiv,
  title = {From {{Word Models}} to {{World Models}}: {{Translating}} from {{Natural Language}} to the {{Probabilistic Language}} of {{Thought}}},
  shorttitle = {From {{Word Models}} to {{World Models}}},
  author = {Wong, Lionel and Grand, Gabriel and Lew, Alexander K. and Goodman, Noah D. and Mansinghka, Vikash K. and Andreas, Jacob and Tenenbaum, Joshua B.},
  year = {2023},
  month = jun,
  number = {arXiv:2306.12672},
  eprint = {2306.12672},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-05},
  abstract = {How does language inform our downstream thinking? In particular, how do humans make meaning from language--and how can we leverage a theory of linguistic meaning to build machines that think in more human-like ways? In this paper, we propose rational meaning construction, a computational framework for language-informed thinking that combines neural language models with probabilistic models for rational inference. We frame linguistic meaning as a context-sensitive mapping from natural language into a probabilistic language of thought (PLoT)--a general-purpose symbolic substrate for generative world modeling. Our architecture integrates two computational tools that have not previously come together: we model thinking with probabilistic programs, an expressive representation for commonsense reasoning; and we model meaning construction with large language models (LLMs), which support broad-coverage translation from natural language utterances to code expressions in a probabilistic programming language. We illustrate our framework through examples covering four core domains from cognitive science: probabilistic reasoning, logical and relational reasoning, visual and physical reasoning, and social reasoning. In each, we show that LLMs can generate context-sensitive translations that capture pragmatically-appropriate linguistic meanings, while Bayesian inference with the generated programs supports coherent and robust commonsense reasoning. We extend our framework to integrate cognitively-motivated symbolic modules (physics simulators, graphics engines, and planning algorithms) to provide a unified commonsense thinking interface from language. Finally, we explore how language can drive the construction of world models themselves. We hope this work will provide a roadmap towards cognitive models and AI systems that synthesize the insights of both modern and classical computational perspectives.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Symbolic Computation},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wong.l2023arxiv From Word Models to World Models Transl.pdf}
}

@article{wood.s:2003tprs,
  title = {Thin Plate Regression Splines},
  author = {Wood, Simon N.},
  year = {2003},
  month = jan,
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {65},
  number = {1},
  pages = {95--114},
  publisher = {Wiley},
  doi = {10.1111/1467-9868.00374},
  bdsk-url-2 = {https://doi.org/10.1111/1467-9868.00374},
  date-added = {2021-12-02 20:55:01 -0500},
  date-modified = {2021-12-02 21:03:21 -0500}
}

@article{wood.s:2004GAMjustGCV,
  title = {Stable and Efficient Multiple Smoothing Parameter Estimation for Generalized Additive Models},
  author = {Wood, Simon N},
  year = {2004},
  month = sep,
  journal = {Journal of the American Statistical Association},
  volume = {99},
  number = {467},
  pages = {673--686},
  publisher = {Informa UK Limited},
  doi = {10.1198/016214504000000980},
  bdsk-url-2 = {https://doi.org/10.1198/016214504000000980},
  date-added = {2021-12-02 20:52:03 -0500},
  date-modified = {2021-12-02 20:52:18 -0500}
}

@article{wood.s:2011GAMmethod,
  title = {Fast Stable Restricted Maximum Likelihood and Marginal Likelihood Estimation of Semiparametric Generalized Linear Models},
  author = {Wood, Simon N.},
  year = {2011},
  journal = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  volume = {73},
  number = {1},
  pages = {3--36},
  publisher = {Wiley},
  doi = {10.1111/j.1467-9868.2010.00749.x},
  bdsk-url-2 = {https://doi.org/10.1111/j.1467-9868.2010.00749.x},
  date-added = {2021-12-02 20:46:39 -0500},
  date-modified = {2021-12-02 20:49:53 -0500}
}

@article{wood.s:2014bam,
  title = {Generalized Additive Models for Large Data Sets},
  author = {Wood, Simon N. and Goude, Yannig and Shaw, Simon},
  year = {2014},
  month = may,
  journal = {Journal of the Royal Statistical Society: Series C (Applied Statistics)},
  volume = {64},
  number = {1},
  pages = {139--155},
  publisher = {Wiley},
  doi = {10.1111/rssc.12068},
  bdsk-url-2 = {https://doi.org/10.1111/rssc.12068},
  date-added = {2021-12-14 20:13:41 -0500},
  date-modified = {2021-12-14 20:21:11 -0500},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wood.s2014bam Generalized additive models for large da.pdf}
}

@article{wood.s:2016GAMbeyondEF,
  title = {Smoothing Parameter and Model Selection for General Smooth Models},
  author = {Wood, Simon N. and Pya, Natalya and S{\"a}fken, Benjamin},
  year = {2016},
  month = oct,
  journal = {Journal of the American Statistical Association},
  volume = {111},
  number = {516},
  pages = {1548--1563},
  publisher = {Taylor \& Francis},
  issn = {0162-1459},
  doi = {10.1080/01621459.2016.1180986},
  urldate = {2022-09-27},
  abstract = {This article discusses a general framework for smoothing parameter estimation for models with regular likelihoods constructed in terms of unknown smooth functions of covariates. Gaussian random effects and parametric terms may also be present. By construction the method is numerically stable and convergent, and enables smoothing parameter uncertainty to be quantified. The latter enables us to fix a well known problem with AIC for such models, thereby improving the range of model selection tools available. The smooth functions are represented by reduced rank spline like smoothers, with associated quadratic penalties measuring function smoothness. Model estimation is by penalized likelihood maximization, where the smoothing parameters controlling the extent of penalization are estimated by Laplace approximate marginal likelihood. The methods cover, for example, generalized additive models for nonexponential family responses (e.g., beta, ordered categorical, scaled t distribution, negative binomial and Tweedie distributions), generalized additive models for location scale and shape (e.g., two stage zero inflation models, and Gaussian location-scale models), Cox proportional hazards models and multivariate additive models. The framework reduces the implementation of new model classes to the coding of some standard derivatives of the log-likelihood. Supplementary materials for this article are available online.},
  keywords = {Additive model,AIC,Distributional regression,GAM,gaulss,location scale additive models,Location scale and shape model,Ordered categorical regression,Penalized regression spline,REML,Smooth Cox model,Smoothing parameter uncertainty,Statistical algorithm,Tweedie distribution.}
}

@book{wood.s:2017GAMoverview,
  title = {Generalized Additive Models},
  author = {Wood, Simon N.},
  year = {2017},
  month = may,
  publisher = {{Chapman and Hall/CRC}},
  doi = {10.1201/9781315370279}
}

@article{wu.b:1999,
  title = {Approximation and Exact Algorithms for Constructing Minimum Ultrametric Trees from Distance Matrices},
  author = {Wu, Bang Ye and Chao, Kun-Mao and Tang, Chuan Yi},
  year = {1999},
  journal = {Journal of Combinatorial Optimization},
  volume = {3},
  number = {2},
  pages = {199--211},
  issn = {1573-2886},
  abstract = {An edge-weighted tree is called ultrametric if the distances from the root to all the leaves in the tree are equal. For an n by n distance matrix M, the minimum ultrametric tree for M is an ultrametric tree T = (V, E, w) with leaf set \{1,..., n\} such that dT(i, j) {$\geq$} M[i, j] for all i, j and \$\${\textbackslash}sum \{\_\{e {\textbackslash}in E\} w(e)\}\$\$is minimum, where dT(i, j) is the distance between i and j on T. Constructing minimum ultrametric trees from distance matrices is an important problem in computational biology. In this paper, we examine its computational complexity and approximability. When the distances satisfy the triangle inequality, we show that the minimum ultrametric tree problem can be approximated in polynomial time with error ratio 1.5(1 + {$\lceil$}log n{$\rceil$}), where n is the number of species. We also develop an efficient branch-and-bound algorithm for constructing the minimum ultrametric tree for both metric and non-metric inputs. The experimental results show that it can find an optimal solution for 25 species within reasonable time, while, to the best of our knowledge, there is no report of algorithms solving the problem even for 12 species.},
  date-added = {2019-07-17 13:39:35 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {ultrametric}
}

@misc{wu.m:2022arxiv,
  title = {Foundation Posteriors for Approximate Probabilistic Inference},
  author = {Wu, Mike and Goodman, Noah},
  year = {2022},
  month = may,
  number = {arXiv:2205.09735},
  eprint = {2205.09735},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.09735},
  urldate = {2022-08-18},
  abstract = {Probabilistic programs provide an expressive representation language for generative models. Given a probabilistic program, we are interested in the task of posterior inference: estimating a latent variable given a set of observed variables. Existing techniques for inference in probabilistic programs often require choosing many hyper-parameters, are computationally expensive, and/or only work for restricted classes of programs. Here we formulate inference as masked language modeling: given a program, we generate a supervised dataset of variables and assignments, and randomly mask a subset of the assignments. We then train a neural network to unmask the random values, defining an approximate posterior distribution. By optimizing a single neural network across a range of programs we amortize the cost of training, yielding a ``foundation'' posterior able to do zero-shot inference for new programs. The foundation posterior can also be fine-tuned for a particular program and dataset by optimizing a variational inference objective. We show the efficacy of the approach, zero-shot and fine-tuned, on a benchmark of STAN programs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,inference,probabilistic programming,Statistics - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wu.m2022 Foundation posteriors for approximate pr.pdf}
}

@inproceedings{wu.s:2010,
  title = {Complexity {{Metrics}} in an {{Incremental Right-Corner Parser}}},
  booktitle = {Proceedings of the 48th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}},
  author = {Wu, Stephen and Bachrach, Asaf and Cardenas, Carlos and Schuler, William},
  year = {2010},
  month = jul,
  pages = {1189--1198},
  publisher = {Association for Computational Linguistics},
  address = {Uppsala, Sweden},
  urldate = {2022-05-31},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wu.s2010 Complexity Metrics in an Incremental Rig.pdf}
}

@inproceedings{wu.z:2021,
  title = {Perturbed Masking: {{Parameter-free}} Probing for Analyzing and Interpreting {{BERT}}},
  booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
  author = {Wu, Zhiyong and Chen, Yun and Kao, Ben and Liu, Qun},
  year = {2020},
  pages = {4166--4176},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.acl-main.383},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.acl-main.383},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/wu.z2021 Perturbed masking Parameter-free probin.pdf}
}

@article{xiang.m:2009,
  title = {Illusory Licensing Effects across Dependency Types: {{ERP}} Evidence},
  shorttitle = {Illusory Licensing Effects across Dependency Types},
  author = {Xiang, Ming and Dillon, Brian and Phillips, Colin},
  year = {2009},
  month = jan,
  journal = {Brain and Language},
  volume = {108},
  number = {1},
  pages = {40--55},
  issn = {0093-934X},
  doi = {10.1016/j.bandl.2008.10.002},
  urldate = {2024-05-27},
  abstract = {A number of recent studies have argued that grammatical illusions can arise in the process of completing linguistic dependencies, such that unlicensed material is temporarily treated as licensed due to the presence of a potential licensor that is semantically appropriate but in a syntactically inappropriate position. A frequently studied case involves illusory licensing of negative polarity items (NPIs) like ever and any, which must appear in the scope (i.e., c-command domain) of a negative element. Speakers often show intrusive licensing effects in sentences where an NPI is preceded but not c-commanded by a negative element, as in {$\ast$}The restaurants that no newspapers have recommended in their reviews have ever gone out of business. Existing accounts of intrusive licensing have focused on the role of general memory retrieval processes. In contrast, we propose that intrusive licensing of NPIs reflects semantic/pragmatic processes that are more specific to NPI licensing. As a test of this claim, we present results from an ERP study that presents a structurally matched comparison of intrusive licensing in two types of linguistic dependencies, namely NPI licensing and the binding of reflexive anaphors like himself, and herself. In the absence of a potential licensor, both NPIs and reflexives elicit a P600 response, but whereas there is an immediate ERP analog of the intrusion effect for NPI licensing, no such effect is found for reflexive binding. This suggests that the NPI intrusion effect does not reflect general-purpose retrieval mechanisms.},
  keywords = {Anaphora,Binding,ERP,Negative polarity items (NPI),Semantic/pragmatic processing,Sentence processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/xiang.m2009 Illusory licensing effects across depend.pdf}
}

@inproceedings{xu.w:2023,
  title = {The Linearity of the Effect of Surprisal on Reading Times across Languages},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Xu, Weijie and Chon, Jason and Liu, Tianran and Futrell, Richard},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {15711--15721},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.findings-emnlp.1052},
  urldate = {2024-01-14},
  abstract = {In psycholinguistics, surprisal theory posits that the amount of online processing effort expended by a human comprehender per word positively correlates with the surprisal of that word given its preceding context. In addition to this overall correlation, more importantly, the specific quantitative form taken by the processing effort as a function of surprisal offers insights into the underlying cognitive mechanisms of language processing. Focusing on English, previous studies have looked into the linearity of surprisal on reading times. Here, we extend the investigation by examining eyetracking corpora of seven languages: Danish, Dutch, English, German, Japanese, Mandarin, and Russian. We find evidence for superlinearity in some languages, but the results are highly sensitive to which language model is used to estimate surprisal.},
  keywords = {superlinearity},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/xu.w2023 The linearity of the effect of surprisal.pdf}
}

@inproceedings{yamakoshi.t:2022,
  title = {Probing {{BERT}}'s Priors with Serial Reproduction Chains},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{ACL}} 2022},
  author = {Yamakoshi, Takateru and Griffiths, Thomas and Hawkins, Robert},
  editor = {Muresan, Smaranda and Nakov, Preslav and Villavicencio, Aline},
  year = {2022},
  month = may,
  pages = {3977--3992},
  publisher = {Association for Computational Linguistics},
  address = {Dublin, Ireland},
  doi = {10.18653/v1/2022.findings-acl.314},
  urldate = {2023-12-27},
  abstract = {Sampling is a promising bottom-up method for exposing what generative models have learned about language, but it remains unclear how to generate representative samples from popular masked language models (MLMs) like BERT. The MLM objective yields a dependency network with no guarantee of consistent conditional distributions, posing a problem for naive approaches. Drawing from theories of iterated learning in cognitive science, we explore the use of serial reproduction chains to sample from BERT's priors. In particular, we observe that a unique and consistent estimator of the ground-truth joint distribution is given by a Generative Stochastic Network (GSN) sampler, which randomly selects which token to mask and reconstruct on each step. We show that the lexical and syntactic statistics of sentences from GSN chains closely match the ground-truth corpus distribution and perform better than other methods in a large corpus of naturalness judgments. Our findings establish a firmer theoretical foundation for bottom-up probing and highlight richer deviations from human priors.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yamakoshi.t2022 Probing BERT's priors with serial reprod.pdf}
}

@inproceedings{yang.k:2020,
  title = {Strongly Incremental Constituency Parsing with Graph Neural Networks},
  booktitle = {Advances in Neural Information Processing Systems},
  author = {Yang, Kaiyu and Deng, Jia},
  editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
  year = {2020},
  volume = {33},
  pages = {21687--21698},
  publisher = {Curran Associates, Inc.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yang.k2020 Strongly incremental constituency parsin.pdf}
}

@inproceedings{yang.s:2020,
  title = {Second-Order Unsupervised Neural Dependency Parsing},
  booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
  author = {Yang, Songlin and Jiang, Yong and Han, Wenjuan and Tu, Kewei},
  year = {2020},
  pages = {3911--3924},
  publisher = {International Committee on Computational Linguistics},
  address = {Barcelona, Spain (Online)},
  doi = {10.18653/v1/2020.coling-main.347},
  bdsk-url-2 = {https://doi.org/10.18653/v1/2020.coling-main.347},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yang.s2020 Second-order unsupervised neural depende.pdf}
}

@inproceedings{yang.z:2018,
  title = {Breaking the Softmax Bottleneck: {{A}} High-Rank {{RNN}} Language Model},
  booktitle = {International Conference on Learning Representations},
  author = {Yang, Zhilin and Dai, Zihang and Salakhutdinov, Ruslan and Cohen, William W.},
  year = {2018}
}

@inproceedings{yang.z:2019,
  title = {{{XLNet}}: {{Generalized}} Autoregressive Pretraining for Language Understanding},
  booktitle = {Advances in Neural Information Processing Systems 32},
  author = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
  editor = {Wallach, Hanna M. and Larochelle, Hugo and Beygelzimer, Alina and {d'Alch{\'e}-Buc}, Florence and Fox, Emily B. and Garnett, Roman},
  year = {2019},
  month = dec,
  volume = {32},
  pages = {5754--5764},
  publisher = {Curran Associates, Inc.},
  address = {Vancouver, British Columbia, Canada},
  bibsource = {dblp computer science bibliography, https://dblp.org},
  biburl = {https://dblp.org/rec/conf/nips/YangDYCSL19.bib},
  date-modified = {2021-09-09 23:04:25 -0400},
  timestamp = {Thu, 21 Jan 2021 00:00:00 +0100},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yang.z2019 XLNet Generalized autoregressive pretra.pdf}
}

@article{ye.l:2020,
  title = {Monte {{Carlo}} Co-Ordinate Ascent Variational Inference},
  author = {Ye, Lifeng and Beskos, Alexandros and De Iorio, Maria and Hao, Jie},
  year = {2020},
  month = jul,
  journal = {Statistics and Computing},
  volume = {30},
  number = {4},
  pages = {887--905},
  issn = {1573-1375},
  doi = {10.1007/s11222-020-09924-y},
  urldate = {2022-06-27},
  abstract = {In variational inference (VI), coordinate-ascent and gradient-based approaches are two major types of algorithms for approximating difficult-to-compute probability densities. In real-world implementations of complex models, Monte Carlo methods are widely used to estimate expectations in coordinate-ascent approaches and gradients in derivative-driven ones. We discuss a Monte Carlo co-ordinate ascent VI (MC-CAVI) algorithm that makes use of Markov chain Monte Carlo (MCMC) methods in the calculation of expectations required within co-ordinate ascent VI (CAVI). We show that, under regularity conditions, an MC-CAVI recursion will get arbitrarily close to a maximiser of the evidence lower bound with any given high probability. In numerical examples, the performance of MC-CAVI algorithm is compared with that of MCMC and---as a representative of derivative-based VI methods---of Black Box VI (BBVI). We discuss and demonstrate MC-CAVI's suitability for models with hard constraints in simulated and real examples. We compare MC-CAVI's performance with that of MCMC in an important complex model used in nuclear magnetic resonance spectroscopy data analysis---BBVI is nearly impossible to be employed in this setting due to the hard constraints involved in the model.},
  langid = {english},
  keywords = {Bayesian inference,Coordinate-ascent,Gradient-based optimisation,Markov chain Monte Carlo,Nuclear magnetic resonance,Variational inference},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/ye.l2020 Monte Carlo co-ordinate ascent variation.pdf}
}

@article{yeung.r:1991,
  title = {A New Outlook on {{Shannon}}'s Information Measures},
  author = {Yeung, Raymond W.},
  year = {1991},
  month = may,
  journal = {IEEE Transactions on Information Theory},
  volume = {37},
  number = {3},
  pages = {466--474},
  publisher = {{Institute of Electrical and Electronics Engineers (IEEE)}},
  doi = {10.1109/18.79902},
  bdsk-url-2 = {https://doi.org/10.1109/18.79902},
  date-added = {2021-09-20 18:00:57 -0400},
  date-modified = {2021-09-21 18:18:41 -0400}
}

@book{yeung.r:2008,
  title = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  year = {2008},
  series = {Information Technology Transmission Processing and Storage},
  publisher = {Springer US},
  doi = {10.1007/978-0-387-79234-7},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7},
  date-added = {2021-09-21 18:16:02 -0400},
  date-modified = {2021-09-21 18:18:28 -0400}
}

@incollection{yeung.r:2008ch3,
  title = {The {{I-Measure}}},
  booktitle = {Information Theory and Network Coding},
  author = {Yeung, Raymond W.},
  series = {Information Technology Transmission Processing and Storage},
  pages = {51--80},
  publisher = {Springer US},
  doi = {10.1007/978-0-387-79234-7_3},
  bdsk-url-2 = {https://doi.org/10.1007/978-0-387-79234-7{$_{3}$}},
  chapter = {3},
  date-added = {2021-09-21 18:19:43 -0400},
  date-modified = {2021-09-21 18:22:32 -0400}
}

@article{yngve.v:1960,
  title = {A Model and an Hypothesis for Language Structure},
  author = {Yngve, Victor H.},
  year = {1960},
  journal = {Proceedings of the American Philosophical Society},
  volume = {104},
  number = {5},
  eprint = {985230},
  eprinttype = {jstor},
  pages = {444--466},
  publisher = {American Philosophical Society},
  issn = {0003-049X},
  urldate = {2023-03-10},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yngve.v1960 A model and an hypothesis for language s.pdf}
}

@inproceedings{yoshida.r:2021,
  title = {Modeling Human Sentence Processing with Left-Corner Recurrent Neural Network Grammars},
  booktitle = {Proceedings of the 2021 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Yoshida, Ryo and Noji, Hiroshi and Oseki, Yohei},
  year = {2021},
  pages = {2964--2973},
  publisher = {Association for Computational Linguistics},
  address = {Online and Punta Cana, Dominican Republic},
  doi = {10.18653/v1/2021.emnlp-main.235},
  urldate = {2022-08-13},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yoshida.r2021 Modeling human sentence processing with.pdf}
}

@inproceedings{yoshida.r:2022,
  title = {Composition, {{Attention}}, or {{Both}}?},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2022},
  author = {Yoshida, Ryo and Oseki, Yohei},
  year = {2022},
  month = dec,
  pages = {5822--5834},
  publisher = {Association for Computational Linguistics},
  address = {Abu Dhabi, United Arab Emirates},
  doi = {10.18653/v1/2022.findings-emnlp.428},
  urldate = {2023-08-18},
  abstract = {In this paper, we propose a novel architecture called Composition Attention Grammars (CAGs) that recursively compose subtrees into a single vector representation with a composition function, and selectively attend to previous structural information with a self-attention mechanism. We investigate whether these components---the composition function and the self-attention mechanism---can both induce human-like syntactic generalization. Specifically, we train language models (LMs) with and without these two components with the model sizes carefully controlled, and evaluate their syntactic generalization performance against six test circuits on the SyntaxGym benchmark. The results demonstrated that the composition function and the self-attention mechanism both play an important role to make LMs more human-like, and closer inspection of linguistic phenomenon implied that the composition function allowed syntactic features, but not semantic features, to percolate into subtree representations.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yoshida.r2022 Composition, Attention, or Both.pdf}
}

@inproceedings{yu.l:2022,
  title = {The Neural Noisy Channel},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Yu, Lei and Blunsom, Phil and Dyer, Chris and Grefenstette, Edward and Kocisky, Tomas},
  year = {2022},
  month = jul,
  urldate = {2022-10-25},
  abstract = {We formulate sequence to sequence transduction as a noisy channel decoding problem and use recurrent neural networks to parameterise the source and channel models. Unlike direct models which can suffer from explaining-away effects during training, noisy channel models must produce outputs that explain their inputs, and their component models can be trained with not only paired training samples but also unpaired samples from the marginal output distribution. Using a latent variable to control how much of the conditioning sequence the channel model needs to read in order to generate a subsequent symbol, we obtain a tractable and effective beam search decoder. Experimental results on abstractive sentence summarisation, morphological inflection, and machine translation show that noisy channel models outperform direct models, and that they significantly benefit from increased amounts of unpaired output data that direct models cannot easily use.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yu.l2022 The neural noisy channel.pdf}
}

@article{yun.j:2014,
  title = {Uncertainty in Processing Relative Clauses across {{East Asian}} Languages},
  author = {Yun, Jiwon and Chen, Zhong and Hunter, Tim and Whitman, John and Hale, John},
  year = {2014},
  journal = {Journal of East Asian Linguistics},
  volume = {24},
  number = {2},
  pages = {113--148},
  publisher = {{Springer Science and Business Media LLC}},
  doi = {10.1007/s10831-014-9126-6},
  bdsk-url-2 = {https://doi.org/10.1007/s10831-014-9126-6},
  date-added = {2021-03-18 10:38:55 -0400},
  date-modified = {2021-03-18 10:39:18 -0400},
  keywords = {entropy reduction,processing},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/yun.j2014 Uncertainty in processing relative claus.pdf}
}

@phdthesis{yuret.d:1998phd,
  title = {Discovery of Linguistic Relations Using Lexical Attraction},
  author = {Yuret, Deniz},
  year = {1998},
  eprint = {cmp-lg/9805009},
  archiveprefix = {arXiv},
  date-added = {2020-04-23 12:44:41 -0400},
  date-modified = {2020-04-24 12:35:24 -0400},
  project = {syntactic embedding},
  school = {Massachusetts Institute of Technology, Dept. of Electrical Engineering and Computer Science},
  keywords = {dependency parsing,dependency structures,mutual information}
}

@article{yuret.d:2006,
  title = {Lexical Attraction Models of Language},
  author = {Yuret, Deniz},
  year = {2006},
  journal = {Ms., Ko{\c c} University, Istanbul, Turkey,},
  date-added = {2019-09-12 19:59:44 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {information theory,lexical attraction,mutual information}
}

@incollection{zaenen.a:1985a,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1985},
  pages = {93--136},
  publisher = {Brill},
  date-added = {2020-02-15 18:32:26 -0500},
  date-modified = {2020-02-15 18:38:25 -0500},
  project = {Icelandic gluttony},
  keywords = {dative subjecthood}
}

@incollection{zaenen.a:1990,
  title = {Case and Grammatical Functions: {{The Icelandic}} Passive},
  booktitle = {Modern Icelandic Syntax},
  author = {Zaenen, Annie and Maling, Joan and Thr{\'a}insson, H{\"o}skuldur},
  year = {1990},
  pages = {93--136},
  publisher = {Brill},
  date-added = {2020-02-03 16:19:23 -0500},
  date-modified = {2020-02-03 16:19:39 -0500},
  project = {Icelandic gluttony},
  keywords = {agreement}
}

@article{zarcone.a:2016,
  title = {Salience and {{Attention}} in {{Surprisal-Based Accounts}} of {{Language Processing}}},
  author = {Zarcone, Alessandra and {van Schijndel}, Marten and Vogels, Jorrig and Demberg, Vera},
  year = {2016},
  month = jun,
  journal = {Frontiers in Psychology},
  volume = {7},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.00844},
  urldate = {2024-07-22},
  abstract = {{$<$}p{$>$}The notion of {$<$}italic{$>$}salience{$<$}/italic{$>$} has been singled out as the explanatory factor for a diverse range of linguistic phenomena. In particular, perceptual salience (e.g., visual salience of objects in the world, acoustic prominence of linguistic sounds) and semantic-pragmatic salience (e.g., prominence of recently mentioned or topical referents) have been shown to influence language comprehension and production. A different line of research has sought to account for behavioral correlates of cognitive load during comprehension as well as for certain patterns in language usage using information-theoretic notions, such as {$<$}italic{$>$}surprisal{$<$}/italic{$>$}. Surprisal and salience both affect language processing at different levels, but the relationship between the two has not been adequately elucidated, and the question of whether salience can be reduced to surprisal / predictability is still open. Our review identifies two main challenges in addressing this question: terminological inconsistency and lack of integration between high and low levels of representations in salience-based accounts and surprisal-based accounts. We capitalize upon work in visual cognition in order to orient ourselves in surveying the different facets of the notion of salience in linguistics and their relation with models of surprisal. We find that work on salience highlights aspects of linguistic communication that models of surprisal tend to overlook, namely the role of attention and relevance to current goals, and we argue that the Predictive Coding framework provides a unified view which can account for the role played by attention and predictability at different levels of processing and which can clarify the interplay between low and high levels of processes and between predictability-driven expectation and attention-driven focus.{$<$}/p{$>$}},
  langid = {english},
  keywords = {accessibility,Attention,goals,Language,predictability,predictive coding,relevance,salience,surprisal},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zarcone.a2016 Salience and Attention in Surprisal-Base.pdf}
}

@article{zaslavsky.n:2018,
  title = {Efficient Compression in Color Naming and Its Evolution},
  author = {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and Tishby, Naftali},
  year = {2018},
  journal = {Proceedings of the National Academy of Sciences},
  volume = {115},
  number = {31},
  eprint = {https://www.pnas.org/content/115/31/7937.full.pdf},
  pages = {7937--7942},
  publisher = {National Academy of Sciences},
  issn = {0027-8424},
  bdsk-url-2 = {https://doi.org/10.1073/pnas.1800521115},
  date-added = {2019-05-15 00:03:28 -0400},
  date-modified = {2020-04-16 12:12:14 -0400},
  project = {syntactic embedding},
  keywords = {rate-distortion theory}
}

@misc{zehr.j:2018PCIbex,
  title = {{{PennController}} for {{Internet Based Experiments}} ({{IBEX}})},
  author = {Zehr, J{\'e}r{\'e}my and Schwarz, Florian},
  year = {2018},
  month = mar,
  doi = {10.17605/OSF.IO/MD832},
  urldate = {2024-02-22},
  abstract = {PennController is a library extension for IBEX. It introduces a flexible and user-friendly syntax to design dynamic (e.g., scripted/timed events) and interactive (e.g., pictures, audio, videos, ...) trials.},
  collaborator = {{Open Science Framework}},
  copyright = {BSD 3-Clause "New"/"Revised" License},
  howpublished = {Open Science Framework}
}

@inproceedings{zeman.d:2020,
  title = {Universal {{Dependencies}}},
  booktitle = {Proceedings of the 15th Conference of the {{European}} Chapter of the Association for Computational Linguistics: {{Tutorial}} Abstracts},
  author = {Nivre, Joakim and Zeman, Daniel and Ginter, Filip and Tyers, Francis},
  year = {2017},
  publisher = {Association for Computational Linguistics},
  address = {Valencia, Spain},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zeman.d2020 Universal Dependencies.pdf}
}

@misc{zeng.a:2022GLM130B,
  title = {{{GLM-130B}}: {{An}} Open Bilingual Pre-Trained Model},
  shorttitle = {Glm-130b},
  author = {Zeng, Aohan and Liu, Xiao and Du, Zhengxiao and Wang, Zihan and Lai, Hanyu and Ding, Ming and Yang, Zhuoyi and Xu, Yifan and Zheng, Wendi and Xia, Xiao and Tam, Weng Lam and Ma, Zixuan and Xue, Yufei and Zhai, Jidong and Chen, Wenguang and Zhang, Peng and Dong, Yuxiao and Tang, Jie},
  year = {2022},
  month = oct,
  number = {arXiv:2210.02414},
  eprint = {2210.02414},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-31},
  abstract = {We introduce GLM-130B, a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 and unveil how models of such a scale can be successfully pre-trained. Over the course of this effort, we face numerous unexpected technical and engineering challenges, particularly on loss spikes and disconvergence. In this paper, we introduce the training process of GLM-130B including its design choices, training strategies for both efficiency and stability, and engineering efforts. The resultant GLM-130B model offers significant outperformance over GPT-3 175B on a wide range of popular English benchmarks while the performance advantage is not observed in OPT-175B and BLOOM-176B. It also consistently and significantly outperforms ERNIE TITAN 3.0 260B -- the largest Chinese language model -- across related benchmarks. Finally, we leverage a unique scaling property of GLM-130B to reach INT4 quantization, without quantization aware training and with almost no performance loss, making it the first among 100B-scale models. More importantly, the property allows its effective inference on 4\${\textbackslash}times\$RTX 3090 (24G) or 8\${\textbackslash}times\$RTX 2080 Ti (11G) GPUs, the most ever affordable GPUs required for using 100B-scale models. The GLM-130B model weights are publicly accessible and its code, training logs, related toolkit, and lessons learned are open-sourced at https://github.com/THUDM/GLM-130B .},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zeng.a2022GLM130B GLM-130B An open bilingual pre-trained.pdf}
}

@inproceedings{zhan.m:2018,
  title = {Comparing Theories of Speaker Choice Using a Model of Classifier Production in {{Mandarin Chinese}}},
  booktitle = {Proceedings of the 2018 {{Conference}} of the {{North American Chapter}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}, {{Volume}} 1 ({{Long Papers}})},
  author = {Zhan, Meilin and Levy, Roger},
  year = {2018},
  month = jun,
  pages = {1997--2005},
  publisher = {Association for Computational Linguistics},
  address = {New Orleans, Louisiana},
  doi = {10.18653/v1/N18-1181},
  urldate = {2022-10-29},
  abstract = {Speakers often have more than one way to express the same meaning. What general principles govern speaker choice in the face of optionality when near semantically invariant alternation exists? Studies have shown that optional reduction in language is sensitive to contextual predictability, such that more predictable a linguistic unit is, the more likely it is to get reduced. Yet it is unclear whether these cases of speaker choice are driven by audience design versus toward facilitating production. Here we argue that for a different optionality phenomenon, namely classifier choice in Mandarin Chinese, Uniform Information Density and at least one plausible variant of availability-based production make opposite predictions regarding the relationship between the predictability of the upcoming material and speaker choices. In a corpus analysis of Mandarin Chinese, we show that the distribution of speaker choices supports the availability-based production account and not the Uniform Information Density.},
  keywords = {uniform information density}
}

@misc{zhang.k:2018,
  title = {Language Modeling Teaches You More Syntax than Translation Does: {{Lessons}} Learned through Auxiliary Task Analysis},
  author = {Zhang, Kelly W. and Bowman, Samuel R.},
  year = {2018},
  number = {arXiv:1809.10040},
  eprint = {1809.10040},
  primaryclass = {cs},
  publisher = {arXiv},
  archiveprefix = {arXiv},
  keywords = {CoVe,ELMo,LSTM,syntactic information},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.k2018 Language modeling teaches you more synta.pdf}
}

@inproceedings{zhang.m:2020,
  title = {Language Generation via Combinatorial Constraint Satisfaction: A Tree Search Enhanced {{Monte-Carlo}} Approach},
  shorttitle = {Language Generation via Combinatorial Constraint Satisfaction},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2020},
  author = {Zhang, Maosen and Jiang, Nan and Li, Lei and Xue, Yexiang},
  year = {2020},
  month = nov,
  pages = {1286--1298},
  publisher = {Association for Computational Linguistics},
  address = {Online},
  doi = {10.18653/v1/2020.findings-emnlp.115},
  urldate = {2023-06-03},
  abstract = {Generating natural language under complex constraints is a principled formulation towards controllable text generation. We present a framework to allow specification of combinatorial constraints for sentence generation. We propose TSMC, an efficient method to generate high likelihood sentences with respect to a pre-trained language model while satisfying the constraints. Our approach is highly flexible, requires no task-specific train- ing, and leverages efficient constraint satisfaction solving techniques. To better handle the combinatorial constraints, a tree search algorithm is embedded into the proposal process of the Markov Chain Monte Carlo (MCMC) to explore candidates that satisfy more constraints. Compared to existing MCMC approaches, our sampling approach has a better mixing performance. Experiments show that TSMC achieves consistent and significant improvement on multiple language generation tasks.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.m2020 Language generation via combinatorial co.pdf}
}

@misc{zhang.s:2022OPT,
  title = {{{OPT}}: {{Open Pre-trained Transformer}} Language Models},
  shorttitle = {{{OPT}}},
  author = {Zhang, Susan and Roller, Stephen and Goyal, Naman and Artetxe, Mikel and Chen, Moya and Chen, Shuohui and Dewan, Christopher and Diab, Mona and Li, Xian and Lin, Xi Victoria and Mihaylov, Todor and Ott, Myle and Shleifer, Sam and Shuster, Kurt and Simig, Daniel and Koura, Punit Singh and Sridhar, Anjali and Wang, Tianlu and Zettlemoyer, Luke},
  year = {2022},
  month = jun,
  number = {arXiv:2205.01068},
  eprint = {2205.01068},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2205.01068},
  urldate = {2023-05-04},
  abstract = {Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.s2022OPT OPT Open Pre-trained Transformer langua.pdf}
}

@article{zhang.t:2021,
  title = {On the Inductive Bias of Masked Language Modeling: {{From}} Statistical to Syntactic Dependencies},
  author = {Zhang, Tianyi and Hashimoto, Tatsunori},
  year = {2021},
  journal = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
  publisher = {Association for Computational Linguistics},
  doi = {10.18653/v1/2021.naacl-main.404},
  date-added = {2021-09-08 11:13:42 -0400},
  date-modified = {2021-09-08 11:13:47 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.t2021 On the inductive bias of masked language.pdf}
}

@inproceedings{zhang.y:2008,
  title = {A Tale of Two Parsers: Investigating and Combining Graph-Based and Transition-Based Dependency Parsing},
  booktitle = {Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing},
  author = {Zhang, Yue and Clark, Stephen},
  year = {2008},
  month = oct,
  pages = {562--571},
  publisher = {Association for Computational Linguistics},
  address = {Honolulu, Hawaii},
  date-added = {2022-03-25 22:14:22 -0400},
  date-modified = {2022-03-25 22:14:24 -0400},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.y2008 A tale of two parsers investigating and.pdf}
}

@article{zhang.y:2023,
  title = {A Noisy-Channel Approach to Depth-Charge Illusions},
  author = {Zhang, Yuhan and Ryskin, Rachel and Gibson, Edward},
  year = {2023},
  month = mar,
  journal = {Cognition},
  volume = {232},
  pages = {105346},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2022.105346},
  urldate = {2023-07-26},
  abstract = {The ``depth-charge'' sentence, No head injury is too trivial to be ignored, is often interpreted as ``no matter how trivial head injuries are, we should not ignore them'' while the literal meaning is the opposite -- ``we should ignore them''. Four decades of research have failed to resolve the source of this entrenched semantic illusion. Here we adopt the noisy-channel framework for language comprehension to provide a potential explanation. We hypothesize that depth-charge sentences result from inferences whereby comprehenders derive the interpretation by weighing the plausibility of possible readings of the depth-charge sentences against the likelihood of plausible sentences being produced with errors. In four experiments, we find that (1) the more plausible the intended meaning of the depth-charge sentence is, the more likely the sentence is to be misinterpreted; and (2) the higher the likelihood of our hypothesized noise operations, the more likely depth-charge sentences are to be misinterpreted. These results suggest that misinterpretation is affected by both world knowledge and the distance between the depth-charge sentence and a plausible alternative, which is consistent with the noisy-channel framework.},
  langid = {english},
  keywords = {Depth-charge sentence,Language comprehension,Noisy-channel framework,noisy-channel processing,Semantic illusion},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.y2023 A noisy-channel approach to depth-charge.pdf}
}

@misc{zhang.y:2023amlap,
  type = {Talk},
  title = {A Noisy-Channel Explanation of the Comparative Illusion},
  author = {Zhang, Yuhan and Kauf, Carina and Gibson, Edward},
  year = {2023},
  month = sep,
  address = {Donostia-San Sebastian, Spain},
  abstract = {The comparative illusion sentence, "More students have been to Russia than I have", is acceptable on first reading, but upon closer examination, its meaning becomes hard to discern. Previous research has identified several factors that affect the illusion degree -- the repeatability of the action ("go to Russia" vs. "escape from Russia"), the than-clause subject form (pronoun "I" vs. noun phrase "the teacher"), and the subject number (singular vs. plural) (O'Connor, 2015; Wellwood et al. 2018), but there has not been a thorough explanation of how the illusion arises. We propose a noisy-channel explanation (Gibson et al. 2013; Levy, 2008) and argue that readers could arrive at any of three possible interpretations (event comparison, event negation, individual comparison) depending on (1) the structural distance between the perceived anomalous sentence and the alternative which carries the interpretation and (2) the plausibility of the associated interpretation. Through a series of experiments, we show that, given equal plausibility of the possible interpretations, participants are more likely to choose the interpretation associated with the structure that necessitates the fewest and more probable edits. This study provides another linguistic phenomenon supporting the rational aspect of human language processing.}
}

@inproceedings{zhang.y:2023conll,
  title = {Can Language Models Be Tricked by Language Illusions? {{Easier}} with Syntax, Harder with Semantics},
  shorttitle = {Can Language Models Be Tricked by Language Illusions?},
  booktitle = {Proceedings of the 27th {{Conference}} on {{Computational Natural Language Learning}} ({{CoNLL}})},
  author = {Zhang, Yuhan and Gibson, Edward and Davis, Forrest},
  editor = {Jiang, Jing and Reitter, David and Deng, Shumin},
  year = {2023},
  month = dec,
  pages = {1--14},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.conll-1.1},
  urldate = {2024-05-27},
  abstract = {Language models (LMs) have been argued to overlap substantially with human beings in grammaticality judgment tasks. But when humans systematically make errors in language processing, should we expect LMs to behave like cognitive models of language and mimic human behavior? We answer this question by investigating LMs' more subtle judgments associated with ``language illusions'' -- sentences that are vague in meaning, implausible, or ungrammatical but receive unexpectedly high acceptability judgments by humans. We looked at three illusions: the comparative illusion (e.g. ``More people have been to Russia than I have''), the depth-charge illusion (e.g. ``No head injury is too trivial to be ignored''), and the negative polarity item (NPI) illusion (e.g. ``The hunter who no villager believed to be trustworthy will ever shoot a bear''). We found that probabilities represented by LMs were more likely to align with human judgments of being ``tricked'' by the NPI illusion which examines a structural dependency, compared to the comparative and the depth-charge illusions which require sophisticated semantic understanding. No single LM or metric yielded results that are entirely consistent with human behavior. Ultimately, we show that LMs are limited both in their construal as cognitive models of human language processing and in their capacity to recognize nuanced but critical information in complicated language materials.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhang.y2023conll Can language models be tricked by langua.pdf}
}

@misc{zhang.y:2024psyarxiv,
  title = {Comparative Illusions Are Evidence of Rational Inference in Language Comprehension},
  author = {Zhang, Yuhan and Kauf, Carina and Levy, Roger Philip and Gibson, Edward},
  year = {2024},
  month = may,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/efr3q},
  urldate = {2024-05-27},
  abstract = {During language comprehension, people sometimes accept sentences that are ungrammatical or semantically implausible and the cognitive mechanism underlying this language illusion is understudied. In this paper, we study the ``comparative illusion'' (CI) phenomenon where people believe the sentence "More people have been to Russia than I have" to be acceptable while in fact it is semantically ill-formed. We provide a potential explanation for the reasons behind the language illusion from the noisy-channel framework. We hypothesize that comprehenders make rational inference over the perceived sentence by entertaining alternative plausible interpretations and weighing the likelihood of the plausible interpretation being produced with errors. In four experiments, we identified the construction that elicited the most salient illusion effect, established the range of the plausible interpretations, and found that the probability for comprehenders to assign a certain plausible interpretation to the CI sentence is proportional to how likely they think that that interpretation is encapsulated into the CI sentence during speech production. This finding is consistent with the noisy-channel predictions and provides another example supporting the rational behavior underlying language comprehension.},
  langid = {american},
  keywords = {information theory,language illusion,language processing,psycholinguistics,rational inference}
}

@inproceedings{zhao.s:2024twistedSMC,
  title = {Probabilistic {{Inference}} in {{Language Models}} via {{Twisted Sequential Monte Carlo}}},
  booktitle = {Proceedings of the 41st {{International Conference}} on {{Machine Learning}}},
  author = {Zhao, Stephen and Brekelmans, Rob and Makhzani, Alireza and Grosse, Roger Baker},
  year = {2024},
  month = jul,
  pages = {60704--60748},
  publisher = {PMLR},
  issn = {2640-3498},
  urldate = {2024-09-30},
  abstract = {Numerous capability and safety techniques of Large Language Models (LLMs), including RLHF, automated red-teaming, prompt engineering, and infilling, can be cast as sampling from an unnormalized target distribution defined by a given reward or potential function over the full sequence. In this work, we leverage the rich toolkit of Sequential Monte Carlo (SMC) for these probabilistic inference problems. In particular, we use learned twist functions to estimate the expected future value of the potential at each timestep, which enables us to focus inference-time computation on promising partial sequences. We propose a novel contrastive method for learning the twist functions, and establish connections with the rich literature of soft reinforcement learning. As a complementary application of our twisted SMC framework, we present methods for evaluating the accuracy of language model inference techniques using novel bidirectional SMC bounds on the log partition function. These bounds can be used to estimate the KL divergence between the inference and target distributions in both directions. We apply our inference evaluation techniques to show that twisted SMC is effective for sampling undesirable outputs from a pretrained model (a useful component of harmlessness training and automated red-teaming), generating reviews with varied sentiment, and performing infilling tasks.},
  langid = {english},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhao.s2024twistedSMC Probabilistic Inference in Language Mode.pdf}
}

@misc{zhao.w:2023arxiv,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = nov,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2024-05-23},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhao.w2023arxiv A survey of large language models.pdf}
}

@misc{zhao.w:2023arxiva,
  title = {A Survey of Large Language Models},
  author = {Zhao, Wayne Xin and Zhou, Kun and Li, Junyi and Tang, Tianyi and Wang, Xiaolei and Hou, Yupeng and Min, Yingqian and Zhang, Beichen and Zhang, Junjie and Dong, Zican and Du, Yifan and Yang, Chen and Chen, Yushuo and Chen, Zhipeng and Jiang, Jinhao and Ren, Ruiyang and Li, Yifan and Tang, Xinyu and Liu, Zikang and Liu, Peiyu and Nie, Jian-Yun and Wen, Ji-Rong},
  year = {2023},
  month = may,
  number = {arXiv:2303.18223},
  eprint = {2303.18223},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.18223},
  urldate = {2023-05-31},
  abstract = {Language is essentially a complex, intricate system of human expressions governed by grammatical rules. It poses a significant challenge to develop capable AI algorithms for comprehending and grasping a language. As a major approach, language modeling has been widely studied for language understanding and generation in the past two decades, evolving from statistical language models to neural language models. Recently, pre-trained language models (PLMs) have been proposed by pre-training Transformer models over large-scale corpora, showing strong capabilities in solving various NLP tasks. Since researchers have found that model scaling can lead to performance improvement, they further study the scaling effect by increasing the model size to an even larger size. Interestingly, when the parameter scale exceeds a certain level, these enlarged language models not only achieve a significant performance improvement but also show some special abilities that are not present in small-scale language models. To discriminate the difference in parameter scale, the research community has coined the term large language models (LLM) for the PLMs of significant size. Recently, the research on LLMs has been largely advanced by both academia and industry, and a remarkable progress is the launch of ChatGPT, which has attracted widespread attention from society. The technical evolution of LLMs has been making an important impact on the entire AI community, which would revolutionize the way how we develop and use AI algorithms. In this survey, we review the recent advances of LLMs by introducing the background, key findings, and mainstream techniques. In particular, we focus on four major aspects of LLMs, namely pre-training, adaptation tuning, utilization, and capacity evaluation. Besides, we also summarize the available resources for developing LLMs and discuss the remaining issues for future directions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhao.w2023 A survey of large language models.pdf}
}

@inproceedings{zhou.j:2019,
  title = {Head-{{Driven Phrase Structure Grammar}} Parsing on {{Penn Treebank}}},
  booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
  author = {Zhou, Junru and Zhao, Hai},
  year = {2019},
  pages = {2396--2408},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/P19-1230},
  bdsk-url-2 = {https://doi.org/10.18653/v1/P19-1230},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhou.j2019 Head-Driven Phrase Structure Grammar par.pdf}
}

@article{zhu.j:2023,
  title = {Computation-{{Limited Bayesian Updating}}},
  author = {Zhu, Jian-Qiao and Sanborn, Adam and Chater, Nick and Griffiths, Tom},
  year = {2023},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {45},
  number = {45},
  urldate = {2023-12-07},
  abstract = {Effectively updating one's beliefs requires sufficient empirical evidence (i.e., data) and the computational capacity to process it. Yet both data and computational resources are limited for human minds. Here, we study the problem of belief updating under limited data and limited computation. Using information theory to characterize constraints on computation, we find that the solution to the resulting optimization problem links the data and computational limitations together: when computational resources are tight, agents may not be able to integrate new empirical evidence. The resource-rational belief updating rule we identify offers a novel interpretation of conservative Bayesian updating.},
  langid = {english},
  keywords = {KL divergence,rational analysis,resource rationality},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zhu.j2023 Computation-Limited Bayesian Updating.pdf}
}

@article{zipf.g:1929,
  title = {Relative Frequency as a Determinant of Phonetic Change},
  author = {Zipf, George Kingsley},
  year = {1929},
  journal = {Harvard Studies in Classical Philology},
  volume = {40},
  eprint = {310585},
  eprinttype = {jstor},
  pages = {1--95},
  publisher = {Department of the Classics, Harvard University},
  issn = {0073-0688},
  doi = {10.2307/310585},
  urldate = {2024-05-05},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zipf.g1929 Relative frequency as a determinant of p.pdf}
}

@book{zipf.g:1935,
  title = {The Psycho-Biology of Language},
  author = {Zipf, George Kingsley},
  year = {2013},
  month = nov,
  edition = {Republished},
  publisher = {Routledge},
  address = {London, United Kingdom},
  doi = {10.4324/9781315009421},
  urldate = {2022-09-28},
  isbn = {978-1-136-31046-1},
  langid = {english}
}

@book{zipf.g:1949,
  title = {Human Behavior and the Principle of Least Effort: An Introduction to Human Ecology},
  shorttitle = {Human Behavior and the Principle of Least Effort},
  author = {Zipf, George Kingsley},
  year = {2012},
  publisher = {Martino Publishing},
  address = {Mansfield Centre, CT},
  isbn = {978-1-61427-312-7},
  langid = {english}
}

@inproceedings{zouhar.v:2023,
  title = {Tokenization and the Noiseless Channel},
  booktitle = {Proceedings of the 61st {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Zouhar, Vil{\'e}m and Meister, Clara and Gastaldi, Juan and Du, Li and Sachan, Mrinmaya and Cotterell, Ryan},
  year = {2023},
  month = jul,
  pages = {5184--5207},
  publisher = {Association for Computational Linguistics},
  address = {Toronto, Canada},
  urldate = {2023-07-24},
  abstract = {Subword tokenization is a key part of most NLP pipelines.However, little is known about why some tokenizer and hyperparameter combinations lead to improved downstream model performance over others. We propose that good tokenizers lead to efficient channel usage, where the channel is the means by which some input is conveyed to the model and efficiency can be quantified in information-theoretic terms as the ratio of the Shannon entropy to the maximum entropy of the subword distribution.Nevertheless, an optimal encoding according to Shannon entropy assigns extremely long codes to low-frequency subwords and very short codes to high-frequency subwords.Defining efficiency in terms of R{\'e}nyi entropy, on the other hand, penalizes distributions with either very high or very low-frequency subwords.We posit that (1) extremely high-frequency subwords are problematic because their meaning is not distinct and (2) that low-frequency subwords may not appear frequently enough for their meaning to be learned properly; encodings that induce unigram distributions with either can harm model performance.In machine translation, we find that across multiple tokenizers, the R{\'e}nyi entropy has a very strong correlation with BLEU: 0.82 in comparison to just -0.30 for compressed length.},
  file = {/Users/j/Dropbox (MIT)/Zotfiles/zouhar.v2023 Tokenization and the noiseless channel.pdf}
}

@inproceedings{zwarts.s:2011,
  title = {The Impact of Language Models and Loss Functions on Repair Disfluency Detection},
  booktitle = {Proceedings of the 49th {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}}: {{Human Language Technologies}}},
  author = {Zwarts, Simon and Johnson, Mark},
  year = {2011},
  month = jun,
  pages = {703--711},
  publisher = {Association for Computational Linguistics},
  address = {Portland, Oregon, USA},
  urldate = {2022-10-24},
  keywords = {noisy channel}
}
